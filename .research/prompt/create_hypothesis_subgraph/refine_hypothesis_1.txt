
LLM Name: o3-2025-04-16
Input:
You are an accomplished researcher in the field of machine learning. Based on the instructions below, please refine the research hypothesis provided in "Hypothesis Info" to make it more novel and academically as well as socially valuable.

# Instructions
- Carefully read the research theme described in "Research Topic" and understand the problems this research should address, as well as the broader impact it aims to achieve.
- "Hypothesis Info" contains a newly proposed research hypothesis related to the "Research Topic." Read the hypothesis thoroughly and refine it so that it becomes a research contribution with stronger novelty and significance.
- The reasoning behind the evaluation of novelty is provided in "Novelty," and the reasoning for significance is provided in "Significance." Use these evaluations as references to improve the research hypothesis.
- "Research Study List" provides a set of related prior studies. Each entry includes a summary of its title, main contributions, methodologies, results, and limitations. Read through these summaries to understand the direction and focus of research in this domain.
- "Hypothesis Info History" contains past research hypotheses and their evaluation reasoning. If provided, use it as a reference for refining the current hypothesis.
- Pay close attention to how each prior study builds upon earlier work and which limitations remain unresolved. Organize this information to form a clear understanding of the current research landscape.
- Identify the key gaps, challenges, or unmet needs that persist across these studies. Also, consider whether methods or concepts from other domains could help address these limitations.
- Reflect on unexplored aspects or areas for improvement (e.g., new techniques, new evaluation metrics, novel datasets, or methods for generalizing findings). Ensure that the refined hypothesis is broadly applicable and not overly dependent on a specific dataset or model.
- Please limit research hypotheses to those that can be validated with a Python script.
- Please also consider ways to enhance the feasibility of validation and improve accordingly.

# Research Topic
Improving fine-tuning performance of language models.

# Current Hypothesis
{
    "Open Problems": "Current PEFT methods equip every selected Transformer layer with the same always-on LoRA adapter. Recent GaLoRA adds a *static* scalar gate per layer, but once training is finished each input still pays the cost of exactly the same surviving adapters. In practice different sentences (or images, code, etc.) stress different depths of the network – a sarcastic review may need higher-level reasoning while a factual statement may not. There is no mechanism that 1) lets a language model decide *per-input* which layers truly need low-rank adaptation, 2) keeps the rest of the adapters completely dormant to save compute, and 3) learns this routing jointly with the downstream task.",
    "Methods": "Conditional Gated-LoRA (CoGaLoRA)\n1.  Adapter & router For every LoRA module attach a tiny router network  f_ℓ  (2-layer MLP,  hidden→4→1) that reads the layer’s pre-adapter activation  h_ℓ  (mean-pooled over tokens) and outputs a gate probability  p_ℓ∈(0,1).\n2.  Stochastic binary gate Sample  ĝ_ℓ∼Bernoulli(p_ℓ)  with a straight-through estimator.\n    W′ = W₀ + ĝ_ℓ · B_ℓ A_ℓ.  If  ĝ_ℓ=0  the LoRA branch is bypassed and receives no gradient.\n3.  Regularisation Add a sparse-compute penalty  λ Σ_ℓ p_ℓ  to the loss.  λ controls the expected number of active adapters per *example* rather than per model.\n4.  Inference Replace sampling by the deterministic hard gate  g_ℓ = 1[p_ℓ>τ].  The set of active layers thus changes with every input; the rest incur zero FLOPs.\n5.  Freezing trick After training, layers whose router is *never* triggered can have their LoRA weights deleted, giving the same memory benefits as GaLoRA.",
    "Experimental Setup": "Base model BERT-base-uncased (110 M).\nTask SST-2 (GLUE) single-sentence sentiment.\nAdapters Rank r=8 on all Q/K/V + FFN up/down matrices.\nBaselines (1) Full fine-tune, (2) Standard LoRA, (3) Static GaLoRA (best λ).\nProposed CoGaLoRA with λ∈{0.1,0.2,0.4}; τ=0.5.\nOptimiser & schedule AdamW lr 2e-5, 3 epochs, batch 32, same seed.\nMetrics • Dev accuracy • Avg #active adapters per example • Training wall-time • Inference FLOPs measured with torch.profiler on 256 random sentences.",
    "Primary Metric": "dev-set accuracy",
    "Experimental Code": "class Router(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(hidden_size, 4), nn.ReLU(), nn.Linear(4, 1))\n    def forward(self, h):                 # h: [B, hidden]\n        return torch.sigmoid(self.net(h))\n\nclass CoGatedLoraLayer(LoraLayer):\n    def __init__(self, *a, hidden_size, **kw):\n        super().__init__(*a, **kw)\n        self.router = Router(hidden_size)\n    def forward(self, x):                 # x: [B, T, hidden]\n        gate_p = self.router(x.mean(1))   # [B,1]\n        bern = (torch.rand_like(gate_p) < gate_p).float() + gate_p.detach() - gate_p  # STE\n        out = super().forward(x)\n        return x + bern.view(-1,1,1) * self.scaling * out, gate_p\n\nlambda_sparse = 0.2\nfor batch in loader:\n    logits, ps = model(**batch, return_gate_ps=True)\n    loss = ce(logits, batch['labels']) + lambda_sparse * ps.mean()\n    loss.backward(); opt.step(); opt.zero_grad()",
    "Expected Result": "• Standard LoRA: 92.0 % accuracy, 100 % layers used.\n• Static GaLoRA (≈35 % layers kept): 92.3 %.\n• CoGaLoRA (λ=0.2): 92.6 % accuracy with only ~20 % adapters active per sentence; 55 % average FLOP reduction at inference; training time ≈+5 % (routers).\n• λ too high (0.4) activates <10 % adapters and drops to 91.5 %.",
    "Expected Conclusion": "Making LoRA activation *input-conditional* via tiny learned routers yields better task accuracy and much larger average compute savings than static layer selection. CoGaLoRA turns parameter-efficient fine-tuning into *computation-efficient* fine-tuning, adapting the network depth it actually modifies on a per-example basis without extra optimisation tricks or post-hoc pruning. The idea is orthogonal to rank-adaptation (AdaLoRA) and can be combined with it in future work."
}

# Novelty
None of the listed PEFT papers let the model decide at inference-time, for every individual input, which LoRA blocks to execute.  • GaLoRA prunes adapters but the surviving ones are always active for every query.  • AdaLoRA reallocates rank per weight matrix, again statically.  • Multi-Head Adapter Routing (MHR) and other routing works learn a task--module matrix Z, not a per-example gate, and in practice still pay the cost of evaluating all adapters because mixing weights are applied to their outputs.  • SHiRA, DoRA, Spectral Adapter, etc. keep full compute during inference.  The proposed CoGaLoRA combines (1) a learnable router that reads the current hidden state, (2) stochastic gating with a straight-through estimator, and (3) a sparsity regulariser that directly penalises expected FLOPs.  This yields input-conditional, zero-FLOP deactivation of adapters – a mechanism absent from existing methods.  The idea is therefore novel with respect to both adapter selection (now dynamic) and compute optimisation (now tied to the downstream loss rather than post-hoc pruning).

# Significance
If successful, CoGaLoRA turns parameter-efficient fine-tuning into compute-efficient inference, which is crucial for deploying many specialised LLMs on latency- or energy-constrained devices.  Saving ~50 % average FLOPs while slightly improving accuracy would translate into lower cloud bills and wider on-device adoption.  Academically, it opens a new axis in PEFT research – conditional depth adaptation – and is orthogonal to rank- or spectral methods, so it can compound with future advances.  The method is simple, adds minimal parameters, and uses standard training tricks, making it easy to reproduce.  Limitations: initial experiments are on BERT-base and a single task; gains on multi-billion-parameter LLMs and real hardware remain to be shown, and routing overhead may offset savings in highly optimised kernels.  Hence the potential impact is high but still provisional.

# Hypothesis History
Hypothesis:
{
    "Open Problems": "LoRA-style PEFT usually attaches low-rank adapters to *all* selected weight matrices with a fixed rank. In practice only a subset of layers are really needed; the rest waste memory, slow training and occasionally hurt generalisation. Today practitioners hand-pick layers or use heavy rank–allocation schemes (e.g.\nAdaLoRA) that still update every layer and add extra bookkeeping. A minimal, train-time mechanism that automatically turns *off* unimportant LoRA modules while keeping the important ones active is missing.",
    "Methods": "Gated-LoRA (GaLoRA)\n1. Architectural change – for every LoRA module add a single learnable scalar gate gℓ∈ℝ that multiplies the LoRA update:\n   W′ = W₀ + gℓ·BA ,   gℓ is initialised to 1.\n2. Regularisation – add an L1 penalty λ·|gℓ| to the loss.  L1 shrinks many gates to ≈0, effectively disabling their LoRA parameters and leaving the underlying pretrained weight untouched.\n3. Objective –  L_total = L_task  +  λ·Σℓ |gℓ|  .  Only this extra term is new; no change to optimiser or inference code (gates are constants at test time).\n\nMotivation\n• L1 on g acts like soft feature selection, letting the optimiser discover which layers truly need adaptation.\n• When gℓ→0 the corresponding BA matrices receive zero gradient, so training time is automatically saved.\n• At inference we can prune modules with |gℓ|<ε, reducing memory and latency with one line of code.",
    "Experimental Setup": "Base model: BERT-base-uncased (110 M).\nTask: SST-2 sentiment classification (GLUE).\nBaselines: (a) Full fine-tune, (b) Standard LoRA (rank=8) on all query/key/value&ffn matrices, (c) AdaLoRA (rank budget equal to LoRA).\nProposed: GaLoRA with same initial rank, λ∈{0.0,1e-3,5e-3,1e-2}.\nTraining: 3 epochs, batch 32, lr 2e-5, AdamW.  Single V100, HuggingFace PEFT.\nMetrics: Dev set Accuracy (primary), number of active gates (#g>0.1), training wall-time, inference FLOPs after pruning.\nAnalysis: draw accuracy vs active-layer count curve.",
    "Primary Metric": "accuracy",
    "Experimental Code": "import torch, torch.nn as nn\nfrom peft.tuners.lora import LoraLayer\nclass GatedLoraLayer(LoraLayer):\n    def __init__(self, *args, **kw):\n        super().__init__(*args, **kw)\n        # scalar gate, initialise to 1\n        self.gate = nn.Parameter(torch.ones(1))\n    def forward(self, x: torch.Tensor):\n        result = super().forward(x)\n        if self.merged:   # inference after merge\n            return result\n        return x + self.gate * self.lora_A(self.lora_dropout(self.lora_B(x)))\n\ndef add_gated_lora(model, r=8, alpha=16, dropout=0.05):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            gated = GatedLoraLayer(module.in_features, module.out_features,\n                                    r=r, lora_alpha=alpha, lora_dropout=dropout,\n                                    fan_in_fan_out=False)\n            setattr(model, name.split('.')[-1], gated)\n    return model\n\n# loss wrapper\nlambda_l1 = 5e-3\nfor batch in loader:\n    logits = model(**batch).logits\n    loss = ce_loss(logits, batch['labels'])\n    l1 = sum(p.abs().mean() for p in model.parameters() if p.shape==torch.Size([1]))\n    loss = loss + lambda_l1 * l1\n    loss.backward(); optimizer.step(); optimizer.zero_grad()\n\n# pruning after training\nwith torch.no_grad():\n    for m in model.modules():\n        if isinstance(m, GatedLoraLayer) and m.gate.abs() < 0.1:\n            m.merge()  # merges zero so nothing added and frees params",
    "Expected Result": "• Standard LoRA expected dev accuracy ≈ 92.0 %  (as in prior work).\n• GaLoRA with λ=5e-3 keeps ~35 % of LoRA layers active and reaches ≈ 92.3 % (+0.3).\n• Training time ~15 % faster (fewer gradients).\n• After pruning, inference FLOPs and trainable parameter count drop by ~60 % vs LoRA.\n• Too large λ (1e-2) prunes too much and accuracy falls to ≈ 91.0 % .",
    "Expected Conclusion": "A single gated scalar with L1 regularisation lets the network *learn* which layers actually need low-rank adaptation.  This tiny change requires only a few lines of code, improves accuracy slightly, and yields large savings in trainable parameters, memory and inference cost—offering an automatic, lightweight alternative to manual layer selection or heavier adaptive-rank methods such as AdaLoRA."
}
Novelty: None of the cited PEFT papers attaches an explicit, learnable scalar gate to each LoRA module and drives it to zero with an L1 penalty.  • AdaLoRA and Spectral-Adapter redistribute or prune *ranks* inside every layer but they still back-prop through all LoRA weights and leave a non-zero update in every matrix.  • SHiRA trains a sparse subset of *base* weights, not LoRA adapters.  • DoRA, MHR, CURROT, etc. focus on different re-parameterisations or curriculum ideas and do not perform on-the-fly *layer selection*.  GaLoRA therefore introduces a distinct mechanism – soft feature selection at the level of whole LoRA blocks – that has not been explored in the referenced literature.  Its novelty is incremental rather than conceptual revolution: gating + L1 is a known sparsification trick, but applying it to LoRA to obtain automatic layer–wise pruning with zero extra bookkeeping is new within the PEFT landscape.
Significance: The hypothesis tackles a practical pain-point for practitioners: deciding which transformer layers to adapt and how large the rank should be. By learning gates that can be pruned at train-time it can 1) cut gradient computation, 2) cut memory, and 3) reduce inference FLOPs – all while matching or slightly improving accuracy.  Because it needs only one extra scalar per layer and a standard L1 term, it can be adopted in any existing LoRA code-base with minimal engineering, potentially influencing everyday fine-tuning workflows.  Academically, it provides a clean test-bed for studying where adaptation is actually needed, but it does not radically change our understanding of PEFT nor enable previously impossible applications.  Thus its practical significance is solid, while its theoretical impact is moderate.


# Research Study List
{
    "Title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "Main Contributions": "The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel Parameter-Efficient Fine-Tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and full fine-tuning (FT) without incurring additional inference costs. It achieves this by decomposing pre-trained weights into magnitude and direction components for fine-tuning, specifically employing LoRA for efficient directional updates. The work also presents a novel weight decomposition analysis to investigate the inherent differences in learning patterns between FT and LoRA. DoRA consistently outperforms LoRA on various downstream tasks across LLaMA, LLaVA, and VL-BART models, including commonsense reasoning, visual instruction tuning, and image/video-text understanding.",
    "Methodology": "DoRA reparameterizes model weights into magnitude (m) and directional (V) components. The magnitude component is a trainable vector, while the directional component is updated using a low-rank adaptation method, specifically LoRA. This decomposition allows DoRA to exhibit a learning pattern similar to FT, characterized by a negative correlation between magnitude and directional updates, in contrast to LoRA's positive correlation. The methodology includes a gradient analysis demonstrating how this decomposition enhances optimization stability. To reduce training memory overhead, the term ||V + ∆V ||c is treated as a constant during backpropagation, dynamically reflecting updates without receiving gradients.",
    "Experimental Setup": "Experiments were conducted across diverse tasks and model architectures. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B were fine-tuned on eight sub-tasks (e.g., BoolQ, PIQA, HellaSwag). For image/video-text understanding, VL-BART was evaluated on VQAv2, GQA, NLVR2, MSCOCO Caption (image) and TVQA, How2QA, TVC, YC2C (video). Visual instruction tuning used LLaVA-1.5-7B, evaluated on VQAv2, GQA, VisWiz, SQA, VQAT, POPE, and MMBench. Instruction tuning of LLaMA-7B/LLaMA2-7B employed a 10K subset of the Alpaca dataset, with evaluation on MT-Bench using GPT-4 scores. QDoRA (DoRA within QLoRA framework) was tested on LLaMA2-7B/LLaMA3-8B using the Orca-Math dataset. Text-to-image generation involved fine-tuning SDXL with DreamBooth on 3D icons and Lego sets. Validation involved comparing DoRA against LoRA, FT, and other PEFT methods like Prefix, Series, Parallel adapters, and VeRA, along with ablation studies on training sample size, rank settings, and tuning granularity.",
    "Limitations": "While DoRA generally enhances LoRA to resemble FT's performance, its improvement over LoRA may be less pronounced in scenarios where FT itself performs worse than LoRA, potentially due to overfitting in FT. The proposed modification to reduce training overhead by treating ||V + ∆V ||c as a constant, while effective for memory reduction (24.4% for LLaMA, 12.4% for VL-BART), results in a negligible accuracy difference (0.2 for LLaMA), suggesting a minor, though not significant, trade-off.",
    "Future Research Directions": "The authors intend to explore the generalizability of DoRA in domains beyond language and vision, specifically mentioning the field of audio.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "Main Contributions": "The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel Parameter-Efficient Fine-Tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and Full Fine-Tuning (FT) while avoiding additional inference costs. DoRA achieves this by performing a weight decomposition analysis to understand the learning patterns of FT and LoRA, and then proposes to fine-tune pre-trained weights by separating them into magnitude and directional components, with LoRA specifically used for efficient directional updates. DoRA consistently outperforms LoRA across various tasks and model architectures without additional inference latency.",
    "Methodology": "The core methodology involves a novel weight decomposition analysis, which reparameterizes model weights into magnitude and directional components to analyze the learning patterns of FT and LoRA. Based on this, DoRA decomposes the pre-trained weight matrix (W0) into a trainable magnitude vector (m) and a directional matrix (V). The directional component is then updated using LoRA, expressed as W' = m * (W0 + BA) / ||W0 + BA||c, where BA represents the low-rank update for the directional component. A gradient analysis shows that this decomposition enhances learning stability by aligning the gradient's covariance matrix with the identity matrix. To reduce training overhead, the norm term ||V + ∆V||c is treated as a constant during backpropagation.",
    "Experimental Setup": "DoRA was evaluated across various tasks and models. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B were fine-tuned on 8 sub-tasks and compared against LoRA, Prompt learning, Series adapter, Parallel adapter, and ChatGPT. For multi-task image/video-text understanding, VL-BART was fine-tuned on 4 image-text tasks (VQAv2, GQA, NLVR2, MSCOCO Caption) and 4 video-text tasks (TVQA, How2QA, TVC, YC2C). Visual instruction tuning was performed on LLaVA-1.5-7B across 7 vision-language benchmarks (VQAv2, GQA, VisWiz, SQA, VQAT, POPE, MMBench). Compatibility with other LoRA variants was shown by combining DoRA with VeRA (DVora) for instruction tuning on LLaMA-7B/LLaMA2-7B using the Alpaca dataset, evaluated on MT-Bench by GPT-4. Ablation studies explored performance with varying training sample sizes and LoRA ranks. QDoRA (DoRA applied to QLoRA) was evaluated on LLaMA2-7B/LLaMA3-8B with the Orca-Math dataset. Text-to-image generation was demonstrated by fine-tuning SDXL with DreamBooth using 3D icons and Lego sets.",
    "Limitations": "The paper primarily highlights the limitations of existing PEFT methods like LoRA and Full Fine-Tuning, which DoRA aims to address. For DoRA itself, specific limitations or weaknesses are not explicitly mentioned. The paper notes an optimization strategy for DoRA to reduce memory (treating ||V + ∆V||c as a constant during backpropagation), stating it has 'no noticeable difference in accuracy,' implying no significant performance compromise due to this simplification.",
    "Future Research Directions": "Future work will explore the generalizability of DoRA in domains beyond language and vision, specifically in the field of audio.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
    "Main Contributions": "The paper addresses the inefficiency of existing parameter-efficient fine-tuning methods (like LoRA) that evenly distribute the budget of incremental updates across all pre-trained weight matrices, leading to suboptimal performance due to neglecting the varying importance of different weight parameters. The main contribution is AdaLoRA, an adaptive budget allocation method for parameter-efficient fine-tuning. AdaLoRA adaptively allocates the parameter budget among weight matrices based on their importance score. It parameterizes incremental updates using Singular Value Decomposition (SVD), enabling effective pruning of singular values for unimportant updates to reduce parameter budget while avoiding intensive exact SVD computations. AdaLoRA consistently outperforms baselines, especially in low-budget settings, achieving notable performance improvements, e.g., a 1.2% F1 improvement on SQuAD2.0 with less than 0.1% trainable parameters compared to full fine-tuning.",
    "Methodology": "AdaLoRA builds on Low-Rank Adaptation (LoRA) by parameterizing incremental updates (∆) of pre-trained weight matrices in the form of SVD: W = W(0) + ∆ = W(0) + PΛQ, where P and Q are orthogonal matrices representing singular vectors and Λ is a diagonal matrix of singular values. An L2-norm regularizer is applied to P and Q to enforce orthogonality. Instead of expensive exact SVD, this parameterization allows for dynamic rank adjustment by manipulating singular values. The method incorporates an importance-aware rank allocation mechanism where singular values are pruned based on a newly designed importance metric. This metric, Sk,i, considers the contribution of the i-th singular value (λk,i) and its corresponding singular vectors (Pk,∗i, Qk,i∗), utilizing a smoothed sensitivity s(·) which is the product of exponential moving average of gradient-weight product (I(t)) and an uncertainty term (U(t)). A global budget scheduler facilitates training by starting with a slightly higher initial budget and gradually reducing it to the target budget via a cubic schedule, allowing initial exploration before focusing on crucial modules. Pruning of singular values is conducted iteratively every ∆T steps.",
    "Experimental Setup": "The effectiveness of AdaLoRA is validated through extensive experiments on a wide range of tasks and models. For Natural Language Understanding (NLU) and Question Answering (QA), DeBERTaV3-base was fine-tuned on the GLUE benchmark (MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B) and SQuAD v1.1/v2.0 datasets, respectively. For Natural Language Generation (NLG), BART-large was fine-tuned on XSum and CNN/DailyMail datasets. Baselines included Full fine-tuning, Bitfit, Houlsby Adapter, Pfeiffer Adapter, and a generalized LoRA applied to all weight matrices (Wq, Wk, Wv, Wf1, Wf2). Performance was evaluated using standard metrics: Accuracy, Matthews correlation, F1-score for GLUE; Exact Match (EM) and F1 for SQuAD; and ROUGE 1/2/L (R-1/2/L) for NLG. Experiments were conducted across various parameter budget levels (e.g., 0.08% to 2.2% of total pre-trained parameters) on NVIDIA V100 GPUs, implemented using PyTorch and Huggingface Transformers. Hyperparameters like learning rates, batch sizes, and the regularization coefficient were tuned, with results averaged over multiple runs (e.g., 5 runs for GLUE) using different random seeds.",
    "Limitations": "The paper does not explicitly list limitations of AdaLoRA in a dedicated section. However, it implicitly acknowledges that exact SVD computation is prohibitively expensive, which AdaLoRA's SVD-based parameterization aims to circumvent. A practical trade-off is observed in training cost, where AdaLoRA incurs an additional training time overhead (e.g., 11% on MNLI and 16% on SQuADv2) compared to LoRA, although it is argued not to be 'significant'. The complexity of the importance scoring mechanism, which involves sensitivity smoothing and uncertainty quantification, is also an added computational aspect.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "import loralib as lora; class Attention(nn.Module):     def __init__(self, nx, n_ctx, config, scale=False):         super(Attention, self).__init__();         n_state = nx;         assert n_state % config.n_head == 0;         self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx));         self.n_head = config.n_head;         self.split_size = n_state;         self.scale = scale;         self.c_attn = lora.MergedLinear(nx, n_state * 3, r=config.lora_attn_dim, lora_alpha=config.lora_attn_alpha, lora_dropout=config.lora_dropout, enable_lora=[True, False, True], fan_in_fan_out=True, merge_weights=False);         self.c_proj = Conv1D(n_state, nx);         self.config = config; # ... (other methods of Attention class) ...; if args.lora_dim > 0: lora.mark_only_lora_as_trainable(lm_net); torch.save({'model_state_dict': lora.lora_state_dict(model)}, model_path);",
    "Experiment Result": "Command-line arguments: --lora_dim (LoRA attention dimension), --lora_alpha (LoRA attention alpha), --lora_dropout (dropout probability for LoRA layers). Configuration parameters in GPT2Config: lora_attn_dim, lora_attn_alpha, lora_dropout. Note: The specific AdaLoRA mechanisms (SVD parameterization of updates, dynamic rank adjustment, importance-aware rank allocation, global cubic budget scheduler for rank, iterative pruning) are not found in the provided repository content. The extracted code and info pertain to a standard Low-Rank Adaptation (LoRA) implementation, which AdaLoRA builds upon."
}{
    "Title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
    "Main Contributions": "This research provides a theoretical analysis of LoRA (Low-Rank Adaptation) fine-tuning within the Neural Tangent Kernel (NTK) regime. The key contributions are: (i) proving that full fine-tuning admits a low-rank solution of rank r \n\n (sqrt(N)); (ii) demonstrating that using LoRA with rank r \n\n (sqrt(N)) eliminates spurious local minima, enabling (stochastic) gradient descent to find these low-rank solutions; and (iii) showing that the low-rank solution found using LoRA generalizes well.",
    "Methodology": "The study employs a theoretical analysis centered on the Neural Tangent Kernel (NTK) regime, approximating network changes using first-order Taylor expansion. It investigates the empirical risk with convex, non-negative, and twice-differentiable loss functions (e.g., cross-entropy, MSE). LoRA's low-rank parameterization (\n\n = uv\n\n) is analyzed, and weight decay is interpreted as nuclear norm regularization. The methodology leverages concepts of second-order stationary points (SOSPs) and established theorems on gradient descent convergence. Proofs for low-rank solution existence are inspired by semi-definite programming results, while the absence of spurious local minima for LoRA is demonstrated using a perturbed loss function, Sard’s theorem, and existing matrix factorization characterizations of SOSPs. Generalization guarantees are derived using Rademacher complexity and McDiarmid's inequality.",
    "Experimental Setup": "Experiments were conducted on linearized pre-trained models across different modalities to validate the theory. For NLP tasks, prompt-based fine-tuning was used on a linearized RoBERTa-base model, training only the query (Wq) and value (Wv) weights of the last layer, on datasets like SST-2, QNLI, MR, CR, QQP, and Subj, with N=32 data points and K=2 labels, using cross-entropy loss. For image classification, a pre-trained vision transformer was fine-tuned on the bean disease dataset (N=48, K=3 labels). For speech classification, a pre-trained wav2vec2 model was fine-tuned on the SUPERB dataset (N=64, K=4 labels). All experiments used full batch gradient descent. The theoretical LoRA rank requirement was set based on the condition r(r+1)/2 > KN (e.g., r \n\n 11 for NLP tasks). Evaluation included training loss curves and test accuracy.",
    "Limitations": "The theoretical analysis relies on the assumption that fine-tuning remains within the NTK (lazy-training) regime, which is empirically supported for prompt-based fine-tuning but may not hold universally. The main theorem on the elimination of spurious local minima applies to a 'perturbed' loss function with a randomly sampled positive semi-definite matrix P; while interpreted to imply generic absence of spurious minima, it doesn't directly apply to standard unperturbed LoRA. The generalization guarantees are primarily shown for cross-entropy loss, although extensions to general convex losses are provided in the appendix. Empirically, while the theory confirms convergence to optimal loss values, the observed rates of convergence differed, with lower LoRA ranks leading to slower convergence, suggesting that lower ranks might create unfavorable loss landscapes (e.g., plateaus or saddle points) that hinder optimization speed.",
    "Future Research Directions": "Future work could involve more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, and achieving a better understanding of the minimum rank requirement for LoRA through lower bounds. Another promising direction is to analyze the observed trade-off between training rate and LoRA rank, particularly exploring why lower LoRA ranks lead to slower convergence, and designing remedies for these potentially unfavorable, flatter training landscapes.",
    "Experiment Code": "class LogitModelWrapper(nn.Module):\n    def __init__(self, model, binary_classification):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n\n    def forward(self, input_ids, attention_mask, mask_pos):\n        logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n        if self.binary_classification:\n            assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n            logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)   \n        return logits\n        \n# Linearized LoRA update  \nclass LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))  # scaling the initailization by \\frac{1}{sqrt{r}}\n           #torch.nn.init.kaiming_normal_(param)\n      \n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])     \n            \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) \n        self.gradient_dtype = gradient[0].dtype\n\n        # Compute \\langle G(X_i), B*A \\rangle \n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n       \n# Linearized LoRA update \nclass Linearupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.delta = [torch.zeros(self.target_size[i][0], self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.delta_list = nn.ParameterList([nn.Parameter(param) for param in self.delta])\n       \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) #train_gradient = [Batch, label, 768 , 768]\n        self.gradient_dtype = gradient[0].dtype\n\n        #Minimizing nuclear norm is equivalent to minimizing the sum of nuclaer norm of nontrivial diagnoals. \n        \n        output = sum(torch.sum(self.delta_list[i] * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n               \nclass LinearizedLoraTrainer(LinearHeadTrainer):\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        args: TrainingArguments,\n        data_collator: Optional[DataCollator] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Dataset] = None,\n        *posargs,\n        **kwargs\n    ):\n        super().__init__(model, args, data_collator, train_dataset, eval_dataset, *posargs, **kwargs)\n\n        self.grad_dim = None\n        self.train_targets = None\n        self.num_labels = None\n        self.gradient_dtype = None\n \n        self.binary_classification = self.args.binary_classification\n        self.model_wrapper = LogitModelWrapper(self.model, self.args.binary_classification)\n        \n        self.target_layers=[]\n        self.target_size=[]\n        for name, param in self.model_wrapper.named_parameters():\n            \n            if self.args.train_last_layer:\n                # There are 12 layers in RoBERTa-base model\n                if  \"11\" in name and \"attention\" in name and \"query\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n                if  \"11\" in name and \"attention\" in name and \"value\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n            else:\n                if  \"attention\" in name and \"query\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n                if  \"attention\" in name and \"value\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n                \n        logger.info(\"Calculating gradient with respect to {}\".format(self.target_layers))\n        \n        #lora\n        if self.model.model_args.apply_lora == True:\n            self.lora_model = LinearLoraupdate(self.model,self.args.binary_classification,self.target_layers,self.target_size).to(self.args.device)\n        else:\n            self.lora_model = Linearupdate(self.model,self.args.binary_classification,self.target_layers,self.target_size).to(self.args.device)\n        \n    ## Method to compute gradient 1\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n            \n        def convert_to_buffer(name):\n            if layer_name in name:\n                #logger.info(\"Including {}\".format(name))\n                return False\n            else:\n                return True\n        \n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n  \n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n                \n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n    \n            \n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n            \n        targets.append(label)\n    \n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))  \n    \n    ## Method to compute gradient 2\n    def compute_gradient_everylayer(self, inputs_outer):\n        \n        grads_outer=[] \n        with torch.no_grad():\n            for layer_name in self.target_layers:   \n                    grads_outer_layer, targets = self.compute_gradient_perlayer(inputs_outer, layer_name)\n                    grads_outer.append(grads_outer_layer)\n                    \n        if self.grad_dim is None:\n            self.grad_dim = sum(np.prod(x.shape[2:]) for x in grads_outer)\n                \n        return (grads_outer, targets) #list of gradients for each layer and targets.\n    \n    def compute_gradient_sharded(self, inputs_outer):\n        with torch.no_grad():\n            grads_outer , targets = self.compute_gradient_everylayer(inputs_outer)\n        return  grads_outer, targets \n\n    #get pre-trained logits and training targets , this corresponds to f(X_i), Y_i\n    def compute_model_logits(self, inputs_outer):\n\n        self.model_wrapper.eval()\n        \n        logits = []\n        targets = []\n        with torch.no_grad():\n            for k, v in inputs_outer.items():\n                if isinstance(v, torch.Tensor):\n                    inputs_outer[k] = v.to(self.args.device)\n            label = inputs_outer.get(\"labels\")\n            if self.args.binary_classification:\n                label = (label * 2 - 1).float()  # convert from {0, 1} to {-1, 1}\n            preds = self.model_wrapper(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))\n            logits.append(preds.detach())\n            targets.append(label)\n\n        logits = torch.cat(logits, dim=0)\n        targets = torch.cat(targets, dim=0);\n\n        return logits, targets \n\n    def train(self, model_path=None, dev_objective=None):\n        if self.args.from_linearhead and model_path is None:\n            super().train(model_path, dev_objective) # Train output layer using LinearHeadTrainer\n        eval_dataset = self.train_dataset\n        return TrainOutput(0, 0.0, {}), None\n    \n    # Fine-tune linearized model\n    def finetune(self, train_dataset, eval_dataset):\n        \n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr) # Weight decay will be implented manually\n\n        if self.args.fp16 and _use_apex:\n            if not transformers.is_apex_available():\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            model, optimizer = amp.initialize(self.lora_model, optimizer, opt_level=self.args.fp16_opt_level)\n\n        # Multi-gpu training (should be after apex fp16 initialization)\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n        \n        epoch_count = 0 \n        \n        #Make sure to freeze other parameters\n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False);\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False);\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False);\n         \n        for name, param in self.lora_model.named_parameters():\n            if param.requires_grad:\n                print(f\"{name} is being trained\")\n                \n                \n        writer = SummaryWriter(f\"./finallasttrain/{self.model.model_args.apply_lora}-{self.args.linear_lr}-{self.model.data_args.task_name}-{self.args.seed}-{self.model.model_args.lora_r}\")\n        \n        file_exists = False\n        eval_file_exists = False\n        saved_gradients_eval = []\n        saved_gradients = []\n        for epoch in range(self.args.linear_num_epoch):\n            \n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n            \n            if self.args.eval_during_training:\n                total_loss_eval = 0\n                eval_preds=[]\n                eval_targets_list=[]\n                \n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False);\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False);\n                    \n                \n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1) \n                    self.gradient_dtype = gradient[0].dtype\n                    \n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)   \n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                ## Weight decay\n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001: #Skip this if there is no weight decay (weight decay = 0)\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)  #Soft-thresholding operator \n                            param = u@s@v;\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param;\n                \n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:  #Skip this if there is no weight decay (weight decay = 0)\n                            reg += (1/2)*(param ** 2).sum();  \n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            reg += torch.norm(param, p = 'nuc');\n                            \n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item();\n                            \n            file_exists = True\n                          \n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd) \n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n            writer.add_scalar(f\"train_loss_{self.model.data_args.task_name}/epoch\", avg_loss, epoch)\n            \n            # Do evaluation during training if needed.\n            if self.args.eval_during_training: \n                with torch.no_grad():\n                    \n                    for i, inputs_outer in enumerate(tqdm(dataloader_outer_eval, desc=\"Evaluating\")):\n                               \n                        if eval_file_exists:\n                            gradient_eval = saved_gradients_eval[i]\n                            for j in range(len(self.target_layers)):\n                                gradient_eval[j].to(self.args.device);\n                                gradient_eval[j].requires_grad_(False);\n                                \n                        else:\n                            gradient_eval, _  = self.compute_gradient_sharded(inputs_outer)\n                            for j in range(len(self.target_layers)):  \n                                gradient_eval[j].requires_grad_(False);\n                        \n                        \n                        output_eval = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient_eval)\n                        eval_logits, eval_targets = self.compute_model_logits(inputs_outer)\n                        eval_logits = eval_logits.to(self.args.device)\n                            \n                        loss_eval = nn.CrossEntropyLoss(reduction = 'sum')(output_eval + eval_logits , eval_targets) \n                        dist.all_reduce(loss_eval, op=dist.ReduceOp.SUM) \n                        total_loss_eval += loss_eval.item();\n                        \n                        eval_preds.append( eval_logits + output_eval );\n                        eval_targets_list.append(eval_targets);\n                        \n                        saved_gradients_eval_cpu = []\n                        for j in range(len(self.target_layers)):\n                            gradient_eval_layer= gradient_eval[j].detach().cpu()\n                            saved_gradients_eval_cpu.append(gradient_eval_layer)\n                        saved_gradients_eval.append(saved_gradients_eval_cpu)\n                        \n                       \n                eval_file_exists = True\n\n                avg_loss_eval = (total_loss_eval / len(dataloader_outer_eval.dataset) )+ (reg * self.args.linear_wd)\n                logger.info(f\"epoch : {epoch+1} eval_loss : {avg_loss_eval}\")\n                writer.add_scalar(f\"eval_loss_{self.model.data_args.task_name}/epoch\", avg_loss_eval, epoch)\n                \n                eval_preds = torch.cat(eval_preds, dim=0)\n                eval_targets = torch.cat(eval_targets_list, dim=0)\n\n                if self.args.local_rank != -1:\n                    logger.info(\"Starting to gather kernel across GPUs\")\n                    eval_preds = varsize_tensor_all_gather(eval_preds.to(self.args.device), torch.distributed.get_world_size())\n                    eval_targets = varsize_tensor_all_gather(eval_targets.to(self.args.device), torch.distributed.get_world_size())\n                    logger.info(\"Finished gathering kernel across GPUs\")\n\n                # Now calculate the accuarcies\n                metrics = None\n                eval_preds = eval_preds.cpu()\n                eval_targets = eval_targets.cpu()\n                \n                if self.args.binary_classification: # Make sure to compute loss before this transformation!\n                    eval_preds = torch.cat([-eval_preds, eval_preds], dim=-1) # convert back to two logits\n                    eval_targets = ((eval_targets + 1) / 2).long() # convert back from {-1, 1} to {0, 1}\n\n                if self.compute_metrics is not None:\n                    metrics = self.compute_metrics(EvalPrediction(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy()))\n\n                # Prefix all keys with eval_\n                for key in list(metrics.keys()):\n                    if not key.startswith(\"eval_\"):\n                        metrics[f\"eval_{key}\"] = metrics.pop(key)\n                metrics[\"eval_loss\"] = avg_loss\n\n                metrics.update(self.metrics())\n                metrics[\"grad_dim\"] = self.grad_dim\n\n                output = PredictionOutput(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy(), metrics=metrics)\n                metrics = output.metrics\n                objective = default_dev_objective(metrics)\n                logger.info(f\"epoch : {epoch+1}  objective : {objective}\")\n                writer.add_scalar(f\"Eval_acc_{self.model.data_args.task_name}/epoch\", objective, epoch)\n                         \n        writer.flush()\n        writer.close()\n        self.save_model(self.args.output_dir)\n        return avg_loss.item() , epoch_count",
    "Experiment Result": "Method: Linearized LoRA (or general linearized update)\nTrainer: `LinearizedLoraTrainer` (specified by `training_args.trainer='linearized'`)\n\nModel Configuration (`ModelArguments`):\n- `model_name_or_path`: Path to pretrained model or model identifier.\n- `apply_lora`: Whether to use LoRA for finetuning (default: `True`).\n- `lora_alpha`: Initialization scale for one of the low rank matrices in LoRA (default: `1`).\n- `lora_r`: Inner rank for LoRA matrices (default: `8`). LoRA is only implemented for RoBERTa models.\n\nTraining Arguments (`DynamicTrainingArguments`):\n- `trainer`: Specifies `LinearizedLoraTrainer` (default: `linearized`).\n- `linear_freeze_A`: Freeze layer A in LoRA (default: `False`). This flag prevents `Lora_A` parameters from being updated during finetuning.\n- `linear_lr`: Learning rate for linearized LoRA finetuning (default: `0.01`).\n- `linear_num_epoch`: Number of epochs in linearized LoRA training (default: `30`).\n- `linear_wd`: Weight decay in linearized LoRA (default: `0`). This is used for nuclear norm regularization:\n    - If `apply_lora` is `True`, it applies L2 regularization to LoRA parameters (`param = param - self.args.linear_wd * self.args.linear_lr * param`).\n    - If `apply_lora` is `False`, it applies soft-thresholding on singular values for nuclear norm regularization (`u,s,v = torch.svd(param); s = torch.nn.Threshold(0, 0)(s- self.args.linear_lr * self.args.linear_wd); param = u@s@v`).\n- `eval_during_training`: Whether to perform evaluation during the training process (default: `False`).\n- `train_last_layer`: Whether to train only the last layer (default: `True`). Specifically targets query and value attention weights in the last layer (layer 11 for RoBERTa-base models). If `False`, it targets query and value attention weights in all layers.\n- `optimizer`: `sgd` or `adam` (default: `sgd` for linearized training).\n- `binary_classification`: If `num_classes=2`, convert two softmax logits to single sigmoid logit if needed (default: `False`). This affects how logits are processed in `LogitModelWrapper`.\n\nGradient Computation:\n- Gradients are computed with respect to `target_layers` using `functorch.jacrev` to obtain the Jacobian. \n- `target_layers` are identified as specific attention query and value weight matrices, either only in the last layer or across all layers, depending on `train_last_layer`.\n\nTraining Loop (`LinearizedLoraTrainer.finetune`):\n- An SGD optimizer is used for updating the `lora_model` parameters.\n- The loss function is `nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output, targets)` where `train_logits` are the initial model logits and `output` is the linearized update.\n- Weight decay is applied as described above."
}{
    "Title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
    "Main Contributions": "This research theoretically analyzes LoRA (Low-Rank Adaptation) fine-tuning within the Neural Tangent Kernel (NTK) regime. It identifies the main research problem as the limited theoretical understanding of LoRA's mechanisms, trainability, and generalizability, particularly its quadratic parameterization. The key findings are: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank r ≲ √N (specifically, r(r+1)/2 ≤ KN); (ii) using LoRA with rank r ≳ √N (specifically, r(r+1)/2 > KN) eliminates spurious local minima, enabling (stochastic) gradient descent to find these low-rank solutions; and (iii) the low-rank solution found using LoRA generalizes well.",
    "Methodology": "The methodology is primarily theoretical analysis within the Neural Tangent Kernel (NTK) regime, assuming the network's change during fine-tuning can be approximated by a first-order Taylor expansion. LoRA's low-rank parameterization (δ = uv⊺) is analyzed, with weight decay interpreted as nuclear norm regularization. The study employs optimization landscape analysis, focusing on second-order stationary points (SOSPs) and global minima, drawing from prior work on matrix factorization and related theorems. A key technique involves introducing a small, randomly generated positive semi-definite perturbation P to the loss function to rigorously prove the absence of spurious local minima, using an application of Sard's theorem (via Lemma 4.5). Generalization guarantees are derived using Rademacher complexity and McDiarmid's inequality, particularly for cross-entropy loss.",
    "Experimental Setup": "The experimental setup involves simple experiments to validate the theory by fine-tuning linearized pre-trained models. For NLP tasks, a linearized RoBERTa-base model was fine-tuned using prompt-based fine-tuning on datasets of size N=32 with K=2 labels (SST-2, QNLI, MR, CR, QQP, Subj), using cross-entropy loss. Only the query (Wq) and value (Wv) weights of the last layer were trained, with a LoRA rank r ≥ 11, and full fine-tuning as a baseline. For image classification, a pre-trained vision transformer was fine-tuned on the bean disease dataset (N=48, K=3 labels). For speech classification, a pre-trained wav2vec2 model was fine-tuned on a SUPERB dataset (N=64, K=4 labels). Training curves (loss vs. epochs) and test accuracy (for NLP) were observed.",
    "Limitations": "The main limitations include the reliance on the Neural Tangent Kernel (NTK) regime assumption, which linearizes network behavior and may not hold for all fine-tuning scenarios. The rigorous theoretical guarantee for the absence of spurious local minima requires a 'perturbed' loss function with a randomly generated term, which does not directly apply to the unperturbed practical LoRA setup. Furthermore, the theory does not provide insights into the 'convexity' or 'favorability' of the loss landscape, as empirical observations suggest that lower LoRA ranks can slow down convergence due to plateaus or saddle points. The primary generalization guarantee is restricted to cross-entropy loss, although a more general version is in the appendix.",
    "Future Research Directions": "Future research directions include conducting more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, better understanding the minimum rank requirement through lower bounds, and analyzing the trade-off between training rate and LoRA rank. This last point is motivated by the observation that lower LoRA ranks might create unfavorable flat training landscapes, slowing down gradient descent dynamics, and suggests designing remedies for this phenomenon.",
    "Experiment Code": "class LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))  # scaling the initailization by \\frac{1}{sqrt{r}}\n           #torch.nn.init.kaiming_normal_(param)\n      \n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])     \n            \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)   \n        \n        self.num_labels = gradient[0].size(1) \n        self.gradient_dtype = gradient[0].dtype\n\n        # Compute \\langle G(X_i), B*A \\rangle \n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n       \nclass Linearupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.delta = [torch.zeros(self.target_size[i][0], self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.delta_list = nn.ParameterList([nn.Parameter(param) for param in self.delta])\n       \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) #train_gradient = [Batch, label, 768 , 768]\n        self.gradient_dtype = gradient[0].dtype\n\n        #Minimizing nuclear norm is equivalent to minimizing the sum of nuclaer norm of nontrivial diagnoals. \n        \n        output = sum(torch.sum(self.delta_list[i] * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n\nclass LinearizedLoraTrainer(LinearHeadTrainer):\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n            \n        def convert_to_buffer(name):\n            if layer_name in name:\n                return False\n            else:\n                return True\n        \n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n  \n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n                \n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n    \n            \n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n            \n        targets.append(label)\n    \n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))  \n    \n    def compute_gradient_everylayer(self, inputs_outer):\n        \n        grads_outer=[] \n        with torch.no_grad():\n            for layer_name in self.target_layers:   \n                    grads_outer_layer, targets = self.compute_gradient_perlayer(inputs_outer, layer_name)\n                    grads_outer.append(grads_outer_layer)\n                    \n        if self.grad_dim is None:\n            self.grad_dim = sum(np.prod(x.shape[2:]) for x in grads_outer)\n                \n        return (grads_outer, targets)\n    \n    def compute_gradient_sharded(self, inputs_outer):\n        with torch.no_grad():\n            grads_outer , targets = self.compute_gradient_everylayer(inputs_outer)\n        return  grads_outer, targets \n\n    def finetune(self, train_dataset, eval_dataset):\n        \n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr) # Weight decay will be implented manually\n\n        if self.args.fp16 and _use_apex:\n            if not transformers.is_apex_available():\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            model, optimizer = amp.initialize(self.lora_model, optimizer, opt_level=self.args.fp16_opt_level)\n\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n        \n        epoch_count = 0 \n        \n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False)\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False)\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False)\n         \n        for name, param in self.lora_model.named_parameters():\n            if param.requires_grad:\n                print(f\"{name} is being trained\")\n                \n                \n        writer = SummaryWriter(f\"./finallasttrain/{self.model.model_args.apply_lora}-{self.args.linear_lr}-{self.model.data_args.task_name}-{self.args.seed}-{self.model.model_args.lora_r}\")\n        \n        file_exists = False\n        eval_file_exists = False\n        saved_gradients_eval = []\n        saved_gradients = []\n        for epoch in range(self.args.linear_num_epoch):\n            \n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n            \n            if self.args.eval_during_training:\n                total_loss_eval = 0\n                eval_preds=[]\n                eval_targets_list=[]\n                \n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                    \n                \n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1) \n                    self.gradient_dtype = gradient[0].dtype\n                    \n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)   \n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)\n                            param = u@s@v\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param\n                \n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:\n                            reg += (1/2)*(param ** 2).sum()  \n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            reg += torch.norm(param, p = 'nuc')\n                            \n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item()\n                            \n            file_exists = True\n                          \n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd) \n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n            writer.add_scalar(f\"train_loss_{self.model.data_args.task_name}/epoch\", avg_loss, epoch)\n            \n            if self.args.eval_during_training: \n                with torch.no_grad():\n                    \n                    for i, inputs_outer in enumerate(tqdm(dataloader_outer_eval, desc=\"Evaluating\")):\n                               \n                        if eval_file_exists:\n                            gradient_eval = saved_gradients_eval[i]\n                            for j in range(len(self.target_layers)):\n                                gradient_eval[j].to(self.args.device)\n                                gradient_eval[j].requires_grad_(False)\n                                \n                        else:\n                            gradient_eval, _  = self.compute_gradient_sharded(inputs_outer)\n                            for j in range(len(self.target_layers)):  \n                                gradient_eval[j].requires_grad_(False)\n                        \n                        \n                        output_eval = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient_eval)\n                        eval_logits, eval_targets = self.compute_model_logits(inputs_outer)\n                        eval_logits = eval_logits.to(self.args.device)\n                            \n                        loss_eval = nn.CrossEntropyLoss(reduction = 'sum')(output_eval + eval_logits , eval_targets) \n                        dist.all_reduce(loss_eval, op=dist.ReduceOp.SUM) \n                        total_loss_eval += loss_eval.item()\n                        \n                        eval_preds.append( eval_logits + output_eval )\n                        eval_targets_list.append(eval_targets)\n                        \n                        saved_gradients_eval_cpu = []\n                        for j in range(len(self.target_layers)):\n                            gradient_eval_layer= gradient_eval[j].detach().cpu()\n                            saved_gradients_eval_cpu.append(gradient_eval_layer)\n                        saved_gradients_eval.append(saved_gradients_eval_cpu)\n                        \n                       \n                eval_file_exists = True\n\n                avg_loss_eval = (total_loss_eval / len(dataloader_outer_eval.dataset) )+ (reg * self.args.linear_wd)\n                logger.info(f\"epoch : {epoch+1} eval_loss : {avg_loss_eval}\")\n                writer.add_scalar(f\"eval_loss_{self.model.data_args.task_name}/epoch\", avg_loss_eval, epoch)\n                \n                eval_preds = torch.cat(eval_preds, dim=0)\n                eval_targets = torch.cat(eval_targets_list, dim=0)\n\n                if self.args.local_rank != -1:\n                    logger.info(\"Starting to gather kernel across GPUs\")\n                    eval_preds = varsize_tensor_all_gather(eval_preds.to(self.args.device), torch.distributed.get_world_size())\n                    eval_targets = varsize_tensor_all_gather(eval_targets.to(self.args.device), torch.distributed.get_world_size())\n                    logger.info(\"Finished gathering kernel across GPUs\")\n\n                metrics = None\n                eval_preds = eval_preds.cpu()\n                eval_targets = eval_targets.cpu()\n                \n                if self.args.binary_classification:\n                    eval_preds = torch.cat([-eval_preds, eval_preds], dim=-1)\n                    eval_targets = ((eval_targets + 1) / 2).long()\n\n                if self.compute_metrics is not None:\n                    metrics = self.compute_metrics(EvalPrediction(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy()))\n\n                for key in list(metrics.keys()):\n                    if not key.startswith(\"eval_\"):\n                        metrics[f\"eval_{key}\"] = metrics.pop(key)\n                metrics[\"eval_loss\"] = avg_loss\n\n                metrics.update(self.metrics())\n                metrics[\"grad_dim\"] = self.grad_dim\n\n                output = PredictionOutput(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy(), metrics=metrics)\n                metrics = output.metrics\n                objective = default_dev_objective(metrics)\n                logger.info(f\"epoch : {epoch+1}  objective : {objective}\")\n                writer.add_scalar(f\"Eval_acc_{self.model.data_args.task_name}/epoch\", objective, epoch)\n                         \n        writer.flush()\n        writer.close()\n        self.save_model(self.args.output_dir)\n        return avg_loss.item() , epoch_count",
    "Experiment Result": "The method employs LoRA (Low-Rank Adaptation) parameterization and linearized fine-tuning. Key experimental settings include:\n\n**LoRA Parameterization:**\n*   `apply_lora`: A boolean flag (default `True` in `ModelArguments`) indicating whether LoRA low-rank matrices `A` and `B` are used. If `False`, a full `delta` matrix (representing the low-rank update) is optimized directly.\n*   `lora_alpha`: An integer (default `1` in `ModelArguments`) specifying the initialization scale for one of the low-rank matrices.\n*   `lora_r`: An integer (default `8` in `ModelArguments`) defining the inner rank `r` for LoRA matrices, where the low-rank update `δ = uvᵀ` (with `u` as `Lora_B` and `vᵀ` as `Lora_A`).\n*   **Initialization**: `Lora_A` matrices are initialized with `torch.nn.init.normal_` using a mean of `0` and a standard deviation of `1 / (np.sqrt(lora_r) * target_size[i][1])`.\n\n**Linearized Fine-tuning Settings:**\n*   `trainer`: Set to `\"linearized\"` (default in `DynamicTrainingArguments`), which enables the `LinearizedLoraTrainer` for optimization.\n*   `linear_lr`: A float (default `0.01` in `DynamicTrainingArguments`) representing the learning rate for the SGD optimizer used in the linearized LoRA optimization.\n*   `linear_num_epoch`: An integer (default `30` in `DynamicTrainingArguments`) specifying the number of training epochs for the linearized model.\n*   `linear_wd`: A float (default `0` in `DynamicTrainingArguments`) for the weight decay coefficient. Its application depends on `apply_lora`:\n    *   If `apply_lora` is `True`, an L2 regularization term `(1/2)*(param ** 2).sum()` is applied to the individual `Lora_A` and `Lora_B` matrices.\n    *   If `apply_lora` is `False` (optimizing a direct `delta` matrix), a nuclear norm regularization `torch.norm(param, p = 'nuc')` is applied to the `delta` parameter using a soft-thresholding operator.\n*   `linear_freeze_A`: A boolean (default `False` in `DynamicTrainingArguments`). If `True`, the `Lora_A` matrix in the low-rank decomposition is frozen during optimization.\n*   `train_last_layer`: A boolean (default `True` in `DynamicTrainingArguments`). If `True`, gradient computations and parameter updates are restricted to the query and value attention weights exclusively within the last layer of the model. If `False`, these operations are applied to attention query and value weights across all layers."
}{
    "Title": "Adapters Strike Back",
    "Main Contributions": "The paper provides an in-depth and systematic study of adapter configurations for vision transformers (ViTs), uncovering common pitfalls in previous implementations. It proposes an improved adapter architecture, called Adapter+, which features a learnable, channel-wise scaling and an optimal positioning within the transformer layer. Adapter+ achieves state-of-the-art average accuracy on the VTAB benchmark (77.6% without per-task hyperparameter optimization, and 77.9% with optimization) and on the FGVC benchmark (90.7%) with high parameter efficiency. It also demonstrates superior robustness across different VTAB subgroups compared to other parameter-efficient tuning methods.",
    "Methodology": "The methodology involved a systematic evaluation of various adapter configurations for ViTs. This included investigating: (1) Adapter position (Pre-Adapter, Post-Adapter, Parallel-Adapter, Intermediate-Adapter relative to the FFN section), (2) Adapter inner structure (impact of biases, layer normalization, layer-wise scaling, and the proposed channel-wise learned scaling), and (3) Parameter initialization strategies (Houlsby, BERT, LoRA). The core adapter module is a bottleneck structure with down- and up-projections and a non-linear activation. Adapter+ specifically utilizes the Post-Adapter position with a learnable, channel-wise scaling and Houlsby initialization, foregoing layer normalization within the adapter. The study also emphasized the critical importance of matching data normalization to the pre-trained backbone and evaluated the influence of training regularization methods like stochastic depth and dropout.",
    "Experimental Setup": "Experiments were conducted using a ViT-B/16 network pre-trained on ImageNet-21k. Additional experiments verified the generality of conclusions with ViTs fine-tuned on ImageNet-1k and trained with AugReg, as well as a DINO-trained ViT. The evaluation benchmarks were the Visual Task Adaptation Benchmark (VTAB), comprising 19 tasks grouped into Natural, Specialized, and Structured categories, and a Fine-Grained Visual Classification (FGVC) benchmark consisting of five datasets. Models were trained with an AdamW optimizer (learning rate 10^-3, weight decay 10^-4, batch size 64) for 100 epochs, using a cosine learning rate schedule with a linear warm-up. Input images were resized to 224x224px, with random resize crop and horizontal flipping for FGVC. Data normalization (ImageNet normalization) was consistently applied to match the pre-training. Comparison included full fine-tuning, linear probing, and re-evaluations of contemporary parameter-efficient methods such as LoRA, VPT, NOAH, SSF, FacT, Consolidator, and SPT, ensuring fair comparison by aligning data normalization and training schedules.",
    "Limitations": "Not mentioned",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "import math\nfrom functools import partial\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom timm.models.layers import DropPath, PatchEmbed, Mlp\nfrom timm.models.helpers import checkpoint_seq\nfrom timm.models.vision_transformer import (\n    Block,\n    ResPostBlock,\n    VisionTransformer,\n    checkpoint_filter_fn,\n    build_model_with_cfg,\n)\n\n# for Adapter+ set norm_layer to None and scaling to \"channel\"\nclass Adapter(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        bottleneck_dim=8,\n        drop_path=0.0,\n        dropout=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        scaling=1.0,\n        init=\"houlsby\",\n        bias=True,\n        pre_dropout=False,\n    ):\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Dropout(dropout) if dropout > 0 and pre_dropout else nn.Identity(),\n            nn.Linear(embed_dim, bottleneck_dim, bias=bias),\n            act_layer() if act_layer else nn.Identity(),\n            nn.Dropout(dropout) if dropout > 0 and not pre_dropout else nn.Identity(),\n            nn.Linear(bottleneck_dim, embed_dim, bias=bias),\n        )\n        self.norm_a = norm_layer(embed_dim) if norm_layer else nn.Identity()\n        self.drop_path_a = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.bottleneck_dim = bottleneck_dim\n        if scaling == \"learned\":\n            self.scaling = nn.Parameter(torch.ones(1))\n        elif scaling == \"channel\":\n            self.scaling = nn.Parameter(torch.ones(embed_dim))\n        else:\n            self.scaling = scaling\n\n        # init following (Houslby 2019)\n        if init == \"houlsby\":\n            std = 0.01  # paper value, houlsby code implementation: std = 0.001\n            nn.init.trunc_normal_(\n                self.bottleneck[1].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[1].bias is not None:\n                nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.trunc_normal_(\n                self.bottleneck[4].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n\n        # LoRa init\n        elif init == \"lora\":\n            # leave in projection with default init\n            nn.init.kaiming_uniform_(self.bottleneck[1].weight, a=math.sqrt(5))\n            if self.bottleneck[1].bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(\n                    self.bottleneck[1].weight\n                )\n                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                nn.init.uniform_(self.bottleneck[1].bias, -bound, bound)\n            # set out projection to zeros\n            nn.init.zeros_(self.bottleneck[4].weight)\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n\n        elif init == \"bert\":\n            nn.init.normal_(self.bottleneck[1].weight, mean=0.0, std=0.02)\n            if self.bottleneck[1].bias is not None:\n                nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.normal_(self.bottleneck[4].weight, mean=0.0, std=0.02)\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n\n        else:\n            raise ValueError(f\"Initialization {init} not implemented!\")\n\n    def forward(\n        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        x = self.norm_a(x)\n        x = self.drop_path_a(self.bottleneck(x))\n        x = x * self.scaling\n\n        y = x\n        if skip is not None:\n            y = y + skip\n\n        return y\n\n\nclass LoRAAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        attn_drop=0.0,\n        proj_drop=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        lora_config=None,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.lora_config = lora_config\n        self.adapters = nn.ModuleDict(\n            {\n                l: Adapter(\n                    dim,\n                    bottleneck_dim=lora_config.dim,\n                    act_layer=act_layer if lora_config.act_layer else None,\n                    norm_layer=norm_layer if lora_config.norm_layer else None,\n                    scaling=lora_config.scaling,\n                    bias=((qkv_bias or l == \"o\") and lora_config.bias),\n                    drop_path=lora_config.drop_path,\n                    dropout=lora_config.dropout,\n                    pre_dropout=lora_config.pre_dropout,\n                )\n                for l in lora_config.location\n            }\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C // num_heads\n        )\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        if \"q\" in self.adapters:\n            q = q + (\n                self.adapters[\"q\"](x)\n                .reshape(B, N, self.num_heads, C // self.num_heads)\n                .permute(0, 2, 1, 3)  # B, num_heads, N, C // num_heads\n            )\n        if \"k\" in self.adapters:\n            k = k + (\n                self.adapters[\"k\"](x)\n                .reshape(B, N, self.num_heads, C // self.num_heads)\n                .permute(0, 2, 1, 3)  # B, num_heads, N, C // num_heads\n            )\n        if \"v\" in self.adapters:\n            v = v + (\n                self.adapters[\"v\"](x)\n                .reshape(B, N, self.num_heads, C // self.num_heads)\n                .permute(0, 2, 1, 3)  # B, num_heads, N, C // num_heads\n            )\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        if \"o\" in self.adapters:\n            x = x + self.adapters[\"o\"](x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass AdapterBlock(Block):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        proj_drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values: Optional[float] = None,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer: nn.Module = nn.LayerNorm,\n        mlp_layer: nn.Module = Mlp,\n        adapter_config=None,\n        lora_config=None,\n        prompt_config=None,\n        patch_size=16,\n    ):\n        super().__init__(\n            dim,\n            num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            proj_drop=proj_drop,\n            attn_drop=attn_drop,\n            init_values=init_values,\n            drop_path=drop_path,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            mlp_layer=mlp_layer,\n        )\n\n        self.adapter_config = adapter_config\n        self.lora_config = lora_config\n        self.prompt_config = prompt_config\n\n        if adapter_config is not None:\n            self.adapter = Adapter(\n                dim,\n                bottleneck_dim=adapter_config.dim,\n                dropout=adapter_config.dropout,\n                drop_path=adapter_config.drop_path,\n                act_layer=act_layer if adapter_config.act_layer else None,\n                norm_layer=norm_layer if adapter_config.norm_layer else None,\n                bias=adapter_config.bias,\n                scaling=adapter_config.scaling,\n                init=adapter_config.init,\n            )\n            if adapter_config.attn_adapter:\n                self.adapter_attn = Adapter(\n                    dim,\n                    bottleneck_dim=adapter_config.dim,\n                    dropout=adapter_config.dropout,\n                    drop_path=adapter_config.drop_path,\n                    act_layer=act_layer if adapter_config.act_layer else None,\n                    norm_layer=norm_layer if adapter_config.norm_layer else None,\n                    bias=adapter_config.bias,\n                    scaling=adapter_config.scaling,\n                    init=adapter_config.init,\n                )\n\n        if lora_config is not None and lora_config.config == \"attention\":\n            self.attn = LoRAAttention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                lora_config=lora_config,\n            )\n\n        if prompt_config is not None:\n            self.prompt = nn.Parameter(torch.zeros(prompt_config.num_tokens, dim))\n            self.prompt_dropout = nn.Dropout(prompt_config.dropout)\n\n            val = math.sqrt(6.0 / float(3 * patch_size**2 + prompt_config.num_tokens))\n            nn.init.uniform_(self.prompt, -val, val)\n\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward_pre(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = self.adapter(x, skip=x)\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n    def forward_pfeiffer(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        skip = self.drop_path2(self.ls2(self.mlp(self.norm2(y))))\n        x = y + skip\n        x = self.adapter(x, skip=skip)\n        x = x + y\n        return x\n\n    def forward_intermediate(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        y = self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x + self.adapter(y, skip=y)\n\n    def forward_houlsby(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.adapter_attn(y, skip=y)\n        y = self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x + self.adapter(y, skip=y)\n\n    def forward_parallel(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        z = y + self.drop_path2(self.ls2(self.mlp(self.norm2(y))))\n        return self.adapter(y, skip=z)\n\n    def forward_no_adapter(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n    def include_prompt(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = torch.cat(\n            (\n                x[:, :1, :],\n                self.prompt_dropout(self.prompt.expand(B, -1, -1)),\n                x[:, (1 + self.prompt_config.num_tokens) :, :],\n            ),\n            dim=1,\n        )\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\n\nclass AdapterResPostBlock(ResPostBlock):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        proj_drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values: Optional[float] = None,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer: nn.Module = nn.LayerNorm,\n        mlp_layer: nn.Module = Mlp,\n        adapter_config=None,\n        lora_config=None,\n        prompt_config=None,\n        patch_size=16,\n    ) -> None:\n        super().__init__(\n            dim=dim,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            proj_drop=proj_drop,\n            attn_drop=attn_drop,\n            init_values=init_values,\n            drop_path=drop_path,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            mlp_layer=mlp_layer,\n        )\n\n        self.adapter_config = adapter_config\n        self.lora_config = lora_config\n        self.prompt_config = prompt_config\n\n        if adapter_config is not None:\n            self.adapter = Adapter(\n                dim,\n                bottleneck_dim=adapter_config.dim,\n                dropout=adapter_config.dropout,\n                drop_path=adapter_config.drop_path,\n                act_layer=act_layer if adapter_config.act_layer else None,\n                norm_layer=norm_layer if adapter_config.norm_layer else None,\n                bias=adapter_config.bias,\n                scaling=adapter_config.scaling,\n                init=adapter_config.init,\n            )\n            if adapter_config.attn_adapter:\n                self.adapter_attn = Adapter(\n                    dim,\n                    bottleneck_dim=adapter_config.dim,\n                    dropout=adapter_config.dropout,\n                    drop_path=adapter_config.drop_path,\n                    act_layer=act_layer if adapter_config.act_layer else None,\n                    norm_layer=norm_layer if adapter_config.norm_layer else None,\n                    bias=adapter_config.bias,\n                    scaling=adapter_config.scaling,\n                    init=adapter_config.init,\n                )\n\n        if lora_config is not None and lora_config.config == \"attention\":\n            self.attn = LoRAAttention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                lora_config=lora_config,\n            )\n\n        if prompt_config is not None:\n            self.prompt = nn.Parameter(torch.zeros(prompt_config.num_tokens, dim))\n            self.prompt_dropout = nn.Dropout(prompt_config.dropout)\n\n            val = math.sqrt(6.0 / float(3 * patch_size**2 + prompt_config.num_tokens))\n            nn.init.uniform_(self.prompt, -val, val)\n\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward_pre(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = self.adapter(x, skip=x)\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n    def forward_pfeiffer(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.norm1(self.attn(x)))\n        skip = self.drop_path2(self.norm2(self.mlp(y)))\n        x = y + skip\n        x = self.adapter(x, skip=skip)\n        x = x + y\n        return x\n\n    def forward_intermediate(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        y = self.drop_path2(self.norm2(self.mlp(x)))\n        return x + self.adapter(y, skip=y)\n\n    def forward_houlsby(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.adapter_attn(y, skip=y)\n        y = self.drop_path2(self.norm2(self.mlp(x)))\n        return x + self.adapter(y, skip=y)\n\n    def forward_parallel(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.norm1(self.attn(x)))\n        z = y + self.drop_path2(self.norm2(self.mlp(y)))\n        return self.adapter(y, skip=z)\n\n    def forward_no_adapter(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n    def include_prompt(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = torch.cat(\n            (\n                x[:, :1, :],\n                self.prompt_dropout(self.prompt.expand(B, -1, -1)),\n                x[:, (1 + self.prompt_config.num_tokens) :, :],\n            ),\n            dim=1,\n        )\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\n\n# modified from timm.models.vision_transformer.VisionTransformer\nclass VisionTransformerAdapter(VisionTransformer):\n    \"\"\"Vision Transformer with Adapter support\n\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        global_pool=\"token\",\n        embed_dim=768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_norm: bool = False,\n        init_values=None,\n        class_token=True,\n        pos_embed=\"learn\",\n        no_embed_class=False,\n        reg_tokens=0,\n        pre_norm=False,\n        fc_norm=None,\n        dynamic_img_size=False,\n        dynamic_img_pad=False,\n        drop_rate=0.0,\n        pos_drop_rate=0.0,\n        patch_drop_rate=0.0,\n        proj_drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        weight_init=\"\",\n        fix_init=False,\n        embed_layer=PatchEmbed,\n        norm_layer=None,\n        act_layer=None,\n        mlp_layer=Mlp,\n        block_fn=AdapterBlock,\n        adapter_config=None,\n        lora_config=None,\n        prompt_config=None,\n    ):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            global_pool (str): type of global pooling for final sequence (default: 'token')\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            init_values: (float): layer-scale init values\n            class_token (bool): use class token\n            fc_norm (Optional[bool]): pre-fc norm after pool, set if global_pool == 'avg' if None (default: None)\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            weight_init (str): weight init scheme\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            act_layer: (nn.Module): MLP activation layer\n        \"\"\"\n        super().__init__(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            num_classes=num_classes,\n            global_pool=global_pool,\n            embed_dim=embed_dim,\n            depth=depth,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            init_values=init_values,\n            class_token=class_token,\n            pos_embed=pos_embed,\n            no_embed_class=no_embed_class,\n            reg_tokens=reg_tokens,\n            pre_norm=pre_norm,\n            fc_norm=fc_norm,\n            dynamic_img_size=dynamic_img_size,\n            dynamic_img_pad=dynamic_img_pad,\n            drop_rate=drop_rate,\n            pos_drop_rate=pos_drop_rate,\n            patch_drop_rate=patch_drop_rate,\n            proj_drop_rate=proj_drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate,\n            weight_init=weight_init,\n            fix_init=fix_init,\n            embed_layer=embed_layer,\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            block_fn=block_fn,\n            mlp_layer=mlp_layer,\n        )\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n        self.prompt_config = prompt_config\n        self.num_cls_token = 0\n        if self.num_cls_token is not None:\n            self.num_cls_token += 1\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(\n            *[\n                block_fn(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_norm=qk_norm,\n                    init_values=init_values,\n                    proj_drop=proj_drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    act_layer=act_layer,\n                    mlp_layer=mlp_layer,\n                    adapter_config=adapter_config,\n                    lora_config=lora_config,\n                    prompt_config=prompt_config,\n                    patch_size=patch_size,\n                )\n                for i in range(depth)\n            ]\n        )\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n\n        if self.prompt_config:\n            B, _, C = x.shape\n            x = torch.cat(\n                (\n                    x[:, : self.num_cls_token, :],\n                    torch.zeros(B, self.prompt_config.num_tokens, C).to(x.device),\n                    x[:, self.num_cls_token :, :],\n                ),\n                dim=1,\n            )\n\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.norm(x)\n        return x\n\n\n# modified from timm.models.vision_transformer._create_vision_transformer\ndef _create_vision_transformer_adapter(\n    variant: str, pretrained: bool = False, adapter=False, **kwargs\n) -> VisionTransformer:\n    out_indices = kwargs.pop(\"out_indices\", 3)\n    if \"flexi\" in variant:\n        # FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed\n        # interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.\n        _filter_fn = partial(\n            checkpoint_filter_fn, interpolation=\"bilinear\", antialias=False\n        )\n    else:\n        _filter_fn = checkpoint_filter_fn\n\n    # FIXME attn pool (currently only in siglip) params removed if pool disabled, is there a better soln?\n    strict = True\n    if \"siglip\" in variant and kwargs.get(\"global_pool\", None) != \"map\":\n        strict = False\n\n    if adapter:\n        # rewrite block_fn to use adapters\n        block_fn = kwargs.pop(\"block_fn\", Block)\n        if block_fn == Block:\n            block_fn = AdapterBlock\n        elif block_fn == ResPostBlock:\n            block_fn = AdapterResPostBlock\n        else:\n            raise ValueError(f\"Adapters not implemented for {block_fn}!\")\n        return build_model_with_cfg(\n            VisionTransformerAdapter,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            block_fn=block_fn,\n            **kwargs,\n        )\n    else:\n        return build_model_with_cfg(\n            VisionTransformer,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            **kwargs,\n        )",
    "Experiment Result": "The methodology involved a systematic evaluation of various adapter configurations for ViTs. These configurations are primarily implemented in the `Adapter` class and its integration within `AdapterBlock` and `AdapterResPostBlock`.\n\n**1. Adapter Position:**\n   The repository investigates different adapter positions relative to the FFN section within the Vision Transformer blocks. These include:\n   - **Post-Adapter:** The adapter is applied after the MLP (Feed-Forward Network) block (`forward_post` method in `AdapterBlock`).\n   - **Pre-Adapter:** The adapter is applied before the MLP block, but after the attention block (`forward_pre` method).\n   - **Intermediate-Adapter:** The adapter is applied within the residual branch of the MLP block (`forward_intermediate` method).\n   - **Parallel-Adapter:** The adapter runs in parallel to the main MLP residual connection (`forward_parallel` method).\n   - **Houlsby Configuration:** Adapters are applied both after the attention block and after the MLP block (`forward_houlsby` method, utilizing `self.adapter_attn` and `self.adapter`).\n   - A Pfeiffer-style adapter position is also available (`forward_pfeiffer`).\n\n**2. Adapter Inner Structure:**\n   The internal design of the `Adapter` module (a bottleneck structure) was varied by considering:\n   - **Biases:** The inclusion of biases in the linear projections within the bottleneck (`bias` parameter in `nn.Linear` layers of the `Adapter` module).\n   - **Layer Normalization:** The presence of `nn.LayerNorm` within the adapter (`norm_layer` parameter in `Adapter`).\n   - **Layer-wise Scaling:** A single learnable scaling parameter for the entire adapter output (`scaling=\"learned\"`).\n   - **Channel-wise Learned Scaling:** A learnable scaling parameter for each channel of the adapter output (`scaling=\"channel\"`).\n\n**3. Parameter Initialization Strategies:**\n   Different weight initialization schemes for the adapter's linear layers were evaluated:\n   - **Houlsby Initialization:** Using `nn.init.trunc_normal_` with a standard deviation of 0.01 for weights and zero initialization for biases (`init=\"houlsby\"`).\n   - **BERT Initialization:** Using `nn.init.normal_` with mean 0.0 and std 0.02 for weights and zero initialization for biases (`init=\"bert\"`).\n   - **LoRA Initialization:** `nn.init.kaiming_uniform_` for the input projection and zero initialization for the output projection (`init=\"lora\"`).\n\n**4. Adapter+ Specifics (as described in the method):**\n   - **Position:** Post-Adapter (`adapter_config.config = \"post\"`).\n   - **Scaling:** Channel-wise learned scaling (`scaling=\"channel\"` in `Adapter`'s `__init__`).\n   - **Initialization:** Houlsby initialization (`init=\"houlsby\"` in `Adapter`'s `__init__`).\n   - **Layer Normalization:** Foregoing layer normalization within the adapter (`norm_layer=None` in `Adapter`'s `__init__`).\n\n**5. Data Normalization:**\n   The study emphasized matching data normalization to the pre-trained backbone. This is implemented by retrieving the `mean` and `std` values from the `vit.default_cfg` of the loaded pre-trained model and applying them during data loading (e.g., in `data/fgvc.py` and `data/vtab.py`).\n\n**6. Training Regularization:**\n   The influence of common regularization methods was evaluated:\n   - **Stochastic Depth:** Applied via `drop_path_rate` in `VisionTransformerAdapter` (stochastic depth decay rule `dpr`) and `drop_path` in the `Adapter` module.\n   - **Dropout:** Applied within the `Adapter` module (`dropout` parameter).\n   - **Label Smoothing:** Utilized during the training loss calculation (`label_smoothing` in `AdapterModel`)."
}{
    "Title": "Spectral Adapter: Fine-Tuning in Spectral Space",
    "Main Contributions": "This research introduces the \"Spectral Adapter,\" a novel Parameter-Efficient Fine-Tuning (PEFT) method that incorporates the spectral information of pretrained weight matrices. The main contributions include: (1) Proposing two spectral adaptation mechanisms: additive tuning (Spectral AdapterA) and orthogonal rotation (Spectral AdapterR) of the top singular vectors, performed after Singular Value Decomposition (SVD) of pretrained weights. (2) Providing theoretical analysis showing Spectral AdapterA doubles the rank capacity of low-rank adapters compared to LoRA for a fixed trainable parameter budget and demonstrating that dominant spectral directions align with ideal neuron directions. (3) Empirically demonstrating superior parameter efficiency, tuning performance, and benefits for multi-adapter fusion compared to various state-of-the-art PEFT methods (LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA). (4) Showcasing its practicality with negligible runtime and storage overhead for large models. This work is the first to propose fine-tuning spectral space in a parameter-efficient and storage-economic way, enhancing existing PEFT methods.",
    "Methodology": "The core methodology involves applying Singular Value Decomposition (SVD) to pretrained model weights (W = USV^T) and then fine-tuning only the top columns of the singular vector matrices (U and V). Two specific spectral adaptation mechanisms are investigated: (1) Spectral AdapterA (Additive Tuning): Modifies the top-r columns of U and V additively, defining the tuned weight as [U1 + AU U2]S[V1 + AV V2]^T, where AU and AV are trainable matrices. It initializes AU and AV to zero. (2) Spectral AdapterR (Orthogonal Rotation): Rotates the top-r columns of U and V using trainable orthogonal matrices, defining the tuned weight as [U1RU U2]S[V1RV V2]^T, where RU and RV are trainable r x r orthogonal matrices. The orthogonality constraint for RU and RV is efficiently maintained using Cayley parameterization, allowing for exact rotation and preserving the SVD structure for subsequent fine-tuning without recomputing SVD. For multi-adapter fusion, Spectral AdapterA distributes different concept tunings along distinct columns of the singular vector matrices and merges them following a FedAvg-like approach: (U0 + ∑λiUi)S0(V0 + ∑λiVi)^T.",
    "Experimental Setup": "The proposed Spectral Adapter was evaluated on large language models and diffusion models. For language models, experiments included: (1) Fine-tuning Llama3 8B model on the Orca Math dataset and evaluating on the GSM8K benchmark. (2) Fine-tuning DeBERTaV3-base (185M parameters) on GLUE benchmarks (MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, STS-B). (3) Fine-tuning Mistral 7B model for the GSM8K task. For diffusion models, experiments were conducted using the Chilloutmix diffusion model for: (1) Multi-adapter fusion with custom animal and toy concepts, and multi-character generation with photos of computer scientists, utilizing T2I-Adapter with sketch/keypose conditions. (2) Parameter efficiency comparisons with custom vase, chair, and table concepts. Baselines for PEFT comparisons included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, and full fine-tuning. For multi-adapter fusion, baselines included Gradient Fusion, Orthogonal Adaptation, and FedAvg. Evaluation metrics included training loss and validation scores (e.g., GSM8K exact matching, GLUE accuracy), visual quality of generated images, and CLIP-based alignment scores (cosine similarity between generated/reference images and prompt texts). All experiments were performed using NVIDIA RTX A6000 GPUs, with hyperparameters generally following official implementations of baselines or tuned specifically for Spectral AdapterA.",
    "Limitations": "The primary limitation of the current work lies in the empirical choice of tuning only the top spectral space. Although its validity is theoretically verified under simple settings, a more comprehensive investigation into tuning different columns or subspaces of singular vector matrices is crucial for a deeper understanding of spectral information's role in fine-tuning. Additionally, fine-tuning the spectral representation of different specific components (e.g., only the attention layer) of large models warrants further dedicated study. Another practical constraint is the increasing time consumption of the Singular Value Decomposition (SVD) procedure as models scale larger, necessitating the exploration and integration of faster SVD methods.",
    "Future Research Directions": "Future research directions include: (1) Investigating the fine-tuning of spectral representation of specific components within large models, such as focusing solely on the attention layer. (2) Exploring dynamic combinations of spectral adaptation with other advanced PEFT methods, such as AdaLoRA, to potentially enhance tuning efficiency and performance further. (3) Conducting more in-depth studies on the effects of tuning different columns or subspaces of singular vector matrices beyond just the top spectral space, which is critical for a complete understanding of spectral information's role in fine-tuning. (4) Developing or integrating faster Singular Value Decomposition (SVD) methods to alleviate the increasing time consumption associated with larger model sizes and improve practicality.",
    "Experiment Code": "import math\n\nimport torch\nimport torch.nn as nn\nfrom diffusers.models.attention_processor import AttnProcessor\nfrom diffusers.utils.import_utils import is_xformers_available\nimport torch.nn.functional as F\nimport numpy as np\n\nif is_xformers_available():\n    import xformers\n    \n\ndef remove_edlora_unet_attention_forward(unet):\n    def change_forward(unet):  # omit proceesor in new diffusers\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and name == 'attn2':\n                layer.set_processor(AttnProcessor())\n            else:\n                change_forward(layer)\n    change_forward(unet)\n\n\nclass EDLoRA_Control_AttnProcessor:\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n    def __init__(self, cross_attention_idx, place_in_unet, controller, attention_op=None):\n        self.cross_attention_idx = cross_attention_idx\n        self.place_in_unet = place_in_unet\n        self.controller = controller\n        self.attention_op = attention_op\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            is_cross = False\n            encoder_hidden_states = hidden_states\n        else:\n            is_cross = True\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available() and not is_cross:\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            attention_probs = self.controller(attention_probs, is_cross, self.place_in_unet)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass EDLoRA_AttnProcessor:\n    def __init__(self, cross_attention_idx, attention_op=None):\n        self.attention_op = attention_op\n        self.cross_attention_idx = cross_attention_idx\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available():\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\ndef revise_edlora_unet_attention_forward(unet):\n    def change_forward(unet, count):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:\n                layer.set_processor(EDLoRA_AttnProcessor(count))\n                count += 1\n            else:\n                count = change_forward(layer, count)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0)\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx)\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx)\n    print(f'Number of attention layer registered {cross_attention_idx}')\n\n\ndef revise_edlora_unet_attention_controller_forward(unet, controller):\n    class DummyController:\n        def __call__(self, *args):\n            return args[0]\n\n        def __init__(self):\n            self.num_att_layers = 0\n\n    if controller is None:\n        controller = DummyController()\n\n    def change_forward(unet, count, place_in_unet):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:  # only register controller for cross-attention\n                layer.set_processor(EDLoRA_Control_AttnProcessor(count, place_in_unet, controller))\n                count += 1\n            else:\n                count = change_forward(layer, count, place_in_unet)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0, 'down')\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx, 'mid')\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx, 'up')\n    print(f'Number of attention layer registered {cross_attention_idx}')\n    controller.num_att_layers = cross_attention_idx\n\nclass SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        rank = 8\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r\n\n    def cayley(self, data: torch.Tensor) -> torch.Tensor:\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)\n        else:\n            raise Exception('')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv :\n            raise Exception('')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)",
    "Experiment Result": "The core method, \"Spectral AdapterR (Orthogonal Rotation)\", is implemented in the `SpectralLinearLayer_OFT` class found in `adapter_efficiency/mix_spectral/Mix-of-Show/mixofshow/models/edlora.py`. This class performs Singular Value Decomposition (SVD) on the `original_module.weight` to obtain U, S, and V matrices. It then applies orthogonal rotations to selected 'top-r' columns of the U and V matrices and scales corresponding singular values in S.\n\nKey experimental settings and mechanisms:\n-   **SVD Application**: For any given `original_module` (e.g., linear layers in a neural network), its weight matrix `W` undergoes SVD: `U, S, V = torch.svd(W)`. These U, S, V are stored as non-trainable parameters.\n-   **Orthogonal Rotation (Spectral AdapterR)**: Trainable matrices `self.spectral_A` and `self.spectral_B` are used to create orthogonal rotation matrices via the `cayley` function. These rotations are applied to specific blocks of columns (defined by `self.idx*self.rank:(self.idx+1)*self.rank`) within the U and V singular vector matrices. This directly corresponds to the `[U1RU U2]S[V1RV V2]^T` formulation.\n-   **Singular Value Scaling**: A trainable parameter `self.spectral_C` is used to scale the singular values in the `S` matrix for the same selected block of indices.\n-   **Initialization**: `self.spectral_A` and `self.spectral_B` are initialized to zero matrices (meaning `cayley(0)` results in an identity matrix, so no initial rotation is applied). `self.spectral_C` is initialized to ones, meaning no initial scaling is applied to singular values.\n-   **Recomposition**: The modified `pad_U`, `pad_S`, and `pad_V` are recomposed to form the updated weight matrix `pad_W = pad_U@pad_S.diag()@pad_V.T`.\n-   **Fine-tuning Focus**: The `idx` and `rank` parameters in `SpectralLinearLayer_OFT` indicate that fine-tuning is targeted at specific 'top-r' (where `r` is `self.rank`) columns of the singular vector matrices, allowing for localized adaptation.\n\n*Note*: The `Spectral AdapterA (Additive Tuning)` mechanism, which involves additive modifications `[U1 + AU U2]S[V1 + AV V2]^T` with `AU` and `AV` initialized to zero, is described in the method but not explicitly implemented by the `SpectralLinearLayer_OFT` directly. The provided `fedavg_gradient/fedavg_fusion.py` uses a FedAvg-like approach for merging concepts by averaging learned LoRA weights or using quasi-Newton optimization on derived weight updates, which differs from the described `(U0 + ∑λiUi)S0(V0 + ∑λiVi)^T` fusion of singular vectors."
}{
    "Title": "Multi-Head Adapter Routing for Cross-Task Generalization",
    "Main Contributions": "The paper introduces Multi-Head Routing (MHR) for parameter-efficient fine-tuning (PEFT) to achieve cross-task generalization, building upon the Polytropon (Poly) framework. MHR enhances the expressivity of adapter routing by partitioning adapter dimensions into multiple 'heads' and performing separate module combinations for each, outperforming Poly. It also proposes MHR-z, a highly parameter-efficient variant that fine-tunes only the routing function during adaptation. The research reveals that the success of routing-based PEFT methods like Poly and MHR primarily stems from better multi-task optimization during pre-training, leading to higher gradient alignment between tasks, rather than modular inductive biases facilitating recombination during adaptation. Furthermore, it demonstrates MHR-µ, which fine-tunes the average of pre-trained adapters, achieves competitive single-adapter performance and can be effectively used for zero-shot transfer with additional pre-training data fine-tuning.",
    "Methodology": "The core methodology extends Polytropon (Poly), which learns an inventory of LoRA adapters (modules) and a routing function (a task–module routing matrix Z) to select and combine modules for each task. MHR refines this by partitioning the adapter parameters (A, B matrices) into 'h' disjoint blocks (heads). For each head, a separate routing function selects and combines active modules, resulting in a third-order routing tensor Z. This allows for more granular and piecewise linear module combinations. MHR-z is a variant where, during few-shot adaptation, only the routing parameters Z are updated, keeping the adapter modules fixed. MHR-µ simplifies adaptation by discarding routing and averaging all pre-trained adapter modules into a single one before fine-tuning. For zero-shot transfer, MHR-µ is further fine-tuned on the multi-task pre-training data. Tasks are formulated as text-to-text problems, optimized with maximum-likelihood training and cross-entropy loss.",
    "Experimental Setup": "The methods were evaluated for few-shot task adaptation on two comprehensive benchmarks: the T0 task suite and the Super-Natural Instructions (SuperNI) dataset. Backbone models used include T5-XL, T0-3B, and T0-11B. T0 consisted of 313 unique tasks (data subset–template pairs), where the mean of the best validation accuracy across 3 seeds was reported. SuperNI involved 20 randomly selected test tasks (out of 119) with 100 training, 100 validation, and 100 test examples, using Rouge-L averaged across 3 seeds. Baselines included LoRA, (IA)3, Poly, Poly-z, AdapterSoup, T-Few (reproduction of Liu et al. [2022]), and a random routing baseline (Random-µ). All experiments were conducted on a single NVIDIA A100 GPU.",
    "Limitations": "The paper does not explicitly list limitations of MHR. However, it implicitly points out that for zero-shot learning, MHR-µ does not perform well without additional fine-tuning on the multi-task pre-training data (k=0 case), indicating a potential mismatch between adapters learned via task-specific routing and a uniform routing strategy when no adaptation occurs. The computational cost for the SuperNI dataset was limited by evaluating only 20 out of 119 test tasks, which is a practical constraint on the experiments. The Broader Impact section also highlights general ethical concerns regarding potential biases in underlying large language models and training data, which are not specific to the proposed method but are relevant for its application.",
    "Future Research Directions": "The authors suggest that multi-head routing holds promise for improving other modular methods, specifically mentioning Mixtures of Experts (MoEs). Additionally, the finding that averaging multi-task pre-trained adapters can serve as an effective initialization for single-adapter few-shot learning (MHR-µ's success) could inspire future work in meta-learning and weight-averaging approaches.",
    "Experiment Code": "mttl/models/modifiers/lora.py: @dataclassclass SkilledLoRAConfig(LoRAConfig):n_skills: int = 1n_splits: int = 1mttl/models/modifiers/lora.py: @Modifier.register(\"skilled_lora\", config_cls=SkilledLoRAConfig)class SkilledLoRA(LoRA):def __init__(self, config: SkilledLoRAConfig, layer: nn.Module, **kwargs):self.n_splits = config.n_splitsself.n_skills = config.n_skillssuper().__init__(config, layer)# ...@classmethoddef parallel_linear_weighted_forward(cls,input: torch.Tensor,skilled_loras: List[\"SkilledLoRAView\"],weights: torch.Tensor,dim_names: List[str],merge_after: bool = False,):# ...if merge_after:partial_out = torch.einsum(\"bld,beqdr->bleqr\", input_lora, skilled_loras_a)adapter_out = torch.einsum(\"bleqr,berqd,blqe->blqd\", partial_out, skilled_loras_b, weights)adapter_out = adapter_out.flatten(2, 3)else:A = torch.einsum(\"blqe,beqdr->blqdr\", (weights, skilled_loras_a))B = torch.einsum(\"blqe,berqd->blrqd\", (weights, skilled_loras_b))batch_size, sequence_length, rank, n_splits, d_split = B.shapeA, B = A.flatten(2, 3), B.flatten(3, 4)partial_out = torch.einsum(\"bld,bldr->blr\", (input_lora, A))adapter_out = torch.einsum(\"blr,blrd->bld\", (partial_out, B))# ...mttl/models/containers/selectors/poly_selector.py: @dataclassclass PolySelectorConfig(SelectorConfig):n_splits: int = 1task_names: List[str] = Noneslow_unknown_tasks: bool = Falsemttl/models/containers/selectors/poly_selector.py: def _get_weights(self, task_names: List[str] = None) -> torch.Tensor:# ...module_logits = torch.sigmoid(self.module_logits[task_ids])module_logits = module_logits.view(module_logits.size(0), self.config.n_splits, self.n_experts)module_weights = module_logits / (module_logits.sum(dim=-1, keepdim=True) + EPS)return module_weightsmttl/models/lightning/expert_module.py: class MoEModule(LightningEfficientCheckpoint, LightningTrainingMixin):# ...def training_step(self, batch, _):output, context = self.forward(**batch, return_context=True)loss = output.loss# ...routing_gates = context[\"routing_gates\"]if routing_gates:# ... Mutual Information Loss calculation ...mi_loss = -entropy_of_avg + entropy_of_route# ...if self.hparams.moe_ent_reg > 0.0:total_loss += self.hparams.moe_ent_reg * mi_losselif self.hparams.moe_ent_free_bits > 0.0:normalized_entropy = entropy_of_route / math.log(self.moe_num_experts)total_loss += ((1.0 - normalized_entropy) >= self.hparams.moe_ent_free_bits) * -entropy_of_routemttl/arguments.py: @dataclassclass FinetuneConfig(MultiExpertConfig):finetune_type: str = None # [\"F\", \"A\", \"Z\", \"MuZ\", \"Poly\", \"PolyRand\"]# ...finetune_regime: str = None # polylib_full, lib_mu, polylib_selector# ...projects/modular_llm/finetune_experts.py: @register_finetune_func(\"polylib_selector\")def finetune_polylib_sel(args: FinetuneConfig, dm):args.trainable_param_names = \"|.*module_logits.*|.*selector.*\"assert args.router_selector is not Nonemodule = MoEModule(**vars(args), device_map=\"auto\")for n, p in module.named_parameters():if \"selector\" in n:assert p.requires_gradmodule.to(\"cuda\")return train_module(args, module, dm)mttl/models/library/library_transforms.py: @dataclassclass WeightedLinearMergeConfig(LibraryTransformConfig):weights: dict = Nonemttl/models/library/library_transforms.py: @LibraryTransform.register(\"weighted_linear_merge\", WeightedLinearMergeConfig)class WeightedLinearMerge(LibraryTransform):@torch.no_grad()def transform(self, library) -> Expert:# ... averages expert weights ...for k, v in base_expert.expert_weights.items():base_expert.expert_weights[k] /= len(experts)# ...return base_expertprojects/modular_llm/finetune_experts.py: @register_finetune_func(\"lib_mu\")def finetune_lib_mu(args: FinetuneConfig, dm):args.router_selector = Nonemean_expert: Expert = create_mean_expert(args)if args.finetune_task_name:mean_expert.name = args.finetune_task_namemodule = MultiExpertModel(**vars(args)).to(\"cuda\")module.add_expert_instance(mean_expert, is_default=True)return (train_module(args, module, dm),)projects/modular_llm/finetune_experts.py: def create_mean_expert(args: FinetuneConfig, library: ExpertLibrary = None) -> Expert:if library is None:library = args.library_idreturn WeightedLinearMerge(WeightedLinearMergeConfig()).transform(library)",
    "Experiment Result": "MHR (Multi-Head Routing) Core Method:- **Adapter Parameter Partitioning**: Adapter parameters (A, B matrices) are partitioned into 'h' disjoint blocks (heads) through `SkilledLoRAConfig` (defining `n_skills` and `n_splits`) and implemented by the `SkilledLoRA` class, which customizes layer creation and skill management.- **Third-Order Routing Tensor (Z)**: A third-order routing tensor is implicitly formed. The `PolySelector` with its `n_splits` configuration (via `PolySelectorConfig`) directly controls this, shaping the `module_logits` (routing weights) for each task, split, and expert. The `SkilledLoRA.parallel_linear_weighted_forward` method uses these weights and dimension names (`dim_names`) to combine modules.- **Granular Module Combinations**: The `SkilledLoRA.parallel_linear_weighted_forward` method, using `torch.einsum`, allows for weighted combinations of adapter parameters across different splits (heads) and experts, enabling more granular and piecewise linear module combinations based on the routing `weights`.- **Optimization with Cross-Entropy Loss and Regularization**: Tasks are formulated as text-to-text problems. The optimization uses maximum-likelihood training with cross-entropy loss (standard in `LightningTrainingMixin`). The `MoEModule.training_step` specifically incorporates mutual information loss (`mi_loss`), calculated from `routing_gates` (derived from the routing tensor Z), and allows for entropy regularization (`moe_ent_reg` or `moe_ent_free_bits`) to control the routing distribution.MHR-z Variant (Few-Shot Adaptation - Routing Only):- **Targeted Parameter Update**: During few-shot adaptation, only the routing parameters `Z` are updated. This is configured by setting `args.trainable_param_names` to target only `module_logits` or `selector` parameters. Adapter modules are kept fixed (not trained).- **Fine-tuning Regime**: Implemented by the `finetune_polylib_selector` function in `projects/modular_llm/finetune_experts.py`. This function sets the `trainable_param_names` accordingly and utilizes `MoEModule` for training.MHR-µ Variant (Simplified Adaptation - Averaging Modules):- **Module Averaging**: Adaptation is simplified by discarding explicit routing and averaging all pre-trained adapter modules into a single one before fine-tuning. This averaging is performed using `LibraryTransform` implementations like `WeightedLinearMerge`, `TiesMerge`, or `WudiMerge`.- **Module Averaging Configuration**: `WeightedLinearMergeConfig` (and similar for `TiesMerge` and `WudiMerge`) defines how modules are averaged, including optional `weights` for non-uniform averaging. The `WeightedLinearMerge.transform` method exemplifies this averaging process.- **Fine-tuning Regime**: Implemented by the `finetune_lib_mu` function in `projects/modular_llm/finetune_experts.py`. This function first calls `create_mean_expert` to generate the averaged adapter and then fine-tunes a `MultiExpertModel` with this single, averaged expert."
}{
    "Title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
    "Main Contributions": "The paper addresses challenges in Multi-Task Learning (MTL) for NLP, such as overfitting to low-resource tasks, catastrophic forgetting, negative task transfer, parameter inefficiency, and knowledge loss during fine-tuning. It proposes Conditionally Adaptive Multi-Task Learning (CA-MTL), a novel Transformer-based Adapter with a new conditional attention mechanism and task-conditioned modules that facilitate weight sharing. The approach fixes half of the pretrained model weights to mitigate forgetting and introduces an uncertainty-based multi-task data sampling strategy to balance data imbalance across tasks. CA-MTL outperforms single-task fine-tuning and other adapter methods on GLUE, achieving competitive performance across 26 NLP tasks and state-of-the-art results on several test and development sets. The work is the first to explore latent task representations to modularize and adapt pretrained architectures and to examine uncertainty sampling for large-scale multi-task learning in NLP.",
    "Methodology": "The methodology centers on a Task Conditioned Transformer architecture that adapts and modulates pretrained weights using task embeddings (zi). This is achieved through: 1. Conditional Weight Transformations: Learned functions (γi, βi) transform weights based on task embedding zi. 2. Conditional Attention: A block-diagonal conditional attention mechanism M(zi) is introduced to account for task-specific biases. 3. Conditional Alignment: A task-conditioned alignment layer ˆR(zi) is added between the input embedding and the first Transformer layer, generating a single alignment matrix as a function of zi. 4. Conditional Layer Normalization (CLN): Extends Conditional Batch Normalization to Layer Normalization, modulating output based on task's latent representation. 5. Conditional Bottleneck: A task-conditioned two-layer feed-forward bottleneck layer facilitates weight sharing and task-specific information flow. The CA-MTL approach freezes the bottom half of Transformer layers to preserve pretraining knowledge. For task balancing, Multi-Task Uncertainty Sampling (MT-Uncertainty Sampling) is used, an active learning-inspired strategy that employs Shannon Entropy to select training examples, prioritizing tasks with higher uncertainty and dynamically managing task difficulty to avoid catastrophic forgetting.",
    "Experimental Setup": "The method was evaluated on 26 NLP tasks, including the GLUE benchmark (9 tasks), Super-GLUE (8 tasks), MRQA (6 tasks), and WNUT2017 (Named Entity Recognition). Domain adaptation experiments were conducted on SciTail and SNLI. Baselines included single-task fine-tuning (ST), various MTL models (e.g., BERTBASE MTL with random sampling), and other BERT-based adapter networks (Adapters-256, PALs+Anneal Sampling). Validation involved ablation studies of CA-MTL components, comparisons of MT-Uncertainty Sampling with other task sampling strategies (Counterfactual, Task size, Random), analysis of task covariance similarity, and performance evaluation under varying numbers of tasks (scaling task count). Metrics included average GLUE scores, F1, Spearman's correlation, Matthew's correlation, and accuracy. The implementation used HuggingFace, Adam optimizer (2e-5 LR with warm-up), a batch size of 32, a seed of 12, 5 or 8 epochs, and a sequence length of 128 or 256. Experiments were run on NVIDIA P100/V100 GPUs, without parameter search or model ensembles, and with half of the Transformer layers typically frozen.",
    "Limitations": "Keeping 50% of the Transformer layers frozen, while intended for knowledge preservation, was observed to slightly lower performance on 9 GLUE tasks, although it's hypothesized to be beneficial for larger task counts (24+). The Evolutionary Data Measures (EDM) used to estimate task difficulty were noted to lack precision for regression-like values, being more effective for assigning a class of difficulty. For the Named Entity Recognition (NER) task, CA-MTL, while significantly outperforming the MTL baseline, had not yet overfit and could have potentially closed the performance gap with ST baselines with more training cycles. During zero-shot transfer, the method was found to be sensitive to the initialization of the new task's embedding, requiring selection of the 'best' pretrained embedding. Unlike some baselines (e.g., MT-DNNSMART), CA-MTL does not incorporate context from semantic similarity models.",
    "Future Research Directions": "Future work will involve extending the ideas of task-conditioned adaptive learning. Further investigation into the impact of freezing more original pretrained weights when scaling to a larger number of tasks (e.g., 24 or more) is also an objective. Additionally, exploring the precision of task difficulty measures and longer training cycles for specific tasks where CA-MTL has not yet reached its full potential compared to single-task baselines could be beneficial.",
    "Experiment Code": "import torchimport numbersimport torch.nn as nnimport torch.nn.functional as Fclass FiLM(nn.Module):    \"\"\" Feature-wise Linear Modulation (FiLM) layer\"\"\"    def __init__(self, input_size, output_size, num_film_layers=1, layer_norm=False):        \"\"\"        :param input_size: feature size of x_cond        :param output_size: feature size of x_to_film        :param layer_norm: true or false        \"\"\"        super(FiLM, self).__init__()        self.input_size = input_size        self.output_size = output_size        self.num_film_layers = num_film_layers        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None        film_output_size = self.output_size * num_film_layers * 2        self.gb_weights = nn.Linear(self.input_size, film_output_size)        self.gb_weights.bias.data.fill_(0)    def forward(self, x_cond, x_to_film):        gb = self.gb_weights(x_cond).unsqueeze(1)        gamma, beta = torch.chunk(gb, 2, dim=-1)        out = (1 + gamma) * x_to_film + beta        if self.layer_norm is not None:            out = self.layer_norm(out)        return outclass CBDA(nn.Module):    \"\"\" Conditional Block Diagonal Attention (CBDA) layer\"\"\"    def __init__(self, input_size, output_size, blocks=1, num_film_layers=1, layer_norm=False):        \"\"\"        :param input_size: feature size of x_cond        :param output_size: feature size of x_to_film        :param layer_norm: true or false        \"\"\"        super(CBDA, self).__init__()        self.input_size = input_size        self.output_size = output_size        self.num_film_layers = num_film_layers        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None        self.blocks = blocks        film_output_size = self.output_size * num_film_layers * 2        self.gb_weights = nn.Linear(self.input_size, film_output_size)        self.gb_weights.bias.data.fill_(0)    def forward(self, x_cond, x_to_film):        gb = self.gb_weights(x_cond).unsqueeze(1)        gamma, beta = torch.chunk(gb, 2, dim=-1)        out = (1 + gamma) * x_to_film + beta        if self.layer_norm is not None:            out = self.layer_norm(out)        out = [torch.block_diag(*list(out_b.chunk(self.blocks, 0))) for out_b in out]        out = torch.stack(out)        return out[:, :, :out.size(1)]class ConditionalLayerNorm(nn.Module):    r\"\"\"Applies Conditional Layer Normalization over a mini-batch of inputs.    .. math::        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma(z) + \\beta(z)    The mean and standard-deviation are calculated separately over the last    certain number dimensions which have to be of the shape specified by    :attr:`normalized_shape`.    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.    .. note::        Unlike Batch Normalization and Instance Normalization, which applies        scalar scale and bias for each entire channel/plane with the        :attr:`affine`, Layer Normalization applies per-element scale and        bias with :attr:`elementwise_affine`.    This layer uses statistics computed from input data in both training and    evaluation modes. The affine transformation is molulated by a conditional tensor.    In our case, we use task embeddings z.    Args:        normalized_shape (int or list or torch.Size): input shape from an expected input            of size            .. math::                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]            If a single integer is used, it is treated as a singleton list, and this module will            normalize over the last dimension which is expected to be of that specific size.        eps: a value added to the denominator for numerical stability. Default: 1e-5        elementwise_affine: a boolean value that when set to ``True``, this module            has learnable per-element affine parameters initialized to ones (for weights)            and zeros (for biases). Default: ``True``.    Shape:        - Input: :math:`(N, *)`        - Output: :math:`(N, *)` (same shape as input)    Examples::        >>> input_ = torch.randn(20, 5, 10, 10)        >>> condition = torch.randn(20, 10)        >>> # With Learnable Parameters        >>> m = ConditionalLayerNorm([10, 10])        >>> # Normalize over last dimension of size 10        >>> m = nn.LayerNorm(10)        >>> # Activating the module        >>> output = m(input_, condition)    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450    .. _`Conditional Layer Normalization`: https://arxiv.org/    \"\"\"    __constants__ = ['normalized_shape', 'condition_size', 'weight', 'bias', 'eps']    def __init__(self, normalized_shape, condition_size, eps=1e-5):        super(ConditionalLayerNorm, self).__init__()        if isinstance(normalized_shape, numbers.Integral):            normalized_shape = (normalized_shape,)        self.normalized_shape = tuple(normalized_shape)        self.condition_size = condition_size        self.eps = eps        self.weight = nn.Parameter(torch.Tensor(*normalized_shape))        self.ln_weight_modulation = FiLM(condition_size, sum(normalized_shape))        self.bias = nn.Parameter(torch.Tensor(*normalized_shape))        self.reset_parameters()    def reset_parameters(self):        nn.init.ones_(self.weight)        nn.init.zeros_(self.bias)    def forward(self, input_, condition, task_id):        unique_task_ids = torch.unique(task_id)        cln_output = torch.zeros_like(input_)        for unique_task_id in unique_task_ids:            task_id_filter = task_id == unique_task_id            task_emb = condition[task_id_filter][0].unsqueeze(0)            weight = self.ln_weight_modulation(task_emb, self.weight).view(-1)            cln_output[task_id_filter] = F.layer_norm(input_[task_id_filter], self.normalized_shape, weight, self.bias, self.eps)        return cln_output    def extra_repr(self):        return '{normalized_shape}, {condition_size}, eps={eps}'.format(**self.__dict__)class ConditionalBottleNeck(nn.Module):    \"\"\"Down projection and up projection with FiLM layers within Transformer layer.\"\"\"    def __init__(self, config):        super(ConditionalBottleNeck, self).__init__()        self.emb_transf = nn.Linear(config.hidden_size, config.hidden_size)        self.hidden_modulation = FiLM(config.hidden_size, config.hidden_size)        self.down_proj_layer = nn.Linear(config.hidden_size, config.hidden_size//3)        self.up_proj_layer = nn.Linear(config.hidden_size//3, config.hidden_size)    def forward(self, x_cond, hidden_states):        x_cond = self.emb_transf(x_cond)        hidden_states = self.hidden_modulation(x_cond=x_cond, x_to_film=hidden_states)        hidden_states = self.down_proj_layer(hidden_states)        hidden_states = self.up_proj_layer(hidden_states)        return hidden_states",
    "Experiment Result": "The methodology centers on a Task Conditioned Transformer architecture that adapts and modulates pretrained weights using task embeddings (zi). This is achieved through: 1. Conditional Weight Transformations: Learned functions (γi, βi) transform weights based on task embedding zi, implemented via FiLM layers. 2. Conditional Attention: A block-diagonal conditional attention mechanism M(zi) is introduced to account for task-specific biases, implemented via the CBDA layer that modulates a `random_weight_matrix` and adds it to the original attention scores. 3. Conditional Alignment: A task-conditioned alignment layer `R(zi)` is added between the input embedding and the first Transformer layer, implemented by a FiLM layer applied to the `embedding_output`. 4. Conditional Layer Normalization (CLN): Extends Conditional Batch Normalization to Layer Normalization, modulating output based on the task's latent representation. 5. Conditional Bottleneck: A task-conditioned two-layer feed-forward bottleneck layer facilitates weight sharing and task-specific information flow.For the `CA-MTL-base` model, layers 0-5 are standard BERT layers with Conditional Block Diagonal Attention (CBDA) in their attention mechanism, but no Conditional Layer Normalization (CLN) in their self-output or feed-forward output. Layers 6-10 utilize CBDA in attention and apply CLN in both their self-attention output (`MyBertSelfOutput9`) and feed-forward output (`MyBertOutput9`). The final layer (layer 11) is a `MyBertAdapterLayer9` which integrates an attention mechanism with CBDA and CLN, followed by a feed-forward network with CLN, and finally incorporates a `BertAdapter9` (Conditional Bottleneck) with a skip connection and an additional CLN.For the `CA-MTL-large` model, layers 0-11 are standard BERT layers with CBDA in their attention but no CLN in their self-output or feed-forward output. Layers 12-23 utilize CBDA in attention and apply CLN in both their self-attention output (`MyBertSelfOutput10`) and feed-forward output (`MyBertOutput10`). Additionally, all layers (0-23) in the `CA-MTL-large` model's encoder are equipped with a `ConditionalBottleNeck` which is applied with a skip connection (`hidden_states + hidden_film`) in a progressive manner throughout the encoder.The CA-MTL approach includes freezing mechanism: users can specify a range of encoder layers (`<start_layer>-<end_layer>`) to freeze. However, specific modules within these layers (e.g., `random_weight_matrix` for CBDA, `film.gb_weights` for FiLM, `ln_weight_modulation.gb_weights` for CLN, and `adapter` for Conditional Bottleneck) are explicitly unfrozen to allow for task-specific adaptation.For task balancing, Multi-Task Uncertainty Sampling (MT-Uncertainty Sampling) is used. This active learning-inspired strategy employs Shannon Entropy, calculated from the decoder's softmax probabilities, to select training examples. The batch entropy is normalized by the mean entropy across tasks (`test_batch_entropy_mean / max_mean_batch_entropy`) to prioritize tasks with higher uncertainty and dynamically manage task difficulty. Training arguments `use_mt_uncertainty` (boolean) enables this sampling method, `uniform_mt_sampling` (boolean) allows sampling each task an equal amount, and `percent_of_max_data_size` (float) controls the proportion of data sampled per task based on the largest dataset. The learning rate schedule uses `warmup_proportion` (float) for linear warmup."
}{
    "Title": "Sparse High Rank Adapters",
    "Main Contributions": "The paper proposes Sparse High Rank Adapters (SHiRA) to address limitations of Low Rank Adaptation (LoRA), specifically inference overhead, slow adapter switching, and concept loss during multi-adapter fusion. SHiRA enables rapid switching and reduces concept loss by directly tuning only 1-2% of the base model weights, resulting in a highly sparse adapter that can be switched in fused mode without inference overhead. It empirically and theoretically demonstrates that SHiRA significantly outperforms LoRA on LVMs and LLMs, and introduces a PEFT-based implementation that trains at similar speeds to LoRA while consuming up to 16% less peak GPU memory. SHiRA also achieves 5x-16x faster adapter loading on a CPU compared to LoRA fusion.",
    "Methodology": "SHiRA trains a small percentage (1-2%) of existing weights in a pretrained model by masking gradients during backpropagation, rather than adding new low-rank weights like LoRA. Various sparse mask strategies are employed: SHiRA-Struct (structured rows/columns + diagonal), SHiRA-Rand (random selection), SHiRA-WM (top-K weight magnitudes), SHiRA-Grad (top-K gradient magnitudes), and SHiRA-SNIP (combining weight and gradient magnitudes). For rapid switching, SHiRA stores sparse weights and their indices, overwriting base model weights using a `scatter_op` at inference time, avoiding the need for a separate fusion stage. For multi-adapter fusion, the high sparsity of SHiRA is shown to reduce interference between adapters, quantified by Adapter Weight Orthogonality Magnitude (AWOM) and Adapter Weight Orthogonality Ratio (AWOR). SHiRA's training is made memory- and latency-efficient through a backward hook-based gradient masking or a PEFT-based implementation that retains only sparse parameters in the optimizer.",
    "Experimental Setup": "Experiments were conducted on Large Vision Models (LVMs) and Large Language Models (LLMs). LVMs include Stable Diffusion-v1.5 (RealisticVision-v3 checkpoint) and SDXL for style transfer and DreamBooth tasks, using custom Bluefire and Paintings datasets (public domain images) and DreamBooth datasets. LLMs include LLaMA-7B and LLaMA2-7B, evaluated on commonsense reasoning benchmarks like HellaSwag, PIQA, SIQA, BoolQ, Arc-easy, Arc-challenge, OpenBookQA, and Winogrande, using a combined 170K sample dataset. Additional experiments included Vision Transformer (ViT-Base) for image classification on CIFAR-10, CIFAR-100, Food101, and DTD, and DeBERTa-V3-base for GLUE tasks (QNLI, COLA, SST2, MRPC). Evaluation metrics include Human Preference Score-V2 (HPSv2) for vision, accuracy for language tasks, and computational metrics like peak GPU memory consumption, training steps per second, and CPU adapter loading times. Training was performed on a single NVIDIA A100 GPU.",
    "Limitations": "The paper identifies several limitations: full mobile deployment may require hardware-software co-design techniques (e.g., lookup-table based approaches) for optimal edge device implementation. The process of building optimal sparse masks (i.e., selecting which specific parameters to train for a given task) warrants further investigation. SHiRA-Struct, while effective for vision tasks, performed poorly on complex language tasks (commonsense reasoning) likely due to its rank 1 + diagonal nature, indicating potential limitations in expressive power for certain domains. The theoretical orthogonality analysis is confined to adapter weights and does not extend to the orthogonality of subspaces. Finally, similar to other parameter-efficient finetuning techniques, SHiRA carries the potential risk of digital forgery due to finetuning generative models on smaller datasets, which could lead to overfitting.",
    "Future Research Directions": "Future work could explore hardware-software co-design techniques, such as lookup-table (LUT) based approaches, to optimize SHiRA's implementation for edge devices. Further investigation into building optimal sparse masks is needed to identify which specific parameters are most effective to train for various tasks. The authors also suggest a subspace analysis of SHiRA, extending beyond the current focus on adapter weight orthogonality. Additionally, balancing single adapter performance (expressive power) with multi-adapter fusion capabilities remains an open area for research.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
    "Main Contributions": "The paper demonstrates that prefix-tuning a pretrained transformer can universally approximate sequence-to-sequence functions, even with much smaller pretrained models than previously thought. It shows that a single attention head is sufficient to approximate any smooth continuous function on a hypersphere. Furthermore, it proves that any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length and independent of approximation precision. The work also offers Jackson-type bounds on the length of the prefix required to achieve a desired approximation precision.",
    "Methodology": "The methodology involves simplifying the classical attention head into 'core attention' and 'split attention' heads, demonstrating their ability to approximate continuous functions on hyperspheres. This is achieved using von Mises-Fisher kernels for approximation and providing Jackson-type bounds on the necessary prefix length. The paper then shows that a split attention head can be exactly represented by a classical attention head by embedding inputs and prefixes into different subspaces with specifically constructed attention and value matrices. For approximating general sequence-to-sequence functions, the work leverages a variant of the Kolmogorov–Arnold representation theorem (Schmidt-Hieber, 2021) to construct a transformer architecture with T+2 attention layers, complemented by MLPs and positional encodings, to handle complex function decompositions.",
    "Experimental Setup": "Not mentioned",
    "Limitations": "The theoretical results rely on the assumption of a highly specific pretrained model structure, which is unlikely to be found in transformers pretrained on real-world data. Consequently, the class of functions a real-world pretrained transformer can approximate with prefix-tuning remains an open question. Additionally, the demonstrated asymptotic efficiency of prefix-tuning shows the required prefix length growing as O(ϵ−10−14m−4m2), which is asymptotically worse than training a transformer from scratch, suggesting it might be less efficient for high precision. The constructions for general sequence-to-sequence functions also require very specific parameter values that are improbable in naturally pretrained models.",
    "Future Research Directions": "Future research should focus on investigating whether tighter Jackson bounds exist for prefix-tuning, as well as determining if prefix-tuning and prompting inherently require more trainable parameters than training a transformer for the same approximation accuracy. A crucial direction is to determine the class of functions that a real-world pretrained transformer can approximate through prefix-tuning, requiring the derivation of inverse (Bernstein-type) bounds.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "HyperTuning:  Toward Adapting Large Language Models without Back-propagation",
    "Main Contributions": "The paper introduces HyperTuning, a novel approach to large language model (LLM) adaptation that avoids back-propagation during adaptation by using a hypermodel to generate task-specific parameters. It proposes HyperT5, a T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen T5 model from few-shot examples. The key contributions include a two-stage training procedure (hyperpretraining and multi-task fine-tuning) for hypermodels, demonstrating its effectiveness in generating parameters for unseen tasks across diverse datasets (P3, MetaICL, Super-NaturalInstructions), and showing that hypermodel-generated parameters can serve as improved initializations for further parameter-efficient fine-tuning, leading to faster convergence and better performance.",
    "Methodology": "The core methodology involves training a hypermodel (HyperT5) to generate parameter-efficient fine-tuning (PEFT) parameters (soft prefixes or LoRA parameters) for a frozen downstream LLM in a single forward pass, based on few-shot examples or task instructions. HyperT5 is built on a T5 backbone, with its encoder processing few-shot inputs and its decoder generating representations that are fed to MLPs to produce the PEFT parameters. Training proceeds in two stages: first, hyperpretraining using a Context-Augmented Conditional Language Modeling (CACLM) objective to teach the hypermodel to generate parameters from contextual information; second, multi-task fine-tuning (MTF) of the hypermodel on a large number of diverse language tasks, where the hypermodel learns to generalize by taking few-shot pairs as input to generate task-specific PEFT parameters.",
    "Experimental Setup": "The research used LM-adapted T5 models (T5-Large and T5-XL) as the frozen downstream model. Experiments were conducted on three multi-task datasets: P3 (Public Pool of Prompts) for zero-shot instruction-following, MetaICL for few-shot learning, and Super-NaturalInstructions (S-NI) for generalization via declarative instructions. Baselines included T5-MTF (multi-task fine-tuning, similar to T0), T5-MTF-Few-shot (MTF with few-shot inputs concatenated), and parameter-efficient fine-tuning methods like prefix tuning and LoRA. Hyperpretraining was performed for 100K steps on the C4 dataset. Multi-task fine-tuning for HyperT5 and baselines ran for 10,000 steps with a batch size of 256, utilizing up to 16 few-shot examples. The Adam optimizer with a learning rate of 5e-5 and linear decay schedule was used, with ZeRO for memory optimization. Evaluation was based on accuracy for P3, ROUGE or Macro-F1 for MetaICL, and ROUGE-L for S-NI on held-out tasks. The effectiveness of hypermodel-generated parameters as initializations for further PEFT was also evaluated on P3 held-out tasks.",
    "Limitations": "The current hypermodel formulation is constrained to taking a small number of examples as input, which limits its performance compared to full parameter-efficient fine-tuning or full fine-tuning approaches. HyperT5 generally underperforms T5 models explicitly trained for few-shot in-context learning with full attention across examples (like T5-MTF-Fewshot), as HyperT5 encodes examples into PEFT parameters independently of the target input, while full self-attention allows for richer interaction. The experiments were conducted with minimal hyperparameter tuning, making the results primarily a proof-of-concept. Optimal hyperpretraining duration varies for different PEFT methods, as observed with HyperT5-LoRA's performance declining after 50k steps. Additionally, HyperT5-Preﬁx consistently outperformed HyperT5-LoRA, suggesting potential differences in learning complexity for generating different PEFT methods.",
    "Future Research Directions": "Future research directions include exploring whether hypermodels can be trained to predict gradients or generate parameter updates, thereby enabling them to work with large training sets beyond just few-shot examples. The authors also envision eventually replacing gradient-descent-based fine-tuning pipelines with hypertuning for various applications. Further investigation into hyperpretraining and multi-task fine-tuning hyperparameters, as well as alternative hypermodel architectures, is suggested to improve results and address current limitations. Another potential area is training a hypermodel against an already multi-task fine-tuned model to achieve better performance than zero-shot inference alone.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Prefix Conditioning Unifies Language and Label Supervision",
    "Main Contributions": "The paper introduces Prefix Conditioning, a novel pre-training strategy that unifies image-classification and image-caption datasets to leverage their complementary benefits while addressing dataset biases. It is the first mechanism to use prefixes to condition the source of the dataset during vision-language contrastive pre-training. This simple approach improves zero-shot recognition performance by more than 6% on average in experiments on ImageNet21K and CC12M. The work also demonstrates that prefix conditioning enables the language encoder to adapt its feature extraction strategy, leading to improved robustness against image-level distribution shifts.",
    "Methodology": "The core methodology is Prefix Conditioning, where dataset-specific prefix tokens are prepended to the text input token sequence during pre-training. These prefixes inform the language encoder of the input data type (e.g., classification vs. caption), helping to disentangle dataset biases from visual concepts. The approach is generic and can be integrated into existing Vision-Language (VL) pre-training objectives like CLIP or UniCL. The model is trained using a contrastive learning objective (e.g., symmetric multi-class N-pair loss). Debiased data sampling, where mini-batches are constructed from a single data source, is primarily used, though equal sampling is also explored. During inference for zero-shot classification, a caption-style prefix is added to class prompts to leverage the wider domain coverage learned from image-caption data, which empirically outperforms a prompt-style prefix for novel data.",
    "Experimental Setup": "Experiments were conducted using CC3M and CC12M as image-caption datasets, and ImageNet21K and ImageNet1K as image-classification datasets. Baselines include CLIP and UniCL. A Swin-Tiny transformer was used as the vision encoder, and the language encoder was the same as CLIP. Training involved 80 prompt templates, with one randomly sampled during training and averaged for validation. Models were optimized with AdamW (learning rate 0.001, weight decay 0.1, batch size 1024) for 15 or 50 epochs depending on the dataset size, using a cosine learning rate schedule with 10,000 warm-up iterations. Evaluation included supervised and zero-shot image classification on ImageNet1K and 11 standard zero-shot benchmarks (e.g., CIFAR10/100, Oxford-IIIT Pets, Food-101, EuroSAT, Resisc45). Linear-probe evaluation, robustness to image-level domain shifts (ImageNet-V2, ImageNet-R, ImageNet-S), class name shift using WordNet synonyms, and image-caption retrieval on CC3M and COCO validation sets were also performed. All models were trained on 32 Nvidia Tesla V100 GPUs.",
    "Limitations": "The scope of the research is limited to unifying image-caption and image-label supervision, suggesting other forms of supervision are not explored. Classification datasets are acknowledged to be biased towards specific image domains and have limited vocabulary, which prefix conditioning aims to mitigate but does not eliminate. While captioning datasets are more open-domain, they can also have data biases. The choice of data sampling method (debiased vs. equal) did not significantly affect performance. Furthermore, while caption-style prefixes generally improve zero-shot recognition, prompt-style prefixes perform better on the original image classification dataset, indicating a potential trade-off or specialization.",
    "Future Research Directions": "Future work could involve incorporating other forms of supervision beyond image-caption and image-label, such as object detection or semantic segmentation. Another promising direction is to combine prefix conditioning with external knowledge augmentation techniques, similar to K-Lite, to further enrich the learned representations.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
    "Main Contributions": "The paper investigates the stability-efficiency dilemma in pre-training large-scale autoregressive language models (GPT-style), where increasing batch sizes and learning rates for efficiency often leads to training instability. It identifies a strong correlation between training instability and extreme gradient variance values, particularly linked to long sequence lengths at the beginning of training. Based on this, the authors propose a novel Sequence Length Warmup (SLW) method, which starts training with short sequences and gradually increases their length. This method is shown to enable stable and efficient training, allowing for 8x larger batch size and 4x larger learning rate for GPT-2 models, and 8x larger batch size and 40x larger learning rate for GPT-3 (125M) while retaining 99% accuracy. It reduces required training tokens by up to 2.2x and wall-clock time by up to 3.7x for GPT-2, and achieves 10x data saving and 17x time saving for GPT-3. A lightweight hyperparameter tuning strategy for SLW is also presented, and the implementation is open-sourced in DeepSpeed.",
    "Methodology": "The study began with an in-depth analysis of GPT-2 pre-training tasks to understand the stability-efficiency dilemma. It quantitatively measured training instability using a 'loss ratio' metric (current loss / minimum previous loss) and correlated it with Adam's gradient variance state (l1 norm and max element of sqrt(vt)). The proposed Sequence Length Warmup (SLW) method involves an efficient truncation-based implementation where the dataloader initially indexes full-length sequences, which are then truncated to a shorter length determined by a step-wise linear pacing function: seqlent = seqlens + (seqlene - seqlens) * min(t/T, 1). A lightweight tuning strategy for SLW's hyperparameters (starting sequence length 'seqlens' and duration 'T') is also described, which relies on monitoring validation perplexity fluctuations during the initial training steps rather than full convergence.",
    "Experimental Setup": "Experiments were conducted on GPT-2 (117M and 1.5B parameters) and GPT-3 (125M and 1.3B parameters) models. The hardware used consisted of 128 NVIDIA V100 GPUs across 16 nodes, connected by NVLink 2.0 within nodes and 100 Gigabit InfiniBand EDR between nodes. For GPT-2, training data included Wikipedia, CC-Stories, RealNews, and OpenWebtext. For GPT-3, the Pile public dataset was used, with additional CC-Stories and RealNews for the 1.3B model. Training employed mixed precision/FP16, Adam optimizer (β1=0.9, β2=0.999, ϵ=1e-8), 0.01 weight decay, and gradient clipping at 1.0. Baselines included standard training configurations, Shortformer, and Batch Size Warmup. Evaluation metrics included 'loss ratio' for stability, validation perplexity for convergence speed/quality, and zero-shot/few-shot accuracy on various downstream NLP tasks such as WikiText-103, LAMBADA, HellaSwag, TriviaQA, WebQs, Winogrande, PIQA, ARC Challenge/Easy, ANLI R1/R2/R3, RACE-h, and BoolQ.",
    "Limitations": "The paper acknowledges that the root cause of the causal relationship between long sequences and training instability has not been fully deciphered, though an assumption is made that shorter sequences are 'simpler' examples leading to less noisy gradients. The correlation analysis between training instability and gradient variance outliers is empirical and not sufficient to prove a causal relationship, as other factors could also contribute to instability. Due to the high computational cost of pre-training, experiments were primarily conducted using a single random seed, which might limit the generalizability of results regarding error bars or sensitivity to initialization.",
    "Future Research Directions": "Future research could focus on thoroughly deciphering the root cause of the causal relationship between long sequences and training instability. Further theoretical and practical studies into the broader topic of training instability in large-scale model training are encouraged. Additionally, exploring more complex and adaptive gradient, variance, or activation clipping techniques could be a promising direction.",
    "Experiment Code": "def __getitem__(self, idx: Optional[int]) -> Dict[str, torch.Tensor]:\n        if idx is None:\n            # Batch padding sequence so the index does not matter\n            text, _ = self._query_document_sample_shuffle_indices(0)\n        else:\n            text, _ = self._query_document_sample_shuffle_indices(idx)\n        text = torch.from_numpy(text).long()\n        if self.config.add_extra_token_to_sequence:\n            tokens = text[:-1].contiguous()\n            labels = text[1:].contiguous()\n        else:\n            tokens = text\n            labels = torch.roll(text, shifts=-1, dims=0)\n            labels[-1] = self._pad_token_id\n        if (\n            not self.masks_and_position_ids_are_cacheable\n            or not self.masks_and_position_ids_are_cached\n        ):\n            attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(\n                tokens,\n                self.config.tokenizer.eod,\n                self.config.reset_position_ids,\n                self.config.reset_attention_mask,\n                self.config.eod_mask_loss,\n                self.config.create_attention_mask,\n            )\n            if self.masks_and_position_ids_are_cacheable:\n                self.cached_attention_mask = attention_mask\n                self.cached_loss_mask = loss_mask\n                self.cached_position_ids = position_ids\n                self.masks_and_position_ids_are_cached = True\n        else:\n            attention_mask = self.cached_attention_mask\n            loss_mask = self.cached_loss_mask.clone()\n            position_ids = self.cached_position_ids\n        # For padded sequences, mask the loss\n        loss_mask[labels == self._pad_token_id] = 0.0\n        # For padded sequences, ensure the embedding layer can map the token ID\n        tokens[tokens == self._pad_token_id] = 0\n        labels[labels == self._pad_token_id] = 0\n        # Batch padding sequence so we mask the loss\n        if idx is None:\n            loss_mask = torch.zeros_like(loss_mask)\n        if self.config.create_attention_mask:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"attention_mask\": attention_mask,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\n        else:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\ndef _build_document_sample_shuffle_indices(\n        self,\n    ) -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n        path_to_cache = self.config.path_to_cache\n        if path_to_cache is None and not self.config.mock:\n            path_to_cache = os.path.join(\n                self.dataset.path_prefix, \"cache\", f\"{type(self).__name__}_indices\"\n            )\n        if path_to_cache:\n            base = f\"{self.unique_description_hash}-{type(self).__name__}-{self.index_split.name}\"\n            get_path_to = lambda affix: os.path.join(path_to_cache, f\"{base}-{affix}\")\n            path_to_description = get_path_to(\"description.txt\")\n            path_to_document_index = get_path_to(\"document_index.npy\")\n            path_to_sample_index = get_path_to(\"sample_index.npy\")\n            path_to_shuffle_index = get_path_to(\"shuffle_index.npy\")\n            cache_hit = all(\n                map(\n                    os.path.isfile,\n                    [\n                        path_to_description,\n                        path_to_document_index,\n                        path_to_sample_index,\n                        path_to_shuffle_index,\n                    ],\n                )\n            )\n        else:\n            cache_hit = False\n        if not path_to_cache or (\n            not cache_hit\n            and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0)\n        ):\n            log_single_rank(\n                logger,\n                logging.INFO,\n                f\"Build and save the {type(self).__name__} {self.index_split.name} indices\",\n            )\n            t_beg = time.time()\n            sequence_length = self.config.sequence_length\n            num_tokens_per_epoch = self._get_num_tokens_per_epoch()\n            num_epochs = self._get_num_epochs(num_tokens_per_epoch)\n            if num_epochs == 1:\n                separate_final_epoch = False\n            else:\n                # Get the number of samples for the last epoch\n                num_samples_sans_final_epoch = (\n                    (num_epochs - 1) * num_tokens_per_epoch\n                    - self.config.add_extra_token_to_sequence\n                ) // sequence_length\n                num_samples_from_final_epoch = self.num_samples - num_samples_sans_final_epoch\n                num_samples_per_epoch = (\n                    num_tokens_per_epoch - self.config.add_extra_token_to_sequence\n                ) // sequence_length\n                # num_samples_from_final_epoch should be non-negative\n                assert num_samples_from_final_epoch >= 0\n                # num_samples_from_final_epoch should not exceed max value\n                assert num_samples_from_final_epoch <= num_samples_per_epoch + 1\n                # Separate the final epoch if it falls below the threshold\n                threshold = 0.80\n                separate_final_epoch = num_samples_from_final_epoch < int(\n                    threshold * num_samples_per_epoch\n                )\n                log_single_rank(\n                    logger,\n                    logging.DEBUG,\n                    f\"> num_samples_from_final_epoch: {num_samples_from_final_epoch}\",\n                )\n                log_single_rank(logger, logging.DEBUG, f\"> threshold: {threshold}\")\n                log_single_rank(\n                    logger, logging.DEBUG, f\"> num_samples_per_epoch: {num_samples_per_epoch}\",\n                )\n            log_single_rank(\n                logger, logging.DEBUG, f\"> separate_final_epoch: {separate_final_epoch}\",\n            )\n            numpy_random_state = numpy.random.RandomState(self.config.random_seed)\n            # Build the document index\n            document_index = _build_document_index(\n                self.indices, num_epochs, numpy_random_state, separate_final_epoch\n            )\n            # Build the sample index\n            from megatron.core.datasets import helpers\n            if self.index_split == Split.valid:\n                drop_last_partial_sequence = self.config.drop_last_partial_validation_sequence\n            else:\n                drop_last_partial_sequence = True\n            assert document_index.dtype == numpy.int32\n            assert self.dataset.sequence_lengths.dtype == numpy.int32\n            if len(document_index) * 2 > len(self.dataset.sequence_lengths):\n                # If \"access density\" of sequence_lengths is high, force load the mmap-ed array\n                # into memory by making a copy.\n                #\n                # System performance benefits come from two aspects:\n                #   1. We sequentially pre-load the whole file, most of which we expect to read\n                #   2. The GIL is held when entering the c++ program, improving the speed of which\n                #      improves parallelism\n                sequence_lengths_for_cpp = self.dataset.sequence_lengths.copy()\n            else:\n                sequence_lengths_for_cpp = self.dataset.sequence_lengths\n            sample_index = helpers.build_sample_idx(\n                sequence_lengths_for_cpp,\n                document_index,\n                sequence_length,\n                num_epochs,\n                num_tokens_per_epoch,\n                drop_last_partial_sequence,\n                self.config.add_extra_token_to_sequence,\n            )\n            # Build the shuffle index\n            if separate_final_epoch:\n                shuffle_index = _build_shuffle_index(\n                    num_samples_sans_final_epoch, sample_index.shape[0] - 1, numpy_random_state\n                )\n            else:\n                shuffle_index = _build_shuffle_index(\n                    sample_index.shape[0] - 1, sample_index.shape[0] - 1, numpy_random_state\n                )\n            if path_to_cache:\n                os.makedirs(path_to_cache, exist_ok=True)\n                # Write the description\n                with open(path_to_description, \"wt\") as writer:\n                    writer.write(self.unique_description)\n                numpy.save(path_to_document_index, document_index, allow_pickle=True)\n                numpy.save(path_to_sample_index, sample_index, allow_pickle=True)\n                numpy.save(path_to_shuffle_index, shuffle_index, allow_pickle=True)\n            else:\n                log_single_rank(\n                    logger,\n                    logging.WARNING,\n                    f\"Unable to save {type(self).__name__} indexes because path_to_cache is None\",\n                )\n            t_end = time.time()\n            log_single_rank(logger, logging.DEBUG, f\"\\t> time elapsed: {t_end - t_beg:4f} seconds\")\n            log_single_rank(\n                logger, logging.INFO, f\"> total number of samples: {sample_index.shape[0] - 1}\"\n            )\n            log_single_rank(logger, logging.INFO, f\"> total number of epochs: {num_epochs}\")\n            return document_index, sample_index, shuffle_index",
    "Experiment Result": "The study quantitatively measured training instability using a 'loss ratio' metric (current loss / minimum previous loss) and correlated it with Adam's gradient variance state (l1 norm and max element of sqrt(vt)). A lightweight tuning strategy for SLW's hyperparameters (starting sequence length 'seqlens' and duration 'T') is also described, which relies on monitoring validation perplexity fluctuations during the initial training steps rather than full convergence. "
}{
    "Title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
    "Main Contributions": "The paper identifies and addresses the \"stability-efficiency dilemma\" in pre-training large-scale autoregressive language models (GPT-style models), where increasing batch sizes and learning rates for efficiency leads to training instability and poor generalization. They discover a strong correlation between training instability and extreme gradient variance values, particularly caused by long sequence lengths at the beginning of training. To solve this, they propose the Sequence Length Warmup (SLW) method, which starts training with short sequences and gradually increases the length, effectively avoiding extreme gradient variance. They also introduce a lightweight hyperparameter tuning strategy for SLW. Empirically, SLW enables stable training with 8x larger batch sizes and 4x larger learning rates for GPT-2 models, reducing required training tokens by up to 2.2x and wall clock time by up to 3.7x. For GPT-3 (125M), SLW allows 8x larger batch size and 40x larger learning rate, retaining 99% of zero-shot accuracy with 10x less data and 17x less time, where baseline training diverges.",
    "Methodology": "The methodology involves an in-depth analysis of GPT-2 pre-training stability under varying batch sizes, learning rates, and sequence lengths. Training instability is quantitatively measured by a \"loss ratio\" (current step loss / minimum previous loss), and its correlation with the l1 norm and max element of Adam's variance state is analyzed. This analysis reveals that long sequence lengths, especially early in training, significantly contribute to extreme gradient variance and instability. Based on this, the Sequence Length Warmup (SLW) method is proposed. SLW uses an efficient truncation-based implementation where the dataloader initially provides full-length sequences, which are then truncated to a shorter length determined by a step-wise linear pacing function: `seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`. A low-cost tuning strategy is developed for SLW's hyperparameters (`seqlens` and `T`) by monitoring validation perplexity fluctuations during the initial training steps, avoiding expensive full training runs. The method's effectiveness is compared against baseline training, Shortformer's two-stage approach, and GPT-3's batch size warmup.",
    "Experimental Setup": "Experiments were conducted on GPT-2 (117M and 1.5B parameters) and GPT-3 (125M and 1.3B parameters) models, using 128 NVIDIA V100 GPUs (32GB memory) configured in 16 nodes with NVLink 2.0 and InfiniBand EDR. For GPT-2, the NVIDIA Megatron-LM training pipeline was followed, utilizing a dataset blend of Wikipedia, CC-Stories, RealNews, and OpenWebtext. Training parameters included batch sizes of 512 and 4K, learning rates of 1.5e-4 and 6e-4, and sequence lengths of 1K and 2K. All experiments used mixed precision/FP16 training, Adam optimizer, 0.01 weight decay, and gradient clipping at 1.0. For GPT-3, experiments replicated the 125M and 1.3B models using the Pile public dataset (with additional data for 1.3B). An aggressive training scenario for GPT-3 125M used 10% of the original training data (30B tokens), batch size 2K, and increased learning rates (30x for baseline, 40x for SLW). Validation was based on perplexity, and zero-shot accuracy was evaluated on WikiText-103/LAMBADA for GPT-2, and 11 tasks (HellaSwag, LAMBADA, TriviaQA, WebQs, Winogrande, PIQA, ARC Challenge/Easy, ANLI R1/R2/R3) for GPT-3. GPT-3 1.3B also included few-shot evaluations on 6 tasks.",
    "Limitations": "The paper acknowledges that while it discovers a correlation between long sequences and training instability, the precise root cause of this causal relationship is not fully deciphered. The authors hypothesize that shorter sequences are ",
    "Future Research Directions": "Future research could delve deeper into deciphering the precise root cause of the causal relationship between long sequence lengths and training instability. The authors also hope this work inspires further studies on training instability in large-scale model training, both theoretically and practically. Additionally, investigating whether more complex and adaptive gradient/variance/activation clipping techniques could achieve a similar level of training stability as the proposed Sequence Length Warmup method is a potential area for exploration.",
    "Experiment Code": "# File Path: examples/multimodal/dataset_helpers.py\ndef search_for_fit(numbers: List[int], capacity: int) -> int:\n    \"\"\"Finds the index of largest number that fits into the knapsack with the given capacity.\"\"\"\n    index = bisect.bisect(numbers, capacity)\n    return -1 if index == 0 else (index - 1)\n\n# File Path: examples/multimodal/dataset_helpers.py\ndef greedy_knapsack(item_sizes: List[int], samples: List, max_capacity: int) -> List:\n    \"\"\"Greedy algorithm with binary search for the knapsack problem.\n\n    Pack as many samples as possible given a maximum capacity and capacities of individual samples.\n    Used if sequence packing is enabled.\n    \"\"\"\n    assert len(item_sizes) == len(samples), \"sample lengths and samples must have the same length.\"\n\n    knapsacks = []\n\n    if len(item_sizes) == 0:\n        return knapsacks\n\n    # Sort sample lengths and samples together.\n    sorted_item_sizes, sorted_samples = zip(*sorted(zip(item_sizes, samples), key=lambda x: x[0]))\n    sorted_item_sizes = list(sorted_item_sizes)\n    sorted_samples = list(sorted_samples)\n\n    # Check if all samples fit in the knapsack capacity.\n    if sorted_item_sizes[-1] > max_capacity:\n        raise ValueError(f\"knapsack: A sample is larger {sorted_item_sizes[-1]} than the max_sequence_length {max_capacity}.\")\n\n    while sorted_item_sizes:\n        current_knapsack = []\n        remaining_capacity = max_capacity\n\n        while True:\n            idx = search_for_fit(sorted_item_sizes, remaining_capacity)\n            if idx == -1:\n                break   # Can't fit more samples.\n\n            remaining_capacity -= sorted_item_sizes[idx]\n\n            sorted_item_sizes.pop(idx)\n            sample = sorted_samples.pop(idx)\n            current_knapsack.append(sample)\n\n        knapsacks.append(current_knapsack)\n\n    return knapsacks\n\n# File Path: examples/multimodal/dataset_helpers.py\n    def _truncate_for_packing(self, input_ids, target, num_tiles):\n        \"\"\"Truncate tokens and labels if they exceed packing sequence length.\"\"\"\n        total_num_images = len(num_tiles)\n        total_num_tiles = sum(num_tiles)\n        total_img_embeddings_len = total_num_tiles * self.num_image_embeddings_per_tile\n        max_text_tokens = self.packing_seq_length - total_img_embeddings_len + total_num_images\n\n        input_ids = input_ids[:max_text_tokens]\n        target = target[:max_text_tokens]\n\n        # If truncate causes all labels to be ignored, then skip the sample\n        if (target == IGNORE_INDEX).all():\n            raise ValueError(f\"all targets will be ignored after truncation: {input_ids}\")\n\n        return input_ids, target\n\n# File Path: examples/multimodal/dataset_helpers.py\n    @stateless\n    def pack_selected_samples(self, samples: List[ImageTaskSample]) -> List[ImageTaskSamplePacked]:\n        \"\"\"\n        Function to pack a list of ImageTaskSample into a single ImageTaskSamplePacked.\n\n        NOTE: Energon dataloader calls this method internally if packing is used.\n        Please see https://nvidia.github.io/Megatron-Energon/advanced/packing.html\n\n        Args:\n            samples: List of ImageTaskSample instances to pack into one sample.\n\n        Returns:\n            ImageTaskSamplePacked instance.\n        \"\"\"\n        packing_seq_len = self.packing_seq_length\n\n        packed_tokens = []\n        packed_labels = []\n        packed_imgs = []\n\n        current_length = 0\n        max_length = 0\n        cu_lengths = [0]\n\n        # Process each sample and build lists that we will concatenate to create the packed sample.\n        for _, sample in enumerate(samples):\n            sample_len = sample.total_len\n\n            if sample_len > max_length:\n                max_length = sample_len\n\n            # If adding this sample exceeds the max length, stop.\n            # This should not happen. The select_samples_to_pack method should have already ensured that the samples fit.\n            if current_length + sample_len > packing_seq_len:\n                raise ValueError(f\"Packed sample exceeds the maximum sequence length of {packing_seq_len}: {samples}\")\n\n            # Add the sample's tokens and labels\n            packed_tokens.append(sample.tokens)\n            packed_labels.append(sample.labels)\n\n            # Add the images\n            packed_imgs += sample.imgs\n\n            current_length += sample_len\n            cu_lengths.append(current_length)\n\n        # Concatenate packed tokens and labels.\n        packed_tokens = torch.cat(packed_tokens, dim=0)\n        packed_labels = torch.cat(packed_labels, dim=0)\n\n        return ImageTaskSamplePacked(\n            __key__=\",\".join([s.__key__ for s in samples]),\n            __restore_key__=(),  # Will be set by energon based on `samples`\n            __subflavor__=None,\n            __subflavors__=samples[0].__subflavors__,\n            tokens=packed_tokens,\n            labels=packed_labels,\n            imgs=packed_imgs,\n            cu_lengths=torch.tensor(cu_lengths, dtype=torch.int32),\n            max_length=max_length,\n            num_tiles=[n for s in samples for n in s.num_tiles],\n        )\n\n# File Path: examples/multimodal/train.py\ndef scaled_loss_func(loss_mask, output_tensor):\n    \"\"\"\n    Scaled loss function\n\n    Scale the loss for each conversation turn using the formula:\n\n    1 / sum_j[ sqrt(length(loss_turn_j)) ] * sum_i[ sum(loss_turn_i) / sqrt(length(loss_turn_i)) ]\n\n    Where we use the loss mask to infer the start / end of the conversation turns.\n    \"\"\"\n    args = get_args()\n    losses = output_tensor.float()\n\n    loss_list = []\n    num_valid_labels_list = []\n    for idx in range(losses.shape[0]):\n        loss_this_sample = losses[idx]\n        turn_start_end_list = get_mask_start_and_end_idx(loss_mask[idx])\n        for turn_start, turn_end in turn_start_end_list:\n            # compute loss for each turn\n            loss_this_turn = loss_this_sample[turn_start:turn_end+1].sum()\n            assert (1 - loss_mask)[idx][turn_start:turn_end+1].sum() < 1.0\n            num_valid_labels_this_turn = turn_end - turn_start + 1\n            loss_this_turn = loss_this_turn / num_valid_labels_this_turn\n            loss_list.append(loss_this_turn)\n            # append num of valid labels for each turn\n            num_valid_labels_list.append(num_valid_labels_this_turn)\n    base_num = sum([math.sqrt(each) for each in num_valid_labels_list])\n    for idx in range(len(loss_list)):\n        # normalize loss for each turn\n        loss_list[idx] = loss_list[idx] * math.sqrt(num_valid_labels_list[idx]) / base_num\n\n    # Some ranks may not get loss tokens due to Context Parallel Sharding\n    if len(loss_list) > 0:\n        total_loss = torch.stack(loss_list).sum()\n        total_tokens = torch.ones_like(total_loss)\n    elif len(loss_list) == 0 and args.context_parallel_size > 1:\n        total_tokens = loss_mask.sum()\n        total_loss = torch.sum(losses.view(-1) * loss_mask)\n    else:\n        raise RuntimeError(\"loss_list for loss scaling per conversation unexpectedly got empty list\")\n\n    num_tokens = total_tokens.clone().detach().to(torch.int)\n    reporting_loss = torch.cat([total_loss.clone().detach().view(1), num_tokens.view(1)])\n\n    return (total_loss, num_tokens, {'lm loss': reporting_loss})\n\n# File Path: examples/multimodal/train.py\ndef get_batch(data_iterator, image_token_index, img_seq_len):\n    # ... (code omitted for brevity)\n    cu_lengths = tensor_parallel.broadcast_data([\"cu_lengths\"], data, torch.int32)[\"cu_lengths\"]\n    max_lengths = tensor_parallel.broadcast_data([\"max_lengths\"], data, torch.int32)[\"max_lengths\"]\n    # ... (code omitted for brevity)\n    if cu_lengths.shape != torch.Size([1, 1]):\n        assert (\n            cu_lengths.shape[0] == max_lengths.shape[0] == 1\n        ), \"micro-batch-size must be 1 for packing\"\n        cu_lengths = cu_lengths[0]\n        max_lengths = max_lengths[0]\n\n        packed_seq_params = PackedSeqParams(\n            qkv_format=\"thd\",\n            cu_seqlens_q=cu_lengths,\n            cu_seqlens_kv=cu_lengths,\n            max_seqlen_q=max_lengths,\n            max_seqlen_kv=max_lengths,\n        )\n    # ... (code omitted for brevity)\n    return (\n        tokens,\n        labels,\n        loss_mask,\n        attention_mask,\n        position_ids,\n        imgs,\n        num_tiles,\n        packed_seq_params,\n    )\n\n# File Path: examples/multimodal/train.py\ndef forward_step(data_iterator, model: LLaVAModel):\n    # ... (code omitted for brevity)\n    (\n        tokens,\n        labels,\n        loss_mask,\n        attention_mask,\n        position_ids,\n        images,\n        num_image_tiles,\n        packed_seq_params,\n    ) = get_batch(data_iterator, model.module.module.image_token_index, model.module.module.img_seq_len)\n    # ... (code omitted for brevity)\n    output_tensor, loss_mask = model(\n        images,\n        tokens,\n        position_ids,\n        attention_mask,\n        labels,\n        loss_mask,\n        num_image_tiles=num_image_tiles,\n        packed_seq_params=packed_seq_params,\n    )\n    args = get_args()\n    if args.use_loss_scaling:\n        loss_function = partial(scaled_loss_func, loss_mask)\n    else:\n        loss_function = partial(loss_func, loss_mask)\n\n    return output_tensor, loss_function",
    "Experiment Result": "The implementation involves sequence packing, a truncation-based approach for managing varying sequence lengths. This is related to the Sequence Length Warmup (SLW) method, although an explicit step-wise linear pacing function (`seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`) is not directly found in the provided code.\n\n**Experimental Settings:**\n-   `--packing-buffer-size`: This argument enables sample packing. Setting the buffer size to a value greater than 0 activates the packing mechanism, influencing how multiple short sequences are combined into longer ones to optimize GPU utilization.\n-   `--packing-seq-length`: When packing is enabled, this argument specifies the maximum target sequence length for the packed samples. This is the length to which sequences are truncated or padded during the packing process.\n-   `--use-loss-scaling`: A boolean flag that, when true, enables a `scaled_loss_func` instead of the standard `loss_func`. This `scaled_loss_func` is a mechanism to address training instability by scaling the loss for each conversation turn. The scaling formula is: `1 / sum_j[ sqrt(length(loss_turn_j)) ] * sum_i[ sum(loss_turn_i) / sqrt(length(loss_turn_i)) ]`.\n\n**Truncation-based Implementation Details:**\n-   The `_truncate_for_packing` method in `examples/multimodal/dataset_helpers.py` is responsible for truncating `input_ids` and `target` labels. It calculates `max_text_tokens` based on `self.packing_seq_length` and the length of image embeddings, then truncates the text tokens and labels accordingly to fit within the `packing_seq_length` limit."
}{
    "Title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
    "Main Contributions": "The paper introduces DP-HyPO, a pioneering adaptive private hyperparameter optimization (HPO) framework designed to bridge the gap between private and non-private HPO. It addresses the significant privacy risks associated with HPO in differentially private (DP) machine learning models, a concern often overlooked. DP-HyPO allows practitioners to adaptively select hyperparameters based on previous runs, utilizing non-DP adaptive HPO methods (such as Gaussian process) for enhanced efficiency while avoiding the substantial privacy costs typically incurred by composition. The framework provides sharp DP guarantees using the R\n\nrnyi DP framework for arbitrary adaptive sampling distributions, strictly generalizing prior non-adaptive methods. Empirically, the Gaussian process-based DP-HyPO algorithm demonstrates superior performance compared to its uniform counterpart across diverse real-world datasets, and offers practitioners flexibility in allocating privacy budgets for adaptivity.",
    "Methodology": "DP-HyPO is built upon an adaptive sampling distribution (π) that updates at each iteration, incorporating information gathered from the dataset and previous outputs. A core mechanism to ensure differential privacy is the requirement that the density of any posterior sampling distribution π(j+1) must be bounded relative to a prior distribution π(0) within constants c and C. To achieve this, the framework includes a general recipe for privatizing non-private HPO methods: projecting the non-private π(j) onto a convex space SC,c of bounded density functions by solving a convex functional programming problem. The paper provides rigorous R\n\nrnyi Differential Privacy (RDP) guarantees for the entire framework, particularly for scenarios where the total number of repetitions (T) follows a random distribution, such as a truncated negative binomial distribution. As an instantiation, DP-HyPO is demonstrated with Gaussian Process (GP), where GP is used to construct a surrogate model for model performance, assign scores (estimated Upper Confidence Bound - UCB) to hyperparameters, and convert these scores into a sampling distribution via a softmax function before applying the density projection technique.",
    "Experimental Setup": "The empirical evaluation of DP-HyPO focused on comparing its Gaussian Process-based implementation (\"GP\") against a Uniform DP-HyPO baseline (\"Uniform\"), which is a non-adaptive special case where C=c=1. Experiments were conducted under two privacy configurations:\n1.  **White-box setting**: Assumes adaptive HPO incurs extra privacy cost, reducing the budget for the base algorithm.\n    *   **MNIST Simulation**: Involved training a standard CNN with DP-SGD, optimizing learning rate (η) and clipping norm (R). A semi-real simulation cached mean accuracy from 5 independently trained models for each discretized hyperparameter combination (16 η values, 20 R values, total 320 HPs), adding Gaussian noise (std dev 0.1) to sampled accuracy. Total privacy budget: ε = 15, δ = 1e-5.\n    *   **CIFAR-10 Simulation**: Utilized the same CNN model and hyperparameters. The hyperparameter landscape (mean and standard error of accuracy) was generated using BoTorch, and an oracle returned noisy scores from a normal distribution. Total privacy budget: ε = 12, δ = 1e-5. η and R were discretized into 50 observations each, leading to 2500 HP combinations.\n2.  **Black-box setting**: Assumes the base algorithm has a fixed privacy cost, allowing an extra privacy budget for HPO adaptivity.\n    *   **Federated Learning (FL) Task**: Explored an FL task on a proprietary dataset, aiming to optimize learning rates for the central server (AdaGrad) and individual users (SGD). The loss landscape (mean and standard error) was generated by BoTorch, and an oracle returned noisy scores from a normal distribution. Experiments investigated the impact of varying the C parameter (1.25, 1.33, 1.5) with c=1/C.\nPerformance was evaluated based on the \"actual accuracy\" for MNIST and CIFAR-10, and loss for the FL task. Results were obtained by averaging across hundreds or thousands of independent runs and reported with 95% confidence error bars.",
    "Limitations": "Existing private hyperparameter optimization (HPO) approaches often neglect privacy costs or rely on unrealistic assumptions like prior knowledge of optimal parameters or the availability of perfectly matched public datasets. Traditional composition-based DP analysis for HPO results in overly loose privacy guarantees. Prior non-adaptive private HPO methods, while improving privacy bounds, are limited to uniform sampling and cannot adapt to past results, which is inefficient for large search spaces. The DP-HyPO framework itself requires the adaptive sampling distribution density to be bounded, which necessitates a projection technique for non-private HPO methods. This projection, for practical computational feasibility, often requires discretizing the continuous hyperparameter space. Empirically, the Gaussian process-based DP-HyPO may initially perform slightly worse than its uniform counterpart due to the privacy budget allocated for adaptivity.",
    "Future Research Directions": "The paper suggests two key directions for future research:\n1.  **Alternative HPO Specification**: Further explore and integrate more advanced HPO methods from the extensive literature to develop alternative HPO specifications that could improve the empirical performance of DP-HyPO in practical scenarios.\n2.  **Theoretical Utility Guarantee**: Establish theoretical utility guarantees for the general DP-HyPO framework, or for specific configurations within it, potentially by adapting and extending proof methodologies similar to those found in related prior work (e.g., Theorem 3.3 in [26]).",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Hyperparameter Optimization through Neural Network Partitioning",
    "Main Contributions": "This research proposes Partitioned Neural Networks, a novel and efficient method for hyperparameter optimization (HPO) that is inspired by marginal likelihood. It addresses the computational expense and validation data dependency of traditional HPO and existing marginal likelihood approximations, particularly in deep learning and federated learning settings. The method allows for optimizing a variety of hyperparameters in a single training run without a separate validation set, significantly reducing computational overhead and communication costs in federated learning.",
    "Methodology": "The core methodology involves partitioning the training data into C shards and the neural network parameters into C partitions. Each parameter partition is associated with and optimized on specific data shards. Subnetworks are formed by concatenating the first k learned parameter partitions and setting subsequent partitions to default values (e.g., initialization). Hyperparameters are optimized by minimizing an 'out-of-training-sample' loss objective (LML), which is an approximation to the marginal likelihood, calculated as the loss on data shards unseen by the respective subnetwork. The parameter updates and hyperparameter updates are interleaved using stochastic gradient descent. The primary partitioning scheme involves randomly assigning a fixed proportion of weights in each layer to a given partition.",
    "Experimental Setup": "The method was evaluated on: 1) A toy input selection task, demonstrating its ability to identify informative features and learn a differentiable input mask. 2) Learning affine image augmentations on MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet). Architectures included CNNs for MNIST, FixupResNet-8 and FixupResNet-14 for CIFAR10, and ResNet-50 with GroupNorm(2) for TinyImagenet. 3) Optimizing a feature extractor (first two stages of Wide ResNet-20) on CIFAR10. 4) Federated Learning (FL) with non-i.i.d. splits of MNIST and CIFAR10 (and their rotated variants) across 100 clients, optimizing data augmentation and dropout rates. Baselines included standard training (no augmentations), Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, traditional training/validation split with finetuning, and FedAvg (with and without Augerino). Validation involved test accuracy and log-likelihood.",
    "Limitations": "The method requires an additional forward-backward pass for hyperparameter updates, although this is computationally less expensive than existing alternatives. It may also require more training iterations to converge. Partitioning the network inherently constrains its capacity, which could lead to a performance loss compared to an optimally tuned full network if the optimal hyperparameters were known a priori. Furthermore, the method introduces an additional hyperparameter: the partitioning strategy (number of chunks, relative proportions of data and parameters per chunk), which might require some tuning for optimal performance.",
    "Future Research Directions": "Future research could explore dynamic partitioning of network parameters during training, rather than a fixed partitioning prior to training. Investigating alternative partitioning schemes, such as node partitioning, could enable more efficient updates. Additionally, methods to alleviate the performance loss due to network partitioning, potentially by adjusting training rounds or increasing the initial network capacity, could be explored. Further optimization of hyperparameter update schedules, such as gradient accumulation or less frequent updates, is also a promising direction.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
    "Main Contributions": "The paper addresses the challenge of setting regularization parameters for Lasso-type estimators, which typically suffers from exponential complexity with grid-search or high memory/instability with gradient-based methods. The main contributions include an efficient implicit differentiation algorithm tailored for Lasso-type problems that avoids matrix inversion and scales to high-dimensional data by leveraging solution sparsity. Key findings demonstrate that forward iterative differentiation of block coordinate descent (BCD) converges to the true gradient, linearly once the support is identified. A novel algorithm (Algorithm 2) decouples Jacobian computation from regression coefficient estimation, circumventing the need to solve potentially ill-conditioned linear systems. Experiments show the proposed method outperforms existing hyperparameter optimization techniques for optimizing held-out error or the Stein Unbiased Risk Estimator (SURE) across both low (Lasso) and high (weighted Lasso) numbers of hyperparameters.",
    "Methodology": "Hyperparameter optimization (HO) is formulated as a bi-level optimization problem, where an outer criterion `L(λ) = C(ˆβ(λ))` is optimized given an inner Lasso-type problem `ˆβ(λ) = arg min β ψ(β,λ)`. The regularization parameter is parametrized as `eλ`. The core challenge is computing the weak Jacobian `ˆJ(λ) = ∇λˆβ(λ)`. The proposed method leverages the fixed-point iteration property of proximal BCD algorithms for Lasso-type problems. By differentiating this fixed-point equation, closed-form, sparse solutions for the Jacobian are derived for Lasso and weighted Lasso (Proposition 1). The 'Implicit Forward Iterdiff' algorithm (Algorithm 2) is introduced, which decouples the computation of regression coefficients and the Jacobian. First, `ˆβ` is obtained using a standard solver, then forward differentiation recursion steps are applied to compute the Jacobian, restricted to the active support, which ensures efficiency and stability. For the SURE criterion, a weakly differentiable approximation based on Finite Differences Monte-Carlo is used.",
    "Experimental Setup": "The Python code for the proposed method is open-sourced (`https://github.com/QB3/sparse-ho`), utilizing Numba for critical parts. For fair comparison across all methods, a vanilla BCD algorithm (Algorithm 5) was used as the inner solver, stopping when `(f(β(k+1)) - f(β(k))) / f(0) < 10^-5`. Gradient-based methods employed a line-search strategy. Lasso was initialized with `λ_init = λ_max - log(10)`, while weighted Lasso used a regularized version of the HO problem for initialization. Competitors included other hypergradient-based methods (Implicit, Forward Iterdiff, Backward Iterdiff) and non-hypergradient methods (Grid-search, Random-search, Lattice Hyp., Bayesian optimization). Experiments were conducted on both synthetic data (Toeplitz correlated matrices with `n=1000, p=2000` or `n=1000, p=10000`) and real high-dimensional datasets: `rcv1` (`p=19,959`), `20news` (`p=130,107`), and `finance` (`p=1,668,737`). Datasets were split into training, validation, and test sets for held-out loss evaluation, or simulated data used with the SURE criterion, varying `p` from 200 to 10,000 for MSE and time efficiency analysis.",
    "Limitations": "The theoretical guarantees for the proposed method do not cover non-convex cases (e.g., MCP penalty), although empirical results show good performance. The proof for Jacobian convergence (Proposition 2) assumes a unique solution for the inner Lasso problem; while non-uniqueness is rare, its occurrence could lead to a non-continuous solution path, potentially undermining gradient-based methods. The overall HO problem `L(λ)` is generally non-convex, so gradient descent might converge only to a local minimum, emphasizing the importance of initialization. The implicit differentiation algorithm (Algorithm 1) can still suffer from ill-conditioned linear systems, especially with large support sizes, leading to numerical instabilities and slower resolution. Additionally, the SURE criterion requires prior knowledge of the noise variance.",
    "Future Research Directions": "Future research directions include extending the theoretical guarantees to handle non-convex penalty functions like MCP. Further investigation is warranted into the behavior and theoretical implications of the algorithm in 'pathological' settings where the Lasso solution might not be unique. The authors also suggest combining the proposed algorithm with state-of-the-art Lasso solvers that employ techniques like active sets or screening rules, to leverage their efficiency in the first step of the decoupled process. Exploring other formulations beyond Lasso and weighted Lasso, such as Elastic-Net, is also mentioned as a potential avenue.",
    "Experiment Code": "import numpy as np\nfrom scipy.linalg import norm\nimport scipy.sparse.linalg as slinalg\nfrom numba import njit\nfrom scipy.sparse.linalg import LinearOperator\n\nfrom sparse_ho.utils import init_dbeta0_new, ST\nfrom sparse_ho.utils import sparse_scalar_product\nfrom sparse_ho.models.base import BaseModel\n\n\nclass Lasso(BaseModel):\n    \"\"\"Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\n    The optimization objective for Lasso is:\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Parameters\n    ----------\n    estimator: sklearn estimator\n        Estimator used to solve the optimization problem. Must follow the\n        scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros(n_features)\n        if jac0 is None or not compute_jac:\n            ddual_var = np.zeros(n_samples)\n        else:\n            dbeta[mask0] = jac0.copy()\n            ddual_var = - X[:, mask0] @ jac0.copy()\n        return dbeta, ddual_var\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n                # compute derivatives\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            # beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero indices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward(X, alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        n_samples, n_features = X.shape\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n            v_t_jac[j] *= np.abs(sign_beta[j])\n            v_t_jac -= v_t_jac[j] / (L[j] * n_samples) * X[:, j] @ X\n\n        return grad\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward_sparse(\n            data, indptr, indices, n_samples, n_features,\n            alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            if L[j] != 0:\n                Xjs = data[indptr[j]:indptr[j+1]]\n                idx_nz = indices[indptr[j]:indptr[j+1]]\n                grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n                v_t_jac[j] *= np.abs(sign_beta[j])\n                cste = v_t_jac[j] / (L[j] * n_samples)\n                for i in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n                    Xis = data[indptr[i]:indptr[i+1]]\n                    idx = indices[indptr[i]:indptr[i+1]]\n                    product = sparse_scalar_product(Xjs, idx_nz, Xis, idx)\n                    v_t_jac[i] -= cste * product\n\n        return grad\n\n    @staticmethod\n    def _get_pobj0(dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    @staticmethod\n    def _get_pobj(dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) +\n            np.abs(alphas * beta).sum())\n\n    @staticmethod\n    def _get_dobj(dual_var, X, beta, alpha, y=None):\n        # the dual variable is theta = (y - X beta) / (alpha n_samples)\n        n_samples = X.shape[0]\n        theta = dual_var / (alpha * n_samples)\n        norm_inf_XTtheta = np.max(np.abs(X.T @ theta))\n        if norm_inf_XTtheta > 1:\n            theta /= norm_inf_XTtheta\n        dobj = alpha * y @ theta\n        dobj -= alpha ** 2 * n_samples / 2 * (theta ** 2).sum()\n        return dobj\n\n    @staticmethod\n    def _get_jac(dbeta, mask):\n        return dbeta[mask]\n\n    @staticmethod\n    def get_full_jac_v(mask, jac_v, n_features):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        n_features: int\n            Number of features.\n        \"\"\"\n        # MM sorry I don't get what this does\n        return jac_v\n\n    @staticmethod\n    def get_mask_jac_v(mask, jac_v):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        \"\"\"\n        return jac_v\n\n    @staticmethod\n    def _init_dbeta0(mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is not None:\n            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)\n        else:\n            dbeta0_new = np.zeros(size_mat)\n        return dbeta0_new\n\n    @staticmethod\n    def _init_dbeta(n_features):\n        dbeta = np.zeros(n_features)\n        return dbeta\n\n    @staticmethod\n    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    @staticmethod\n    def _init_g_backward(jac_v0, n_features):\n        if jac_v0 is None:\n            return 0.0\n        else:\n            return jac_v0\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            # dbeta_old = dbeta[j].copy()\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        for j in range(n_features):\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            # store old beta j for fast update\n            dbeta_old = dbeta[j]\n            # update of the Jacobian dbeta\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alphas[j] * np.sign(beta[j]) / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _reduce_alpha(alpha, mask):\n        return alpha\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        \"\"\"Project hyperparameter on an admissible range of values.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float\n            Logarithm of hyperparameter.\n\n        Returns\n        -------\n        log_alpha: float\n            Logarithm of projected hyperparameter.\n        \"\"\"\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\n    @staticmethod\n    def get_L(X):\n        \"\"\"Compute Lipschitz constant of datafit.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n\n        Returns\n        -------\n        L: float\n            The Lipschitz constant.\n        \"\"\"\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    def _use_estimator(self, X, y, alpha, tol):\n        if self.estimator is None:\n            raise ValueError(\"You did not pass a solver with sklearn API\")\n        self.estimator.set_params(tol=tol, alpha=alpha)\n        self.estimator.fit(X, y)\n        mask = self.estimator.coef_ != 0\n        dense = self.estimator.coef_[mask]\n        return mask, dense, None\n\n    @staticmethod\n    def reduce_X(X, mask):\n        \"\"\"Reduce design matrix to generalized support.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        mask : ndarray, shape (n_features,)\n            Generalized support.\n        \"\"\"\n        return X[:, mask]\n\n    @staticmethod\n    def reduce_y(y, mask):\n        \"\"\"Reduce observation vector to generalized support.\n\n        Parameters\n        ----------\n        y : ndarray, shape (n_samples,)\n            Observation vector.\n        mask : ndarray, shape (n_features,)  TODO shape n_samples right?\n            Generalized support.\n        \"\"\"\n        return y\n\n    def sign(self, x, log_alpha):\n        \"\"\"Get sign of iterate.\n\n        Parameters\n        ----------\n        x : ndarray, shape TODO\n        log_alpha : ndarray, shape TODO\n            Logarithm of hyperparameter.\n        \"\"\"\n        return np.sign(x)\n\n    def get_beta(self, X, y, mask, dense):\n        \"\"\"Return primal iterate.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        \"\"\"\n        return mask, dense\n\n    def get_jac_v(self, X, y, mask, dense, jac, v):\n        \"\"\"Compute hypergradient.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        jac: TODO\n        v: TODO\n        \"\"\"\n        return jac.T @ v(mask, dense)\n\n    @staticmethod\n    def get_mat_vec(X, y, mask, dense, log_alpha):\n        \"\"\"Returns a LinearOperator computing the matrix vector product\n        with the Hessian of datafit. It is necessary to avoid storing a\n        potentially large matrix, and keep advantage of the sparsity of X.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        log_alpha: ndarray\n            Logarithm of hyperparameter.\n        \"\"\"\n        X_m = X[:, mask]\n        n_samples, size_supp = X_m.shape\n\n        def mv(v):\n            return X_m.T @ (X_m @ v) / n_samples\n        return LinearOperator((size_supp, size_supp), matvec=mv)\n\n    def generalized_supp(self, X, v, log_alpha):\n        \"\"\"Generalized support of iterate.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        v : TODO\n        log_alpha : float\n            Log of hyperparameter.\n\n        Returns\n        -------\n        TODO\n        \"\"\"\n        return v\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta,\n                              dual_var, ddual_var, alpha):\n        return norm(ddual_var.T @ ddual_var +\n                    n_samples * alpha * sign_beta @ dbeta)",
    "Experiment Result": "This `Lasso` model class provides the specific implementations for computing regression coefficients (`_update_beta_jac_bcd`) and their Jacobian (`_update_only_jac`) using proximal coordinate descent, for both dense and sparse design matrices. It also includes methods for objective function calculation, Lipschitz constant, and projection of hyperparameters. The `_update_only_jac` methods directly implement the differentiation of the fixed-point equation to derive sparse Jacobian solutions."
}{
    "Title": "Bayesian Optimization for Iterative Learning",
    "Main Contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL), a novel Bayesian optimization (BO) approach designed for efficient hyperparameter tuning of deep (reinforcement) learning systems. It addresses the high computational cost and the limitation of traditional BO methods that only consider final performance, ignoring valuable intermediate training information. BOIL proposes to learn an evaluation function that compresses the entire learning progress at any stage of training into a single numeric score, accounting for both training success and stability. The framework balances the benefit of evaluating a hyperparameter setting over additional training steps against their computational cost. It also enhances efficiency by selectively including scores from different training steps (data augmentation). The algorithm demonstrates superior performance over existing baselines in identifying optimal hyperparameters in minimal wall-clock time for Deep Reinforcement Learning (DRL) agents and convolutional neural networks (CNNs).",
    "Methodology": "The methodology is centered on a Bayesian Optimization framework that models the cost-sensitive black-box objective function f(x,t) (hyperparameters x and iterations t) using a Gaussian Process (GP) with a product kernel k([x,t],[x',t']). The training time cost c(x,t) is approximated using a linear regressor. Crucially, BOIL introduces a training curve compression technique, transforming the entire learning curve into a numeric utility score via a Sigmoid (Logistic) preference function. The parameters of this Sigmoid function (growth parameter g0 and middle point m0) are learned dynamically from the observed data by maximizing the GP's log marginal likelihood. To improve sample efficiency and prevent ill-conditioning of the GP covariance matrix, a selective data augmentation technique is used. This involves actively selecting a subset of points from the observed learning curve (at fixed hyperparameters x) by maximizing the GP predictive uncertainty, ensuring the natural log of the covariance matrix's condition number remains below a specified threshold. The next hyperparameter-iteration pair is chosen by maximizing a cost-aware acquisition function (a modified Expected Improvement criterion divided by the predicted cost).",
    "Experimental Setup": "Experiments were conducted on an NVIDIA 1080 GTX GPU using TensorFlow. All results were averaged over 20 independent runs with distinct random seeds, with final performance estimated by evaluating hyperparameters over the maximum number of iterations. DRL tasks included Dueling DQN (DDQN) on the CartPole-v0 environment and Advantage Actor Critic (A2C) on InvertedPendulum-v2 and Reacher-v2 environments, utilizing OpenAI gym and Mujoco, with implementations based on OpenAI Baselines. CNN tuning involved a convolutional neural network on the SVHN and CIFAR10 datasets. The GP model employed square-exponential kernels, with parameters estimated by maximizing marginal likelihood. Data augmentation was set with a maximum of 15 augmented points and a threshold of 20 for the natural log of the GP covariance matrix's condition number. Baselines for comparison included Hyperband, Continuous Multi-Task/Fidelity BO (CM-T/F-BO), vanilla Bayesian Optimization (BO), and BO with curve compression (BO-L). Detailed hyperparameter search ranges for each task and specifications for DRL agents were provided.",
    "Limitations": "The authors identify that a naive approach of augmenting the GP model with an entire learning curve's points can lead to redundancy and severe ill-conditioning of the GP covariance matrix, necessitating their proposed selective augmentation strategy. A more general limitation noted is that methods assuming an exponential decay in training performance, such as Freeze-thaw BO, are often unsuitable for Deep Reinforcement Learning (DRL) due to the unpredictable and noisy fluctuations typical of DRL reward curves. From a broader societal perspective, the paper acknowledges that an increasingly automated machine learning pipeline, while efficient, could potentially detach humans from the modeling process, making critical failures harder to identify and contributing to the growing opacity of ML models. Practitioners are encouraged to reflect on potential biases in data and models used in such automated processes.",
    "Future Research Directions": "The paper suggests that the proposed BOIL framework is not limited to machine learning algorithms but can be broadly applied to any iterative process that depends on a set of parameters and whose iterative structure can be exploited. A specific example given is the optimization of manufacturing pipelines, where factory settings could be adjusted to enhance productivity. Furthermore, the authors implicitly suggest a direction related to broader impact, encouraging the integration of rapidly developing work on interpretability of machine learning models to rigorously analyze the outcomes of automatically trained models, helping to address concerns about model opacity.",
    "Experiment Code": "import numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import MinMaxScaler\nimport scipy\n\n\nclass GaussianProcess(object):\n    def __init__ (self,SearchSpace,noise_delta=1e-8,verbose=0):\n        self.noise_delta=noise_delta\n        self.noise_upperbound=noise_delta\n        self.mycov=self.cov_RBF\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        self.hyper={}\n        self.hyper['var']=1 # standardise the data\n        self.hyper['lengthscale']=0.04 #to be optimised\n        self.noise_delta=noise_delta\n        return None\n        \n    def set_optimum_value(self,fstar_scaled):\n        self.fstar=fstar_scaled\n        \n    def fit(self,X,Y,IsOptimize=0):\n        \"\"\"\n        Fit a Gaussian Process model\n        X: input 2d array [N*d]\n        Y: output 2d array [N*1]\n        \"\"\"       \n        self.X_ori=X # this is the output in original scale\n        #self.X= self.Xscaler.transform(X) #this is the normalised data [0-1] in each column\n        self.X=X\n        self.Y_ori=Y # this is the output in original scale\n        self.Y=(Y-np.mean(Y))/np.std(Y) # this is the standardised output N(0,1)\n        \n        if IsOptimize:\n            self.hyper['lengthscale']=self.optimise()         # optimise GP hyperparameters\n            \n        self.KK_x_x=self.mycov(self.X,self.X,self.hyper)+np.eye(len(X))*self.noise_delta     \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x !\")\n      \n        self.L=scipy.linalg.cholesky(self.KK_x_x,lower=True)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        \n    def cov_RBF(self,x1, x2,hyper):        \n        \"\"\"\n        Radial Basic function kernel (or SE kernel)\n        \"\"\"\n        variance=hyper['var']\n        lengthscale=hyper['lengthscale']\n\n        if x1.shape[1]!=x2.shape[1]:\n            x1=np.reshape(x1,(-1,x2.shape[1]))\n        Euc_dist=euclidean_distances(x1,x2)\n\n        return variance*np.exp(-np.square(Euc_dist)/lengthscale)\n    \n\n    def log_llk(self,X,y,hyper_values):\n        \n        #print(hyper_values)\n        hyper={}\n        hyper['var']=1\n        hyper['lengthscale']=hyper_values[0]\n        noise_delta=self.noise_delta\n\n        KK_x_x=self.mycov(X,X,hyper)+np.eye(len(X))*noise_delta     \n        if np.isnan(KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x !\")   \n\n        try:\n            L=scipy.linalg.cholesky(KK_x_x,lower=True)\n            alpha=np.linalg.solve(KK_x_x,y)\n\n        except: # singular\n            return -np.inf\n        try:\n            first_term=-0.5*np.dot(self.Y.T,alpha)\n            W_logdet=np.sum(np.log(np.diag(L)))\n            second_term=-W_logdet\n\n        except: # singular\n            return -np.inf\n\n        logmarginal=first_term+second_term-0.5*len(y)*np.log(2*3.14)\n        \n        #print(hyper_values,logmarginal)\n        return np.asscalar(logmarginal)\n    \n    def set_ls(self,lengthscale):\n        self.hyper['lengthscale']=lengthscale\n        \n    def optimise(self):\n        \"\"\"\n        Optimise the GP kernel hyperparameters\n        Returns\n        x_t\n        \"\"\"\n        opts ={'maxiter':200,'maxfun':200,'disp': False}\n\n        # epsilon, ls, var, noise var\n        #bounds=np.asarray([[9e-3,0.007],[1e-2,self.noise_upperbound]])\n        bounds=np.asarray([[1e-3,1]])\n\n        init_theta = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(10, 1))\n        logllk=[0]*init_theta.shape[0]\n        for ii,val in enumerate(init_theta):           \n            logllk[ii]=self.log_llk(self.X,self.Y,hyper_values=val) #noise_delta=self.noise_delta\n            \n        x0=init_theta[np.argmax(logllk)]\n\n        res = minimize(lambda x: -self.log_llk(self.X,self.Y,hyper_values=x),x0,\n                                   bounds=bounds,method=\"L-BFGS-B\",options=opts)#L-BFGS-B\n        \n        if self.verbose:\n            print(\"estimated lengthscale\",res.x)\n            \n        return res.x  \n   \n    def predict(self,Xtest,isOriScale=False):\n        \"\"\"\n        ----------\n        Xtest: the testing points  [N*d]\n        Returns\n        -------\n        pred mean, pred var, pred mean original scale, pred var original scale\n        \"\"\"    \n        \n        if isOriScale:\n            Xtest=self.Xscaler.transform(Xtest)\n            \n        if len(Xtest.shape)==1: # 1d\n            Xtest=np.reshape(Xtest,(-1,self.X.shape[1]))\n            \n        if Xtest.shape[1] != self.X.shape[1]: # different dimension\n            Xtest=np.reshape(Xtest,(-1,self.X.shape[1]))\n       \n        KK_xTest_xTest=self.mycov(Xtest,Xtest,self.hyper)+np.eye(Xtest.shape[0])*self.noise_delta\n        KK_xTest_x=self.mycov(Xtest,self.X,self.hyper)\n\n        mean=np.dot(KK_xTest_x,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_x.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n\n        #mean_ori=mean*np.std(self.Y_ori)+np.mean(self.Y_ori)\n        std=np.reshape(np.diag(var),(-1,1))\n        \n        #std_ori=std*np.std(self.Y_ori)#+np.mean(self.Y_ori)\n        \n        #return mean,std,mean_ori,std_ori\n        return  np.reshape(mean,(-1,1)),std  \n\n",
    "Experiment Result": "Gaussian Process:\n- Kernel: Radial Basis Function (RBF) with product kernel k([x,t],[x',t']) = k(x,x')*k(t,t').\n- Initial lengthscale_x: 0.02\n- Initial lengthscale_t: 0.2\n- Initial midpoint (logistic function): 0.0\n- Initial growth (logistic function): 1.0\n- Noise delta: 5e-4\n\nHyperparameter Optimization for GP and Logistic function:\n- GP lengthscale_x search space: [0.03, 0.3]\n- GP lengthscale_t search space: [0.3, 0.6] (derived from [10*SearchSpace_l_min,2*SearchSpace_l_max] with SearchSpace_l_min=0.03 and SearchSpace_l_max=0.3)\n- Logistic midpoint search space: [-2, 3]\n- Logistic growth search space: [0.5, 2]\n- Optimization method: L-BFGS-B (scipy.optimize.minimize)\n- Number of random initializations for hyperparameter search: 20\n- L-BFGS-B options: {'maxiter': 30*dim, 'maxfun': 30*dim} where dim is the search space dimension of the hyperparameter vector (4 in this case: lengthscale_x, lengthscale_t, midpoint, growth).\n- Frequency of hyperparameter optimization: Every 2*dim iterations (where dim is the total search space dimension of x and t).\n\nCost Model:\n- Type: Linear Regressor (sklearn.linear_model.LinearRegression)\n- Input: Combined hyperparameters (x) and iteration (t)\n- Output: Predicted cost\n\nAcquisition Function:\n- Name: 'ei_mu_max' (Expected Improvement using the maximum of the GP's predictive mean as the incumbent).\n- Cost-aware formulation: The acquisition function is computed as log(utility) - log(mean_cost). This combined function is then minimized using `acq_min_scipy_kwargs` which effectively maximizes the ratio (or difference of logs).\n- Acquisition function maximization (for next point selection):\n    - Optimization method: L-BFGS-B (scipy.optimize.minimize)\n    - Number of random initializations: 50*dim points generated randomly, then `range(1*dim)` multi-starts for L-BFGS-B in `acq_max_scipy`. For cost-aware acquisition (minimization), `range(3*dim)` multi-starts with `size=(50*dim, dim)` random points.\n    - L-BFGS-B options: {'maxiter': 10*dim, 'maxfun': 20*dim} (for `acq_max_scipy`), and {'maxiter': 10*dim, 'maxfun': 20*dim} (for `acq_min_scipy_kwargs`).\n\nData Augmentation:\n- Technique: Selective data augmentation based on maximizing GP predictive uncertainty.\n- Maximum number of augmentations: `max_n_augmentation=10` (per real observation's curve), scaled by the iteration `t_max` (i.e., `np.int(t_max * self.max_n_augmentation)`).\n- Selection criteria: Maximizes GP predictive uncertainty ('pure_exploration').\n- Stopping condition for augmentation: If the natural log of the covariance matrix's condition number `log_cond > 15` or if the predictive uncertainty `pred_var_value < (gp.noise_delta + 1e-3)` or if a repetition occurs.\n- Virtual observations: Are generated for a fixed hyperparameter `x` at different iteration `t` values along its learning curve. These virtual points are then transformed into utility scores using the current logistic function parameters and an interpolated cost.\n\nInitialization:\n- Number of initial real points: 3 (default).\n- Initial points are randomly sampled.\n- Virtual observations are generated for each initial real point.\n\nTraining Curve Compression:\n- Method: Sigmoid (Logistic) preference function `1.0/(1+np.exp(-growth*(x-midpoint)))`.\n- Parameters: `midpoint` and `growth` are learned dynamically by maximizing the GP's log marginal likelihood.\n- Score calculation: Average of `curve * logistic_value` from a threshold `(midpoint+6-2)*len(curve)/(12)` to the end of the curve.\n"
}{
    "Title": "An Analytical Theory of Curriculum Learning in Teacher-Student Networks",
    "Main Contributions": "This research analyzes a prototypical neural network model of curriculum learning using statistical physics methods to understand when and why it helps. The key findings include demonstrating a clear speed benefit for curriculum learning in the online setting. However, it also shows that this advantage disappears in standard neural networks when training experiences can be stored and replayed (batch learning), aligning with observations from the deep learning literature. To overcome this, the paper proposes curriculum-aware algorithms that consolidate synapses at curriculum change points through elastic coupling (L2 regularization), which significantly improves test performance in both online and batch settings. The work helps reconcile conflicting empirical results, traces regimes where curriculum learning yields the largest gains (especially with sparse, informative features amidst noise), and suggests that fully exploiting a curriculum may require explicit adjustments in the loss function.",
    "Methodology": "The study employs a teacher-student framework with shallow 1-layer neural networks (perceptrons). The learning task is binary classification where the teacher network is sparse, focusing on a subset of relevant input features. Sample difficulty is modeled by varying the variance of irrelevant input components (∆µ), creating 'easy' (smaller ∆µ) and 'hard' (larger ∆µ) examples. The learning protocols examined are curriculum (easy-to-hard), anti-curriculum (hard-to-easy), and no-curriculum (randomly shuffled). Statistical physics methods, particularly high-dimensional limit analysis, are used to derive exact analytical expressions for online learning dynamics (mean square error loss with L2 regularization via gradient descent) and batch learning performance (L2-regularized logistic loss). For batch learning, a Gaussian prior, acting as an elastic coupling (γ12), is introduced between successive learning phases to instantiate a long-term memory effect, and its impact is analyzed using the replica method for free-energy computation.",
    "Experimental Setup": "The primary experimental setup involves synthetic data generated from the teacher-student model. The student networks are 1-layer perceptrons trained on datasets with varying total sizes (α1 + α2), proportions of easy (α1) and hard (α2) samples, and teacher sparsity (ρ=0.5 in most cases). Difficulty is controlled by ∆1 (easy) and ∆2 (hard). Hyperparameters like learning rate, weight decay, initialisation norm, and elastic coupling strength (γ12) are systematically optimized for each curriculum strategy. Numerical simulations validate the theoretical predictions, showing typical learning trajectories. A real-world demonstration is conducted using a CIFAR10-derived dataset where 32x64 input images are created by concatenating a task-relevant image with an irrelevant distractor, with difficulty varied by scaling the distractor's contrast. A single-layer network is trained with cross-entropy loss and the proposed curriculum protocol with Gaussian prior, using PyTorch Lightning with optimized hyperparameters (elastic penalties, weight decay) and evaluated over multiple random seeds.",
    "Limitations": "The theoretical analysis focuses on a simple teacher-student model with convex loss landscapes, which may not fully capture the complexities of curriculum learning in modern deep learning with highly non-convex problems (e.g., in large language models or reinforcement learning). The model's reliance on a sparse set of informative features amidst noise, while crucial for the observed curriculum benefits, might not generalize to all learning tasks. In the online learning setting, achieving benefits from curriculum learning depends on careful tuning of learning rate and training epochs, as suboptimal choices can lead to performance deterioration or anti-curriculum outperforming curriculum. For the batch setting, when the fraction of relevant features (ρ) is sufficiently high (>0.5), splitting data into curriculum phases can become detrimental due to the intrinsic cost of information partitioning. The batch learning analysis primarily focuses on a two-phase curriculum, with extensions to multiple phases mentioned but not explicitly detailed.",
    "Future Research Directions": "Future work should extend this analysis beyond simple loss landscapes to address the impact of curricula in more complex tasks, such as reinforcement learning. The research also suggests exploring other curriculum-aware approaches, including adapting the learning algorithm (e.g., dynamic loss functions) or the network architecture itself. On the psychology side, the analytical predictions can guide the design of new experiments, particularly to test the counter-intuitive benefit of anti-curriculum learning for intermediate sparsity levels. Further experiments are proposed to systematically manipulate training and transfer difficulties and to extensively sample sparsity levels, testing for interactions with the fraction of easy and hard examples, especially in human learning studies (e.g., fading experiments). An interesting biological question is how animals might naturally learn with 'near-optimal hyperparameters' to consistently exploit curriculum benefits without deterioration.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Understanding the Complexity Gains of Single-Task RL with a Curriculum",
    "Main Contributions": "The paper addresses the challenge of learning in Reinforcement Learning (RL) with sparse or delayed rewards by reformulating single-task problems as multi-task problems with an implicit curriculum. The main contribution is the theoretical framework and algorithm, ROLLIN, which shows that sequentially solving tasks in a curriculum is more computationally efficient than solving a single challenging task from scratch. ROLLIN reduces the exponential iteration and sample complexity bounds of entropy-regularized softmax policy gradient to a polynomial dependency on the state space size (S) by initializing the current task's policy with the previous task's optimal policy and constructing a mixed initial state distribution. This approach achieves computational efficiency without explicit exploration bonuses. Empirical validation is provided on tabular MDPs and simulated robotic tasks.",
    "Methodology": "The core methodology revolves around the **ROLLIN algorithm** operating within a contextual MDP framework. It utilizes entropy-regularized softmax policy gradient (PG) with stochastic gradient estimation. ROLLIN enhances learning by: 1) initializing the current task's policy parameters with the optimal parameters of the preceding task, and 2) constructing the initial state distribution as a mixture of the previous task's optimal state visitation distribution and the original initial state distribution. This 'rolling in' strategy aims to start learning from a near-optimal regime, reducing density mismatch. The theoretical analysis relies on Lipschitz continuity of the reward function and a similarity assumption between consecutive contexts in the curriculum. For practical implementation, ROLLIN is adapted to **Soft-Actor-Critic (SAC)**, using two parallel agents (a main agent `πmain` and an exploration agent `πexp`) to collect and learn from experience, with a probability `β` for the 'roll-in' exploration strategy. Curriculum progression is managed by a performance threshold `R` for switching tasks.",
    "Experimental Setup": "The paper validates its theory and practical implementation across two settings. For **theoretical validation**, experiments were conducted on a tabular 12x12 grid-world MDP with 144 states and 105 actions (including dummy actions). A curriculum of 17 goal-reaching tasks was used, with two reward function types (easy and hard). Stochastic PG with Adam optimizer was employed, and performance was measured by curriculum progress and final return over 50,000 gradient steps. For **practical implementation validation**, Soft-Actor-Critic (SAC) was used on: 1) a **goal-reaching task** (`antmaze-umaze` environment with an oracle curriculum), comparing ROLLIN with vanilla SAC, goal relabeling, and Go-Explore, under varying curriculum step sizes and replay buffer sampling strategies (uniform vs. geometric); and 2) **four non-goal-reaching tasks** (`walker2d`, `hopper`, `humanoid`, `ant` in OpenAI Gym), where the objective was to gradually increase the x-velocity, with increasing task difficulty. Performance was evaluated at 0.5 and 1.0 million environment steps based on curriculum progress, average x-velocity, and average return.",
    "Limitations": "The theoretical analysis acknowledges that reducing the state space size itself requires additional assumptions beyond the scope of this work, implying that while exponential dependency on 'S' is reduced to polynomial, 'S' can still be large. A practical limitation is the requirement for domain-specific knowledge to define a 'good' return value (performance threshold `R`) for switching between curriculum contexts. Additionally, the ROLLIN algorithm introduces an extra hyperparameter `β` which may require careful tuning, although experiments suggest that simple choices (e.g., 0.1 or 0.2) can yield reasonable improvements. The theoretical results also assume access to a 'good' curriculum where adjacent tasks are sufficiently similar.",
    "Future Research Directions": "Future work could explore how ROLLIN's insights (reducing density mismatch by constructing new initial distributions) can be applied to other policy gradient variants, such as Natural Policy Gradient (NPG) or PG in feature spaces. Another promising direction is to integrate ROLLIN with context-based meta-RL methods, specifically for learning to generate sub-goals or contexts to accelerate the learning of current sub-goals. The authors also suggest combining their state coverage analysis with representation analysis from prior work (e.g., Klink et al., 2022). Lastly, the method's applicability is not restricted to goal-conditioned settings, opening up possibilities for its use in more challenging and diverse RL domains.",
    "Experiment Code": "@functools.partial(jax.jit,static_argnames=('backup_entropy', 'update_target'))def _update_jit(rng: PRNGKey, actor: Model, critic: Model, target_critic: Model,temp: Model, batch: Batch, discount: float, tau: float,target_entropy: float, backup_entropy: bool, update_target: bool) -> Tuple[PRNGKey, Model, Model, Model, Model, InfoDict]:rng, key = jax.random.split(rng)new_critic, critic_info = update_critic(key,actor,critic,target_critic,temp,batch,discount,backup_entropy=backup_entropy)if update_target:new_target_critic = target_update(new_critic, target_critic, tau)else:new_target_critic = target_criticrng, key = jax.random.split(rng)new_actor, actor_info = update_actor(key, actor, new_critic, temp, batch)new_temp, alpha_info = temperature.update(temp, actor_info['entropy'],target_entropy)return rng, new_actor, new_critic, new_target_critic, new_temp, {**critic_info,**actor_info,**alpha_info}class SACLearner(object):def __init__(self,seed: int,observations: jnp.ndarray,actions: jnp.ndarray,actor_lr: float = 3e-4,critic_lr: float = 3e-4,temp_lr: float = 3e-4,hidden_dims: Sequence[int] = (256, 256),discount: float = 0.99,tau: float = 0.005,target_update_period: int = 1,target_entropy: Optional[float] = None,backup_entropy: bool = True,init_temperature: float = 1.0,init_mean: Optional[np.ndarray] = None,policy_final_fc_init_scale: float = 1.0):\"\"\"An implementation of the version of Soft-Actor-Critic described in https://arxiv.org/abs/1812.05905\"\"\"action_dim = actions.shape[-1]if target_entropy is None:self.target_entropy = -action_dim / 2else:self.target_entropy = target_entropyself.backup_entropy = backup_entropyself.tau = tauself.target_update_period = target_update_periodself.discount = discountrng = jax.random.PRNGKey(seed)rng, actor_key, critic_key, temp_key = jax.random.split(rng, 4)actor_def = policies.NormalTanhPolicy(hidden_dims,action_dim,init_mean=init_mean,final_fc_init_scale=policy_final_fc_init_scale)actor = Model.create(actor_def,inputs=[actor_key, observations],tx=optax.adam(learning_rate=actor_lr))critic_def = critic_net.DoubleCritic(hidden_dims)critic = Model.create(critic_def,inputs=[critic_key, observations, actions],tx=optax.adam(learning_rate=critic_lr))target_critic = Model.create(critic_def, inputs=[critic_key, observations, actions])temp = Model.create(temperature.Temperature(init_temperature),inputs=[temp_key],tx=optax.adam(learning_rate=temp_lr))self.actor = actorself.critic = criticself.target_critic = target_criticself.temp = tempself.rng = rngself.step = 1def sample_actions(self,observations: np.ndarray,temperature: float = 1.0) -> jnp.ndarray:rng, actions = policies.sample_actions(self.rng, self.actor.apply_fn,self.actor.params, observations,temperature)self.rng = rngactions = np.asarray(actions)return np.clip(actions, -1, 1)def update(self, batch: Batch) -> InfoDict:self.step += 1new_rng, new_actor, new_critic, new_target_critic, new_temp, info = _update_jit(self.rng, self.actor, self.critic, self.target_critic, self.temp,batch, self.discount, self.tau, self.target_entropy,self.backup_entropy, self.step % self.target_update_period == 0)self.rng = new_rngself.actor = new_actorself.critic = new_criticself.target_critic = new_target_criticself.temp = new_tempreturn infodefine update(key: PRNGKey, actor: Model, critic: Model, temp: Model,batch: Batch) -> Tuple[Model, InfoDict]:def actor_loss_fn(actor_params: Params) -> Tuple[jnp.ndarray, InfoDict]:dist = actor.apply_fn({'params': actor_params}, batch.observations)actions = dist.sample(seed=key)log_probs = dist.log_prob(actions)q1, q2 = critic(batch.observations, actions)q = jnp.minimum(q1, q2)actor_loss = (log_probs * temp() - q).mean()return actor_loss, {'actor_loss': actor_loss,'entropy': -log_probs.mean()}new_actor, info = actor.apply_gradient(actor_loss_fn)return new_actor, infodefine target_update(critic: Model, target_critic: Model, tau: float) -> Model:new_target_params = jax.tree_multimap(lambda p, tp: p * tau + tp * (1 - tau), critic.params,target_critic.params)return target_critic.replace(params=new_target_params)def update(key: PRNGKey, actor: Model, critic: Model, target_critic: Model,temp: Model, batch: Batch, discount: float,backup_entropy: bool) -> Tuple[Model, InfoDict]:dist = actor(batch.next_observations)next_actions = dist.sample(seed=key)next_log_probs = dist.log_prob(next_actions)next_q1, next_q2 = target_critic(batch.next_observations, next_actions)next_q = jnp.minimum(next_q1, next_q2)target_q = batch.rewards + discount * batch.masks * next_qif backup_entropy:target_q -= discount * batch.masks * temp() * next_log_probsdef critic_loss_fn(critic_params: Params) -> Tuple[jnp.ndarray, InfoDict]:q1, q2 = critic.apply_fn({'params': critic_params}, batch.observations,batch.actions)critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()return critic_loss, {'critic_loss': critic_loss,'q1': q1.mean(),'q2': q2.mean()}new_critic, info = critic.apply_gradient(critic_loss_fn)return new_critic, infodefine update(temp: Model, entropy: float,target_entropy: float) -> Tuple[Model, InfoDict]:def temperature_loss_fn(temp_params):temperature = temp.apply_fn({'params': temp_params})temp_loss = temperature * (entropy - target_entropy).mean()return temp_loss, {'temperature': temperature, 'temp_loss': temp_loss}new_temp, info = temp.apply_gradient(temperature_loss_fn)return new_temp, infoimport osimport numpy as npimport tqdmfrom absl import app, flagsfrom ml_collections import config_flagsfrom tensorboardX import SummaryWriterfrom jaxrl.agents import AWACLearner, SACLearnerfrom jaxrl.datasets import ReplayBufferfrom jaxrl.datasets.dataset_utils import make_env_and_datasetfrom jaxrl.evaluation import evaluatefrom jaxrl.utils import make_envFLAGS = flags.FLAGSflags.DEFINE_string('env_name', 'HalfCheetah-v2', 'Environment name.')flags.DEFINE_enum('dataset_name', 'awac', ['d4rl', 'awac'], 'Dataset name.')flags.DEFINE_string('save_dir', './tmp/', 'Tensorboard logging dir.')flags.DEFINE_integer('seed', 42, 'Random seed.')flags.DEFINE_integer('eval_episodes', 10,'Number of episodes used for evaluation.')flags.DEFINE_integer('log_interval', 1000, 'Logging interval.')flags.DEFINE_integer('eval_interval', 10000, 'Eval interval.')flags.DEFINE_integer('batch_size', 1024, 'Mini batch size.')flags.DEFINE_integer('max_steps', int(1e6), 'Number of training steps.')flags.DEFINE_integer('init_dataset_size', None,'Number of samples from the dataset to initialize the replay buffer.')flags.DEFINE_integer('num_pretraining_steps', int(5e4),'Number of pretraining steps.')flags.DEFINE_boolean('tqdm', True, 'Use tqdm progress bar.')flags.DEFINE_boolean('save_video', False, 'Save videos during evaluation.')config_flags.DEFINE_config_file('config','configs/awac_default.py','File path to the training hyperparameter configuration.',lock_config=False)def main(_):summary_writer = SummaryWriter(os.path.join(FLAGS.save_dir, 'tb', str(FLAGS.seed)))if FLAGS.save_video:video_train_folder = os.path.join(FLAGS.save_dir, 'video', 'train')video_eval_folder = os.path.join(FLAGS.save_dir, 'video', 'eval')else:video_train_folder = Nonevideo_eval_folder = Noneenv, dataset = make_env_and_dataset(FLAGS.env_name, FLAGS.seed,FLAGS.dataset_name, video_train_folder)eval_env = make_env(FLAGS.env_name, FLAGS.seed + 42, video_eval_folder)np.random.seed(FLAGS.seed)kwargs = dict(FLAGS.config)algo = kwargs.pop('algo')replay_buffer_size = kwargs.pop('replay_buffer_size')if algo == 'sac':agent = SACLearner(FLAGS.seed,env.observation_space.sample()[np.newaxis],env.action_space.sample()[np.newaxis], **kwargs)elif algo == 'awac':agent = AWACLearner(FLAGS.seed,env.observation_space.sample()[np.newaxis],env.action_space.sample()[np.newaxis], **kwargs)else:raise NotImplementedError()replay_buffer = ReplayBuffer(env.observation_space, env.action_space,replay_buffer_size or FLAGS.max_steps)replay_buffer.initialize_with_dataset(dataset, FLAGS.init_dataset_size)eval_returns = []observation, done = env.reset(), Falsefor i in tqdm.tqdm(range(1 - FLAGS.num_pretraining_steps,FLAGS.max_steps + 1),smoothing=0.1,disable=not FLAGS.tqdm):if i >= 1:action = agent.sample_actions(observation)next_observation, reward, done, info = env.step(action)if not done or 'TimeLimit.truncated' in info:mask = 1.0else:mask = 0.0replay_buffer.insert(observation, action, reward, mask,float(done), next_observation)observation = next_observationif done:observation, done = env.reset(), Falsefor k, v in info['episode'].items():summary_writer.add_scalar(f'training/{k}', v,info['total']['timesteps'])else:info = {}info['total'] = {'timesteps': i}batch = replay_buffer.sample(FLAGS.batch_size)update_info = agent.update(batch)if i % FLAGS.log_interval == 0:for k, v in update_info.items():summary_writer.add_scalar(f'training/{k}', v, i)summary_writer.flush()if i % FLAGS.eval_interval == 0:eval_stats = evaluate(agent, eval_env, FLAGS.eval_episodes)for k, v in eval_stats.items():summary_writer.add_scalar(f'evaluation/average_{k}s', v,info['total']['timesteps'])summary_writer.flush()eval_returns.append((info['total']['timesteps'], eval_stats['return']))np.savetxt(os.path.join(FLAGS.save_dir, f'{FLAGS.seed}.txt'),eval_returns,fmt=['%d', '%.1f'])if __name__ == '__main__':app.run(main)",
    "Experiment Result": "The core method is an adaptation of Soft-Actor-Critic (SAC). The default hyperparameters for SAC are as follows:\n`config.algo = 'sac'`\n`config.actor_lr = 3e-4`\n`config.critic_lr = 3e-4`\n`config.temp_lr = 3e-4`\n`config.hidden_dims = (256, 256)`\n`config.discount = 0.99`\n`config.tau = 0.005`\n`config.target_update_period = 1`\n`config.init_temperature = 1.0`\n`config.target_entropy = None`\n`config.backup_entropy = True`\n`config.replay_buffer_size = None`\n\nFor aspects related to sequential learning and initializing from prior knowledge, the `examples/train_finetuning.py` script is relevant. Its key experimental settings include:\n- `env_name`: 'HalfCheetah-v2'\n- `dataset_name`: 'awac' (used to create an initial dataset)\n- `seed`: 42\n- `batch_size`: 1024\n- `max_steps`: 1e6 (total training steps)\n- `init_dataset_size`: None (defaults to using the full initial dataset for replay buffer initialization)\n- `num_pretraining_steps`: 5e4 (number of steps to train solely on the initial dataset before interacting with the environment)\n- `log_interval`: 1000\n- `eval_interval`: 10000\n\nThe `replay_buffer` is initialized with data using `replay_buffer.initialize_with_dataset(dataset, FLAGS.init_dataset_size)`. The training loop then proceeds, allowing for pretraining (`i < 1`) where the agent updates only from the pre-filled replay buffer, followed by online data collection and updates (`i >= 1`).\n\n**Note on ROLLIN specific features:**\nThe provided repository content implements a Soft-Actor-Critic (SAC) learner as a base. The `examples/train_finetuning.py` script provides a mechanism for initializing the replay buffer with an existing dataset and performing `num_pretraining_steps` before online interaction. This serves as a basic form of policy initialization and sequential learning, which is conceptually related to 'initializing the current task's policy parameters with the optimal parameters of the preceding task' and using a 'previous task's optimal state visitation distribution' for initial states, although it does not explicitly implement the 'optimal parameters' transfer or 'mixture' components described in the ROLLIN method. Explicit implementations for 'two parallel agents (πmain and πexp)', a specific 'β for 'roll-in' exploration strategy' (distinct from AWAC's advantage weighting beta), or 'curriculum progression managed by a performance threshold R for switching tasks' are not found within the provided code."
}{
    "Title": "Growing Action Spaces",
    "Main Contributions": "The paper introduces Growing Action Spaces (GAS), an off-policy reinforcement learning algorithm that uses a curriculum of progressively growing action spaces to accelerate learning in complex tasks with large combinatorial action spaces. It proposes simultaneously estimating optimal value functions for multiple action spaces and efficiently transferring data, value estimates, and state representations from restricted action spaces to the full task. The approach is demonstrated to be effective in proof-of-concept control tasks and challenging large-scale StarCraft micromanagement scenarios, significantly improving sample efficiency and asymptotic performance compared to learning from scratch or existing baselines.",
    "Methodology": "The core methodology is built on off-policy value-based reinforcement learning (Q-learning). It introduces 'off-action-space learning' where estimated optimal value functions (Q*l) are learned simultaneously for a hierarchy of action spaces (A0 subset of A1 ... subset of AN-1). Experiences generated by a behavior policy from a restricted action space (Al) are used to update all Q* values for less restricted action spaces (A>=l). Value estimates are transferred by leveraging the monotonicity property (V*i(s) <= V*j(s) for i<j) and initializing Q*l+1(s,a) as Q*l(s,parentl(a)) + Delta_l(s,a) when a hierarchical structure exists. State representations are transferred by sharing parameters between function approximators of each Ql, allowing for a shared embedding. A simple linear schedule is used to grow the action space over training. For multi-agent control, hierarchical k-means clustering (k=2) assigns the same action to nearby agents, progressively increasing the number of independent groups. The joint-action value is estimated as the mean of group-action values, and a specific architecture handles dynamic clustering and produces spatially resolved embeddings.",
    "Experimental Setup": "Experiments were conducted on two classes of problems: discretised continuous control and combinatorial action spaces. Discretised continuous control involved versions of Acrobot and Mountain Car with sparse rewards (+1 for goal, -1 for timeout, 0.05 actuation cost), using action spaces (Al) where each action in Al-1 splits into two children in Al, doubling the action count. Combinatorial action spaces involved large-scale StarCraft micromanagement with 50-100 units per side in scenarios like 50 hydralisks vs 50 hydralisks and 80 marines vs 80/85 marines. The opponent uses scripted logic (holds position, then attack-closest). Performance was evaluated using mean returns (continuous control) and mean evaluation winrate (StarCraft), with moving averages over episodes. Baselines included training from scratch on different action spaces (A0, A2), an ablation with slower epsilon decay, and a Mix&Match (MM) actor-critic baseline. Ablations included 'GAS (2): ON-AC' (without off-action-space update) and 'SEP-Q' (without iterative value function composition). The StarCraft model used a custom architecture with MLP unit embeddings, a residual CNN encoder for spatial features, and separate MLP decoders for state-value and group-action scores. Training used n-step Q-learning, 64 parallel actors, Adam optimizer, epsilon-greedy exploration, and a linear schedule for action space growth.",
    "Limitations": "The approach requires domain knowledge to identify a suitable hierarchy of action spaces. While a simple linear schedule for curriculum progression worked well, a more sophisticated or automated schedule could be beneficial but adds overhead. The Mix&Match baseline's underperformance in some scenarios might be due to hyperparameter sensitivity, and while population-based training could help, it incurs significant computational cost. Asymptotic performance degraded slightly with very high action space levels (A3 or higher), possibly due to decreased average group size pushing against the spatial resolution limits of the CNN architecture or leading to degenerate masked pooling when units are fewer than groups. The 'SEP-Q' ablation's failure highlights the criticality of iteratively computing the value function for proper initialization and preventing competing gradients. Maximizing over coarser action spaces in the Bellman target did not improve learning speed, possibly due to increased maximization bias or the n-step objective reducing its impact. A less restrictive representation for multi-agent value learning (e.g., QMIX) could further improve results but was not explored.",
    "Future Research Directions": "Future work includes automatically identifying how to restrict action spaces for efficient exploration, potentially through meta-optimization. Exploring more complex and deeper hierarchies of action spaces is another avenue. Additionally, investigating less restrictive multi-agent representations, such as those proposed by QMIX, for the multi-agent value learning problem at each hierarchical level could be beneficial.",
    "Experiment Code": "The provided repository content does not contain the direct implementation code for the core reinforcement learning methodology described (off-policy Q-learning, off-action-space learning, value/state transfer, action space growth, or multi-agent clustering). The `submit-collect-samples.py` script is a job submission utility that calls an external binary (`bos-train`), and the `cherryvis` directory contains code for a replay visualization web server, neither of which directly implement the machine learning method.",
    "Experiment Result": "The `submit-collect-samples.py` script configures and submits SLURM jobs to run an executable named `/workspace/build/scripts/bo-switch/bos-train`. The experimental settings passed to this binary and the SLURM environment are as follows: Command-line arguments for `bos-train`: `-mode online`, `-v -1`, `-vmodule bos-train-supervised-online=1`, `-bos_model_type idle`, `-playoutput /workspace/playoutput`, `-num_game_threads 12`, `-bandit ucb1rolling`, `-strategy training`. SLURM job settings include: `partition`: 'scavenge' (default, configurable), `time`: '24:00:00', `nodes`: '1', `ntasks-per-node`: 1, `cpus-per-task`: 20, `mem-per-cpu`: '6G', `num_jobs`: 1000 (configurable), `num_parallel_jobs`: 1000 (configurable)."
}{
    "Title": "Curriculum Reinforcement Learning via Constrained Optimal Transport",
    "Main Contributions": "The paper addresses the problem of automating curriculum generation in Reinforcement Learning (RL) by framing it as an interpolation between task distributions. It identifies shortcomings in existing interpolation-based methods, such as issues with KL divergence for measuring task similarity and expected performance constraints that can lead to non-gradual task changes or sampling overly complex/simple tasks. The main contribution is the proposal of CURROT, a novel approach that generates curricula by formulating it as a constrained optimal transport problem. CURROT explicitly encodes task similarity via a distance function and restricts the generated task distributions to only encode tasks that satisfy a specified performance threshold. The paper also contrasts CURROT with GRADIENT, a recently proposed optimal transport-based method. Experimental results demonstrate that both CURROT and GRADIENT improve upon existing CRL methods, achieving high performance across various tasks with discrete and continuous context spaces and different distance metrics. Specifically, CURROT's adaptive nature proves more effective in scenarios with infeasible target tasks.",
    "Methodology": "The methodology centers on framing curriculum generation as a constrained optimal transport problem. To overcome limitations of KL divergence, the authors propose explicitly defining task similarity using a distance function d(c, c') and leveraging optimal transport to generate interpolating distributions (Wasserstein barycenters), ensuring gradual task changes regardless of distribution parameterization. To address issues with expected performance constraints: 1) CURROT restricts the support of the task distribution p(c) to only contexts c that satisfy a performance constraint J(π, c) ≥ δ, formulated as 'min_p W2(p, µ) s.t. p(V(π, δ))=1', and includes a trust region constraint W2(p, q) ≤ ε. 2) GRADIENT uses a barycentric interpolation pα(c) = B2([1−α, α], [p0(c), µ(c)]) between initial and target distributions, where α is adjusted based on an expected performance constraint. For practical application, approximate algorithms are introduced: particle-based approximations for Wasserstein barycenters using empirical distributions and solving linear assignment problems. Approximate GRADIENT updates α step-wise. Approximate CURROT uses Nadaraya-Watson kernel regression to estimate J(π, c) from task buffers and updates p(c) by minimizing distance to µ(c) under performance and trust region constraints via candidate sampling. The paper also investigates entropy regularization for both methods and employs a search procedure for feasible contexts when initial distributions are uninformative.",
    "Experimental Setup": "The algorithms were benchmarked across diverse environments: 1) E-Maze: a 20x20 grid sparse-reward task, using PPO, with goal position as context. Evaluated using Euclidean (dE), Shortest-Path (dS), and performance-based pseudo-distances (dP*, dP). 2) Unlock-Pickup: a Minigrid environment with a large discrete context space (81,920 tasks), using DQN. Context defines agent, key, box, door positions, and door state, employing a custom 'highway distance function'. 3) Point-Mass: a continuous environment (c ∈ R^2) where context controls gate position and width, using PPO. Also tested higher-dimensional versions (up to R^30) with over-parameterized contexts. 4) Sparse Goal-Reaching: a continuous, sparse-reward environment (c ∈ R^3) with goal position and tolerance as context, using SAC. 5) Teach My Agent (Bipedal Walker): an environment where context controls obstacle spacing and height, using SAC, for 'mostly infeasible' and 'mostly trivial' scenarios. Baselines included ALP-GMM, GOALGAN, PLR, VDS, ACL, Random curriculum, Default (direct training on target), and SPRL. Hindsight Experience Replay (HER) was also evaluated in the Sparse Goal-Reaching task. Evaluation metrics included episodic return, success rate, median goal tolerance, median minimum distance to target contexts, and percentage of solved tasks.",
    "Limitations": "The primary limitations include the high computational burden of optimal transport, especially for large particle counts or high-dimensional spaces, although approximations alleviate this. The performance of the proposed methods is highly sensitive to the choice of the distance metric; a poorly chosen metric can severely diminish learning performance. CURROT, without entropy regularization, can exhibit 'aggressive targeting' where its sampling distribution collapses to a Dirac distribution at the edge of the performance constraint. GRADIENT's curriculum is pre-determined and lacks adaptability to infeasible task regions within the context space, which can hinder learning in environments with such challenges. Both methods are sensitive to the initial context distribution, struggling if it does not contain sufficiently easy tasks to reach the performance threshold. The approximate optimization schemes, particularly for CURROT, may become delicate in higher-dimensional contexts, requiring more sophisticated approaches.",
    "Future Research Directions": "Future research should focus on developing algorithms that synthesize the adaptivity of CURROT with the algorithmic simplicity of GRADIENT to maximize the benefits of optimal transport in curriculum RL. Another key direction is to advance the understanding and learning of task similarity by developing methods that learn distance functions d(c1, c2) from experience, potentially merging intrinsic motivation with the targeted learning capabilities of CURROT and GRADIENT. Additionally, there is a need for robust and efficient estimators for agent performance J(π, c) for performance-based distance metrics. For high-dimensional context spaces, more sophisticated optimization methods are required for CURROT's particle-based updates, possibly involving parallelized gradient-based schemes. Finally, developing problem-specific search procedures for feasible contexts or providing more informed initial distributions p0(c) could address scenarios where easy tasks are initially scarce.",
    "Experiment Code": null,
    "Experiment Result": null
}
Output:
{'open_problems': '(1) PEFT methods still spend the same compute for every input because all surviving adapters are executed, and (2) they store a *fixed* adapter for every layer although most sentences only need a handful of small modifications.  We lack a mechanism that, for each example,\n  • decides *which* layers to adapt,\n  • decides *how large* (rank-k) an adaptation is needed there, and\n  • materialises the corresponding parameters on-the-fly so that inactive layers occupy exactly 0 FLOPs *and* 0 RAM.\nThe challenge is to learn such depth-and-rank decisions jointly with the downstream objective while respecting a global latency / memory budget.', 'method': 'Rank-And-Depth Adaptive LoRA (RAD-LoRA)\n1.  Per-layer router  f_ℓ  (2-layer MLP, hidden→8→2+r_max) takes the mean-pooled pre-adapter activation h_ℓ.\n    • First logit z0 chooses a *binary depth gate* ĝ_ℓ∈{0,1} (straight-through Bernoulli).\n    • The remaining r_max logits pass through softmax to obtain a categorical rank  k_ℓ∈{0,…,r_max}.  Rank 0 means no adapter, reproducing depth gating; higher k activates rank-k low-rank update.\n2.  HyperLoRA  g_ℓ  – a tiny hyper-network (hidden→4r_max·d_in) that outputs the vectorised A_ℓ and B_ℓ **conditioned on k_ℓ**.  Weight generation is masked so FLOPs scale with k.\n3.  Update rule  W′ = W₀ + ĝ_ℓ · (B_ℓ[:k]  A_ℓ[:k]).\n4.  Budget regulariser  λ_c · Σℓ ĝ_ℓ + λ_r · Σℓ E[k_ℓ]  enforces expected #active layers and expected total rank.\n5.  Inference – replace stochastic gates by hard argmax; only active layers materialise their A/B, others allocate no memory (weights are generated on-the-fly then discarded).\n6.  Optional cache – if the same (k_ℓ,h_ℓ) pair repeats, reuse the previously generated matrices.', 'experimental_setup': 'Model  BERT-base-uncased.\nTask   SST-2 classification.\nRouters  f_ℓ shared across all layers (parameter tying).\nHyperLoRA   single linear layer per Transformer block.\nMax rank r_max = 8.\nBaselines  (1) Full FT, (2) LoRA-8 all layers, (3) GaLoRA (static pruning), (4) CoGaLoRA (depth gates only).\nRAD-LoRA   λ_c∈{0,0.1,0.2}, λ_r∈{0,0.05,0.1}.\nTraining   AdamW 2e-5, 3 epochs, batch 32.\nMetrics   dev accuracy, avg #active layers, avg total rank, wall-time, profiled inference FLOPs & RAM on 256 sentences.', 'primary_metric': 'Dev-set accuracy under a fixed compute budget (≤25 % of full LoRA FLOPs).', 'experimental_code': 'class Router(nn.Module):\n    def __init__(self, h, r_max):\n        super().__init__(); self.fc = nn.Sequential(nn.Linear(h,8), nn.ReLU(), nn.Linear(8,2+r_max))\n        self.r_max = r_max\n    def forward(self,h):\n        logits = self.fc(h)                # [B,2+r_max]\n        gate_p = torch.sigmoid(logits[:,0:1])\n        rank_p = torch.softmax(logits[:,1:],-1)  # categorical over r_max+1 ranks\n        gate = ((torch.rand_like(gate_p)<gate_p).float()+gate_p.detach()-gate_p) # STE\n        rank_idx = torch.multinomial(rank_p,1).squeeze(-1)   # STE via gumbel-softmax optional\n        return gate, rank_idx, gate_p, rank_p\nclass HyperLoRA(nn.Module):\n    def __init__(self,h,d_in,d_out,r_max):\n        super().__init__(); self.gen = nn.Linear(h, (d_in+d_out)*r_max)\n        self.d_in, self.d_out, self.r_max = d_in,d_out,r_max\n    def forward(self, h, k):\n        vec = self.gen(h)                  # [B, r_max*(d_in+d_out)]\n        A,B = torch.split(vec, [self.d_in*self.r_max, self.d_out*self.r_max], dim=-1)\n        A = A.view(-1,self.r_max,self.d_in); B = B.view(-1,self.r_max,self.d_out)\n        idx = k.unsqueeze(-1).unsqueeze(-1)\n        A = A.gather(1, idx.expand(-1,1,self.d_in)).squeeze(1)\n        B = B.gather(1, idx.expand(-1,1,self.d_out)).squeeze(1)\n        return A,B\n# inside Transformer layer\nh_pool = x.mean(1)                     # [B,hidden]\ngate, k, gp, rp = self.router(h_pool)\nA,B = self.hyper(h_pool,k)             # shapes depend on k\nupdate = (B.transpose(1,2) @ A) / max(k,1)  # low-rank product\nout = x + gate.view(-1,1,1)*self.alpha*F.linear(x,update)\nloss = task_loss + λ_c*gp.mean() + λ_r*(rp*torch.arange(r_max+1,device=x.device)).mean()', 'expected_result': 'At λ_c=0.1, λ_r=0.05 ⇒ 92.5 % accuracy with ≈3 active layers and total rank ≈12 (vs 72 ranks for full LoRA).  Inference FLOPs ↓60 %, GPU memory for A/B ↓95 % (weights generated transiently).  Ablations show: (i) removing rank branch reverts to CoGaLoRA accuracy 92.0 % and higher compute; (ii) removing gate branch equals AdaLoRA-style rank adaptation but still evaluates all layers, giving only 25 % FLOP saving.', 'expected_conclusion': 'RAD-LoRA converts parameter-efficient fine-tuning into *instance-adaptive depth *and* width* tuning: every input triggers just the minimal set of low-rank updates, generated on demand.  This halves inference compute and almost eliminates adapter storage while matching or exceeding prior PEFT accuracy.  The framework unifies dynamic layer selection and dynamic rank allocation, and opens the door to stateless, cache-friendly hyper-generated adapters for large LLM deployment on memory-constrained devices.'}
