
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/runs/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/runs/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
       ```json
       {
         "primary_metric": "Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).",
         "metrics": {
           "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
           "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
         },
         "best_proposed": {
           "run_id": "proposed-iter2-model-dataset",
           "value": 0.92
         },
         "best_baseline": {
           "run_id": "comparative-1-model-dataset",
           "value": 0.88
         },
         "gap": 4.55
       }
       ```
       The structure must include:
       - "primary_metric": The primary evaluation metric name from the hypothesis
       - "metrics": All collected metrics organized by metric name, then by run_id
       - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
       - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
       - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
         * Must use the expected results from the hypothesis to determine metric direction (higher vs lower is better)
         * If the metric should be minimized, reverse the sign of the gap
         * The gap represents the percentage improvement of the proposed method over the best baseline
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Hypothesis
open_problems='Existing cross-layer LoRA sharing schemes (SAT-LoRA, HF-SAT-LoRA) still presuppose a *fixed host model*: the functional mixer g_θ is learned **per architecture** and its output dimensionality is hard-wired to the in/out size of every weight matrix.  Hence the learned mixers cannot be reused across (i) different hidden sizes (e.g. BERT-base → DeBERTa-large), (ii) pruned or compressed variants, or (iii) future model versions that add more layers.  Practically this forces practitioners to retrain a new generator for each model, while societally it prevents community-shared, device-ready adapter libraries.  We lack a PEFT mechanism that 1) keeps the global atom dictionary *model-agnostic*, 2) generates layer-specific LoRA updates for *arbitrary* (d_in , d_out) on-the-fly, and 3) still fits into a sub-megabyte memory budget with per-token sparsity.' method='Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2\u2009k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.' experimental_setup='Source pre-training  – BERT-base (L=12, d=768) on GLUE-SST-2 (3 epochs).  Target models  – DistilBERT-6L-d=768, BERT-large-24L-d=1024, and TinyBERT-4L-d=312.  Downstream tasks  – CoNLL-2003, BoolQ, Amazon-Review.  Baselines  – (a) SAT-LoRA retrained for every model, (b) standard 8-rank LoRA, (c) full fine-tune.  Budgets  – ≤1 MB total adapter weights, ≤10 % extra FLOPs.  Metrics  – dev F1 / accuracy, extra RAM, atoms · token, *zero-shot* performance before δθ tuning, and after 500 fine-tune steps.' primary_metric='Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).' experimental_code="class DimProjector(nn.Module):\n    def __init__(self,d0=256):\n        super().__init__(); self.d0=d0; self.mlp=nn.Sequential(nn.Linear(1,64),nn.SiLU(),nn.Linear(64,d0*d0))\n    def forward(self,dim):                       # dim: int\n        w=self.mlp(torch.log10(torch.tensor([[dim]],dtype=torch.float32)))\n        return w.view(dim,self.d0)\n\nclass UniHyperLoRA(nn.Module):\n    def __init__(self,M=128,d0=256):\n        super().__init__(); self.u=nn.Parameter(torch.randn(M,d0)); self.v=nn.Parameter(torch.randn(M,d0))\n        self.mix=nn.Sequential(nn.Linear(6,128),nn.SiLU(),nn.Linear(128,M))   # depth+one-hot(5)\n        self.dim_proj=DimProjector(d0)\n    def forward(self,h,meta):                     # h:[B,T,H]\n        depth,kind,task=meta\n        g=self.mix(torch.cat([depth[:,None],kind],-1))          # [B,M]\n        mask=(g.topk(2).indices)                               # k=2\n        U_sel=self.u[mask]; V_sel=self.v[mask]\n        P_out=self.dim_proj(h.size(-1)); P_in=self.dim_proj(h.size(-1)).t()\n        delta=torch.einsum('bmk,kd->bmd',U_sel,P_out)           # simplified demo\n        # full std LoRA update skipped for brevity\n        return h+delta" expected_result='1) Zero-shot: without any extra tuning UniHyperLoRA transfers to BERT-large with <1 % accuracy loss vs source model. 2) After 500 update steps on each task, UniHyperLoRA matches SAT-LoRA that was *retrained* for that architecture while using 97 % less additional memory (0.97 MB vs 33 MB) and identical FLOPs. 3) On TinyBERT the projector correctly shrinks atoms; accuracy +1.1 % over plain LoRA under the same budget.' expected_conclusion='A single tiny hyper-network that conditions on layer *dimensions* as well as depth and type lets one global LoRA dictionary adapt *any* Transformer family without retraining architecture-specific mixers.  UniHyperLoRA therefore turns adapter weights into true reusable assets—enabling community-shared, phone-sized PEFT libraries and dramatically lowering the entry barrier for fine-tuning ever-evolving language models.'

# Current Research Method
Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2 k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.

# Experimental Design
- Strategy: We first pre-train one universal UniHyperLoRA adapter on a single 12-layer BERT-base model with the SST-2 corpus.  This pre-training jointly learns (1) the canonical atom dictionary U,V, (2) the tiny continuous mixer g_θ, (3) the dimension-hyper-network h_ψ that predicts thin projectors P_in / P_out from log-scaled layer sizes, and (4) the sparse router r_φ that chooses at most k=2 atoms per token.  Only U,V,θ,ψ,φ are updated; the host model’s weights stay frozen.  After three epochs we freeze everything except a 4-bit bias δθ.

For the transfer phase we load the *same* frozen adapter weights into three unseen target architectures (DistilBERT-6L-768, BERT-large-1024, TinyBERT-4L-312).  Because the projector h_ψ is dimension-aware, it instantly produces size-compatible low-rank updates for every (d_out,d_in) pair, so no re-training is needed (zero-shot evaluation).  We then run a short 500-step PEFT fine-tuning on each downstream task (CoNLL-2003 NER, BoolQ, Amazon-Review) updating only δθ.

We compare against a strong baseline—SAT-LoRA—that must be re-learnt per architecture, as well as classical rank-8 LoRA and full fine-tuning, while enforcing the shared resource budget of ≤1 MB extra weights and ≤10 % extra FLOPs.  All experiments are executed on a single NVIDIA A100/H200 with 80 GB VRAM; the unified adapter occupies 0.97 MB in 4-bit form and adds <8 % theoretical FLOPs, satisfying the constraint.

After training we report: (a) dev-set performance of the target model with the adapter inserted but *before* any task-specific updates (zero-shot portability) and (b) the score after 500 δθ optimisation steps.  We also log memory overhead, FLOP overhead, and average active atoms per token.  Results are expected to show that the single UniHyperLoRA file achieves near-lossless zero-shot transfer (≤1 % drop) and, after light tuning, matches or exceeds SAT-LoRA while using ~97 % less additional memory.
- Proposed Method: Universal Hyper-LoRA (UniHyperLoRA) is a model-agnostic parameter-efficient fine-tuning mechanism that decouples the learned LoRA dictionary from the host architecture.
1. Atom dictionary: Two 8-bit matrices U,V ∈ ℝ^{M×d₀} with M=128 atoms and anchor width d₀=256 hold the canonical bases that will be reused across *all* layers and models.
2. Dimension-aware projectors: A shared two-layer MLP h_ψ takes log10(d_in) or log10(d_out) as a scalar input and outputs flattened weights for thin projectors P_in(d_in) ∈ ℝ^{d₀×d_in} and P_out(d_out) ∈ ℝ^{d_out×d₀}.  Because the mapping is continuous, it can generate projectors for unseen dimensions at inference time, enabling cross-size portability.
3. Continuous mixer g_θ: Given the normalised depth of the layer (ℓ/L) and a one-hot encoding of the sub-module type (Q,K,V,O,FFN), a tiny sine-activated MLP (≈2 k parameters) produces real-valued mixing weights over the M atoms.  These weights are quantised to 4-bit when saved.
4. Sparse router r_φ: A light router attends over the current hidden state h_{ℓ,i} concatenated with a learned task embedding and selects the Top-k (k≤2) atoms to activate, enforcing sparsity.
5. Low-rank update: For every weight W_{ℓ,i} we form ΔW_{ℓ,i}= Σ_{j=1..k} g_top(j) · P_out u_j v_jᵀ P_in, and fuse it with the host forward pass.
6. Zero-shot portability: After training on a source model we freeze U,V,ψ,φ and only fine-tune a 4-bit bias δθ per task, achieving <1 MB total adapter size.
7. Budget regularisation: The loss L = L_task + λ_F·E[k] + λ_Q·‖δθ‖² penalises FLOPs and bias magnitude to respect the deployment budget.
- Evaluation Metrics: ['Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).', 'Memory overhead (MB)', 'FLOP overhead (%)', 'Active atoms per token', 'F1 Score', 'Accuracy']

# Experiment Runs

- Run ID: proposed-iter1-CoNLL-2003
  Method: proposed
  Model: None
  Dataset: CoNLL-2003
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-CoNLL-2003
method: proposed
model:
  host_model_name: distilbert-base-uncased
  hidden_size: 768
  num_layers: 6
  max_position_embeddings: 512
  vocab_size: 30522
adapter:
  name: UniHyperLoRA
  atoms: 128
  anchor_dim: 256
  router_top_k: 2
  quant_bits: 4
  freeze_base_adapter: true
  train_bias_delta_only: true
  bias_bits: 4
  lambda_F: 0.05
  lambda_Q: 1.0e-5
dataset:
  name: conll2003
  hf_id: eriktks/conll2003
  splits:
    train: train
    validation: validation
    test: test
  max_length: 128
training:
  optimizer: adamw
  learning_rate: 2.0e-4
  batch_size: 32
  max_steps: 500
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  lr_scheduler: linear
  warmup_steps: 50
  seed: 42
  update_param_groups:
    - adapter.bias_delta
evaluation:
  eval_steps: 100
  metric: seqeval
budget_constraints:
  max_extra_params_mb: 1.0
  max_flop_overhead_pct: 10
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    lambda_F:
      type: uniform
      low: 0.01
      high: 0.1
    lambda_Q:
      type: loguniform
      low: 1.0e-6
      high: 1.0e-4
    router_top_k:
      type: categorical
      choices: [1, 2]
hardware:
  accelerator: gpu
  precision: bf16
  devices: 1

  ```
  

- Run ID: comparative-1-iter1-CoNLL-2003
  Method: comparative-1
  Model: None
  Dataset: CoNLL-2003
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-CoNLL-2003
method: comparative-1
model:
  host_model_name: distilbert-base-uncased
  hidden_size: 768
  num_layers: 6
adapter:
  name: SAT-LoRA
  lora_rank: 8
  lora_alpha: 16
  target_modules: [q_proj, k_proj, v_proj, o_proj, ffn_down, ffn_up]
  share_across_layers: false
  precision: int8
dataset:
  name: conll2003
  hf_id: eriktks/conll2003
  splits:
    train: train
    validation: validation
    test: test
  max_length: 128
training:
  optimizer: adamw
  learning_rate: 3.0e-4
  batch_size: 32
  max_steps: 3000
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  lr_scheduler: linear
  warmup_steps: 300
  seed: 42
  update_param_groups:
    - adapter.lora
evaluation:
  eval_steps: 200
  metric: seqeval
budget_constraints:
  max_extra_params_mb: 32
  max_flop_overhead_pct: 10
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 1.0e-3
    lora_rank:
      type: int
      low: 4
      high: 16
    batch_size:
      type: categorical
      choices: [16, 32, 64]
hardware:
  accelerator: gpu
  precision: bf16
  devices: 1

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n\n# These three CLI parameters *must* be supplied when invoking main.py\nrun: \"\"\nresults_dir: outputs\nmode: full  # full | trial\n\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online  # switched to disabled automatically in trial mode\n\noptuna:\n  n_trials: 0  # overridden by run-specific yaml\n  direction: maximize\n\nevaluation:\n  metric: accuracy  # default \u2013 run configs may change (e.g. seqeval)\n  eval_steps: 50\n\ntraining:\n  seed: 42\n  batch_size: 32\n  max_steps: 1000\n  learning_rate: 5.0e-5\n  weight_decay: 0.01\n", "evaluate_py": "import argparse\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nimport wandb\nfrom omegaconf import OmegaConf\n\n\n# -----------------------------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------------------------\n\ndef _load_wandb_credentials():\n    cfg_file = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    cfg = OmegaConf.load(cfg_file)\n    return cfg.wandb.entity, cfg.wandb.project\n\n\ndef _fetch_run(run_id: str, entity: str, project: str):\n    api = wandb.Api()\n    return api.run(f\"{entity}/{project}/{run_id}\")\n\n\ndef _export_metrics(run, save_dir: Path):\n    save_dir.mkdir(parents=True, exist_ok=True)\n    history_df = run.history()  # Pandas DF of all logged events\n    summary_dict = dict(run.summary._json_dict)\n    cfg_dict = dict(run.config)\n\n    # Save json metrics\n    with open(save_dir / \"metrics.json\", \"w\") as fp:\n        json.dump({\"summary\": summary_dict, \"config\": cfg_dict}, fp, indent=2)\n\n    # Save history to parquet for efficiency\n    history_df.to_parquet(save_dir / \"history.parquet\")\n    print(f\"Saved metrics for {run.id} \u2192 {save_dir}\")\n\n    # Plot learning curves for common metrics\n    figure_fp = save_dir / f\"{run.id}_learning_curve.pdf\"\n    plt.figure()\n    for col in [c for c in history_df.columns if c.startswith(\"train\") or c.startswith(\"eval\")]:\n        sns.lineplot(x=history_df.index, y=history_df[col], label=col)\n    plt.xlabel(\"step\")\n    plt.ylabel(\"value\")\n    plt.title(run.id)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(figure_fp)\n    plt.close()\n    print(f\"Figure saved \u2192 {figure_fp}\")\n\n\ndef _aggregate(runs: Dict[str, wandb.apis.public.Run], out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    metrics: Dict[str, Dict[str, float]] = {}\n    for run_id, run in runs.items():\n        summary: Dict = run.summary._json_dict\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                metrics.setdefault(k, {})[run_id] = v\n\n    # Decide primary metric key heuristically\n    primary_key_candidates = [\n        k\n        for k in [\"final_f1\", \"final_accuracy\", \"f1\", \"accuracy\", \"overall_f1\"]\n        if k in metrics\n    ]\n    if not primary_key_candidates:\n        raise RuntimeError(\"No common primary metric across runs to compare.\")\n    primary_key = primary_key_candidates[0]\n\n    # Identify best runs\n    proposed_runs = {k: v for k, v in metrics[primary_key].items() if \"proposed\" in k}\n    baseline_runs = {\n        k: v\n        for k, v in metrics[primary_key].items()\n        if any(tok in k for tok in [\"baseline\", \"comparative\"])\n    }\n    best_proposed_id = max(proposed_runs, key=proposed_runs.get)\n    best_baseline_id = max(baseline_runs, key=baseline_runs.get)\n    best_proposed_val = proposed_runs[best_proposed_id]\n    best_baseline_val = baseline_runs[best_baseline_id]\n    gap_pct = (best_proposed_val - best_baseline_val) / best_baseline_val * 100.0\n\n    aggregate_dict = {\n        \"primary_metric\": primary_key,\n        \"metrics\": metrics,\n        \"best_proposed\": {\"run_id\": best_proposed_id, \"value\": best_proposed_val},\n        \"best_baseline\": {\"run_id\": best_baseline_id, \"value\": best_baseline_val},\n        \"gap\": gap_pct,\n    }\n    with open(out_dir / \"aggregated_metrics.json\", \"w\") as fp:\n        json.dump(aggregate_dict, fp, indent=2)\n    print(f\"Aggregated metrics saved \u2192 {fp.name}\")\n\n    # Comparison bar chart\n    plt.figure(figsize=(10, 6))\n    sns.barplot(\n        x=list(metrics[primary_key].keys()),\n        y=list(metrics[primary_key].values()),\n        palette=\"viridis\",\n    )\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(primary_key)\n    plt.tight_layout()\n    fig_fp = out_dir / \"comparison_primary_metric_bar_chart.pdf\"\n    plt.savefig(fig_fp)\n    plt.close()\n    print(f\"Comparison figure saved \u2192 {fig_fp}\")\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Evaluate multiple runs from wandb and create comparison plots\")\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\"run_ids\", type=str, help=\"JSON list, e.g. [\u0027run1\u0027,\u0027run2\u0027]\")\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    results_root = Path(args.results_dir)\n    run_id_list: List[str] = json.loads(args.run_ids)\n\n    entity, project = _load_wandb_credentials()\n\n    runs: Dict[str, wandb.apis.public.Run] = {}\n    for rid in run_id_list:\n        run = _fetch_run(rid, entity, project)\n        runs[rid] = run\n        _export_metrics(run, results_root / rid)\n\n    _aggregate(runs, results_root / \"comparison\")", "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    if not cfg.run:\n        raise ValueError(\"Usage: python -m src.main run=\u003crun_id\u003e results_dir=\u003cdir\u003e mode=\u003ctrial|full\u003e\")\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    print(\"\ud83d\ude80 Launching:\", \" \".join(cmd))\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "import math\nfrom typing import List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import AutoConfig, AutoModelForTokenClassification\n\n__all__ = [\n    \"build_model_with_adapters\",\n    \"compute_adapter_params\",\n]\n\n\n# -----------------------------------------------------------------------------\n# Baseline: plain LoRA wrapper\n# -----------------------------------------------------------------------------\n\nclass LoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.0):\n        super().__init__()\n        self.base = base\n        self.r = r\n        self.dropout = nn.Dropout(dropout) if dropout \u003e 0.0 else nn.Identity()\n        if r \u003e 0:\n            self.lora_a = nn.Linear(base.in_features, r, bias=False)\n            self.lora_b = nn.Linear(r, base.out_features, bias=False)\n            nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_b.weight)\n            self.scaling = alpha / r\n        else:\n            self.lora_a = self.lora_b = None\n            self.scaling = 1.0\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        result = self.base(x)\n        if self.r \u003e 0:\n            result = result + self.lora_b(self.lora_a(self.dropout(x))) * self.scaling\n        return result\n\n\n# -----------------------------------------------------------------------------\n# Proposed: UniHyperLoRA components\n# -----------------------------------------------------------------------------\n\nclass DimAwareProjector(nn.Module):\n    \"\"\"h_\u03c8: log10(dim) \u2192 R^{dim \u00d7 d0}\"\"\"\n\n    def __init__(self, d0: int = 256):\n        super().__init__()\n        self.d0 = d0\n        self.mlp = nn.Sequential(\n            nn.Linear(1, 64),\n            nn.SiLU(),\n            nn.Linear(64, d0 * 2),  # produce two vectors which we\u0027ll outer-product with ones(:dim)\n        )\n\n    def forward(self, size: int):  # returns (size, d0)\n        inp = torch.log10(torch.tensor([[float(size)]], dtype=torch.float32, device=self.mlp[0].weight.device))\n        vec = self.mlp(inp).view(2, self.d0)  # 2 \u00d7 d0\n        row, col = vec[0], vec[1]\n        return torch.outer(torch.ones(size, device=row.device), row) + torch.outer(torch.arange(1, size + 1, device=row.device, dtype=row.dtype), col)\n\n\nclass SharedUniHyperLoRA(nn.Module):\n    def __init__(self, M: int = 128, d0: int = 256, router_top_k: int = 2):\n        super().__init__()\n        self.M = M\n        self.d0 = d0\n        self.router_top_k = router_top_k\n        self.U = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.V = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.dim_proj = DimAwareProjector(d0)\n        self.mixer = nn.Sequential(\n            nn.Linear(2, 128),  # depth + type-id scalar\n            nn.SiLU(),\n            nn.Linear(128, M),\n        )\n\n\nclass UniHyperLoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, shared: SharedUniHyperLoRA, depth_norm: float, layer_type_id: int):\n        super().__init__()\n        self.base = base\n        for p in self.base.parameters():\n            p.requires_grad = False\n        self.shared = shared\n        self.register_buffer(\"depth_norm\", torch.tensor([depth_norm], dtype=torch.float32))\n        self.register_buffer(\"type_id\", torch.tensor([float(layer_type_id) / 4.0], dtype=torch.float32))  # scaled 0-1\n\n    def forward(self, x):\n        base_out = self.base(x)\n        if not self.training and self.shared.router_top_k == 0:\n            return base_out  # inference shortcut if k=0\n\n        d_out, d_in = self.base.out_features, self.base.in_features\n        device = x.device\n        # --- compute delta W\n        P_out = self.shared.dim_proj(d_out).to(device)  # (d_out, d0)\n        P_in = self.shared.dim_proj(d_in).to(device).t()  # (d0, d_in)\n\n        meta_vec = torch.cat([self.depth_norm, self.type_id]).unsqueeze(0).to(device)  # (1,2)\n        gates = self.shared.mixer(meta_vec)[0]  # (M)\n        values, indices = torch.topk(gates, k=self.shared.router_top_k)\n\n        delta_W = torch.zeros(d_out, d_in, device=device)\n        for val, idx in zip(values, indices):\n            u = self.shared.U[idx]\n            v = self.shared.V[idx]\n            delta_j = torch.outer(u, v)  # d0 \u00d7 d0\n            delta_W += val * (P_out @ delta_j @ P_in)\n\n        return base_out + F.linear(x, delta_W, bias=None)\n\n\n# -----------------------------------------------------------------------------\n# Utility functions\n# -----------------------------------------------------------------------------\n\ndef _get_parent_module(model: nn.Module, module_name: str):\n    parts = module_name.split(\".\")\n    parent = model\n    for p in parts[:-1]:\n        if p.isdigit():  # ModuleList / Sequential numerical index\n            parent = parent[int(p)]\n        else:\n            parent = getattr(parent, p)\n    return parent\n\n\ndef compute_adapter_params(model: nn.Module) -\u003e float:\n    bytes_total = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return bytes_total / (1024 ** 2)  # MB\n\n\n# -----------------------------------------------------------------------------\n# Entry point to build model with chosen adapter type\n# -----------------------------------------------------------------------------\n\ndef build_model_with_adapters(cfg: DictConfig, num_labels: int, device):\n    model_name = cfg.model.host_model_name\n    base_cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=\".cache/\")\n    model = AutoModelForTokenClassification.from_pretrained(model_name, config=base_cfg, cache_dir=\".cache/\")\n\n    if cfg.method.startswith(\"proposed\") or cfg.adapter.name.lower() == \"unihyperlora\":\n        shared = SharedUniHyperLoRA(\n            M=cfg.adapter.atoms,\n            d0=cfg.adapter.anchor_dim,\n            router_top_k=cfg.adapter.router_top_k,\n        )\n        total_linear = sum(1 for m in model.modules() if isinstance(m, nn.Linear))\n        idx = 0\n        for name, module in list(model.named_modules()):\n            if isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                depth = idx / max(1, total_linear - 1)\n                wrapped = UniHyperLoRALinear(module, shared, depth, layer_type_id=0)\n                setattr(parent, child_name, wrapped)\n                idx += 1\n    elif cfg.method.startswith(\"comparative\") or cfg.adapter.name.lower() == \"sat-lora\":\n        targets: List[str] = cfg.adapter.target_modules\n        for name, module in list(model.named_modules()):\n            if any(t in name for t in targets) and isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                wrapped = LoRALinear(module, r=cfg.adapter.lora_rank, alpha=cfg.adapter.lora_alpha)\n                setattr(parent, child_name, wrapped)\n    else:\n        raise ValueError(f\"Unknown adapter type: {cfg.adapter.name}\")\n\n    model.to(device)\n    return model", "preprocess_py": "from typing import Tuple, List\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n)\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n\n# -----------------------------------------------------------------------------\n# Token-classification helpers\n# -----------------------------------------------------------------------------\n\ndef _tokenize_align_labels(examples, tokenizer, label_all_tokens: bool):\n    tokenized = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=tokenizer.model_max_length,\n    )\n    labels = []\n    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_seq[word_idx])\n            else:\n                label_ids.append(label_seq[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized[\"labels\"] = labels\n    return tokenized\n\n\n# -----------------------------------------------------------------------------\n# Data-loading entry point\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg: DictConfig, cache_dir: str):\n    ds_cfg = cfg.dataset\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.host_model_name, cache_dir=cache_dir, use_fast=True)\n    task_name = ds_cfg.name.lower()\n\n    if task_name == \"conll2003\":\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        label_list = raw[\"train\"].features[\"ner_tags\"].feature.names\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n        train_ds = train_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=train_ds.column_names,\n            desc=\"Tokenising train\",\n        )\n        val_ds = val_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=val_ds.column_names,\n            desc=\"Tokenising val\",\n        )\n        collator = DataCollatorForTokenClassification(tokenizer)\n    else:\n        # generic single-sentence classification\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        label_list = raw[\"train\"].features[\"label\"].names\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n\n        def tok_fn(ex):\n            return tokenizer(ex[\"text\"], truncation=True, max_length=ds_cfg.max_length)\n\n        train_ds = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names)\n        val_ds = val_ds.map(tok_fn, batched=True, remove_columns=val_ds.column_names)\n        collator = DataCollatorWithPadding(tokenizer)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        collate_fn=collator,\n    )\n    return train_loader, val_loader, label_list\n\n\ndef get_num_labels(cfg: DictConfig):\n    ds_cfg = cfg.dataset\n    if ds_cfg.name.lower() == \"conll2003\":\n        raw = load_dataset(ds_cfg.hf_id, split=ds_cfg.splits.train)\n        label_list = raw.features[\"ner_tags\"].feature.names\n    else:\n        raw = load_dataset(ds_cfg.hf_id, split=ds_cfg.splits.train)\n        label_list = raw.features[\"label\"].names\n    return len(label_list)", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"unihyperlora-experiments\"\nversion = \"0.1.0\"\ndescription = \"Universal Hyper-LoRA experiments managed by Hydra\"\nrequires-python = \"\u003e=3.10\"\ndependencies = [\n    \"torch\u003e=2.1.0\",\n    \"transformers\u003e=4.36.0\",\n    \"datasets\u003e=2.14.0\",\n    \"hydra-core\u003e=1.3.2\",\n    \"wandb\u003e=0.16.0\",\n    \"optuna\u003e=3.4.0\",\n    \"evaluate\u003e=0.4.1\",\n    \"seqeval\u003e=1.2.2\",\n    \"scikit-learn\u003e=1.3.0\",\n    \"matplotlib\u003e=3.8.0\",\n    \"seaborn\u003e=0.13.0\",\n    \"tqdm\u003e=4.66.0\",\n]", "train_py": "import os\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport hydra\nfrom omegaconf import OmegaConf, DictConfig\n\nimport wandb\nimport optuna\n\n# -----------------------------------------------------------------------------\n# Local imports (package \"src\" is an implicit namespace, PEP-420 \u2011 no __init__.py\n# needed).  Always use absolute \"src.xxx\" imports to be robust when the file is\n# executed as a module (python -m src.train).\n# -----------------------------------------------------------------------------\nfrom src.preprocess import build_dataloaders, get_num_labels\nfrom src.model import (\n    build_model_with_adapters,\n    compute_adapter_params,\n)\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    import random, numpy as np\n\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _write_cfg(cfg: DictConfig, out_dir: str):\n    cfg_path = Path(out_dir) / f\"{cfg.run}_final_cfg.yaml\"\n    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\ndef _prepare_cfg(cfg: DictConfig) -\u003e DictConfig:\n    \"\"\"Merge the base config with run-specific YAML and apply mode overrides.\"\"\"\n\n    run_cfg_fp = Path(__file__).resolve().parent.parent / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_fp.exists():\n        raise FileNotFoundError(f\"No run configuration found for id={cfg.run}: {run_cfg_fp}\")\n\n    run_cfg = OmegaConf.load(run_cfg_fp)\n    cfg = OmegaConf.merge(cfg, run_cfg)  # CLI-level (base) -\u003e run config (more specific)\n\n    # ------------------------------------------------------------------\n    # Mode specific tweaks (trial vs full)\n    # ------------------------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.max_steps = 2  # keep *very* small in trial mode\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be either \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Ensure absolute results_dir path\n    cfg.results_dir = os.path.abspath(cfg.results_dir)\n    os.makedirs(cfg.results_dir, exist_ok=True)\n\n    _write_cfg(cfg, cfg.results_dir)\n    return cfg\n\n\n# -----------------------------------------------------------------------------\n# Evaluation helper\n# -----------------------------------------------------------------------------\n\ndef _evaluate(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    device: torch.device,\n    metric_name: str,\n) -\u003e Dict[str, float]:\n    import evaluate as hf_evaluate\n\n    metric = hf_evaluate.load(metric_name, cache_dir=CACHE_DIR)\n    model.eval()\n\n    id2label: Dict[int, str] = {\n        int(k): v for k, v in getattr(model.config, \"id2label\", {}).items()\n    }\n\n    for batch in dataloader:\n        labels = batch.pop(\"labels\").to(device)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            logits = model(**batch).logits\n        preds = logits.argmax(-1).cpu()\n        labels_cpu = labels.cpu()\n\n        if metric_name == \"seqeval\":\n            preds_list: List[List[str]] = []\n            refs_list: List[List[str]] = []\n            for pred_seq, label_seq in zip(preds, labels_cpu):\n                p_sent: List[str] = []\n                l_sent: List[str] = []\n                for p_id, l_id in zip(pred_seq.tolist(), label_seq.tolist()):\n                    if l_id == -100:\n                        continue\n                    p_sent.append(id2label.get(p_id, str(p_id)))\n                    l_sent.append(id2label.get(l_id, str(l_id)))\n                preds_list.append(p_sent)\n                refs_list.append(l_sent)\n            metric.add_batch(predictions=preds_list, references=refs_list)\n        else:\n            metric.add_batch(predictions=preds, references=labels_cpu)\n\n    result = metric.compute()\n    if metric_name == \"seqeval\":\n        # seqeval returns many values \u2013 we surface overall_f1 and overall_accuracy if present\n        out = {\n            \"f1\": result.get(\"overall_f1\", 0.0),\n            \"accuracy\": result.get(\"overall_accuracy\", 0.0),\n        }\n        return out\n    # for single-value metrics (accuracy, f1, etc.) evaluate returns scalar\n    if isinstance(result, dict):\n        return result  # e.g., {\"accuracy\": 0.9}\n    return {metric_name: float(result)}\n\n\n# -----------------------------------------------------------------------------\n# Training utilities\n# -----------------------------------------------------------------------------\n\ndef _training_step(\n    model: torch.nn.Module,\n    batch: Dict[str, torch.Tensor],\n    optimizer: torch.optim.Optimizer,\n    scheduler: Optional[torch.optim.lr_scheduler.LambdaLR],\n    device: torch.device,\n    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n) -\u003e float:\n    model.train()\n    labels = batch.pop(\"labels\").to(device)\n    batch = {k: v.to(device) for k, v in batch.items()}\n\n    if scaler is not None:\n        with torch.cuda.amp.autocast():\n            outputs = model(**batch, labels=labels)\n            loss = outputs.loss\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        outputs = model(**batch, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    optimizer.zero_grad(set_to_none=True)\n    if scheduler is not None:\n        scheduler.step()\n    return loss.item()\n\n\n# -----------------------------------------------------------------------------\n# Optuna Objective\n# -----------------------------------------------------------------------------\n\ndef _suggest_and_apply(trial: optuna.Trial, cfg: DictConfig):\n    \"\"\"Mutate cfg in-place with sampled hyper-parameters according to cfg.optuna.search_space.\"\"\"\n\n    search_space: Dict[str, Any] = cfg.optuna.get(\"search_space\", {})\n    for param_path, space in search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            sampled = trial.suggest_float(param_path, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            sampled = trial.suggest_float(param_path, space[\"low\"], space[\"high\"], log=False)\n        elif space[\"type\"] == \"categorical\":\n            sampled = trial.suggest_categorical(param_path, space[\"choices\"])\n        elif space[\"type\"] == \"int\":\n            sampled = trial.suggest_int(param_path, space[\"low\"], space[\"high\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna space type: {space[\u0027type\u0027]}\")\n        OmegaConf.update(cfg, param_path, sampled, merge=True)\n\n\ndef _objective(\n    trial: optuna.Trial,\n    cfg: DictConfig,\n    model_init_fn,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    device: torch.device,\n):\n    cfg_tmp = OmegaConf.deepcopy(cfg)  # work on a copy so that parallel trials don\u0027t clash\n    _suggest_and_apply(trial, cfg_tmp)\n\n    model = model_init_fn(cfg_tmp)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg_tmp.training.learning_rate,\n        weight_decay=cfg_tmp.training.weight_decay,\n    )\n\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n    max_steps = min(100, cfg_tmp.training.max_steps)  # keep Optuna fast\n    eval_every = max(10, max_steps // 5)\n\n    step = 0\n    best_val = -float(\"inf\")\n    train_iter = iter(train_loader)\n    while step \u003c max_steps:\n        try:\n            batch = next(train_iter)\n        except StopIteration:\n            train_iter = iter(train_loader)\n            batch = next(train_iter)\n\n        loss = _training_step(model, batch, optimizer, None, device, scaler)\n        step += 1\n        if step % eval_every == 0 or step == max_steps:\n            metrics = _evaluate(model, val_loader, device, cfg_tmp.evaluation.metric)\n            key = \"f1\" if \"f1\" in metrics else list(metrics.keys())[0]\n            score = metrics[key]\n            trial.report(score, step)\n            if score \u003e best_val:\n                best_val = score\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    return best_val\n\n\n# -----------------------------------------------------------------------------\n# wandb\n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        dir=cfg.results_dir,\n    )\n    print(f\"wandb\u2006URL: {run.url}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # ------------------------------------------------------------------\n    # Prepare configuration \u0026 environment\n    # ------------------------------------------------------------------\n    cfg = _prepare_cfg(cfg)\n    set_seed(cfg.training.seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, label_list = build_dataloaders(cfg, CACHE_DIR)\n    num_labels = len(label_list)\n\n    # ------------------------------------------------------------------\n    # Model factory so Optuna can re-instantiate quickly\n    # ------------------------------------------------------------------\n    def model_init(local_cfg: DictConfig):\n        return build_model_with_adapters(local_cfg, num_labels, device)\n\n    # ------------------------------------------------------------------\n    # Hyper-parameter search (Optuna)\n    # ------------------------------------------------------------------\n    if cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(\n            lambda tr: _objective(\n                tr, cfg, model_init, train_loader, val_loader, device\n            ),\n            n_trials=cfg.optuna.n_trials,\n        )\n        print(f\"Optuna finished \u2013 best value {study.best_value:.4f} in trial {study.best_trial.number}\")\n        for k, v in study.best_trial.params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n\n    # ------------------------------------------------------------------\n    # Final training run (logged to wandb)\n    # ------------------------------------------------------------------\n    model = model_init(cfg)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    # simple linear LR schedule \u2013 from 1.0 \u2192 0.0 over max_steps\n    lr_lambda = lambda cur: max(0.0, 1.0 - (cur / cfg.training.max_steps))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    wb_run = _init_wandb(cfg)\n    if wb_run is not None:\n        wb_run.summary[\"adapter_param_MB\"] = compute_adapter_params(model)\n\n    global_step = 0\n    epoch = 0\n    pbar = tqdm(total=cfg.training.max_steps, desc=\"Training\")\n\n    while global_step \u003c cfg.training.max_steps:\n        epoch += 1\n        for batch in train_loader:\n            loss_val = _training_step(model, batch, optimizer, scheduler, device, scaler)\n            global_step += 1\n            pbar.update(1)\n\n            if wb_run is not None:\n                wandb.log({\"train_loss\": loss_val, \"step\": global_step})\n\n            if global_step % cfg.evaluation.eval_steps == 0 or global_step == cfg.training.max_steps:\n                eval_metrics = _evaluate(model, val_loader, device, cfg.evaluation.metric)\n                if wb_run is not None:\n                    wandb.log({**{f\"eval_{k}\": v for k, v in eval_metrics.items()}, \"step\": global_step})\n\n            if global_step \u003e= cfg.training.max_steps:\n                break\n    pbar.close()\n\n    # Final evaluation\n    final_metrics = _evaluate(model, val_loader, device, cfg.evaluation.metric)\n    print(f\"Final metrics: {final_metrics}\")\n\n    if wb_run is not None:\n        for k, v in final_metrics.items():\n            wb_run.summary[f\"final_{k}\"] = v\n        wb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()"}



Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The code will not run in its current state and violates several mandatory requirements:\n\n1. model.py ‒ runtime NameError: DictConfig is used in type annotations but never imported. In Python ≥3.10 annotations are evaluated eagerly, so the module crashes on import.\n2. preprocess.py ‒ single-sentence classification branch drops the ‘label’ column and never writes a ‘labels’ field; _training_step expects batch[\"labels\"]. Any classification dataset will raise a KeyError.\n3. evaluate.py ‒ aggregated_metrics.json must store the *exact* primary metric string from the paper hypothesis, but the script infers a heuristic key instead, breaking the required output specification.\n4. Figure naming conventions in evaluate.py do not follow the strictly prescribed pattern (run_id_<topic>.pdf and comparison_<topic>.pdf without dots in run_id, etc.).\n5. SharedUniHyperLoRA has no forward() method; although not currently called directly, its absence makes the component incomplete for independent reuse and contradicts the “fully functional / publication-ready” criterion.\n\nBecause of these issues the experiment code is not immediately executable nor fully compliant with the review requirements."
}
