
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Hypothesis
open_problems='Existing cross-layer LoRA sharing schemes (SAT-LoRA, HF-SAT-LoRA) still presuppose a *fixed host model*: the functional mixer g_θ is learned **per architecture** and its output dimensionality is hard-wired to the in/out size of every weight matrix.  Hence the learned mixers cannot be reused across (i) different hidden sizes (e.g. BERT-base → DeBERTa-large), (ii) pruned or compressed variants, or (iii) future model versions that add more layers.  Practically this forces practitioners to retrain a new generator for each model, while societally it prevents community-shared, device-ready adapter libraries.  We lack a PEFT mechanism that 1) keeps the global atom dictionary *model-agnostic*, 2) generates layer-specific LoRA updates for *arbitrary* (d_in , d_out) on-the-fly, and 3) still fits into a sub-megabyte memory budget with per-token sparsity.' method='Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2\u2009k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.' experimental_setup='Source pre-training  – BERT-base (L=12, d=768) on GLUE-SST-2 (3 epochs).  Target models  – DistilBERT-6L-d=768, BERT-large-24L-d=1024, and TinyBERT-4L-d=312.  Downstream tasks  – CoNLL-2003, BoolQ, Amazon-Review.  Baselines  – (a) SAT-LoRA retrained for every model, (b) standard 8-rank LoRA, (c) full fine-tune.  Budgets  – ≤1 MB total adapter weights, ≤10 % extra FLOPs.  Metrics  – dev F1 / accuracy, extra RAM, atoms · token, *zero-shot* performance before δθ tuning, and after 500 fine-tune steps.' primary_metric='Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).' experimental_code="class DimProjector(nn.Module):\n    def __init__(self,d0=256):\n        super().__init__(); self.d0=d0; self.mlp=nn.Sequential(nn.Linear(1,64),nn.SiLU(),nn.Linear(64,d0*d0))\n    def forward(self,dim):                       # dim: int\n        w=self.mlp(torch.log10(torch.tensor([[dim]],dtype=torch.float32)))\n        return w.view(dim,self.d0)\n\nclass UniHyperLoRA(nn.Module):\n    def __init__(self,M=128,d0=256):\n        super().__init__(); self.u=nn.Parameter(torch.randn(M,d0)); self.v=nn.Parameter(torch.randn(M,d0))\n        self.mix=nn.Sequential(nn.Linear(6,128),nn.SiLU(),nn.Linear(128,M))   # depth+one-hot(5)\n        self.dim_proj=DimProjector(d0)\n    def forward(self,h,meta):                     # h:[B,T,H]\n        depth,kind,task=meta\n        g=self.mix(torch.cat([depth[:,None],kind],-1))          # [B,M]\n        mask=(g.topk(2).indices)                               # k=2\n        U_sel=self.u[mask]; V_sel=self.v[mask]\n        P_out=self.dim_proj(h.size(-1)); P_in=self.dim_proj(h.size(-1)).t()\n        delta=torch.einsum('bmk,kd->bmd',U_sel,P_out)           # simplified demo\n        # full std LoRA update skipped for brevity\n        return h+delta" expected_result='1) Zero-shot: without any extra tuning UniHyperLoRA transfers to BERT-large with <1 % accuracy loss vs source model. 2) After 500 update steps on each task, UniHyperLoRA matches SAT-LoRA that was *retrained* for that architecture while using 97 % less additional memory (0.97 MB vs 33 MB) and identical FLOPs. 3) On TinyBERT the projector correctly shrinks atoms; accuracy +1.1 % over plain LoRA under the same budget.' expected_conclusion='A single tiny hyper-network that conditions on layer *dimensions* as well as depth and type lets one global LoRA dictionary adapt *any* Transformer family without retraining architecture-specific mixers.  UniHyperLoRA therefore turns adapter weights into true reusable assets—enabling community-shared, phone-sized PEFT libraries and dramatically lowering the entry barrier for fine-tuning ever-evolving language models.'

## Research Method
Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2 k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.

## Experimental Design
experiment_summary='We first pre-train one universal UniHyperLoRA adapter on a single 12-layer BERT-base model with the SST-2 corpus.  This pre-training jointly learns (1) the canonical atom dictionary U,V, (2) the tiny continuous mixer g_θ, (3) the dimension-hyper-network h_ψ that predicts thin projectors P_in / P_out from log-scaled layer sizes, and (4) the sparse router r_φ that chooses at most k=2 atoms per token.  Only U,V,θ,ψ,φ are updated; the host model’s weights stay frozen.  After three epochs we freeze everything except a 4-bit bias δθ.\n\nFor the transfer phase we load the *same* frozen adapter weights into three unseen target architectures (DistilBERT-6L-768, BERT-large-1024, TinyBERT-4L-312).  Because the projector h_ψ is dimension-aware, it instantly produces size-compatible low-rank updates for every (d_out,d_in) pair, so no re-training is needed (zero-shot evaluation).  We then run a short 500-step PEFT fine-tuning on each downstream task (CoNLL-2003 NER, BoolQ, Amazon-Review) updating only δθ.\n\nWe compare against a strong baseline—SAT-LoRA—that must be re-learnt per architecture, as well as classical rank-8 LoRA and full fine-tuning, while enforcing the shared resource budget of ≤1 MB extra weights and ≤10 % extra FLOPs.  All experiments are executed on a single NVIDIA A100/H200 with 80 GB VRAM; the unified adapter occupies 0.97 MB in 4-bit form and adds <8 % theoretical FLOPs, satisfying the constraint.\n\nAfter training we report: (a) dev-set performance of the target model with the adapter inserted but *before* any task-specific updates (zero-shot portability) and (b) the score after 500 δθ optimisation steps.  We also log memory overhead, FLOP overhead, and average active atoms per token.  Results are expected to show that the single UniHyperLoRA file achieves near-lossless zero-shot transfer (≤1 % drop) and, after light tuning, matches or exceeds SAT-LoRA while using ~97 % less additional memory.' evaluation_metrics=['Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).', 'Memory overhead (MB)', 'FLOP overhead (%)', 'Active atoms per token', 'F1 Score', 'Accuracy'] proposed_method='Universal Hyper-LoRA (UniHyperLoRA) is a model-agnostic parameter-efficient fine-tuning mechanism that decouples the learned LoRA dictionary from the host architecture.\n1. Atom dictionary: Two 8-bit matrices U,V ∈ ℝ^{M×d₀} with M=128 atoms and anchor width d₀=256 hold the canonical bases that will be reused across *all* layers and models.\n2. Dimension-aware projectors: A shared two-layer MLP h_ψ takes log10(d_in) or log10(d_out) as a scalar input and outputs flattened weights for thin projectors P_in(d_in) ∈ ℝ^{d₀×d_in} and P_out(d_out) ∈ ℝ^{d_out×d₀}.  Because the mapping is continuous, it can generate projectors for unseen dimensions at inference time, enabling cross-size portability.\n3. Continuous mixer g_θ: Given the normalised depth of the layer (ℓ/L) and a one-hot encoding of the sub-module type (Q,K,V,O,FFN), a tiny sine-activated MLP (≈2 k parameters) produces real-valued mixing weights over the M atoms.  These weights are quantised to 4-bit when saved.\n4. Sparse router r_φ: A light router attends over the current hidden state h_{ℓ,i} concatenated with a learned task embedding and selects the Top-k (k≤2) atoms to activate, enforcing sparsity.\n5. Low-rank update: For every weight W_{ℓ,i} we form ΔW_{ℓ,i}= Σ_{j=1..k} g_top(j) · P_out u_j v_jᵀ P_in, and fuse it with the host forward pass.\n6. Zero-shot portability: After training on a source model we freeze U,V,ψ,φ and only fine-tune a 4-bit bias δθ per task, achieving <1 MB total adapter size.\n7. Budget regularisation: The loss L = L_task + λ_F·E[k] + λ_Q·‖δθ‖² penalises FLOPs and bias magnitude to respect the deployment budget.' comparative_methods=['SAT-LoRA'] models_to_use=[] datasets_to_use=['CoNLL-2003'] hyperparameters_to_search={'learning_rate': '5e-5-5e-4', 'batch_size': '32,64,128', 'lambda_F': '0.01-0.1', 'lambda_Q': '1e-6-1e-4', 'router_top_k': '1,2,3'} external_resources=ExternalResources(hugging_face=HuggingFace(models=[], datasets=[HuggingFaceResource(id='eriktks/conll2003', author='eriktks', sha='3f1cce917ab38486481b062921eb137e7bd3c205', created_at=datetime.datetime(2022, 3, 2, 23, 29, 22, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2024, 1, 18, 9, 34, 17, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=21175, likes=154, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='conll2003.py', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license=['other'], language=['en'], library_name=None, pipeline_tag=None, tags=[], datasets=[], model_type=None, base_model=None, task_categories=['token-classification'], size_categories=['10K<n<100K'], metrics=[], widget=[]), tags=['task_categories:token-classification', 'task_ids:named-entity-recognition', 'task_ids:part-of-speech', 'annotations_creators:crowdsourced', 'language_creators:found', 'multilinguality:monolingual', 'source_datasets:extended|other-reuters-corpus', 'language:en', 'license:other', 'size_categories:10K<n<100K', 'region:us'], pipeline_tag=None, library_name=None, readme='---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- other\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-reuters-corpus\ntask_categories:\n- token-classification\ntask_ids:\n- named-entity-recognition\n- part-of-speech\npaperswithcode_id: conll-2003\npretty_name: CoNLL-2003\ndataset_info:\n  features:\n  - name: id\n    dtype: string\n  - name: tokens\n    sequence: string\n  - name: pos_tags\n    sequence:\n      class_label:\n        names:\n          \'0\': \'"\'\n          \'1\': \'\'\'\'\'\'\n          \'2\': \'#\'\n          \'3\': $\n          \'4\': (\n          \'5\': )\n          \'6\': \',\'\n          \'7\': .\n          \'8\': \':\'\n          \'9\': \'``\'\n          \'10\': CC\n          \'11\': CD\n          \'12\': DT\n          \'13\': EX\n          \'14\': FW\n          \'15\': IN\n          \'16\': JJ\n          \'17\': JJR\n          \'18\': JJS\n          \'19\': LS\n          \'20\': MD\n          \'21\': NN\n          \'22\': NNP\n          \'23\': NNPS\n          \'24\': NNS\n          \'25\': NN|SYM\n          \'26\': PDT\n          \'27\': POS\n          \'28\': PRP\n          \'29\': PRP$\n          \'30\': RB\n          \'31\': RBR\n          \'32\': RBS\n          \'33\': RP\n          \'34\': SYM\n          \'35\': TO\n          \'36\': UH\n          \'37\': VB\n          \'38\': VBD\n          \'39\': VBG\n          \'40\': VBN\n          \'41\': VBP\n          \'42\': VBZ\n          \'43\': WDT\n          \'44\': WP\n          \'45\': WP$\n          \'46\': WRB\n  - name: chunk_tags\n    sequence:\n      class_label:\n        names:\n          \'0\': O\n          \'1\': B-ADJP\n          \'2\': I-ADJP\n          \'3\': B-ADVP\n          \'4\': I-ADVP\n          \'5\': B-CONJP\n          \'6\': I-CONJP\n          \'7\': B-INTJ\n          \'8\': I-INTJ\n          \'9\': B-LST\n          \'10\': I-LST\n          \'11\': B-NP\n          \'12\': I-NP\n          \'13\': B-PP\n          \'14\': I-PP\n          \'15\': B-PRT\n          \'16\': I-PRT\n          \'17\': B-SBAR\n          \'18\': I-SBAR\n          \'19\': B-UCP\n          \'20\': I-UCP\n          \'21\': B-VP\n          \'22\': I-VP\n  - name: ner_tags\n    sequence:\n      class_label:\n        names:\n          \'0\': O\n          \'1\': B-PER\n          \'2\': I-PER\n          \'3\': B-ORG\n          \'4\': I-ORG\n          \'5\': B-LOC\n          \'6\': I-LOC\n          \'7\': B-MISC\n          \'8\': I-MISC\n  config_name: conll2003\n  splits:\n  - name: train\n    num_bytes: 6931345\n    num_examples: 14041\n  - name: validation\n    num_bytes: 1739223\n    num_examples: 3250\n  - name: test\n    num_bytes: 1582054\n    num_examples: 3453\n  download_size: 982975\n  dataset_size: 10252622\ntrain-eval-index:\n- config: conll2003\n  task: token-classification\n  task_id: entity_extraction\n  splits:\n    train_split: train\n    eval_split: test\n  col_mapping:\n    tokens: tokens\n    ner_tags: tags\n  metrics:\n  - type: seqeval\n    name: seqeval\n---\n\n# Dataset Card for "conll2003"\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** [https://www.aclweb.org/anthology/W03-0419/](https://www.aclweb.org/anthology/W03-0419/)\n- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n- **Size of downloaded dataset files:** 4.85 MB\n- **Size of the generated dataset:** 10.26 MB\n- **Total amount of disk used:** 15.11 MB\n\n### Dataset Summary\n\nThe shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\nnot belong to the previous three groups.\n\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\ntagging scheme, whereas the original dataset uses IOB1.\n\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n\n### Supported Tasks and Leaderboards\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Languages\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n## Dataset Structure\n\n### Data Instances\n\n#### conll2003\n\n- **Size of downloaded dataset files:** 4.85 MB\n- **Size of the generated dataset:** 10.26 MB\n- **Total amount of disk used:** 15.11 MB\n\nAn example of \'train\' looks as follows.\n\n```\n{\n    "chunk_tags": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\n    "id": "0",\n    "ner_tags": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    "pos_tags": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\n    "tokens": ["The", "European", "Commission", "said", "on", "Thursday", "it", "disagreed", "with", "German", "advice", "to", "consumers", "to", "shun", "British", "lamb", "until", "scientists", "determine", "whether", "mad", "cow", "disease", "can", "be", "transmitted", "to", "sheep", "."]\n}\n```\n\nThe original data files have `-DOCSTART-` lines used to separate documents, but these lines are removed here.\nIndeed `-DOCSTART-` is a special line that acts as a boundary between two different documents, and it is filtered out in this implementation.\n\n### Data Fields\n\nThe data fields are the same among all splits.\n\n#### conll2003\n- `id`: a `string` feature.\n- `tokens`: a `list` of `string` features.\n- `pos_tags`: a `list` of classification labels (`int`). Full tagset with indices:\n\n```python\n{\'"\': 0, "\'\'": 1, \'#\': 2, \'$\': 3, \'(\': 4, \')\': 5, \',\': 6, \'.\': 7, \':\': 8, \'``\': 9, \'CC\': 10, \'CD\': 11, \'DT\': 12,\n \'EX\': 13, \'FW\': 14, \'IN\': 15, \'JJ\': 16, \'JJR\': 17, \'JJS\': 18, \'LS\': 19, \'MD\': 20, \'NN\': 21, \'NNP\': 22, \'NNPS\': 23,\n \'NNS\': 24, \'NN|SYM\': 25, \'PDT\': 26, \'POS\': 27, \'PRP\': 28, \'PRP$\': 29, \'RB\': 30, \'RBR\': 31, \'RBS\': 32, \'RP\': 33,\n \'SYM\': 34, \'TO\': 35, \'UH\': 36, \'VB\': 37, \'VBD\': 38, \'VBG\': 39, \'VBN\': 40, \'VBP\': 41, \'VBZ\': 42, \'WDT\': 43,\n \'WP\': 44, \'WP$\': 45, \'WRB\': 46}\n```\n\n- `chunk_tags`: a `list` of classification labels (`int`). Full tagset with indices:\n\n```python\n{\'O\': 0, \'B-ADJP\': 1, \'I-ADJP\': 2, \'B-ADVP\': 3, \'I-ADVP\': 4, \'B-CONJP\': 5, \'I-CONJP\': 6, \'B-INTJ\': 7, \'I-INTJ\': 8,\n \'B-LST\': 9, \'I-LST\': 10, \'B-NP\': 11, \'I-NP\': 12, \'B-PP\': 13, \'I-PP\': 14, \'B-PRT\': 15, \'I-PRT\': 16, \'B-SBAR\': 17,\n \'I-SBAR\': 18, \'B-UCP\': 19, \'I-UCP\': 20, \'B-VP\': 21, \'I-VP\': 22}\n```\n\n- `ner_tags`: a `list` of classification labels (`int`). Full tagset with indices:\n\n```python\n{\'O\': 0, \'B-PER\': 1, \'I-PER\': 2, \'B-ORG\': 3, \'I-ORG\': 4, \'B-LOC\': 5, \'I-LOC\': 6, \'B-MISC\': 7, \'I-MISC\': 8}\n```\n\n### Data Splits\n\n|  name   |train|validation|test|\n|---------|----:|---------:|---:|\n|conll2003|14041|      3250|3453|\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n#### Who are the source language producers?\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n#### Who are the annotators?\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Personal and Sensitive Information\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Discussion of Biases\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Other Known Limitations\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Licensing Information\n\nFrom the [CoNLL2003 shared task](https://www.clips.uantwerpen.be/conll2003/ner/) page:\n\n> The English data is a collection of news wire articles from the Reuters Corpus. The annotation has been done by people of the University of Antwerp. Because of copyright reasons we only make available the annotations. In order to build the complete data sets you will need access to the Reuters Corpus. It can be obtained for research purposes without any charge from NIST.\n\nThe copyrights are defined below, from the [Reuters Corpus page](https://trec.nist.gov/data/reuters/reuters.html):\n\n> The stories in the Reuters Corpus are under the copyright of Reuters Ltd and/or Thomson Reuters, and their use is governed by the following agreements:\n>\n> [Organizational agreement](https://trec.nist.gov/data/reuters/org_appl_reuters_v4.html)\n>\n> This agreement must be signed by the person responsible for the data at your organization, and sent to NIST.\n>\n> [Individual agreement](https://trec.nist.gov/data/reuters/ind_appl_reuters_v4.html)\n>\n> This agreement must be signed by all researchers using the Reuters Corpus at your organization, and kept on file at your organization.\n\n### Citation Information\n\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",\n    author = "Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien",\n    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",\n    year = "2003",\n    url = "https://www.aclweb.org/anthology/W03-0419",\n    pages = "142--147",\n}\n\n```\n\n\n### Contributions\n\nThanks to [@jplu](https://github.com/jplu), [@vblagoje](https://github.com/vblagoje), [@lhoestq](https://github.com/lhoestq) for adding this dataset.', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code=None)])) experiment_code=None

## Experiment Runs

- Run ID: proposed-iter1-CoNLL-2003
  Method: proposed
  Model: None
  Dataset: CoNLL-2003

- Run ID: comparative-1-iter1-CoNLL-2003
  Method: comparative-1
  Model: None
  Dataset: CoNLL-2003


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-iter1-CoNLL-2003",
      "run_config_yaml": "run_id: proposed-iter1-CoNLL-2003\nmethod: proposed\nmodel:\n  host_model_name: distilbert-base-uncased\n  hidden_size: 768\n  num_layers: 6\n  max_position_embeddings: 512\n  vocab_size: 30522\nadapter:\n  name: UniHyperLoRA\n  atoms: 128\n  anchor_dim: 256\n  router_top_k: 2\n  quant_bits: 4\n  freeze_base_adapter: true\n  train_bias_delta_only: true\n  bias_bits: 4\n  lambda_F: 0.05\n  lambda_Q: 1.0e-5\ndataset:\n  name: conll2003\n  hf_id: eriktks/conll2003\n  splits:\n    train: train\n    validation: validation\n    test: test\n  max_length: 128\ntraining:\n  optimizer: adamw\n  learning_rate: 2.0e-4\n  batch_size: 32\n  max_steps: 500\n  weight_decay: 0.0\n  gradient_accumulation_steps: 1\n  lr_scheduler: linear\n  warmup_steps: 50\n  seed: 42\n  update_param_groups:\n    - adapter.bias_delta\nevaluation:\n  eval_steps: 100\n  metric: seqeval\nbudget_constraints:\n  max_extra_params_mb: 1.0\n  max_flop_overhead_pct: 10\noptuna:\n  n_trials: 25\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 5.0e-4\n    lambda_F:\n      type: uniform\n      low: 0.01\n      high: 0.1\n    lambda_Q:\n      type: loguniform\n      low: 1.0e-6\n      high: 1.0e-4\n    router_top_k:\n      type: categorical\n      choices: [1, 2]\nhardware:\n  accelerator: gpu\n  precision: bf16\n  devices: 1\n"
    },
    {
      "run_id": "comparative-1-iter1-CoNLL-2003",
      "run_config_yaml": "run_id: comparative-1-iter1-CoNLL-2003\nmethod: comparative-1\nmodel:\n  host_model_name: distilbert-base-uncased\n  hidden_size: 768\n  num_layers: 6\nadapter:\n  name: SAT-LoRA\n  lora_rank: 8\n  lora_alpha: 16\n  target_modules: [q_proj, k_proj, v_proj, o_proj, ffn_down, ffn_up]\n  share_across_layers: false\n  precision: int8\ndataset:\n  name: conll2003\n  hf_id: eriktks/conll2003\n  splits:\n    train: train\n    validation: validation\n    test: test\n  max_length: 128\ntraining:\n  optimizer: adamw\n  learning_rate: 3.0e-4\n  batch_size: 32\n  max_steps: 3000\n  weight_decay: 0.01\n  gradient_accumulation_steps: 1\n  lr_scheduler: linear\n  warmup_steps: 300\n  seed: 42\n  update_param_groups:\n    - adapter.lora\nevaluation:\n  eval_steps: 200\n  metric: seqeval\nbudget_constraints:\n  max_extra_params_mb: 32\n  max_flop_overhead_pct: 10\noptuna:\n  n_trials: 25\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 1.0e-3\n    lora_rank:\n      type: int\n      low: 4\n      high: 16\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\nhardware:\n  accelerator: gpu\n  precision: bf16\n  devices: 1\n"
    }
  ]
}
