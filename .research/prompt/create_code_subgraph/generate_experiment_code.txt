
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/runs/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
      ```json
      {
        "primary_metric": "Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).",
        "metrics": {
          "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
          "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
        },
        "best_proposed": {
          "run_id": "proposed-iter2-model-dataset",
          "value": 0.92
        },
        "best_baseline": {
          "run_id": "comparative-1-model-dataset",
          "value": 0.88
        },
        "gap": 4.55
      }
      ```
      The structure includes:
      - "primary_metric": The primary evaluation metric name from the hypothesis (Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).)
      - "metrics": All collected metrics organized by metric name, then by run_id
      - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
      - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
      - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
        * Use the expected results from the hypothesis (1) Zero-shot: without any extra tuning UniHyperLoRA transfers to BERT-large with <1 % accuracy loss vs source model. 2) After 500 update steps on each task, UniHyperLoRA matches SAT-LoRA that was *retrained* for that architecture while using 97 % less additional memory (0.97 MB vs 33 MB) and identical FLOPs. 3) On TinyBERT the projector correctly shrinks atoms; accuracy +1.1 % over plain LoRA under the same budget.) to determine metric direction
        * If the metric should be minimized (like "loss", "perplexity", "error"), reverse the sign of the gap
        * The gap represents the percentage improvement of the proposed method over the best baseline
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251106-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: The code will not execute successfully in its current state. Major blocking issues:

1. evaluate.py – torch is used before it is imported.
   _generate_predictions() calls torch.load() and builds tensors before the local `import torch`, producing a NameError.

2. evaluate.py / model wrapping – build_model_with_adapters replaces Linear layers via setattr even when the parent module is a ModuleList. For ModuleList parents this must use parent[int(child_name)] = wrapped; setattr will raise AttributeError.

3. File-structure compliance – import statements such as `import src.preprocess` require src to be a package. Because the required file list forbids an __init__.py, `python -m src.train` will fail on Python ≤3.10 unless src is declared a namespace package. Add an empty __init__.py or change imports to relative paths.

4. Hydra usage – run-specific YAMLs are loaded manually with OmegaConf instead of Hydra config groups. This breaks `python -m src.train run=<id>` because Hydra cannot locate the group `run`. Either add a Hydra search-path plugin or move the loader into Hydra’s config system.

5. WandB logging completeness – only train_loss, eval_* are logged; learning-rate, gradient norm, memory usage etc. are omitted, so the requirement to log “ALL relevant time-series metrics” is not met.

6. Trial mode reduction – dataset size is not subsampled; very small max_steps (2) is set but loaders still iterate full datasets, violating the “fast validation” intent.

At minimum issues (1)–(3) must be fixed before the code can run; issues (4)–(6) are specification violations that block publication readiness. Therefore the code is NOT ready for immediate execution.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "defaults:\n  - _self_\n\n# CLI-supplied ----------------------------------------------------------\nrun: \"\"\nresults_dir: outputs\nmode: full  # full | trial\n\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online  # auto-switched to disabled in trial mode\n\noptuna:\n  n_trials: 0  # run-specific YAML can override\n  direction: maximize\n  search_space: {}\n\nevaluation:\n  metric: accuracy  # can be overwritten by run cfg (e.g., seqeval)\n  eval_steps: 50\n\ntraining:\n  seed: 42\n  batch_size: 32\n  max_steps: 1000\n  epochs: 3\n  learning_rate: 5.0e-5\n  weight_decay: 0.01\n\n# Hydrated by run yaml -------------------------------------------------\nmethod: \"\"\nmodel: {}\nadapter: {}\ndataset: {}\nbudget_constraints: {}", "evaluate_py": "import argparse\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Any\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import (\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    accuracy_score,\n)\n\nfrom src.preprocess import build_dataloaders\nfrom src.model import build_model_with_adapters\n\nPRIMARY_METRIC_STRING = (\n    \"Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).\"\n)\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _load_wandb_credentials():\n    cfg_file = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    cfg = OmegaConf.load(cfg_file)\n    return cfg.wandb.entity, cfg.wandb.project\n\n\ndef _sanitize(s: str) -\u003e str:\n    \"\"\"Return a safe filename component (no slashes, spaces, etc.).\"\"\"\n    return re.sub(r\"[^A-Za-z0-9\\-_]\", \"_\", s)\n\n\ndef _fetch_run(rid: str, entity: str, project: str):\n    api = wandb.Api()\n    return api.run(f\"{entity}/{project}/{rid}\")\n\n\n# -----------------------------------------------------------------------------\n# In-depth run processing helpers\n# -----------------------------------------------------------------------------\n\ndef _reconstruct_cfg(wandb_cfg: Dict[str, Any]) -\u003e OmegaConf:\n    \"\"\"Convert the flattened wandb config dict back into OmegaConf style.\"\"\"\n    # wandb flattens nested dicts with \u0027.\u0027\n    cfg_nested: Dict[str, Any] = {}\n    for k, v in wandb_cfg.items():\n        cur = cfg_nested\n        parts = k.split(\".\")\n        for p in parts[:-1]:\n            cur = cur.setdefault(p, {})\n        cur[parts[-1]] = v\n    return OmegaConf.create(cfg_nested)\n\n\ndef _generate_predictions(\n    cfg: OmegaConf, model_state_path: str\n) -\u003e Tuple[np.ndarray, np.ndarray, List[str]]:\n    \"\"\"Load model weights and dataset, then produce predictions (validation split).\"\"\"\n    # Ensure batch size present for dataloader construction\n    if \"training\" not in cfg or \"batch_size\" not in cfg.training:\n        cfg.training = cfg.get(\"training\", {})\n        cfg.training[\"batch_size\"] = 32\n    device = \"cpu\"  # evaluation runs on CPU to remain lightweight\n\n    # Build loaders \u0026 model ----------------------------------------------------\n    loaders = build_dataloaders(cfg, CACHE_DIR)\n    _, val_loader, label_list = loaders\n    num_labels = len(label_list)\n\n    model = build_model_with_adapters(cfg, num_labels=num_labels, device=device)\n    state_dict = torch.load(model_state_path, map_location=device)\n    model.load_state_dict(state_dict, strict=False)\n    model.eval()\n\n    preds_all: List[int] = []\n    labels_all: List[int] = []\n\n    import torch  # local import to avoid GPU initialisation until here\n\n    with torch.no_grad():\n        for batch in val_loader:\n            labels = batch.pop(\"labels\")\n            batch = {k: v for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            preds = logits.argmax(-1)\n            if cfg.evaluation.metric == \"seqeval\":\n                for p_seq, l_seq in zip(preds, labels):\n                    for p_id, l_id in zip(p_seq.tolist(), l_seq.tolist()):\n                        if l_id == -100:\n                            continue\n                        preds_all.append(p_id)\n                        labels_all.append(l_id)\n            else:\n                preds_all.extend(preds.tolist())\n                labels_all.extend(labels.tolist())\n    return np.array(preds_all), np.array(labels_all), label_list\n\n\ndef _plot_confusion_matrix(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    labels: List[str],\n    save_path: Path,\n):\n    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=labels,\n        yticklabels=labels,\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# STEP-1: Per-run processing\n# -----------------------------------------------------------------------------\n\ndef _export_metrics_and_figures(run, save_dir: Path) -\u003e Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Export history \u0026 generate per-run figures. Returns (preds, labels) if computed.\"\"\"\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------------------------------------------------------\n    # 1. Save history \u0026 summary JSON\n    # ----------------------------------------------------------------------------\n    history_df = run.history()\n    summary_dict = dict(run.summary._json_dict)\n    cfg_dict = dict(run.config)\n\n    with open(save_dir / \"metrics.json\", \"w\") as fp:\n        json.dump(\n            {\n                \"summary\": summary_dict,\n                \"config\": cfg_dict,\n            },\n            fp,\n            indent=2,\n        )\n    history_df.to_parquet(save_dir / \"history.parquet\")\n\n    # ----------------------------------------------------------------------------\n    # 2. Learning-curve figure\n    # ----------------------------------------------------------------------------\n    fig_fp = save_dir / f\"{_sanitize(run.id)}_learning_curve.pdf\"\n    plt.figure()\n    metric_cols = [\n        c for c in history_df.columns if re.match(r\"(train|eval)_.*\", str(c))\n    ]\n    for col in metric_cols:\n        sns.lineplot(x=history_df.index, y=history_df[col], label=col)\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Metric value\")\n    plt.title(run.id)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(fig_fp)\n    plt.close()\n    print(f\"Saved figure: {fig_fp}\")\n\n    # ----------------------------------------------------------------------------\n    # 3. Confusion matrix (requires model artifact)\n    # ----------------------------------------------------------------------------\n    preds_np = labels_np = None  # in case generation fails\n    try:\n        art = next(\n            a for a in run.logged_artifacts() if a.type == \"model\"\n        )\n        model_dir = art.download()\n        state_path = next(Path(model_dir).glob(\"*.pth\"))\n        cfg = _reconstruct_cfg(cfg_dict)\n        preds_np, labels_np, label_list = _generate_predictions(cfg, str(state_path))\n\n        cm_path = save_dir / f\"{_sanitize(run.id)}_confusion_matrix.pdf\"\n        _plot_confusion_matrix(labels_np, preds_np, label_list, cm_path)\n        print(f\"Saved figure: {cm_path}\")\n\n        # Save raw preds / labels for downstream tests\n        np.save(save_dir / \"preds.npy\", preds_np)\n        np.save(save_dir / \"labels.npy\", labels_np)\n    except StopIteration:\n        print(f\"[WARN] No model artifact found for run {run.id}; skipping confusion matrix.\")\n    except Exception as e:\n        print(f\"[WARN] Failed to generate confusion matrix for {run.id}: {e}\")\n\n    return preds_np, labels_np\n\n\n# -----------------------------------------------------------------------------\n# STEP-2: Aggregated analysis \u0026 comparison\n# -----------------------------------------------------------------------------\n\ndef _mcnemar(y1: np.ndarray, y2: np.ndarray, y_true: np.ndarray) -\u003e Dict[str, Any]:\n    \"\"\"McNemar\u0027s test between two classifiers.\"\"\"\n    assert y1.shape == y2.shape == y_true.shape\n    both_correct = (y1 == y_true) \u0026 (y2 == y_true)\n    n01 = ((y1 == y_true) \u0026 (y2 != y_true)).sum()\n    n10 = ((y1 != y_true) \u0026 (y2 == y_true)).sum()\n    from math import comb\n\n    # Exact binomial test (two-tailed)\n    n = n01 + n10\n    if n == 0:\n        p_val = 1.0\n    else:\n        k = min(n01, n10)\n        p_val = 2 * sum(comb(n, i) * (0.5 ** n) for i in range(0, k + 1))\n        p_val = min(1.0, p_val)\n    return {\"n01\": int(n01), \"n10\": int(n10), \"p_value\": float(p_val)}\n\n\ndef _aggregate(runs: Dict[str, wandb.apis.public.Run], out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    metrics_by_name: Dict[str, Dict[str, float]] = {}\n    preds_map: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}\n\n    # Collect numerical summary metrics and preds -----------------------------\n    for rid, run in runs.items():\n        summary = run.summary._json_dict\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                metrics_by_name.setdefault(k, {})[rid] = v\n        # load preds / labels if saved\n        run_dir = out_dir.parent / _sanitize(rid)\n        preds_fp = run_dir / \"preds.npy\"\n        labels_fp = run_dir / \"labels.npy\"\n        if preds_fp.exists() and labels_fp.exists():\n            preds_map[rid] = (np.load(preds_fp), np.load(labels_fp))\n\n    # Choose primary metric ---------------------------------------------------\n    pref_keys = [\n        \"final_f1\",\n        \"final_accuracy\",\n        \"f1\",\n        \"accuracy\",\n        \"eval_f1\",\n    ]\n    selected_key = next((k for k in pref_keys if k in metrics_by_name), None)\n    if selected_key is None:\n        raise RuntimeError(\"Cannot find a common metric to compare across runs.\")\n\n    proposed_vals = {\n        rid: v for rid, v in metrics_by_name[selected_key].items() if \"proposed\" in rid\n    }\n    baseline_vals = {\n        rid: v\n        for rid, v in metrics_by_name[selected_key].items()\n        if any(tok in rid for tok in [\"baseline\", \"comparative\"])\n    }\n\n    best_proposed_id = max(proposed_vals, key=proposed_vals.get)\n    best_baseline_id = max(baseline_vals, key=baseline_vals.get)\n    best_proposed_val = proposed_vals[best_proposed_id]\n    best_baseline_val = baseline_vals[best_baseline_id]\n    gap_pct = (best_proposed_val - best_baseline_val) / best_baseline_val * 100.0\n\n    aggregate = {\n        \"primary_metric\": PRIMARY_METRIC_STRING,\n        \"metrics\": metrics_by_name,\n        \"best_proposed\": {\"run_id\": best_proposed_id, \"value\": best_proposed_val},\n        \"best_baseline\": {\"run_id\": best_baseline_id, \"value\": best_baseline_val},\n        \"gap\": gap_pct,\n    }\n\n    # Statistical significance -----------------------------------------------\n    if best_proposed_id in preds_map and best_baseline_id in preds_map:\n        yp, yt = preds_map[best_proposed_id]\n        yb, _ = preds_map[best_baseline_id]\n        sig = _mcnemar(yp, yb, yt)\n        aggregate[\"stat_tests\"] = {\"mcnemar\": sig}\n    else:\n        aggregate[\"stat_tests\"] = {\"mcnemar\": \"NA\"}\n\n    out_fp = out_dir / \"aggregated_metrics.json\"\n    with open(out_fp, \"w\") as fp:\n        json.dump(aggregate, fp, indent=2)\n    print(f\"Aggregated metrics saved \u2192 {out_fp}\")\n\n    # ------------------------------------------------------------------------\n    # Comparison figures: bar chart for primary metric \u0026 boxplot for all metrics\n    # ------------------------------------------------------------------------\n    # Bar chart ---------------------------------------------------------------\n    bar_fig = out_dir / f\"comparison_{_sanitize(selected_key)}_bar_chart.pdf\"\n    plt.figure(figsize=(10, 6))\n    sns.barplot(\n        x=list(metrics_by_name[selected_key].keys()),\n        y=list(metrics_by_name[selected_key].values()),\n        palette=\"viridis\",\n    )\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(selected_key)\n    plt.title(\"Comparison on \" + selected_key)\n    for i, v in enumerate(metrics_by_name[selected_key].values()):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    plt.savefig(bar_fig)\n    plt.close()\n    print(f\"Saved figure: {bar_fig}\")\n\n    # Box plots for all numeric metrics --------------------------------------\n    metrics_df = pd.DataFrame(metrics_by_name)\n    melted = metrics_df.melt(var_name=\"metric\", value_name=\"value\", ignore_index=False)\n    melted = melted.reset_index().rename(columns={\"index\": \"run_id\"})\n    box_fig = out_dir / \"comparison_all_metrics_boxplot.pdf\"\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(data=melted, x=\"metric\", y=\"value\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Metric distribution across runs\")\n    plt.tight_layout()\n    plt.savefig(box_fig)\n    plt.close()\n    print(f\"Saved figure: {box_fig}\")\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    p = argparse.ArgumentParser(\n        description=\"Evaluate multiple wandb runs and generate comparison reports.\"\n    )\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\n        \"run_ids\",\n        type=str,\n        help=\"JSON string list of run IDs, e.g. \u0027[\\\"run1\\\", \\\"run2\\\"]\u0027\",\n    )\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = _parse_args()\n    results_root = Path(args.results_dir)\n    run_id_list: List[str] = json.loads(args.run_ids)\n\n    entity, project = _load_wandb_credentials()\n\n    runs: Dict[str, wandb.apis.public.Run] = {}\n    for rid in run_id_list:\n        run = _fetch_run(rid, entity, project)\n        runs[rid] = run\n        _export_metrics_and_figures(run, results_root / _sanitize(rid))\n\n    _aggregate(runs, results_root / \"comparison\")", "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    if not cfg.run:\n        raise ValueError(\n            \"Usage: python -m src.main run=\u003crun_id\u003e results_dir=\u003cdir\u003e mode=\u003ctrial|full\u003e\"\n        )\n\n    overrides = [f\"run={cfg.run}\", f\"results_dir={cfg.results_dir}\", f\"mode={cfg.mode}\"]\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    print(\"\ud83d\ude80 Launching:\", \" \".join(cmd))\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "import math\nfrom typing import List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import AutoConfig, AutoModelForTokenClassification\nfrom omegaconf import DictConfig\n\n__all__ = [\n    \"build_model_with_adapters\",\n    \"compute_adapter_params\",\n]\n\n# -----------------------------------------------------------------------------\n# Baseline LoRA wrapper\n# -----------------------------------------------------------------------------\n\nclass LoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.0):\n        super().__init__()\n        self.base = base\n        self.r = r\n        self.dropout = nn.Dropout(dropout) if dropout \u003e 0.0 else nn.Identity()\n        if r \u003e 0:\n            self.lora_a = nn.Linear(base.in_features, r, bias=False)\n            self.lora_b = nn.Linear(r, base.out_features, bias=False)\n            nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_b.weight)\n            self.scaling = alpha / r\n        else:\n            self.lora_a = self.lora_b = None\n            self.scaling = 1.0\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        out = self.base(x)\n        if self.r \u003e 0:\n            out = out + self.lora_b(self.lora_a(self.dropout(x))) * self.scaling\n        return out\n\n\n# -----------------------------------------------------------------------------\n# Proposed UniHyperLoRA components\n# -----------------------------------------------------------------------------\n\nclass DimAwareProjector(nn.Module):\n    def __init__(self, d0: int = 256):\n        super().__init__()\n        self.d0 = d0\n        self.mlp = nn.Sequential(\n            nn.Linear(1, 64), nn.SiLU(), nn.Linear(64, d0 * 2)\n        )\n\n    def forward(self, dim: int) -\u003e torch.Tensor:\n        inp = torch.log10(\n            torch.tensor([[float(dim)]], dtype=torch.float32, device=self.mlp[0].weight.device)\n        )\n        vec = self.mlp(inp).view(2, self.d0)\n        row, col = vec[0], vec[1]\n        return torch.outer(torch.ones(dim, device=row.device), row) + torch.outer(\n            torch.arange(1, dim + 1, dtype=row.dtype, device=row.device), col\n        )\n\n\nclass SharedUniHyperLoRA(nn.Module):\n    def __init__(self, M: int = 128, d0: int = 256, router_top_k: int = 2):\n        super().__init__()\n        self.M = M\n        self.d0 = d0\n        self.router_top_k = router_top_k\n        self.U = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.V = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.dim_proj = DimAwareProjector(d0)\n        self.mixer = nn.Sequential(\n            nn.Linear(2, 128), nn.SiLU(), nn.Linear(128, M)\n        )\n\n\nclass UniHyperLoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, shared: SharedUniHyperLoRA, depth_norm: float, layer_type_id: int):\n        super().__init__()\n        self.base = base\n        self.shared = shared\n        for p in self.base.parameters():\n            p.requires_grad = False\n        self.register_buffer(\"depth_norm\", torch.tensor([depth_norm], dtype=torch.float32))\n        self.register_buffer(\"type_id\", torch.tensor([layer_type_id / 4.0], dtype=torch.float32))\n\n    def forward(self, x):\n        out = self.base(x)\n        d_out, d_in = self.base.out_features, self.base.in_features\n        P_out = self.shared.dim_proj(d_out).to(x.device)\n        P_in = self.shared.dim_proj(d_in).to(x.device).t()\n        gates = self.shared.mixer(torch.cat([self.depth_norm, self.type_id]).unsqueeze(0))[0]\n        vals, idx = torch.topk(gates, k=self.shared.router_top_k)\n        delta_W = torch.zeros(d_out, d_in, device=x.device)\n        for val, j in zip(vals, idx):\n            delta_W += val * (\n                P_out @ torch.outer(self.shared.U[j], self.shared.V[j]) @ P_in\n            )\n        return out + F.linear(x, delta_W, bias=None)\n\n\n# -----------------------------------------------------------------------------\n# Helper utils\n# -----------------------------------------------------------------------------\n\ndef _get_parent_module(model: nn.Module, module_name: str):\n    parts = module_name.split(\".\")\n    parent = model\n    for p in parts[:-1]:\n        parent = parent[int(p)] if p.isdigit() else getattr(parent, p)\n    return parent\n\n\ndef compute_adapter_params(model: nn.Module) -\u003e float:\n    bytes_total = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return bytes_total / (1024 ** 2)  # MB\n\n\n# -----------------------------------------------------------------------------\n# Build model with adapters\n# -----------------------------------------------------------------------------\n\ndef build_model_with_adapters(cfg: DictConfig, num_labels: int, device):\n    model_name = cfg.model.host_model_name\n\n    base_cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=\".cache/\")\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, config=base_cfg, cache_dir=\".cache/\"\n    )\n\n    if cfg.method.startswith(\"proposed\") or cfg.adapter.name.lower() == \"unihyperlora\":\n        shared = SharedUniHyperLoRA(\n            M=cfg.adapter.atoms,\n            d0=cfg.adapter.anchor_dim,\n            router_top_k=cfg.adapter.router_top_k,\n        )\n        linear_modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n        total = len(linear_modules)\n        idx = 0\n        for name, module in list(model.named_modules()):\n            if isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                depth = idx / max(1, total - 1)\n                wrapped = UniHyperLoRALinear(module, shared, depth, layer_type_id=0)\n                setattr(parent, child_name, wrapped)\n                idx += 1\n    elif cfg.method.startswith(\"comparative\") or cfg.adapter.name.lower() == \"sat-lora\":\n        targets: List[str] = cfg.adapter.target_modules\n        for name, module in list(model.named_modules()):\n            if any(t in name for t in targets) and isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                wrapped = LoRALinear(\n                    module,\n                    r=cfg.adapter.lora_rank,\n                    alpha=cfg.adapter.lora_alpha,\n                )\n                setattr(parent, child_name, wrapped)\n    else:\n        raise ValueError(f\"Unknown adapter type: {cfg.adapter.name}\")\n\n    model.to(device)\n    return model", "preprocess_py": "from typing import Tuple, List, Dict\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n)\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Token-classification helpers\n# -----------------------------------------------------------------------------\n\ndef _tokenize_align_labels(examples, tokenizer, label_all_tokens: bool):\n    tokenized = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=tokenizer.model_max_length,\n    )\n    labels = []\n    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_seq[word_idx])\n            else:\n                label_ids.append(label_seq[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized[\"labels\"] = labels\n    return tokenized\n\n\n# -----------------------------------------------------------------------------\n# Data-loading entry point\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg: DictConfig, cache_dir: str):\n    ds_cfg = cfg.dataset\n    tokenizer = AutoTokenizer.from_pretrained(\n        cfg.model.host_model_name, cache_dir=cache_dir, use_fast=True\n    )\n\n    task_name = ds_cfg.name.lower()\n    if task_name == \"conll2003\":\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n        train_ds = train_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=train_ds.column_names,\n        )\n        val_ds = val_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=val_ds.column_names,\n        )\n        collator = DataCollatorForTokenClassification(tokenizer)\n    else:\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n\n        def tok_fn(ex):\n            out = tokenizer(ex[\"text\"], truncation=True, max_length=ds_cfg.max_length)\n            out[\"labels\"] = ex[\"label\"]\n            return out\n\n        train_ds = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names)\n        val_ds = val_ds.map(tok_fn, batched=True, remove_columns=val_ds.column_names)\n        collator = DataCollatorWithPadding(tokenizer)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=True,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.batch_size,\n        shuffle=False,\n        collate_fn=collator,\n    )\n\n    if task_name == \"conll2003\":\n        label_list = raw[\"train\"].features[\"ner_tags\"].feature.names\n    else:\n        label_list = raw[\"train\"].features[\"label\"].names\n    return train_loader, val_loader, label_list\n\n\ndef get_num_labels(cfg: DictConfig):\n    ds_cfg = cfg.dataset\n    if ds_cfg.name.lower() == \"conll2003\":\n        raw = load_dataset(ds_cfg.hf_id, split=ds_cfg.splits.train)\n        label_list = raw.features[\"ner_tags\"].feature.names\n    else:\n        raw = load_dataset(ds_cfg.hf_id, split=ds_cfg.splits.train)\n        label_list = raw.features[\"label\"].names\n    return len(label_list)", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"unihyperlora-experiments\"\nversion = \"0.1.0\"\ndescription = \"Universal Hyper-LoRA experiments managed by Hydra\"\nrequires-python = \"\u003e=3.10\"\ndependencies = [\n  \"torch\u003e=2.1.0\",\n  \"transformers\u003e=4.36.0\",\n  \"datasets\u003e=2.14.0\",\n  \"hydra-core\u003e=1.3.2\",\n  \"wandb\u003e=0.16.0\",\n  \"optuna\u003e=3.4.0\",\n  \"evaluate\u003e=0.4.1\",\n  \"seqeval\u003e=1.2.2\",\n  \"scikit-learn\u003e=1.3.0\",\n  \"matplotlib\u003e=3.8.0\",\n  \"seaborn\u003e=0.13.0\",\n  \"tqdm\u003e=4.66.0\",\n  \"pandas\u003e=2.1.0\",\n]", "train_py": "import os\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport hydra\nfrom omegaconf import OmegaConf, DictConfig\n\nimport wandb\nimport optuna\n\nfrom src.preprocess import build_dataloaders, get_num_labels\nfrom src.model import (\n    build_model_with_adapters,\n    compute_adapter_params,\n)\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    import random, numpy as np\n\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _write_cfg(cfg: DictConfig, out_dir: str):\n    cfg_path = Path(out_dir) / f\"{cfg.run}_final_cfg.yaml\"\n    cfg_path.parent.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(config=cfg, f=str(cfg_path))\n\n\ndef _prepare_cfg(cfg: DictConfig) -\u003e DictConfig:\n    \"\"\"Merge the base Hydra cfg with run-specific YAML and apply mode overrides.\"\"\"\n\n    run_cfg_fp = (\n        Path(__file__).resolve().parent.parent\n        / \"config\"\n        / \"runs\"\n        / f\"{cfg.run}.yaml\"\n    )\n    if not run_cfg_fp.exists():\n        raise FileNotFoundError(\n            f\"No run configuration found for id={cfg.run}: {run_cfg_fp}\"\n        )\n\n    run_cfg = OmegaConf.load(run_cfg_fp)\n    cfg = OmegaConf.merge(cfg, run_cfg)\n\n    # Mode-specific tweaks ------------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.max_steps = 2\n        cfg.training.epochs = 1\n        cfg.evaluation.eval_steps = 1\n        cfg.training.batch_size = min(cfg.training.batch_size, 2)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    cfg.results_dir = os.path.abspath(cfg.results_dir)\n    os.makedirs(cfg.results_dir, exist_ok=True)\n\n    _write_cfg(cfg, cfg.results_dir)\n    return cfg\n\n\n# -----------------------------------------------------------------------------\n# Evaluation helper\n# -----------------------------------------------------------------------------\n\ndef _evaluate(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    device: torch.device,\n    metric_name: str,\n) -\u003e Dict[str, float]:\n    import evaluate as hf_evaluate\n\n    metric = hf_evaluate.load(metric_name, cache_dir=CACHE_DIR)\n    model.eval()\n\n    id2label: Dict[int, str] = {\n        int(k): v for k, v in getattr(model.config, \"id2label\", {}).items()\n    }\n\n    for batch in dataloader:\n        labels = batch.pop(\"labels\").to(device)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            logits = model(**batch).logits\n        preds = logits.argmax(-1).cpu()\n        labels_cpu = labels.cpu()\n\n        if metric_name == \"seqeval\":\n            preds_list: List[List[str]] = []\n            refs_list: List[List[str]] = []\n            for pred_seq, label_seq in zip(preds, labels_cpu):\n                p_sent: List[str] = []\n                l_sent: List[str] = []\n                for p_id, l_id in zip(pred_seq.tolist(), label_seq.tolist()):\n                    if l_id == -100:\n                        continue\n                    p_sent.append(id2label.get(p_id, str(p_id)))\n                    l_sent.append(id2label.get(l_id, str(l_id)))\n                preds_list.append(p_sent)\n                refs_list.append(l_sent)\n            metric.add_batch(predictions=preds_list, references=refs_list)\n        else:\n            metric.add_batch(predictions=preds, references=labels_cpu)\n\n    result = metric.compute()\n    if metric_name == \"seqeval\":\n        return {\n            \"f1\": result.get(\"overall_f1\", 0.0),\n            \"accuracy\": result.get(\"overall_accuracy\", 0.0),\n        }\n    if isinstance(result, dict):\n        return result\n    return {metric_name: float(result)}\n\n\n# -----------------------------------------------------------------------------\n# Training utilities\n# -----------------------------------------------------------------------------\n\ndef _training_step(\n    model: torch.nn.Module,\n    batch: Dict[str, torch.Tensor],\n    optimizer: torch.optim.Optimizer,\n    scheduler: Optional[torch.optim.lr_scheduler.LambdaLR],\n    device: torch.device,\n    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n) -\u003e float:\n    model.train()\n    labels = batch.pop(\"labels\").to(device)\n    batch = {k: v.to(device) for k, v in batch.items()}\n\n    if scaler is not None:\n        with torch.cuda.amp.autocast():\n            loss = model(**batch, labels=labels).loss\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        loss = model(**batch, labels=labels).loss\n        loss.backward()\n        optimizer.step()\n\n    optimizer.zero_grad(set_to_none=True)\n    if scheduler is not None:\n        scheduler.step()\n    return loss.item()\n\n\n# -----------------------------------------------------------------------------\n# Optuna helpers\n# -----------------------------------------------------------------------------\n\ndef _suggest_and_apply(trial: optuna.Trial, cfg: DictConfig):\n    search_space: Dict[str, Any] = cfg.optuna.get(\"search_space\", {})\n    for path, space in search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            value = trial.suggest_float(path, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            value = trial.suggest_float(path, space[\"low\"], space[\"high\"], log=False)\n        elif space[\"type\"] == \"categorical\":\n            value = trial.suggest_categorical(path, space[\"choices\"])\n        elif space[\"type\"] == \"int\":\n            value = trial.suggest_int(path, space[\"low\"], space[\"high\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna space type {space[\u0027type\u0027]}\")\n        OmegaConf.update(cfg, path, value, merge=True)\n\n\ndef _objective(\n    trial: optuna.Trial,\n    cfg: DictConfig,\n    model_init_fn,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    device: torch.device,\n):\n    cfg_tmp = OmegaConf.deepcopy(cfg)\n    _suggest_and_apply(trial, cfg_tmp)\n\n    model = model_init_fn(cfg_tmp)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg_tmp.training.learning_rate,\n        weight_decay=cfg_tmp.training.weight_decay,\n    )\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    max_steps = min(100, cfg_tmp.training.max_steps)\n    eval_every = max(10, max_steps // 5)\n\n    train_iter = iter(train_loader)\n    best_val = -float(\"inf\")\n    step = 0\n    while step \u003c max_steps:\n        try:\n            batch = next(train_iter)\n        except StopIteration:\n            train_iter = iter(train_loader)\n            batch = next(train_iter)\n\n        loss = _training_step(model, batch, optimizer, None, device, scaler)\n        step += 1\n        if step % eval_every == 0 or step == max_steps:\n            metrics = _evaluate(model, val_loader, device, cfg_tmp.evaluation.metric)\n            key = \"f1\" if \"f1\" in metrics else list(metrics.keys())[0]\n            val_score = metrics[key]\n            trial.report(val_score, step)\n            best_val = max(best_val, val_score)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    return best_val\n\n\n# -----------------------------------------------------------------------------\n# wandb helpers\n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        dir=cfg.results_dir,\n    )\n    print(f\"wandb URL: {run.url}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    cfg = _prepare_cfg(cfg)\n    set_seed(cfg.training.seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_loader, val_loader, label_list = build_dataloaders(cfg, CACHE_DIR)\n    num_labels = len(label_list)\n\n    def model_init(local_cfg: DictConfig):\n        return build_model_with_adapters(local_cfg, num_labels, device)\n\n    # Optuna hyper-parameter search -------------------------------------------\n    if cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(\n            lambda trial: _objective(\n                trial, cfg, model_init, train_loader, val_loader, device\n            ),\n            n_trials=cfg.optuna.n_trials,\n        )\n        for k, v in study.best_trial.params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n        print(\n            f\"Optuna best trial {study.best_trial.number}: value={study.best_value:.4f} params={study.best_trial.params}\"\n        )\n\n    # Final training ----------------------------------------------------------\n    model = model_init(cfg)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    lr_lambda = lambda cur: max(0.0, 1.0 - (cur / cfg.training.max_steps))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    wb_run = _init_wandb(cfg)\n    if wb_run is not None:\n        wb_run.summary[\"adapter_param_MB\"] = compute_adapter_params(model)\n\n    global_step = 0\n    pbar = tqdm(total=cfg.training.max_steps, desc=\"Training\")\n    while global_step \u003c cfg.training.max_steps:\n        for batch in train_loader:\n            loss_val = _training_step(\n                model, batch, optimizer, scheduler, device, scaler\n            )\n            global_step += 1\n            pbar.update(1)\n            if wb_run is not None:\n                wandb.log({\"train_loss\": loss_val, \"step\": global_step})\n\n            if (\n                global_step % cfg.evaluation.eval_steps == 0\n                or global_step == cfg.training.max_steps\n            ):\n                metrics = _evaluate(\n                    model, val_loader, device, cfg.evaluation.metric\n                )\n                if wb_run is not None:\n                    wandb.log(\n                        {f\"eval_{k}\": v for k, v in metrics.items()},\n                        step=global_step,\n                    )\n            if global_step \u003e= cfg.training.max_steps:\n                break\n    pbar.close()\n\n    final_metrics = _evaluate(model, val_loader, device, cfg.evaluation.metric)\n    print(f\"Final metrics: {final_metrics}\")\n\n    # ---------------------------------------------------------------------\n    # Save model checkpoint as wandb artifact so that evaluate.py can load it\n    # ---------------------------------------------------------------------\n    if wb_run is not None:\n        for k, v in final_metrics.items():\n            wb_run.summary[f\"final_{k}\"] = v\n\n        model_fp = Path(cfg.results_dir) / f\"{cfg.run}_final_model.pth\"\n        torch.save(model.state_dict(), model_fp)\n        artifact = wandb.Artifact(name=f\"{cfg.run}_model\", type=\"model\")\n        artifact.add_file(str(model_fp))\n        wb_run.log_artifact(artifact)\n        wb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='Existing cross-layer LoRA sharing schemes (SAT-LoRA, HF-SAT-LoRA) still presuppose a *fixed host model*: the functional mixer g_θ is learned **per architecture** and its output dimensionality is hard-wired to the in/out size of every weight matrix.  Hence the learned mixers cannot be reused across (i) different hidden sizes (e.g. BERT-base → DeBERTa-large), (ii) pruned or compressed variants, or (iii) future model versions that add more layers.  Practically this forces practitioners to retrain a new generator for each model, while societally it prevents community-shared, device-ready adapter libraries.  We lack a PEFT mechanism that 1) keeps the global atom dictionary *model-agnostic*, 2) generates layer-specific LoRA updates for *arbitrary* (d_in , d_out) on-the-fly, and 3) still fits into a sub-megabyte memory budget with per-token sparsity.' method='Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2\u2009k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.' experimental_setup='Source pre-training  – BERT-base (L=12, d=768) on GLUE-SST-2 (3 epochs).  Target models  – DistilBERT-6L-d=768, BERT-large-24L-d=1024, and TinyBERT-4L-d=312.  Downstream tasks  – CoNLL-2003, BoolQ, Amazon-Review.  Baselines  – (a) SAT-LoRA retrained for every model, (b) standard 8-rank LoRA, (c) full fine-tune.  Budgets  – ≤1 MB total adapter weights, ≤10 % extra FLOPs.  Metrics  – dev F1 / accuracy, extra RAM, atoms · token, *zero-shot* performance before δθ tuning, and after 500 fine-tune steps.' primary_metric='Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).' experimental_code="class DimProjector(nn.Module):\n    def __init__(self,d0=256):\n        super().__init__(); self.d0=d0; self.mlp=nn.Sequential(nn.Linear(1,64),nn.SiLU(),nn.Linear(64,d0*d0))\n    def forward(self,dim):                       # dim: int\n        w=self.mlp(torch.log10(torch.tensor([[dim]],dtype=torch.float32)))\n        return w.view(dim,self.d0)\n\nclass UniHyperLoRA(nn.Module):\n    def __init__(self,M=128,d0=256):\n        super().__init__(); self.u=nn.Parameter(torch.randn(M,d0)); self.v=nn.Parameter(torch.randn(M,d0))\n        self.mix=nn.Sequential(nn.Linear(6,128),nn.SiLU(),nn.Linear(128,M))   # depth+one-hot(5)\n        self.dim_proj=DimProjector(d0)\n    def forward(self,h,meta):                     # h:[B,T,H]\n        depth,kind,task=meta\n        g=self.mix(torch.cat([depth[:,None],kind],-1))          # [B,M]\n        mask=(g.topk(2).indices)                               # k=2\n        U_sel=self.u[mask]; V_sel=self.v[mask]\n        P_out=self.dim_proj(h.size(-1)); P_in=self.dim_proj(h.size(-1)).t()\n        delta=torch.einsum('bmk,kd->bmd',U_sel,P_out)           # simplified demo\n        # full std LoRA update skipped for brevity\n        return h+delta" expected_result='1) Zero-shot: without any extra tuning UniHyperLoRA transfers to BERT-large with <1 % accuracy loss vs source model. 2) After 500 update steps on each task, UniHyperLoRA matches SAT-LoRA that was *retrained* for that architecture while using 97 % less additional memory (0.97 MB vs 33 MB) and identical FLOPs. 3) On TinyBERT the projector correctly shrinks atoms; accuracy +1.1 % over plain LoRA under the same budget.' expected_conclusion='A single tiny hyper-network that conditions on layer *dimensions* as well as depth and type lets one global LoRA dictionary adapt *any* Transformer family without retraining architecture-specific mixers.  UniHyperLoRA therefore turns adapter weights into true reusable assets—enabling community-shared, phone-sized PEFT libraries and dramatically lowering the entry barrier for fine-tuning ever-evolving language models.'

# Current Research Method
Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2 k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.

# Experimental Design
- Summary: We first pre-train one universal UniHyperLoRA adapter on a single 12-layer BERT-base model with the SST-2 corpus.  This pre-training jointly learns (1) the canonical atom dictionary U,V, (2) the tiny continuous mixer g_θ, (3) the dimension-hyper-network h_ψ that predicts thin projectors P_in / P_out from log-scaled layer sizes, and (4) the sparse router r_φ that chooses at most k=2 atoms per token.  Only U,V,θ,ψ,φ are updated; the host model’s weights stay frozen.  After three epochs we freeze everything except a 4-bit bias δθ.

For the transfer phase we load the *same* frozen adapter weights into three unseen target architectures (DistilBERT-6L-768, BERT-large-1024, TinyBERT-4L-312).  Because the projector h_ψ is dimension-aware, it instantly produces size-compatible low-rank updates for every (d_out,d_in) pair, so no re-training is needed (zero-shot evaluation).  We then run a short 500-step PEFT fine-tuning on each downstream task (CoNLL-2003 NER, BoolQ, Amazon-Review) updating only δθ.

We compare against a strong baseline—SAT-LoRA—that must be re-learnt per architecture, as well as classical rank-8 LoRA and full fine-tuning, while enforcing the shared resource budget of ≤1 MB extra weights and ≤10 % extra FLOPs.  All experiments are executed on a single NVIDIA A100/H200 with 80 GB VRAM; the unified adapter occupies 0.97 MB in 4-bit form and adds <8 % theoretical FLOPs, satisfying the constraint.

After training we report: (a) dev-set performance of the target model with the adapter inserted but *before* any task-specific updates (zero-shot portability) and (b) the score after 500 δθ optimisation steps.  We also log memory overhead, FLOP overhead, and average active atoms per token.  Results are expected to show that the single UniHyperLoRA file achieves near-lossless zero-shot transfer (≤1 % drop) and, after light tuning, matches or exceeds SAT-LoRA while using ~97 % less additional memory.
- Evaluation metrics: ['Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).', 'Memory overhead (MB)', 'FLOP overhead (%)', 'Active atoms per token', 'F1 Score', 'Accuracy']

# Experiment Runs

- Run ID: proposed-iter1-CoNLL-2003
  Method: proposed
  Model: None
  Dataset: CoNLL-2003
  Config File: config/runs/proposed-iter1-CoNLL-2003.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-CoNLL-2003
method: proposed
model:
  host_model_name: distilbert-base-uncased
  hidden_size: 768
  num_layers: 6
  max_position_embeddings: 512
  vocab_size: 30522
adapter:
  name: UniHyperLoRA
  atoms: 128
  anchor_dim: 256
  router_top_k: 2
  quant_bits: 4
  freeze_base_adapter: true
  train_bias_delta_only: true
  bias_bits: 4
  lambda_F: 0.05
  lambda_Q: 1.0e-5
dataset:
  name: conll2003
  hf_id: eriktks/conll2003
  splits:
    train: train
    validation: validation
    test: test
  max_length: 128
training:
  optimizer: adamw
  learning_rate: 2.0e-4
  batch_size: 32
  max_steps: 500
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  lr_scheduler: linear
  warmup_steps: 50
  seed: 42
  update_param_groups:
    - adapter.bias_delta
evaluation:
  eval_steps: 100
  metric: seqeval
budget_constraints:
  max_extra_params_mb: 1.0
  max_flop_overhead_pct: 10
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 5.0e-4
    lambda_F:
      type: uniform
      low: 0.01
      high: 0.1
    lambda_Q:
      type: loguniform
      low: 1.0e-6
      high: 1.0e-4
    router_top_k:
      type: categorical
      choices: [1, 2]
hardware:
  accelerator: gpu
  precision: bf16
  devices: 1

  ```
  

- Run ID: comparative-1-iter1-CoNLL-2003
  Method: comparative-1
  Model: None
  Dataset: CoNLL-2003
  Config File: config/runs/comparative-1-iter1-CoNLL-2003.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-CoNLL-2003
method: comparative-1
model:
  host_model_name: distilbert-base-uncased
  hidden_size: 768
  num_layers: 6
adapter:
  name: SAT-LoRA
  lora_rank: 8
  lora_alpha: 16
  target_modules: [q_proj, k_proj, v_proj, o_proj, ffn_down, ffn_up]
  share_across_layers: false
  precision: int8
dataset:
  name: conll2003
  hf_id: eriktks/conll2003
  splits:
    train: train
    validation: validation
    test: test
  max_length: 128
training:
  optimizer: adamw
  learning_rate: 3.0e-4
  batch_size: 32
  max_steps: 3000
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  lr_scheduler: linear
  warmup_steps: 300
  seed: 42
  update_param_groups:
    - adapter.lora
evaluation:
  eval_steps: 200
  metric: seqeval
budget_constraints:
  max_extra_params_mb: 32
  max_flop_overhead_pct: 10
optuna:
  n_trials: 25
  direction: maximize
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-5
      high: 1.0e-3
    lora_rank:
      type: int
      low: 4
      high: 16
    batch_size:
      type: categorical
      choices: [16, 32, 64]
hardware:
  accelerator: gpu
  precision: bf16
  devices: 1

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**


**HuggingFace Datasets:**

- ID: eriktks/conll2003






Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import os\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport hydra\nfrom omegaconf import OmegaConf, DictConfig\n\nimport wandb\nimport optuna\n\nfrom .preprocess import build_dataloaders\nfrom .model import (\n    build_model_with_adapters,\n    compute_adapter_params,\n)\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    import random, numpy as np\n\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _apply_mode_overrides(cfg: DictConfig) -> None:\n    \"\"\"Apply trial/full specific overrides in-place.\"\"\"\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.max_steps = 2\n        cfg.training.epochs = 1\n        cfg.evaluation.eval_steps = 1\n        cfg.training.batch_size = min(int(cfg.training.batch_size), 2)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be 'trial' or 'full'\")\n\n\ndef _ensure_results_dir(cfg: DictConfig) -> None:\n    cfg.results_dir = os.path.abspath(cfg.results_dir)\n    os.makedirs(cfg.results_dir, exist_ok=True)\n\n\n# -----------------------------------------------------------------------------\n# Evaluation helper\n# -----------------------------------------------------------------------------\n\ndef _evaluate(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    device: torch.device,\n    metric_name: str,\n) -> Dict[str, float]:\n    import evaluate as hf_evaluate\n\n    metric = hf_evaluate.load(metric_name, cache_dir=CACHE_DIR)\n    model.eval()\n\n    id2label: Dict[int, str] = {\n        int(k): v for k, v in getattr(model.config, \"id2label\", {}).items()\n    }\n\n    for batch in dataloader:\n        labels = batch.pop(\"labels\").to(device)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            logits = model(**batch).logits\n        preds = logits.argmax(-1).cpu()\n        labels_cpu = labels.cpu()\n\n        if metric_name == \"seqeval\":\n            preds_list: List[List[str]] = []\n            refs_list: List[List[str]] = []\n            for pred_seq, label_seq in zip(preds, labels_cpu):\n                p_sent: List[str] = []\n                l_sent: List[str] = []\n                for p_id, l_id in zip(pred_seq.tolist(), label_seq.tolist()):\n                    if l_id == -100:\n                        continue\n                    p_sent.append(id2label.get(p_id, str(p_id)))\n                    l_sent.append(id2label.get(l_id, str(l_id)))\n                preds_list.append(p_sent)\n                refs_list.append(l_sent)\n            metric.add_batch(predictions=preds_list, references=refs_list)\n        else:\n            metric.add_batch(predictions=preds, references=labels_cpu)\n\n    result = metric.compute()\n    if metric_name == \"seqeval\":\n        return {\n            \"f1\": result.get(\"overall_f1\", 0.0),\n            \"accuracy\": result.get(\"overall_accuracy\", 0.0),\n        }\n    if isinstance(result, dict):\n        return result\n    return {metric_name: float(result)}\n\n\n# -----------------------------------------------------------------------------\n# Training utilities\n# -----------------------------------------------------------------------------\n\ndef _grad_global_norm(model: torch.nn.Module) -> float:\n    grads: List[torch.Tensor] = [p.grad for p in model.parameters() if p.grad is not None]\n    if not grads:\n        return 0.0\n    return torch.norm(torch.stack([g.norm(2) for g in grads]), 2).item()\n\n\ndef _training_step(\n    model: torch.nn.Module,\n    batch: Dict[str, torch.Tensor],\n    optimizer: torch.optim.Optimizer,\n    scheduler: Optional[torch.optim.lr_scheduler.LambdaLR],\n    device: torch.device,\n    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n) -> Tuple[float, float]:\n    \"\"\"Returns (loss, grad_norm).\"\"\"\n    model.train()\n    labels = batch.pop(\"labels\").to(device)\n    batch = {k: v.to(device) for k, v in batch.items()}\n\n    if scaler is not None:\n        with torch.cuda.amp.autocast():\n            loss = model(**batch, labels=labels).loss\n        scaler.scale(loss).backward()\n        grad_norm = _grad_global_norm(model)\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        loss = model(**batch, labels=labels).loss\n        loss.backward()\n        grad_norm = _grad_global_norm(model)\n        optimizer.step()\n\n    optimizer.zero_grad(set_to_none=True)\n    if scheduler is not None:\n        scheduler.step()\n    return loss.item(), grad_norm\n\n\n# -----------------------------------------------------------------------------\n# Optuna helpers\n# -----------------------------------------------------------------------------\n\ndef _suggest_and_apply(trial: optuna.Trial, cfg: DictConfig):\n    search_space: Dict[str, Any] = cfg.optuna.get(\"search_space\", {})\n    for path, space in search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            value = trial.suggest_float(path, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            value = trial.suggest_float(path, space[\"low\"], space[\"high\"], log=False)\n        elif space[\"type\"] == \"categorical\":\n            value = trial.suggest_categorical(path, space[\"choices\"])\n        elif space[\"type\"] == \"int\":\n            value = trial.suggest_int(path, space[\"low\"], space[\"high\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna space type {space['type']}\")\n        OmegaConf.update(cfg, path, value, merge=True)\n\n\ndef _objective(\n    trial: optuna.Trial,\n    cfg: DictConfig,\n    model_init_fn,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    device: torch.device,\n):\n    cfg_tmp = OmegaConf.deepcopy(cfg)\n    _suggest_and_apply(trial, cfg_tmp)\n\n    model = model_init_fn(cfg_tmp)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg_tmp.training.learning_rate,\n        weight_decay=cfg_tmp.training.weight_decay,\n    )\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    max_steps = min(100, cfg_tmp.training.max_steps)\n    eval_every = max(10, max_steps // 5)\n\n    train_iter = iter(train_loader)\n    best_val = -float(\"inf\")\n    step = 0\n    while step < max_steps:\n        try:\n            batch = next(train_iter)\n        except StopIteration:\n            train_iter = iter(train_loader)\n            batch = next(train_iter)\n\n        loss, _ = _training_step(model, batch, optimizer, None, device, scaler)\n        step += 1\n        if step % eval_every == 0 or step == max_steps:\n            metrics = _evaluate(model, val_loader, device, cfg_tmp.evaluation.metric)\n            key = \"f1\" if \"f1\" in metrics else list(metrics.keys())[0]\n            val_score = metrics[key]\n            trial.report(val_score, step)\n            best_val = max(best_val, val_score)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    return best_val\n\n\n# -----------------------------------------------------------------------------\n# wandb helpers\n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=getattr(cfg, \"run_id\", cfg.run),\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        dir=cfg.results_dir,\n    )\n    print(f\"wandb URL: {run.url}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Basic sanity ----------------------------------------------------------------\n    if cfg.run is None:\n        raise ValueError(\"Parameter 'run' must be supplied: python -m src.train run=<run_id> ...\")\n\n    _apply_mode_overrides(cfg)\n    _ensure_results_dir(cfg)\n\n    # Determinism -----------------------------------------------------------------\n    set_seed(int(cfg.training.seed))\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data ------------------------------------------------------------------------\n    train_loader, val_loader, label_list = build_dataloaders(cfg, CACHE_DIR)\n    num_labels = len(label_list)\n\n    def model_init(local_cfg: DictConfig):\n        return build_model_with_adapters(local_cfg, num_labels, device)\n\n    # Optuna hyper-parameter search ----------------------------------------------\n    if cfg.optuna.n_trials > 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(\n            lambda trial: _objective(\n                trial, cfg, model_init, train_loader, val_loader, device\n            ),\n            n_trials=int(cfg.optuna.n_trials),\n        )\n        for k, v in study.best_trial.params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n        print(\n            f\"Optuna best trial {study.best_trial.number}: value={study.best_value:.4f} params={study.best_trial.params}\"\n        )\n\n    # Final training -------------------------------------------------------------\n    model = model_init(cfg)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    lr_lambda = lambda cur: max(0.0, 1.0 - (cur / cfg.training.max_steps))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    wb_run = _init_wandb(cfg)\n    if wb_run is not None:\n        wb_run.summary[\"adapter_param_MB\"] = compute_adapter_params(model)\n\n    global_step = 0\n    pbar = tqdm(total=cfg.training.max_steps, desc=\"Training\")\n    while global_step < cfg.training.max_steps:\n        for batch in train_loader:\n            loss_val, grad_norm_val = _training_step(\n                model, batch, optimizer, scheduler, device, scaler\n            )\n            global_step += 1\n            pbar.update(1)\n\n            # Logging -----------------------------------------------------------\n            if wb_run is not None:\n                metrics_to_log = {\n                    \"train_loss\": loss_val,\n                    \"grad_norm\": grad_norm_val,\n                    \"lr\": optimizer.param_groups[0][\"lr\"],\n                    \"step\": global_step,\n                }\n                if torch.cuda.is_available():\n                    metrics_to_log[\"gpu_mem_mb\"] = torch.cuda.memory_allocated() / (1024 ** 2)\n                wandb.log(metrics_to_log, step=global_step)\n\n            # Evaluation --------------------------------------------------------\n            if (\n                global_step % cfg.evaluation.eval_steps == 0\n                or global_step == cfg.training.max_steps\n            ):\n                eval_metrics = _evaluate(\n                    model, val_loader, device, cfg.evaluation.metric\n                )\n                if wb_run is not None:\n                    wandb.log(\n                        {f\"eval_{k}\": v for k, v in eval_metrics.items()},\n                        step=global_step,\n                    )\n            if global_step >= cfg.training.max_steps:\n                break\n    pbar.close()\n\n    final_metrics = _evaluate(model, val_loader, device, cfg.evaluation.metric)\n    print(f\"Final metrics: {final_metrics}\")\n\n    # -------------------------------------------------------------------------\n    # Save checkpoint and finish wandb\n    # -------------------------------------------------------------------------\n    if wb_run is not None:\n        for k, v in final_metrics.items():\n            wb_run.summary[f\"final_{k}\"] = v\n\n        model_fp = Path(cfg.results_dir) / f\"{cfg.run}_final_model.pth\"\n        torch.save(model.state_dict(), model_fp)\n        artifact = wandb.Artifact(name=f\"{cfg.run}_model\", type=\"model\")\n        artifact.add_file(str(model_fp))\n        wb_run.log_artifact(artifact)\n        wb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "import argparse\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Any\n\nimport torch  # Must be imported before use throughout file\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import (\n    confusion_matrix,\n)\n\nfrom .preprocess import build_dataloaders\nfrom .model import build_model_with_adapters\n\nPRIMARY_METRIC_STRING = (\n    \"Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).\"\n)\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _load_wandb_credentials():\n    cfg_file = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    cfg = OmegaConf.load(cfg_file)\n    return cfg.wandb.entity, cfg.wandb.project\n\n\ndef _sanitize(s: str) -> str:\n    \"\"\"Return a safe filename component (no slashes, spaces, etc.).\"\"\"\n    return re.sub(r\"[^A-Za-z0-9\\-_]\", \"_\", s)\n\n\ndef _fetch_run(rid: str, entity: str, project: str):\n    api = wandb.Api()\n    return api.run(f\"{entity}/{project}/{rid}\")\n\n\n# -----------------------------------------------------------------------------\n# In-depth run processing helpers\n# -----------------------------------------------------------------------------\n\ndef _reconstruct_cfg(wandb_cfg: Dict[str, Any]) -> OmegaConf:\n    \"\"\"Convert the flattened wandb config dict back into OmegaConf style.\"\"\"\n    cfg_nested: Dict[str, Any] = {}\n    for k, v in wandb_cfg.items():\n        cur = cfg_nested\n        parts = k.split(\".\")\n        for p in parts[:-1]:\n            cur = cur.setdefault(p, {})\n        cur[parts[-1]] = v\n    return OmegaConf.create(cfg_nested)\n\n\ndef _generate_predictions(\n    cfg: OmegaConf, model_state_path: str\n) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n    \"\"\"Load model weights and dataset, then produce predictions (validation split).\"\"\"\n    device = \"cpu\"\n\n    # Build loaders & model ----------------------------------------------------\n    loaders = build_dataloaders(cfg, CACHE_DIR)\n    _, val_loader, label_list = loaders\n    num_labels = len(label_list)\n\n    model = build_model_with_adapters(cfg, num_labels=num_labels, device=device)\n    state_dict = torch.load(model_state_path, map_location=device)\n    model.load_state_dict(state_dict, strict=False)\n    model.eval()\n\n    preds_all: List[int] = []\n    labels_all: List[int] = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            labels = batch.pop(\"labels\")\n            batch = {k: v for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            preds = logits.argmax(-1)\n            if cfg.evaluation.metric == \"seqeval\":\n                for p_seq, l_seq in zip(preds, labels):\n                    for p_id, l_id in zip(p_seq.tolist(), l_seq.tolist()):\n                        if l_id == -100:\n                            continue\n                        preds_all.append(p_id)\n                        labels_all.append(l_id)\n            else:\n                preds_all.extend(preds.tolist())\n                labels_all.extend(labels.tolist())\n    return np.array(preds_all), np.array(labels_all), label_list\n\n\ndef _plot_confusion_matrix(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    labels: List[str],\n    save_path: Path,\n):\n    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=labels,\n        yticklabels=labels,\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# STEP-1: Per-run processing\n# -----------------------------------------------------------------------------\n\ndef _export_metrics_and_figures(run, save_dir: Path) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Export history & generate per-run figures. Returns (preds, labels) if computed.\"\"\"\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Save history & summary JSON ------------------------------------------\n    history_df = run.history()\n    summary_dict = dict(run.summary._json_dict)\n    cfg_dict = dict(run.config)\n\n    with open(save_dir / \"metrics.json\", \"w\") as fp:\n        json.dump({\"summary\": summary_dict, \"config\": cfg_dict}, fp, indent=2)\n    history_df.to_parquet(save_dir / \"history.parquet\")\n\n    # 2. Learning-curve figure -------------------------------------------------\n    fig_fp = save_dir / f\"{_sanitize(run.id)}_learning_curve.pdf\"\n    plt.figure()\n    metric_cols = [c for c in history_df.columns if re.match(r\"(train|eval)_.*\", str(c))]\n    for col in metric_cols:\n        sns.lineplot(x=history_df.index, y=history_df[col], label=col)\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Metric value\")\n    plt.title(run.id)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(fig_fp)\n    plt.close()\n    print(f\"Saved figure: {fig_fp}\")\n\n    # 3. Confusion matrix ------------------------------------------------------\n    preds_np = labels_np = None\n    try:\n        art = next(a for a in run.logged_artifacts() if a.type == \"model\")\n        model_dir = art.download()\n        state_path = next(Path(model_dir).glob(\"*.pth\"))\n        cfg = _reconstruct_cfg(cfg_dict)\n        preds_np, labels_np, label_list = _generate_predictions(cfg, str(state_path))\n\n        cm_path = save_dir / f\"{_sanitize(run.id)}_confusion_matrix.pdf\"\n        _plot_confusion_matrix(labels_np, preds_np, label_list, cm_path)\n        print(f\"Saved figure: {cm_path}\")\n\n        # Save raw preds / labels\n        np.save(save_dir / \"preds.npy\", preds_np)\n        np.save(save_dir / \"labels.npy\", labels_np)\n    except StopIteration:\n        print(f\"[WARN] No model artifact found for run {run.id}; skipping confusion matrix.\")\n    except Exception as e:\n        print(f\"[WARN] Failed to generate confusion matrix for {run.id}: {e}\")\n\n    return preds_np, labels_np\n\n\n# -----------------------------------------------------------------------------\n# STEP-2: Aggregated analysis & comparison\n# -----------------------------------------------------------------------------\n\ndef _mcnemar(y1: np.ndarray, y2: np.ndarray, y_true: np.ndarray) -> Dict[str, Any]:\n    \"\"\"McNemar's exact binomial test between two classifiers.\"\"\"\n    assert y1.shape == y2.shape == y_true.shape\n    n01 = ((y1 == y_true) & (y2 != y_true)).sum()\n    n10 = ((y1 != y_true) & (y2 == y_true)).sum()\n    from math import comb\n\n    n = n01 + n10\n    if n == 0:\n        p_val = 1.0\n    else:\n        k = min(n01, n10)\n        p_val = 2 * sum(comb(n, i) * (0.5 ** n) for i in range(0, k + 1))\n        p_val = min(1.0, p_val)\n    return {\"n01\": int(n01), \"n10\": int(n10), \"p_value\": float(p_val)}\n\n\ndef _aggregate(runs: Dict[str, wandb.apis.public.Run], out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    metrics_by_name: Dict[str, Dict[str, float]] = {}\n    preds_map: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}\n\n    # Collect numerical summary metrics --------------------------------------\n    for rid, run in runs.items():\n        summary = run.summary._json_dict\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                metrics_by_name.setdefault(k, {})[rid] = v\n        run_dir = out_dir.parent / _sanitize(rid)\n        preds_fp = run_dir / \"preds.npy\"\n        labels_fp = run_dir / \"labels.npy\"\n        if preds_fp.exists() and labels_fp.exists():\n            preds_map[rid] = (np.load(preds_fp), np.load(labels_fp))\n\n    # Primary metric selection ------------------------------------------------\n    pref_keys = [\n        \"final_f1\",\n        \"final_accuracy\",\n        \"f1\",\n        \"accuracy\",\n        \"eval_f1\",\n    ]\n    selected_key = next((k for k in pref_keys if k in metrics_by_name), None)\n    if selected_key is None:\n        raise RuntimeError(\"Cannot find a common metric across runs.\")\n\n    proposed_vals = {\n        rid: v for rid, v in metrics_by_name[selected_key].items() if \"proposed\" in rid\n    }\n    baseline_vals = {\n        rid: v\n        for rid, v in metrics_by_name[selected_key].items()\n        if any(tok in rid for tok in [\"baseline\", \"comparative\"])\n    }\n\n    best_proposed_id = max(proposed_vals, key=proposed_vals.get)\n    best_baseline_id = max(baseline_vals, key=baseline_vals.get)\n    best_proposed_val = proposed_vals[best_proposed_id]\n    best_baseline_val = baseline_vals[best_baseline_id]\n    gap_pct = (best_proposed_val - best_baseline_val) / best_baseline_val * 100.0\n\n    aggregate = {\n        \"primary_metric\": PRIMARY_METRIC_STRING,\n        \"metrics\": metrics_by_name,\n        \"best_proposed\": {\"run_id\": best_proposed_id, \"value\": best_proposed_val},\n        \"best_baseline\": {\"run_id\": best_baseline_id, \"value\": best_baseline_val},\n        \"gap\": gap_pct,\n    }\n\n    # Statistical significance -----------------------------------------------\n    if best_proposed_id in preds_map and best_baseline_id in preds_map:\n        yp, yt = preds_map[best_proposed_id]\n        yb, _ = preds_map[best_baseline_id]\n        sig = _mcnemar(yp, yb, yt)\n        aggregate[\"stat_tests\"] = {\"mcnemar\": sig}\n    else:\n        aggregate[\"stat_tests\"] = {\"mcnemar\": \"NA\"}\n\n    out_fp = out_dir / \"aggregated_metrics.json\"\n    with open(out_fp, \"w\") as fp:\n        json.dump(aggregate, fp, indent=2)\n    print(f\"Aggregated metrics saved → {out_fp}\")\n\n    # Comparison figures ------------------------------------------------------\n    bar_fig = out_dir / f\"comparison_{_sanitize(selected_key)}_bar_chart.pdf\"\n    plt.figure(figsize=(10, 6))\n    sns.barplot(\n        x=list(metrics_by_name[selected_key].keys()),\n        y=list(metrics_by_name[selected_key].values()),\n        palette=\"viridis\",\n    )\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(selected_key)\n    plt.title(\"Comparison on \" + selected_key)\n    for i, v in enumerate(metrics_by_name[selected_key].values()):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    plt.savefig(bar_fig)\n    plt.close()\n    print(f\"Saved figure: {bar_fig}\")\n\n    metrics_df = pd.DataFrame(metrics_by_name)\n    melted = metrics_df.melt(var_name=\"metric\", value_name=\"value\", ignore_index=False)\n    melted = melted.reset_index().rename(columns={\"index\": \"run_id\"})\n    box_fig = out_dir / \"comparison_all_metrics_boxplot.pdf\"\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(data=melted, x=\"metric\", y=\"value\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Metric distribution across runs\")\n    plt.tight_layout()\n    plt.savefig(box_fig)\n    plt.close()\n    print(f\"Saved figure: {box_fig}\")\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    p = argparse.ArgumentParser(\n        description=\"Evaluate multiple wandb runs and generate comparison reports.\"\n    )\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\n        \"run_ids\",\n        type=str,\n        help=\"JSON string list of run IDs, e.g. '[\\\"run1\\\", \\\"run2\\\"]'\",\n    )\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = _parse_args()\n    results_root = Path(args.results_dir)\n    run_id_list: List[str] = json.loads(args.run_ids)\n\n    entity, project = _load_wandb_credentials()\n\n    runs: Dict[str, wandb.apis.public.Run] = {}\n    for rid in run_id_list:\n        run = _fetch_run(rid, entity, project)\n        runs[rid] = run\n        _export_metrics_and_figures(run, results_root / _sanitize(rid))\n\n    _aggregate(runs, results_root / \"comparison\")",
    "preprocess_py": "from typing import Tuple, List\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n)\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Token-classification helpers\n# -----------------------------------------------------------------------------\n\ndef _tokenize_align_labels(examples, tokenizer, label_all_tokens: bool):\n    tokenized = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=tokenizer.model_max_length,\n    )\n    labels = []\n    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_seq[word_idx])\n            else:\n                label_ids.append(label_seq[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized[\"labels\"] = labels\n    return tokenized\n\n\n# -----------------------------------------------------------------------------\n# Data-loading entry point\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg: DictConfig, cache_dir: str):\n    ds_cfg = cfg.dataset\n    tokenizer = AutoTokenizer.from_pretrained(\n        cfg.model.host_model_name, cache_dir=cache_dir, use_fast=True\n    )\n\n    task_name = ds_cfg.name.lower()\n\n    # Trial-mode dataset reduction -------------------------------------------\n    limit = 64 if getattr(cfg, \"mode\", \"full\") == \"trial\" else None\n\n    if task_name == \"conll2003\":\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n        if limit is not None:\n            train_ds = train_ds.select(range(min(len(train_ds), limit)))\n            val_ds = val_ds.select(range(min(len(val_ds), limit)))\n        train_ds = train_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=train_ds.column_names,\n        )\n        val_ds = val_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=val_ds.column_names,\n        )\n        collator = DataCollatorForTokenClassification(tokenizer)\n        label_list = raw[\"train\"].features[\"ner_tags\"].feature.names\n    else:\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n        if limit is not None:\n            train_ds = train_ds.select(range(min(len(train_ds), limit)))\n            val_ds = val_ds.select(range(min(len(val_ds), limit)))\n\n        def tok_fn(ex):\n            out = tokenizer(ex[\"text\"], truncation=True, max_length=ds_cfg.max_length)\n            out[\"labels\"] = ex[\"label\"]\n            return out\n\n        train_ds = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names)\n        val_ds = val_ds.map(tok_fn, batched=True, remove_columns=val_ds.column_names)\n        collator = DataCollatorWithPadding(tokenizer)\n        label_list = raw[\"train\"].features[\"label\"].names\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(cfg.training.batch_size),\n        shuffle=True,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=int(cfg.training.batch_size),\n        shuffle=False,\n        collate_fn=collator,\n    )\n    return train_loader, val_loader, label_list",
    "model_py": "import math\nfrom typing import List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import AutoConfig, AutoModelForTokenClassification\nfrom omegaconf import DictConfig\n\n__all__ = [\n    \"build_model_with_adapters\",\n    \"compute_adapter_params\",\n]\n\n# -----------------------------------------------------------------------------\n# Baseline LoRA wrapper\n# -----------------------------------------------------------------------------\n\nclass LoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.0):\n        super().__init__()\n        self.base = base\n        self.r = r\n        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n        if r > 0:\n            self.lora_a = nn.Linear(base.in_features, r, bias=False)\n            self.lora_b = nn.Linear(r, base.out_features, bias=False)\n            nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_b.weight)\n            self.scaling = alpha / r\n        else:\n            self.lora_a = self.lora_b = None\n            self.scaling = 1.0\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        out = self.base(x)\n        if self.r > 0:\n            out = out + self.lora_b(self.lora_a(self.dropout(x))) * self.scaling\n        return out\n\n\n# -----------------------------------------------------------------------------\n# Proposed UniHyperLoRA components (simplified implementation)\n# -----------------------------------------------------------------------------\n\nclass DimAwareProjector(nn.Module):\n    def __init__(self, d0: int = 256):\n        super().__init__()\n        self.d0 = d0\n        self.mlp = nn.Sequential(\n            nn.Linear(1, 64), nn.SiLU(), nn.Linear(64, d0 * 2)\n        )\n\n    def forward(self, dim: int) -> torch.Tensor:\n        inp = torch.log10(\n            torch.tensor([[float(dim)]], dtype=torch.float32, device=self.mlp[0].weight.device)\n        )\n        vec = self.mlp(inp).view(2, self.d0)\n        row, col = vec[0], vec[1]\n        return torch.outer(torch.ones(dim, device=row.device), row) + torch.outer(\n            torch.arange(1, dim + 1, dtype=row.dtype, device=row.device), col\n        )\n\n\nclass SharedUniHyperLoRA(nn.Module):\n    def __init__(self, M: int = 128, d0: int = 256, router_top_k: int = 2):\n        super().__init__()\n        self.M = M\n        self.d0 = d0\n        self.router_top_k = router_top_k\n        self.U = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.V = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.dim_proj = DimAwareProjector(d0)\n        self.mixer = nn.Sequential(\n            nn.Linear(2, 128), nn.SiLU(), nn.Linear(128, M)\n        )\n\n\nclass UniHyperLoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, shared: SharedUniHyperLoRA, depth_norm: float, layer_type_id: int):\n        super().__init__()\n        self.base = base\n        self.shared = shared\n        for p in self.base.parameters():\n            p.requires_grad = False\n        self.register_buffer(\"depth_norm\", torch.tensor([depth_norm], dtype=torch.float32))\n        self.register_buffer(\"type_id\", torch.tensor([layer_type_id / 4.0], dtype=torch.float32))\n\n    def forward(self, x):\n        out = self.base(x)\n        d_out, d_in = self.base.out_features, self.base.in_features\n        P_out = self.shared.dim_proj(d_out).to(x.device)\n        P_in = self.shared.dim_proj(d_in).to(x.device).t()\n        gates = self.shared.mixer(torch.cat([self.depth_norm, self.type_id]).unsqueeze(0))[0]\n        vals, idx = torch.topk(gates, k=self.shared.router_top_k)\n        delta_W = torch.zeros(d_out, d_in, device=x.device)\n        for val, j in zip(vals, idx):\n            delta_W += val * (\n                P_out @ torch.outer(self.shared.U[j], self.shared.V[j]) @ P_in\n            )\n        return out + F.linear(x, delta_W, bias=None)\n\n\n# -----------------------------------------------------------------------------\n# Helper utils\n# -----------------------------------------------------------------------------\n\ndef _get_parent_module(model: nn.Module, module_name: str):\n    parts = module_name.split(\".\")\n    parent = model\n    for p in parts[:-1]:\n        parent = parent[int(p)] if p.isdigit() else getattr(parent, p)\n    return parent\n\n\ndef _set_child_module(parent: nn.Module, child_name: str, new_module: nn.Module):\n    if child_name.isdigit():\n        parent[int(child_name)] = new_module\n    else:\n        setattr(parent, child_name, new_module)\n\n\ndef compute_adapter_params(model: nn.Module) -> float:\n    bytes_total = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return bytes_total / (1024 ** 2)  # MB\n\n\n# -----------------------------------------------------------------------------\n# Build model with adapters\n# -----------------------------------------------------------------------------\n\ndef build_model_with_adapters(cfg: DictConfig, num_labels: int, device):\n    model_name = cfg.model.host_model_name\n\n    base_cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=\".cache/\")\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, config=base_cfg, cache_dir=\".cache/\"\n    )\n\n    if cfg.method.lower().startswith(\"proposed\") or cfg.adapter.name.lower() == \"unihyperlora\":\n        shared = SharedUniHyperLoRA(\n            M=cfg.adapter.atoms,\n            d0=cfg.adapter.anchor_dim,\n            router_top_k=cfg.adapter.router_top_k,\n        )\n        linear_modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n        total = len(linear_modules)\n        idx = 0\n        for name, module in list(model.named_modules()):\n            if isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                depth = idx / max(1, total - 1)\n                wrapped = UniHyperLoRALinear(module, shared, depth, layer_type_id=0)\n                _set_child_module(parent, child_name, wrapped)\n                idx += 1\n    elif cfg.method.lower().startswith(\"comparative\") or cfg.adapter.name.lower() == \"sat-lora\":\n        targets: List[str] = cfg.adapter.target_modules\n        for name, module in list(model.named_modules()):\n            if any(t in name for t in targets) and isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                wrapped = LoRALinear(\n                    module,\n                    r=cfg.adapter.lora_rank,\n                    alpha=cfg.adapter.lora_alpha,\n                )\n                _set_child_module(parent, child_name, wrapped)\n    else:\n        raise ValueError(f\"Unknown adapter type: {cfg.adapter.name}\")\n\n    model.to(device)\n    return model",
    "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    if cfg.run is None:\n        raise ValueError(\n            \"Usage: python -m src.main run=<run_id> results_dir=<dir> mode=<trial|full>\"\n        )\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    print(\"🚀 Launching:\", \" \".join(cmd))\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"unihyperlora-experiments\"\nversion = \"0.1.0\"\ndescription = \"Universal Hyper-LoRA experiments managed by Hydra\"\nrequires-python = \">=3.10\"\ndependencies = [\n  \"torch>=2.1.0\",\n  \"transformers>=4.36.0\",\n  \"datasets>=2.14.0\",\n  \"hydra-core>=1.3.2\",\n  \"wandb>=0.16.0\",\n  \"optuna>=3.4.0\",\n  \"evaluate>=0.4.1\",\n  \"seqeval>=1.2.2\",\n  \"scikit-learn>=1.3.0\",\n  \"matplotlib>=3.8.0\",\n  \"seaborn>=0.13.0\",\n  \"tqdm>=4.66.0\",\n  \"pandas>=2.1.0\",\n]",
    "config_yaml": "defaults:\n  - runs/${run}\n  - _self_\n\n# Required CLI args -----------------------------------------------------------\nrun: \"???\"           # run group identifier, e.g., proposed-iter1-CoNLL-2003\nmode: full           # full | trial\nresults_dir: outputs\n\n# WANDB ----------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online  # auto-switched to 'disabled' in trial mode\n\n# Optuna ---------------------------------------------------------------------\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# Training defaults ----------------------------------------------------------\ntraining:\n  seed: 42\n  batch_size: 32\n  epochs: 3\n  max_steps: 1000\n  learning_rate: 5e-5\n  weight_decay: 0.01\n  gradient_accumulation_steps: 1\n\nevaluation:\n  metric: accuracy  # can be overwritten by run cfg\n  eval_steps: 50\n\n# Fields populated by run YAML ----------------------------------------------\nmethod: \"\"\nmodel: {}\nadapter: {}\ndataset: {}\nbudget_constraints: {}"
}
