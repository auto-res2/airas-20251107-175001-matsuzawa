Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 665ms
Installed 99 packages in 4.97s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 238784.84 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 260610.41 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 348256.23 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 2067.38 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 1645.30 examples/s]
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:303: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
Training:   0%|          | 0/2 [00:00<?, ?it/s]/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.92s/it]
Downloading builder script: 0.00B [00:00, ?B/s][ADownloading builder script: 6.34kB [00:00, 11.2MB/s]
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.90s/it]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.15s/it]
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 108 packages in 589ms
Installed 99 packages in 7.12s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 164689.73 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 159079.10 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 207828.31 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 2186.83 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 7167.45 examples/s]
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:303: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
Training:   0%|          | 0/2 [00:00<?, ?it/s]/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.58it/s]
Downloading builder script: 0.00B [00:00, ?B/s][ADownloading builder script: 6.34kB [00:00, 8.21MB/s]
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.51s/it]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.99s/it]
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 108 packages in 151ms
Installed 99 packages in 6.79s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 194036.53 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 182312.26 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 212899.76 examples/s]
Map:   0%|          | 0/14041 [00:00<?, ? examples/s]Map:  14%|â–ˆâ–        | 2000/14041 [00:00<00:00, 14594.98 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4000/14041 [00:00<00:00, 16508.27 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 7000/14041 [00:00<00:00, 17445.36 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9000/14041 [00:00<00:00, 16695.20 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12000/14041 [00:00<00:00, 16279.54 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 14000/14041 [00:00<00:00, 15850.44 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 15687.67 examples/s]
Map:   0%|          | 0/3250 [00:00<?, ? examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3250 [00:00<00:00, 15566.11 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 7961.59 examples/s] 
[I 2025-11-08 04:24:19,817] A new study created in memory with name: no-name-a310e09c-fad6-4504-ad22-9a772ed8b9a8
[W 2025-11-08 04:24:19,817] Trial 0 failed with parameters: {} because of the following error: AttributeError("type object 'OmegaConf' has no attribute 'deepcopy'").
Traceback (most recent call last):
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 283, in <lambda>
    lambda trial: _objective(
                  ^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 184, in _objective
    cfg_tmp = OmegaConf.deepcopy(cfg)
              ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OmegaConf' has no attribute 'deepcopy'
[W 2025-11-08 04:24:19,818] Trial 0 failed with value None.
Error executing job with overrides: ['run=comparative-1-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 282, in main
    study.optimize(
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 63, in _optimize
    _optimize_sequential(
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 258, in _run_trial
    raise func_err
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 283, in <lambda>
    lambda trial: _objective(
                  ^^^^^^^^^^^
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 184, in _objective
    cfg_tmp = OmegaConf.deepcopy(cfg)
              ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OmegaConf' has no attribute 'deepcopy'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['run=comparative-1-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/main.py", line 38, in main
    subprocess.run(cmd, check=True, env=env)
  File "/mnt/home/toma/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 104ms
Installed 99 packages in 5.63s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 267391.71 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 262988.60 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 331417.20 examples/s]
Map:   0%|          | 0/14041 [00:00<?, ? examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4000/14041 [00:00<00:00, 23257.24 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8000/14041 [00:00<00:00, 23995.17 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12000/14041 [00:00<00:00, 22841.39 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 22065.86 examples/s]
Map:   0%|          | 0/3250 [00:00<?, ? examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3250 [00:00<00:00, 18606.08 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 7697.71 examples/s] 
[I 2025-11-07 19:26:59,946] A new study created in memory with name: no-name-e94f52ce-fbf1-4636-b9b5-f46096918e2d
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:193: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[I 2025-11-07 19:27:25,968] Trial 0 finished with value: 0.885847945772205 and parameters: {'learning_rate': 2.1606877353075694e-05, 'lora_rank': 5, 'batch_size': 32}. Best is trial 0 with value: 0.885847945772205.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:27:46,667] Trial 1 finished with value: 0.9048288196218702 and parameters: {'learning_rate': 0.000967894722314015, 'lora_rank': 15, 'batch_size': 32}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:28:07,487] Trial 2 finished with value: 0.877121347454383 and parameters: {'learning_rate': 0.0003609698723361945, 'lora_rank': 5, 'batch_size': 64}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:28:28,100] Trial 3 finished with value: 0.8973719547285631 and parameters: {'learning_rate': 6.557833574608277e-05, 'lora_rank': 5, 'batch_size': 16}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:28:48,912] Trial 4 finished with value: 0.8344091438647313 and parameters: {'learning_rate': 3.1292849542847255e-05, 'lora_rank': 12, 'batch_size': 32}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:28:57,317] Trial 5 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:29:01,721] Trial 6 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:29:18,557] Trial 7 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:29:22,920] Trial 8 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:29:39,399] Trial 9 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:29:48,022] Trial 10 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:00,523] Trial 11 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:09,000] Trial 12 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:13,650] Trial 13 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:26,154] Trial 14 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:46,847] Trial 15 finished with value: 0.8898455305140542 and parameters: {'learning_rate': 0.0002464048953856699, 'lora_rank': 8, 'batch_size': 32}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:59,322] Trial 16 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:07,720] Trial 17 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:12,106] Trial 18 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:16,458] Trial 19 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:25,162] Trial 20 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:45,983] Trial 21 finished with value: 0.893622416634969 and parameters: {'learning_rate': 0.00019664947682632055, 'lora_rank': 6, 'batch_size': 32}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:50,380] Trial 22 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:32:10,985] Trial 23 finished with value: 0.9000610363326801 and parameters: {'learning_rate': 4.79358341167532e-05, 'lora_rank': 4, 'batch_size': 32}. Best is trial 1 with value: 0.9048288196218702.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:32:28,116] Trial 24 pruned. 
Error executing job with overrides: ['run=comparative-1-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 289, in main
    OmegaConf.update(cfg, k, v, merge=True)
omegaconf.errors.ConfigAttributeError: Key 'learning_rate' is not in struct
    full_key: learning_rate
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['run=comparative-1-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/main.py", line 38, in main
    subprocess.run(cmd, check=True, env=env)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 108 packages in 575ms
Installed 99 packages in 7.22s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 184246.05 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 146113.23 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 175655.02 examples/s]
Map:   0%|          | 0/14041 [00:00<?, ? examples/s]Map:  14%|â–ˆâ–        | 2000/14041 [00:00<00:00, 13653.71 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4000/14041 [00:00<00:00, 15688.47 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 7000/14041 [00:00<00:00, 17019.97 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9000/14041 [00:00<00:00, 16687.35 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11000/14041 [00:00<00:00, 16714.34 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 13000/14041 [00:00<00:00, 16262.85 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 15763.88 examples/s]
Map:   0%|          | 0/3250 [00:00<?, ? examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 1000/3250 [00:00<00:00, 4154.12 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3000/3250 [00:00<00:00, 9003.89 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 8106.06 examples/s]
[I 2025-11-08 04:36:01,654] A new study created in memory with name: no-name-0c9ccd3c-513a-47bf-987b-9d695054f1e1
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:193: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[I 2025-11-08 04:36:23,200] Trial 0 finished with value: 0.9033736473583706 and parameters: {'training.learning_rate': 0.00014025472753572118, 'adapter.lora_rank': 10, 'training.batch_size': 16}. Best is trial 0 with value: 0.9033736473583706.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:36:39,757] Trial 1 finished with value: 0.9029484423626039 and parameters: {'training.learning_rate': 9.619112495044093e-05, 'adapter.lora_rank': 7, 'training.batch_size': 64}. Best is trial 0 with value: 0.9033736473583706.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:36:56,183] Trial 2 finished with value: 0.9037569131015193 and parameters: {'training.learning_rate': 0.00024193439703533383, 'adapter.lora_rank': 5, 'training.batch_size': 64}. Best is trial 2 with value: 0.9037569131015193.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:12,337] Trial 3 finished with value: 0.9036434133367514 and parameters: {'training.learning_rate': 0.00014399467271357906, 'adapter.lora_rank': 5, 'training.batch_size': 16}. Best is trial 2 with value: 0.9037569131015193.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:28,827] Trial 4 finished with value: 0.9060896014194284 and parameters: {'training.learning_rate': 0.00016650810091857352, 'adapter.lora_rank': 9, 'training.batch_size': 32}. Best is trial 4 with value: 0.9060896014194284.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:32,393] Trial 5 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:35,869] Trial 6 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:42,369] Trial 7 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:45,647] Trial 8 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:48,921] Trial 9 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:52,257] Trial 10 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:37:55,708] Trial 11 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:09,265] Trial 12 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:12,735] Trial 13 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:19,498] Trial 14 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:22,945] Trial 15 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:33,173] Trial 16 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:36,766] Trial 17 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:40,222] Trial 18 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:38:56,745] Trial 19 finished with value: 0.9059003051881994 and parameters: {'training.learning_rate': 0.00019845407479569184, 'adapter.lora_rank': 15, 'training.batch_size': 32}. Best is trial 4 with value: 0.9060896014194284.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:39:00,252] Trial 20 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:39:10,079] Trial 21 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:39:13,695] Trial 22 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:39:17,285] Trial 23 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-08 04:39:24,430] Trial 24 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:306: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
wandb: Currently logged in as: gengaru617 (gengaru617-personal) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run comparative-1-iter1-CoNLL-2003
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.research/iteration1/wandb/run-20251108_043925-comparative-1-iter1-CoNLL-2003
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comparative-1-iter1-CoNLL-2003
wandb: â­ï¸ View project at https://wandb.ai/gengaru617-personal/251106-test
wandb: ðŸš€ View run at https://wandb.ai/gengaru617-personal/251106-test/runs/comparative-1-iter1-CoNLL-2003
Training:   0%|          | 0/3000 [00:00<?, ?it/s]/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Training:   0%|          | 4/3000 [00:00<01:31, 32.74it/s]Training:   0%|          | 8/3000 [00:00<01:40, 29.66it/s]Training:   0%|          | 11/3000 [00:00<01:44, 28.58it/s]Training:   0%|          | 15/3000 [00:00<01:42, 28.99it/s]Training:   1%|          | 18/3000 [00:00<01:42, 29.01it/s]Training:   1%|          | 21/3000 [00:00<01:44, 28.51it/s]Training:   1%|          | 25/3000 [00:00<01:42, 28.91it/s]Training:   1%|          | 28/3000 [00:00<01:41, 29.18it/s]Training:   1%|          | 31/3000 [00:01<01:43, 28.63it/s]Training:   1%|          | 35/3000 [00:01<01:40, 29.51it/s]Training:   1%|â–         | 38/3000 [00:01<01:42, 28.83it/s]Training:   1%|â–         | 42/3000 [00:01<01:41, 29.15it/s]Training:   2%|â–         | 45/3000 [00:01<01:41, 29.18it/s]Training:   2%|â–         | 48/3000 [00:01<01:42, 28.87it/s]Training:   2%|â–         | 52/3000 [00:01<01:39, 29.56it/s]Training:   2%|â–         | 55/3000 [00:01<01:42, 28.82it/s]Training:   2%|â–         | 59/3000 [00:02<01:41, 29.09it/s]Training:   2%|â–         | 63/3000 [00:02<01:39, 29.61it/s]Training:   2%|â–         | 66/3000 [00:02<01:41, 28.99it/s]Training:   2%|â–         | 70/3000 [00:02<01:39, 29.32it/s]Training:   2%|â–         | 73/3000 [00:02<01:39, 29.32it/s]Training:   3%|â–Ž         | 76/3000 [00:02<01:41, 28.78it/s]Training:   3%|â–Ž         | 80/3000 [00:02<01:40, 29.17it/s]Training:   3%|â–Ž         | 83/3000 [00:02<01:41, 28.68it/s]Training:   3%|â–Ž         | 88/3000 [00:02<01:25, 34.04it/s]Training:   3%|â–Ž         | 92/3000 [00:03<01:29, 32.54it/s]Training:   3%|â–Ž         | 96/3000 [00:03<01:31, 31.87it/s]Training:   3%|â–Ž         | 100/3000 [00:03<01:32, 31.22it/s]Training:   3%|â–Ž         | 104/3000 [00:03<01:36, 30.00it/s]Training:   4%|â–Ž         | 108/3000 [00:03<01:36, 30.12it/s]Training:   4%|â–Ž         | 112/3000 [00:03<01:38, 29.34it/s]Training:   4%|â–         | 115/3000 [00:03<01:38, 29.37it/s]Training:   4%|â–         | 118/3000 [00:03<01:39, 28.98it/s]Training:   4%|â–         | 122/3000 [00:04<01:38, 29.29it/s]Training:   4%|â–         | 125/3000 [00:04<01:39, 28.79it/s]Training:   4%|â–         | 129/3000 [00:04<01:39, 28.99it/s]Training:   4%|â–         | 132/3000 [00:04<01:38, 29.20it/s]Training:   4%|â–         | 135/3000 [00:04<01:39, 28.73it/s]Training:   5%|â–         | 139/3000 [00:04<01:38, 29.15it/s]Training:   5%|â–         | 142/3000 [00:04<01:39, 28.68it/s]Training:   5%|â–         | 146/3000 [00:04<01:38, 29.12it/s]Training:   5%|â–         | 149/3000 [00:05<01:38, 28.92it/s]Training:   5%|â–Œ         | 152/3000 [00:05<01:38, 28.80it/s]Training:   5%|â–Œ         | 156/3000 [00:05<01:37, 29.15it/s]Training:   5%|â–Œ         | 159/3000 [00:05<01:37, 29.14it/s]Training:   5%|â–Œ         | 162/3000 [00:05<01:38, 28.90it/s]Training:   6%|â–Œ         | 166/3000 [00:05<01:36, 29.41it/s]Training:   6%|â–Œ         | 169/3000 [00:05<01:36, 29.33it/s]Training:   6%|â–Œ         | 172/3000 [00:05<01:37, 28.96it/s]Training:   6%|â–Œ         | 176/3000 [00:05<01:35, 29.45it/s]Training:   6%|â–Œ         | 179/3000 [00:06<01:37, 28.89it/s]Training:   6%|â–Œ         | 183/3000 [00:06<01:36, 29.34it/s]Training:   6%|â–Œ         | 186/3000 [00:06<01:36, 29.26it/s]Training:   6%|â–‹         | 189/3000 [00:06<01:37, 28.95it/s]Training:   6%|â–‹         | 192/3000 [00:06<01:36, 29.22it/s]Training:   6%|â–‹         | 195/3000 [00:06<01:37, 28.83it/s]Training:   7%|â–‹         | 199/3000 [00:06<01:35, 29.19it/s]/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
Training:   7%|â–‹         | 202/3000 [00:09<13:09,  3.55it/s]Training:   7%|â–‹         | 206/3000 [00:09<09:03,  5.14it/s]Training:   7%|â–‹         | 209/3000 [00:09<07:04,  6.57it/s]Training:   7%|â–‹         | 212/3000 [00:09<05:32,  8.37it/s]Training:   7%|â–‹         | 215/3000 [00:10<04:25, 10.50it/s]Training:   7%|â–‹         | 218/3000 [00:10<03:36, 12.85it/s]Training:   7%|â–‹         | 222/3000 [00:10<02:52, 16.13it/s]Training:   8%|â–Š         | 225/3000 [00:10<02:33, 18.11it/s]Training:   8%|â–Š         | 229/3000 [00:10<02:10, 21.26it/s]Training:   8%|â–Š         | 232/3000 [00:10<02:02, 22.55it/s]Training:   8%|â–Š         | 235/3000 [00:10<01:55, 23.95it/s]Training:   8%|â–Š         | 238/3000 [00:10<01:49, 25.21it/s]Training:   8%|â–Š         | 241/3000 [00:10<01:44, 26.29it/s]Training:   8%|â–Š         | 244/3000 [00:11<01:43, 26.66it/s]Training:   8%|â–Š         | 247/3000 [00:11<01:42, 26.99it/s]Training:   8%|â–Š         | 250/3000 [00:11<01:40, 27.30it/s]Training:   8%|â–Š         | 254/3000 [00:11<01:37, 28.12it/s]Training:   9%|â–Š         | 257/3000 [00:11<01:38, 27.87it/s]Training:   9%|â–Š         | 261/3000 [00:11<01:35, 28.55it/s]Training:   9%|â–‰         | 264/3000 [00:11<01:35, 28.66it/s]Training:   9%|â–‰         | 267/3000 [00:11<01:35, 28.56it/s]Training:   9%|â–‰         | 271/3000 [00:11<01:33, 29.15it/s]Training:   9%|â–‰         | 274/3000 [00:12<01:35, 28.68it/s]Training:   9%|â–‰         | 278/3000 [00:12<01:33, 28.99it/s]Training:   9%|â–‰         | 281/3000 [00:12<01:34, 28.88it/s]Training:  10%|â–‰         | 285/3000 [00:12<01:32, 29.20it/s]Training:  10%|â–‰         | 288/3000 [00:12<01:32, 29.39it/s]Training:  10%|â–‰         | 291/3000 [00:12<01:33, 29.08it/s]Training:  10%|â–‰         | 294/3000 [00:12<01:33, 28.87it/s]Training:  10%|â–‰         | 297/3000 [00:12<01:32, 29.14it/s]Training:  10%|â–ˆ         | 300/3000 [00:12<01:34, 28.60it/s]Training:  10%|â–ˆ         | 304/3000 [00:13<01:32, 29.23it/s]Training:  10%|â–ˆ         | 307/3000 [00:13<01:33, 28.70it/s]Training:  10%|â–ˆ         | 311/3000 [00:13<01:32, 29.16it/s]Training:  10%|â–ˆ         | 314/3000 [00:13<01:31, 29.29it/s]Training:  11%|â–ˆ         | 317/3000 [00:13<01:33, 28.76it/s]Training:  11%|â–ˆ         | 320/3000 [00:13<01:32, 28.87it/s]Training:  11%|â–ˆ         | 324/3000 [00:13<01:31, 29.10it/s]Training:  11%|â–ˆ         | 327/3000 [00:13<01:33, 28.71it/s]Training:  11%|â–ˆ         | 331/3000 [00:14<01:30, 29.36it/s]Training:  11%|â–ˆ         | 334/3000 [00:14<01:31, 29.17it/s]Training:  11%|â–ˆâ–        | 338/3000 [00:14<01:31, 29.14it/s]Training:  11%|â–ˆâ–        | 341/3000 [00:14<01:31, 29.20it/s]Training:  11%|â–ˆâ–        | 344/3000 [00:14<01:32, 28.78it/s]Training:  12%|â–ˆâ–        | 348/3000 [00:14<01:30, 29.43it/s]Training:  12%|â–ˆâ–        | 351/3000 [00:14<01:31, 28.93it/s]Training:  12%|â–ˆâ–        | 355/3000 [00:14<01:30, 29.15it/s]Training:  12%|â–ˆâ–        | 359/3000 [00:14<01:30, 29.23it/s]Training:  12%|â–ˆâ–        | 362/3000 [00:15<01:30, 29.29it/s]Training:  12%|â–ˆâ–        | 365/3000 [00:15<01:29, 29.33it/s]Training:  12%|â–ˆâ–        | 368/3000 [00:15<01:32, 28.57it/s]Training:  12%|â–ˆâ–        | 372/3000 [00:15<01:29, 29.24it/s]Training:  12%|â–ˆâ–Ž        | 375/3000 [00:15<01:29, 29.24it/s]Training:  13%|â–ˆâ–Ž        | 378/3000 [00:15<01:31, 28.74it/s]Training:  13%|â–ˆâ–Ž        | 382/3000 [00:15<01:29, 29.24it/s]Training:  13%|â–ˆâ–Ž        | 385/3000 [00:15<01:29, 29.08it/s]Training:  13%|â–ˆâ–Ž        | 388/3000 [00:15<01:30, 28.84it/s]Training:  13%|â–ˆâ–Ž        | 392/3000 [00:16<01:28, 29.34it/s]Training:  13%|â–ˆâ–Ž        | 395/3000 [00:16<01:30, 28.78it/s]Training:  13%|â–ˆâ–Ž        | 398/3000 [00:16<01:29, 29.02it/s]Training:  13%|â–ˆâ–Ž        | 401/3000 [00:19<12:37,  3.43it/s]Training:  14%|â–ˆâ–Ž        | 405/3000 [00:19<08:34,  5.05it/s]Training:  14%|â–ˆâ–Ž        | 409/3000 [00:19<06:11,  6.98it/s]Training:  14%|â–ˆâ–        | 413/3000 [00:19<04:38,  9.28it/s]Training:  14%|â–ˆâ–        | 417/3000 [00:19<03:37, 11.88it/s]Training:  14%|â–ˆâ–        | 420/3000 [00:19<03:05, 13.87it/s]Training:  14%|â–ˆâ–        | 423/3000 [00:19<02:40, 16.09it/s]Training:  14%|â–ˆâ–        | 427/3000 [00:19<02:14, 19.10it/s]Training:  14%|â–ˆâ–        | 430/3000 [00:20<02:01, 21.12it/s]Training:  14%|â–ˆâ–        | 433/3000 [00:20<01:53, 22.68it/s]Training:  15%|â–ˆâ–        | 437/3000 [00:20<01:43, 24.86it/s]Training:  15%|â–ˆâ–        | 440/3000 [00:20<01:45, 24.19it/s]Training:  15%|â–ˆâ–        | 446/3000 [00:20<01:18, 32.55it/s]Training:  15%|â–ˆâ–Œ        | 450/3000 [00:20<01:14, 34.00it/s]Training:  15%|â–ˆâ–Œ        | 454/3000 [00:20<01:18, 32.31it/s]Training:  15%|â–ˆâ–Œ        | 458/3000 [00:20<01:20, 31.48it/s]Training:  15%|â–ˆâ–Œ        | 462/3000 [00:21<01:21, 31.10it/s]Training:  16%|â–ˆâ–Œ        | 466/3000 [00:21<01:23, 30.38it/s]Training:  16%|â–ˆâ–Œ        | 470/3000 [00:21<01:24, 30.02it/s]Training:  16%|â–ˆâ–Œ        | 474/3000 [00:21<01:24, 29.99it/s]Training:  16%|â–ˆâ–Œ        | 478/3000 [00:21<01:23, 30.08it/s]Training:  16%|â–ˆâ–Œ        | 482/3000 [00:21<01:23, 30.04it/s]Training:  16%|â–ˆâ–Œ        | 486/3000 [00:21<01:25, 29.30it/s]Training:  16%|â–ˆâ–‹        | 490/3000 [00:22<01:24, 29.75it/s]Training:  16%|â–ˆâ–‹        | 493/3000 [00:22<01:25, 29.32it/s]Training:  17%|â–ˆâ–‹        | 496/3000 [00:22<01:24, 29.49it/s]Training:  17%|â–ˆâ–‹        | 499/3000 [00:22<01:26, 29.03it/s]Training:  17%|â–ˆâ–‹        | 503/3000 [00:22<01:24, 29.41it/s]Training:  17%|â–ˆâ–‹        | 507/3000 [00:22<01:24, 29.48it/s]Training:  17%|â–ˆâ–‹        | 510/3000 [00:22<01:24, 29.54it/s]Training:  17%|â–ˆâ–‹        | 513/3000 [00:22<01:25, 29.20it/s]Training:  17%|â–ˆâ–‹        | 517/3000 [00:22<01:23, 29.68it/s]Training:  17%|â–ˆâ–‹        | 520/3000 [00:23<01:25, 29.04it/s]Training:  17%|â–ˆâ–‹        | 524/3000 [00:23<01:23, 29.58it/s]Training:  18%|â–ˆâ–Š        | 527/3000 [00:23<01:25, 29.08it/s]Training:  18%|â–ˆâ–Š        | 531/3000 [00:23<01:23, 29.43it/s]Training:  18%|â–ˆâ–Š        | 534/3000 [00:23<01:23, 29.56it/s]Training:  18%|â–ˆâ–Š        | 538/3000 [00:23<01:22, 29.80it/s]Training:  18%|â–ˆâ–Š        | 541/3000 [00:23<01:23, 29.58it/s]Training:  18%|â–ˆâ–Š        | 545/3000 [00:23<01:20, 30.50it/s]Training:  18%|â–ˆâ–Š        | 549/3000 [00:24<01:22, 29.88it/s]Training:  18%|â–ˆâ–Š        | 552/3000 [00:24<01:23, 29.45it/s]Training:  19%|â–ˆâ–Š        | 556/3000 [00:24<01:21, 29.83it/s]Training:  19%|â–ˆâ–Š        | 559/3000 [00:24<01:22, 29.77it/s]Training:  19%|â–ˆâ–Š        | 562/3000 [00:24<01:23, 29.09it/s]Training:  19%|â–ˆâ–‰        | 566/3000 [00:24<01:22, 29.49it/s]Training:  19%|â–ˆâ–‰        | 569/3000 [00:24<01:24, 28.94it/s]Training:  19%|â–ˆâ–‰        | 573/3000 [00:24<01:22, 29.28it/s]Training:  19%|â–ˆâ–‰        | 576/3000 [00:24<01:22, 29.26it/s]Training:  19%|â–ˆâ–‰        | 579/3000 [00:25<01:22, 29.19it/s]Training:  19%|â–ˆâ–‰        | 583/3000 [00:25<01:22, 29.42it/s]Training:  20%|â–ˆâ–‰        | 586/3000 [00:25<01:22, 29.31it/s]Training:  20%|â–ˆâ–‰        | 590/3000 [00:25<01:20, 30.09it/s]Training:  20%|â–ˆâ–‰        | 593/3000 [00:25<01:20, 29.87it/s]Training:  20%|â–ˆâ–‰        | 596/3000 [00:25<01:22, 29.28it/s]Training:  20%|â–ˆâ–ˆ        | 600/3000 [00:25<01:20, 29.74it/s]Training:  20%|â–ˆâ–ˆ        | 603/3000 [00:28<11:24,  3.50it/s]Training:  20%|â–ˆâ–ˆ        | 607/3000 [00:28<07:55,  5.03it/s]Training:  20%|â–ˆâ–ˆ        | 610/3000 [00:28<06:10,  6.45it/s]Training:  20%|â–ˆâ–ˆ        | 613/3000 [00:28<04:50,  8.21it/s]Training:  21%|â–ˆâ–ˆ        | 617/3000 [00:29<03:37, 10.97it/s]Training:  21%|â–ˆâ–ˆ        | 621/3000 [00:29<02:52, 13.82it/s]Training:  21%|â–ˆâ–ˆ        | 625/3000 [00:29<02:22, 16.68it/s]Training:  21%|â–ˆâ–ˆ        | 628/3000 [00:29<02:07, 18.56it/s]Training:  21%|â–ˆâ–ˆ        | 632/3000 [00:29<01:51, 21.32it/s]Training:  21%|â–ˆâ–ˆ        | 635/3000 [00:29<01:42, 23.02it/s]Training:  21%|â–ˆâ–ˆâ–       | 638/3000 [00:29<01:37, 24.17it/s]Training:  21%|â–ˆâ–ˆâ–       | 642/3000 [00:29<01:30, 25.93it/s]Training:  22%|â–ˆâ–ˆâ–       | 645/3000 [00:30<01:29, 26.45it/s]Training:  22%|â–ˆâ–ˆâ–       | 649/3000 [00:30<01:22, 28.39it/s]Training:  22%|â–ˆâ–ˆâ–       | 653/3000 [00:30<01:22, 28.38it/s]Training:  22%|â–ˆâ–ˆâ–       | 657/3000 [00:30<01:21, 28.91it/s]Training:  22%|â–ˆâ–ˆâ–       | 660/3000 [00:30<01:22, 28.53it/s]Training:  22%|â–ˆâ–ˆâ–       | 664/3000 [00:30<01:20, 29.04it/s]Training:  22%|â–ˆâ–ˆâ–       | 667/3000 [00:30<01:20, 28.95it/s]Training:  22%|â–ˆâ–ˆâ–       | 670/3000 [00:30<01:19, 29.18it/s]Training:  22%|â–ˆâ–ˆâ–       | 673/3000 [00:31<01:20, 28.84it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 677/3000 [00:31<01:19, 29.15it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 680/3000 [00:31<01:20, 28.66it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 684/3000 [00:31<01:18, 29.62it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 687/3000 [00:31<01:18, 29.55it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 690/3000 [00:31<01:18, 29.31it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 693/3000 [00:31<01:19, 28.92it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 697/3000 [00:31<01:18, 29.25it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 700/3000 [00:31<01:18, 29.20it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 703/3000 [00:32<01:19, 28.88it/s]Training:  24%|â–ˆâ–ˆâ–Ž       | 706/3000 [00:32<01:20, 28.65it/s]Training:  24%|â–ˆâ–ˆâ–Ž       | 709/3000 [00:32<01:19, 28.99it/s]Training:  24%|â–ˆâ–ˆâ–Ž       | 712/3000 [00:32<01:20, 28.60it/s]Training:  24%|â–ˆâ–ˆâ–       | 716/3000 [00:32<01:18, 29.19it/s]Training:  24%|â–ˆâ–ˆâ–       | 719/3000 [00:32<01:18, 29.19it/s]Training:  24%|â–ˆâ–ˆâ–       | 723/3000 [00:32<01:17, 29.28it/s]Training:  24%|â–ˆâ–ˆâ–       | 726/3000 [00:32<01:17, 29.36it/s]Training:  24%|â–ˆâ–ˆâ–       | 730/3000 [00:32<01:17, 29.29it/s]Training:  24%|â–ˆâ–ˆâ–       | 733/3000 [00:33<01:17, 29.42it/s]Training:  25%|â–ˆâ–ˆâ–       | 736/3000 [00:33<01:18, 29.02it/s]Training:  25%|â–ˆâ–ˆâ–       | 740/3000 [00:33<01:16, 29.38it/s]Training:  25%|â–ˆâ–ˆâ–       | 743/3000 [00:33<01:17, 28.96it/s]Training:  25%|â–ˆâ–ˆâ–       | 747/3000 [00:33<01:16, 29.52it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 750/3000 [00:33<01:16, 29.44it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 753/3000 [00:33<01:17, 29.12it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 757/3000 [00:33<01:15, 29.77it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 760/3000 [00:33<01:15, 29.63it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 763/3000 [00:34<01:15, 29.60it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 766/3000 [00:34<01:15, 29.60it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 769/3000 [00:34<01:15, 29.67it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 772/3000 [00:34<01:15, 29.68it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 775/3000 [00:34<01:15, 29.66it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 778/3000 [00:34<01:14, 29.65it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 781/3000 [00:34<01:15, 29.54it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 784/3000 [00:34<01:15, 29.53it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 787/3000 [00:34<01:14, 29.55it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 790/3000 [00:35<01:15, 29.20it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 793/3000 [00:35<01:15, 29.24it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 796/3000 [00:35<01:15, 29.34it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 799/3000 [00:35<01:15, 29.32it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 802/3000 [00:38<11:06,  3.30it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 806/3000 [00:38<07:24,  4.93it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 809/3000 [00:38<05:42,  6.40it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 813/3000 [00:38<04:05,  8.90it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 816/3000 [00:38<03:19, 10.97it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 819/3000 [00:38<02:44, 13.24it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 822/3000 [00:38<02:18, 15.73it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 825/3000 [00:38<02:00, 18.11it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 828/3000 [00:38<01:47, 20.23it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 832/3000 [00:39<01:33, 23.17it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 835/3000 [00:39<01:29, 24.28it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 839/3000 [00:39<01:23, 25.99it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 842/3000 [00:39<01:20, 26.83it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 845/3000 [00:39<01:18, 27.56it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 848/3000 [00:39<01:16, 28.18it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 851/3000 [00:39<01:16, 28.12it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 854/3000 [00:39<01:16, 28.07it/s]Training:  29%|â–ˆâ–ˆâ–Š       | 858/3000 [00:39<01:14, 28.84it/s]Training:  29%|â–ˆâ–ˆâ–Š       | 861/3000 [00:40<01:13, 28.95it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 864/3000 [00:40<01:13, 29.20it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 867/3000 [00:40<01:12, 29.41it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 870/3000 [00:40<01:13, 28.90it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 874/3000 [00:40<01:11, 29.82it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 877/3000 [00:40<01:11, 29.54it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 880/3000 [00:40<01:12, 29.06it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 886/3000 [00:40<00:57, 36.52it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 890/3000 [00:40<00:57, 36.56it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 894/3000 [00:41<01:02, 33.67it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 898/3000 [00:41<01:05, 32.30it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 902/3000 [00:41<01:06, 31.40it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 906/3000 [00:41<01:09, 30.23it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 910/3000 [00:41<01:09, 30.25it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 914/3000 [00:41<01:10, 29.43it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 917/3000 [00:41<01:11, 29.06it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 921/3000 [00:42<01:10, 29.57it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 924/3000 [00:42<01:11, 29.05it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 928/3000 [00:42<01:10, 29.44it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 931/3000 [00:42<01:11, 28.94it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 935/3000 [00:42<01:10, 29.40it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 938/3000 [00:42<01:10, 29.40it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 941/3000 [00:42<01:10, 29.12it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 945/3000 [00:42<01:09, 29.46it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 948/3000 [00:42<01:10, 28.91it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 951/3000 [00:43<01:10, 28.88it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 955/3000 [00:43<01:09, 29.28it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 958/3000 [00:43<01:09, 29.37it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 961/3000 [00:43<01:09, 29.20it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 964/3000 [00:43<01:09, 29.24it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 967/3000 [00:43<01:09, 29.29it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 970/3000 [00:43<01:10, 28.98it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 974/3000 [00:43<01:09, 29.24it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 978/3000 [00:43<01:06, 30.23it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 982/3000 [00:44<01:09, 29.25it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 986/3000 [00:44<01:06, 30.46it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 990/3000 [00:44<01:07, 29.85it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 994/3000 [00:44<01:07, 29.69it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 998/3000 [00:44<01:06, 29.93it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1001/3000 [00:47<08:46,  3.79it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1005/3000 [00:47<06:14,  5.32it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 1009/3000 [00:47<04:38,  7.16it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1013/3000 [00:47<03:32,  9.36it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1016/3000 [00:48<02:55, 11.27it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1020/3000 [00:48<02:20, 14.08it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1024/3000 [00:48<01:56, 16.92it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1027/3000 [00:48<01:45, 18.75it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1031/3000 [00:48<01:32, 21.38it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 1034/3000 [00:48<01:25, 22.96it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 1037/3000 [00:48<01:21, 24.19it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 1041/3000 [00:48<01:15, 25.86it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 1045/3000 [00:49<01:12, 27.08it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 1048/3000 [00:49<01:10, 27.60it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1051/3000 [00:49<01:09, 27.91it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1055/3000 [00:49<01:08, 28.59it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1058/3000 [00:49<01:07, 28.91it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1061/3000 [00:49<01:07, 28.55it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1065/3000 [00:49<01:06, 29.05it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1069/3000 [00:49<01:05, 29.45it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1072/3000 [00:49<01:05, 29.46it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1075/3000 [00:50<01:05, 29.47it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1078/3000 [00:50<01:05, 29.50it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1081/3000 [00:50<01:05, 29.43it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1084/3000 [00:50<01:05, 29.45it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1087/3000 [00:50<01:04, 29.45it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 1090/3000 [00:50<01:04, 29.51it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 1093/3000 [00:50<01:04, 29.55it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1096/3000 [00:50<01:04, 29.56it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1099/3000 [00:50<01:04, 29.56it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1102/3000 [00:50<01:04, 29.49it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1105/3000 [00:51<01:04, 29.49it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1108/3000 [00:51<01:04, 29.44it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1111/3000 [00:51<01:04, 29.39it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1114/3000 [00:51<01:04, 29.39it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1117/3000 [00:51<01:04, 29.41it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1120/3000 [00:51<01:04, 29.24it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1123/3000 [00:51<01:04, 29.00it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1127/3000 [00:51<01:03, 29.34it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1131/3000 [00:51<01:03, 29.56it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1134/3000 [00:52<01:03, 29.52it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1137/3000 [00:52<01:03, 29.32it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1141/3000 [00:52<01:02, 29.66it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1144/3000 [00:52<01:02, 29.61it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1147/3000 [00:52<01:03, 29.22it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1150/3000 [00:52<01:03, 29.22it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1153/3000 [00:52<01:02, 29.43it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1156/3000 [00:52<01:03, 28.99it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1160/3000 [00:52<01:02, 29.55it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1163/3000 [00:53<01:02, 29.50it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1166/3000 [00:53<01:02, 29.22it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1170/3000 [00:53<01:02, 29.42it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1173/3000 [00:53<01:03, 28.88it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1177/3000 [00:53<01:09, 26.35it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1183/3000 [00:53<00:53, 33.96it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1187/3000 [00:53<00:52, 34.63it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1191/3000 [00:53<00:55, 32.41it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1195/3000 [00:54<00:56, 31.76it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1199/3000 [00:54<00:57, 31.15it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1203/3000 [00:57<07:12,  4.16it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1207/3000 [00:57<05:17,  5.65it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1210/3000 [00:57<04:16,  6.98it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1214/3000 [00:57<03:13,  9.23it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1217/3000 [00:57<02:39, 11.15it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1220/3000 [00:57<02:13, 13.29it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1224/3000 [00:57<01:48, 16.40it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1227/3000 [00:57<01:36, 18.39it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1231/3000 [00:58<01:23, 21.31it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1234/3000 [00:58<01:17, 22.71it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1238/3000 [00:58<01:11, 24.78it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1241/3000 [00:58<01:07, 25.97it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1244/3000 [00:58<01:06, 26.44it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1248/3000 [00:58<01:03, 27.72it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1251/3000 [00:58<01:02, 28.20it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1254/3000 [00:58<01:01, 28.44it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1257/3000 [00:58<01:01, 28.36it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1261/3000 [00:59<01:00, 28.86it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1264/3000 [00:59<00:59, 29.04it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1268/3000 [00:59<00:58, 29.44it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1271/3000 [00:59<00:58, 29.48it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1274/3000 [00:59<01:00, 28.68it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1278/3000 [00:59<00:56, 30.21it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1282/3000 [00:59<00:57, 29.64it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1285/3000 [00:59<00:58, 29.26it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1289/3000 [00:59<00:58, 29.44it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1292/3000 [01:00<00:58, 29.01it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1295/3000 [01:00<00:58, 29.09it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1298/3000 [01:00<00:59, 28.62it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1302/3000 [01:00<00:58, 29.17it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1305/3000 [01:00<00:59, 28.56it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1309/3000 [01:00<00:57, 29.25it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1312/3000 [01:00<00:59, 28.43it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1316/3000 [01:00<00:57, 29.27it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1320/3000 [01:01<00:56, 30.00it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1324/3000 [01:01<00:54, 30.72it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1328/3000 [01:01<00:53, 31.16it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1332/3000 [01:01<00:53, 31.44it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1336/3000 [01:01<00:52, 31.61it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1340/3000 [01:01<00:52, 31.71it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1344/3000 [01:01<00:52, 31.84it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1348/3000 [01:01<00:51, 32.00it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1352/3000 [01:02<00:51, 31.98it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1356/3000 [01:02<00:51, 32.02it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1360/3000 [01:02<00:51, 32.06it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1364/3000 [01:02<00:50, 32.09it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1368/3000 [01:02<00:50, 32.15it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1372/3000 [01:02<00:50, 32.12it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1376/3000 [01:02<00:51, 31.38it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1380/3000 [01:02<00:51, 31.39it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1384/3000 [01:03<00:53, 30.34it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1388/3000 [01:03<00:54, 29.48it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1392/3000 [01:03<00:53, 29.79it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1395/3000 [01:03<00:54, 29.54it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1398/3000 [01:03<00:54, 29.26it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1401/3000 [01:06<07:31,  3.54it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1405/3000 [01:06<05:12,  5.10it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1409/3000 [01:06<03:47,  7.00it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1412/3000 [01:06<03:02,  8.69it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1416/3000 [01:06<02:19, 11.32it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1420/3000 [01:07<01:51, 14.13it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1423/3000 [01:07<01:37, 16.23it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1426/3000 [01:07<01:25, 18.34it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1430/3000 [01:07<01:14, 21.13it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1433/3000 [01:07<01:08, 22.73it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1437/3000 [01:07<01:03, 24.60it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1441/3000 [01:07<00:59, 26.24it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1444/3000 [01:07<00:58, 26.58it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1448/3000 [01:08<00:56, 27.59it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1451/3000 [01:08<00:55, 28.11it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1454/3000 [01:08<00:55, 27.96it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1458/3000 [01:08<00:53, 28.73it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1461/3000 [01:08<00:53, 28.81it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1464/3000 [01:08<00:53, 28.66it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1467/3000 [01:08<00:53, 28.69it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1470/3000 [01:08<00:52, 28.98it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1473/3000 [01:08<00:53, 28.54it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1477/3000 [01:09<00:52, 28.90it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1480/3000 [01:09<00:52, 28.93it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1484/3000 [01:09<00:52, 29.10it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1487/3000 [01:09<00:51, 29.22it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1490/3000 [01:09<00:52, 28.70it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1494/3000 [01:09<00:51, 29.18it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1497/3000 [01:09<00:52, 28.63it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1501/3000 [01:09<00:50, 29.48it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1504/3000 [01:09<00:50, 29.44it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1507/3000 [01:10<00:51, 28.73it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1511/3000 [01:10<00:50, 29.26it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1514/3000 [01:10<00:50, 29.29it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1517/3000 [01:10<00:51, 29.00it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1521/3000 [01:10<00:50, 29.29it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1524/3000 [01:10<00:50, 29.28it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1528/3000 [01:10<00:49, 29.71it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1532/3000 [01:10<00:49, 29.90it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1536/3000 [01:11<00:48, 30.00it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1540/3000 [01:11<00:48, 29.99it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1543/3000 [01:11<00:48, 29.98it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1547/3000 [01:11<00:48, 30.12it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1551/3000 [01:11<00:48, 30.12it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1555/3000 [01:11<00:47, 30.14it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1559/3000 [01:11<00:47, 30.21it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1563/3000 [01:11<00:48, 29.34it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1567/3000 [01:12<00:48, 29.52it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1570/3000 [01:12<00:48, 29.40it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1573/3000 [01:12<00:49, 28.98it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1577/3000 [01:12<00:48, 29.44it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1580/3000 [01:12<00:49, 28.69it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1584/3000 [01:12<00:47, 29.54it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1587/3000 [01:12<00:48, 28.95it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1591/3000 [01:12<00:47, 29.47it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1594/3000 [01:12<00:47, 29.53it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1597/3000 [01:13<00:47, 29.40it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1600/3000 [01:13<00:48, 29.07it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1603/3000 [01:15<06:46,  3.43it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1607/3000 [01:16<04:37,  5.03it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1610/3000 [01:16<03:34,  6.47it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1614/3000 [01:16<02:36,  8.88it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1617/3000 [01:16<02:06, 10.90it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1620/3000 [01:16<01:44, 13.24it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1623/3000 [01:16<01:27, 15.72it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1626/3000 [01:16<01:16, 18.00it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1629/3000 [01:16<01:08, 20.14it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1633/3000 [01:16<00:59, 22.98it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1636/3000 [01:17<00:56, 24.16it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1639/3000 [01:17<00:54, 25.18it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1642/3000 [01:17<00:52, 26.01it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1645/3000 [01:17<00:51, 26.55it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1649/3000 [01:17<00:48, 27.69it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1652/3000 [01:17<00:48, 28.06it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1655/3000 [01:17<00:47, 28.17it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1659/3000 [01:17<00:46, 28.84it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1662/3000 [01:18<00:46, 28.54it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1665/3000 [01:18<00:46, 28.55it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1668/3000 [01:18<00:46, 28.44it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1672/3000 [01:18<00:45, 29.11it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1676/3000 [01:18<00:44, 29.70it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1679/3000 [01:18<00:45, 28.97it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1683/3000 [01:18<00:44, 29.73it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1686/3000 [01:18<00:44, 29.32it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1689/3000 [01:18<00:44, 29.32it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1692/3000 [01:19<00:44, 29.24it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1695/3000 [01:19<00:45, 28.94it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1699/3000 [01:19<00:44, 29.26it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1702/3000 [01:19<00:45, 28.75it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1705/3000 [01:19<00:44, 28.83it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1709/3000 [01:19<00:44, 29.14it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1712/3000 [01:19<00:44, 29.01it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1715/3000 [01:19<00:45, 28.55it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1719/3000 [01:19<00:44, 29.10it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1722/3000 [01:20<00:45, 28.35it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1726/3000 [01:20<00:42, 29.78it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1729/3000 [01:20<00:43, 29.31it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1732/3000 [01:20<00:43, 29.05it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1736/3000 [01:20<00:42, 29.46it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1739/3000 [01:20<00:43, 29.06it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1743/3000 [01:20<00:42, 29.53it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1746/3000 [01:20<00:43, 28.90it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1750/3000 [01:21<00:43, 29.07it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1753/3000 [01:21<00:42, 29.22it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1756/3000 [01:21<00:47, 26.07it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1762/3000 [01:21<00:36, 33.99it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1766/3000 [01:21<00:35, 35.24it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1770/3000 [01:21<00:35, 35.10it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1774/3000 [01:21<00:34, 35.29it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1778/3000 [01:21<00:37, 32.76it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1782/3000 [01:21<00:38, 31.68it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1786/3000 [01:22<00:39, 31.04it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1790/3000 [01:22<00:39, 30.50it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1794/3000 [01:22<00:39, 30.20it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1798/3000 [01:22<00:40, 29.89it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1802/3000 [01:25<04:53,  4.09it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1806/3000 [01:25<03:34,  5.58it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1809/3000 [01:25<02:52,  6.92it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1813/3000 [01:25<02:08,  9.22it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1816/3000 [01:25<01:46, 11.14it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1820/3000 [01:26<01:24, 13.98it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1824/3000 [01:26<01:09, 16.85it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1827/3000 [01:26<01:02, 18.76it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1831/3000 [01:26<00:54, 21.33it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1835/3000 [01:26<00:49, 23.52it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1838/3000 [01:26<00:47, 24.38it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1842/3000 [01:26<00:44, 26.02it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1846/3000 [01:26<00:42, 27.10it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1849/3000 [01:27<00:42, 27.27it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1853/3000 [01:27<00:40, 28.14it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1856/3000 [01:27<00:40, 28.44it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1859/3000 [01:27<00:40, 28.50it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1863/3000 [01:27<00:39, 29.14it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1866/3000 [01:27<00:38, 29.20it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1869/3000 [01:27<00:39, 28.91it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1873/3000 [01:27<00:38, 29.41it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1876/3000 [01:27<00:38, 29.02it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1879/3000 [01:28<00:38, 29.13it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1882/3000 [01:28<00:38, 29.35it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1885/3000 [01:28<00:38, 29.03it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1889/3000 [01:28<00:37, 29.43it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1892/3000 [01:28<00:37, 29.54it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1895/3000 [01:28<00:38, 29.02it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1899/3000 [01:28<00:37, 29.29it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1902/3000 [01:28<00:37, 29.40it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1905/3000 [01:28<00:37, 28.95it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1909/3000 [01:29<00:37, 29.44it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1912/3000 [01:29<00:36, 29.46it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1916/3000 [01:29<00:36, 29.37it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1919/3000 [01:29<00:36, 29.51it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1922/3000 [01:29<00:37, 28.91it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1926/3000 [01:29<00:36, 29.44it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1930/3000 [01:29<00:36, 29.50it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3000 [01:29<00:36, 29.40it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1937/3000 [01:30<00:36, 29.48it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1940/3000 [01:30<00:35, 29.46it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1943/3000 [01:30<00:35, 29.49it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1946/3000 [01:30<00:35, 29.49it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1949/3000 [01:30<00:35, 29.33it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1953/3000 [01:30<00:35, 29.57it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1956/3000 [01:30<00:35, 29.09it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1959/3000 [01:30<00:35, 29.32it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1962/3000 [01:30<00:35, 28.94it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1966/3000 [01:31<00:35, 29.37it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1969/3000 [01:31<00:35, 29.41it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1973/3000 [01:31<00:34, 29.53it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1976/3000 [01:31<00:34, 29.38it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1979/3000 [01:31<00:35, 28.88it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1983/3000 [01:31<00:34, 29.77it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1986/3000 [01:31<00:34, 29.43it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1989/3000 [01:31<00:34, 29.09it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1993/3000 [01:31<00:33, 30.21it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1997/3000 [01:32<00:33, 29.65it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2001/3000 [01:35<04:17,  3.88it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2005/3000 [01:35<03:04,  5.40it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2008/3000 [01:35<02:25,  6.80it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2011/3000 [01:35<01:55,  8.54it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2014/3000 [01:35<01:33, 10.55it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2018/3000 [01:35<01:12, 13.54it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2022/3000 [01:35<00:59, 16.53it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2025/3000 [01:35<00:52, 18.64it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2028/3000 [01:35<00:47, 20.51it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2032/3000 [01:36<00:41, 23.22it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2035/3000 [01:36<00:39, 24.32it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2039/3000 [01:36<00:36, 26.03it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2043/3000 [01:36<00:35, 27.12it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2046/3000 [01:36<00:34, 27.71it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2049/3000 [01:36<00:33, 28.13it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2052/3000 [01:36<00:33, 28.56it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2055/3000 [01:36<00:32, 28.71it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2058/3000 [01:36<00:32, 29.02it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2061/3000 [01:37<00:32, 28.59it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2065/3000 [01:37<00:32, 29.11it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2069/3000 [01:37<00:31, 29.42it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2072/3000 [01:37<00:32, 28.86it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2076/3000 [01:37<00:31, 29.75it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2079/3000 [01:37<00:31, 29.41it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2083/3000 [01:37<00:31, 29.23it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2087/3000 [01:37<00:30, 29.53it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2090/3000 [01:38<00:30, 29.48it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2093/3000 [01:38<00:31, 29.18it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2097/3000 [01:38<00:30, 29.62it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2100/3000 [01:38<00:30, 29.50it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2103/3000 [01:38<00:30, 29.25it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2107/3000 [01:38<00:30, 29.50it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2110/3000 [01:38<00:30, 29.30it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2113/3000 [01:38<00:30, 29.01it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2117/3000 [01:38<00:29, 29.53it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2120/3000 [01:39<00:30, 29.02it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2124/3000 [01:39<00:29, 29.89it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2127/3000 [01:39<00:29, 29.29it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2131/3000 [01:39<00:29, 29.95it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2134/3000 [01:39<00:29, 29.30it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2138/3000 [01:39<00:29, 29.68it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2141/3000 [01:39<00:29, 29.02it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2145/3000 [01:39<00:28, 29.49it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2148/3000 [01:39<00:29, 28.99it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2152/3000 [01:40<00:28, 29.52it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2155/3000 [01:40<00:28, 29.53it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2159/3000 [01:40<00:28, 29.54it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2162/3000 [01:40<00:28, 29.65it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2165/3000 [01:40<00:28, 29.13it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2169/3000 [01:40<00:28, 29.67it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2172/3000 [01:40<00:28, 29.07it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2176/3000 [01:40<00:27, 29.63it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2179/3000 [01:41<00:28, 29.04it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2183/3000 [01:41<00:27, 29.87it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2186/3000 [01:41<00:27, 29.70it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2189/3000 [01:41<00:27, 29.46it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2193/3000 [01:41<00:27, 29.72it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2196/3000 [01:41<00:28, 28.05it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2201/3000 [01:44<03:16,  4.06it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2205/3000 [01:44<02:22,  5.60it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2209/3000 [01:44<01:46,  7.43it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2213/3000 [01:44<01:21,  9.64it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2216/3000 [01:44<01:08, 11.48it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2220/3000 [01:45<00:54, 14.35it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2223/3000 [01:45<00:47, 16.44it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2226/3000 [01:45<00:41, 18.60it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2229/3000 [01:45<00:37, 20.58it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2233/3000 [01:45<00:33, 23.09it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2236/3000 [01:45<00:31, 24.05it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2240/3000 [01:45<00:29, 25.99it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2243/3000 [01:45<00:28, 26.42it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2247/3000 [01:46<00:26, 27.89it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2250/3000 [01:46<00:26, 27.87it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2254/3000 [01:46<00:26, 28.62it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2257/3000 [01:46<00:25, 28.79it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2261/3000 [01:46<00:25, 29.05it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2264/3000 [01:46<00:25, 28.76it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2268/3000 [01:46<00:24, 29.48it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2271/3000 [01:46<00:25, 28.91it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2275/3000 [01:47<00:24, 29.04it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2279/3000 [01:47<00:23, 30.32it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2283/3000 [01:47<00:24, 29.71it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2287/3000 [01:47<00:24, 29.63it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2291/3000 [01:47<00:23, 29.78it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2294/3000 [01:47<00:23, 29.65it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2297/3000 [01:47<00:24, 28.98it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2301/3000 [01:47<00:22, 30.76it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2305/3000 [01:47<00:23, 30.11it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2309/3000 [01:48<00:23, 29.75it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2313/3000 [01:48<00:23, 29.66it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2316/3000 [01:48<00:23, 29.66it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2319/3000 [01:48<00:23, 29.15it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2323/3000 [01:48<00:22, 29.45it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2326/3000 [01:48<00:23, 28.88it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2330/3000 [01:48<00:22, 29.32it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2333/3000 [01:48<00:23, 28.90it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2337/3000 [01:49<00:22, 29.29it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2340/3000 [01:49<00:22, 29.26it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2344/3000 [01:49<00:22, 29.40it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2347/3000 [01:49<00:22, 29.35it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2350/3000 [01:49<00:22, 29.45it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2353/3000 [01:49<00:22, 29.21it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2357/3000 [01:49<00:22, 29.15it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2361/3000 [01:49<00:21, 29.91it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2364/3000 [01:50<00:21, 29.31it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2368/3000 [01:50<00:21, 30.05it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2371/3000 [01:50<00:21, 29.84it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2374/3000 [01:50<00:21, 28.87it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2378/3000 [01:50<00:20, 30.04it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2381/3000 [01:50<00:21, 29.37it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2385/3000 [01:50<00:20, 29.81it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2388/3000 [01:50<00:20, 29.25it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2392/3000 [01:50<00:20, 29.59it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2395/3000 [01:51<00:20, 29.35it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2398/3000 [01:51<00:20, 29.02it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2401/3000 [01:54<02:55,  3.41it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2405/3000 [01:54<01:59,  4.99it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2409/3000 [01:54<01:25,  6.91it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2412/3000 [01:54<01:08,  8.64it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2415/3000 [01:54<00:54, 10.71it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2418/3000 [01:54<00:44, 12.98it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2421/3000 [01:54<00:37, 15.39it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2425/3000 [01:54<00:30, 18.56it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2428/3000 [01:54<00:27, 20.64it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2431/3000 [01:55<00:25, 22.26it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2435/3000 [01:55<00:23, 24.53it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2439/3000 [01:55<00:21, 26.24it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2442/3000 [01:55<00:20, 26.65it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2446/3000 [01:55<00:20, 27.65it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2449/3000 [01:55<00:19, 28.04it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2453/3000 [01:55<00:19, 28.58it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2456/3000 [01:55<00:18, 28.72it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2460/3000 [01:56<00:18, 29.08it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2464/3000 [01:56<00:18, 29.47it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2467/3000 [01:56<00:18, 29.48it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2470/3000 [01:56<00:18, 29.22it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2474/3000 [01:56<00:17, 29.41it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2477/3000 [01:56<00:17, 29.24it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2480/3000 [01:56<00:17, 29.24it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2483/3000 [01:56<00:17, 29.03it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2486/3000 [01:56<00:17, 29.30it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2489/3000 [01:57<00:17, 28.84it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2492/3000 [01:57<00:17, 29.13it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2495/3000 [01:57<00:17, 28.69it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2498/3000 [01:57<00:17, 28.93it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2502/3000 [01:57<00:16, 29.69it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2505/3000 [01:57<00:16, 29.40it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2508/3000 [01:57<00:16, 29.15it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2511/3000 [01:57<00:16, 29.39it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2514/3000 [01:57<00:16, 29.09it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2517/3000 [01:57<00:16, 29.08it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2521/3000 [01:58<00:16, 29.36it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2525/3000 [01:58<00:16, 29.67it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2528/3000 [01:58<00:16, 29.00it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2532/3000 [01:58<00:15, 29.59it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2535/3000 [01:58<00:15, 29.44it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2538/3000 [01:58<00:15, 29.11it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2541/3000 [01:58<00:15, 29.12it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2544/3000 [01:58<00:15, 28.87it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2548/3000 [01:59<00:15, 29.36it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2551/3000 [01:59<00:15, 29.32it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2555/3000 [01:59<00:15, 29.59it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2559/3000 [01:59<00:14, 29.41it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2562/3000 [01:59<00:14, 29.53it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2565/3000 [01:59<00:14, 29.07it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2569/3000 [01:59<00:14, 29.51it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2572/3000 [01:59<00:14, 29.28it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2575/3000 [01:59<00:14, 28.88it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2578/3000 [02:00<00:14, 28.48it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2582/3000 [02:00<00:13, 30.54it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2586/3000 [02:00<00:13, 30.28it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2590/3000 [02:00<00:13, 29.81it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2594/3000 [02:00<00:13, 30.13it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2598/3000 [02:00<00:13, 29.17it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2601/3000 [02:03<01:45,  3.77it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2605/3000 [02:03<01:14,  5.32it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2609/3000 [02:03<00:54,  7.17it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2613/3000 [02:03<00:41,  9.39it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2617/3000 [02:04<00:32, 11.92it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2620/3000 [02:04<00:27, 13.87it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2624/3000 [02:04<00:22, 16.78it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2627/3000 [02:04<00:19, 18.66it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2631/3000 [02:04<00:17, 21.32it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2634/3000 [02:04<00:17, 21.37it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2640/3000 [02:04<00:12, 28.87it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2644/3000 [02:04<00:11, 30.52it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2648/3000 [02:05<00:11, 29.86it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2652/3000 [02:05<00:11, 29.60it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2656/3000 [02:05<00:11, 29.75it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2660/3000 [02:05<00:11, 29.50it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2664/3000 [02:05<00:11, 29.59it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2668/3000 [02:05<00:11, 29.65it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2672/3000 [02:05<00:11, 29.56it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2675/3000 [02:06<00:11, 29.23it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2679/3000 [02:06<00:10, 29.64it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2682/3000 [02:06<00:10, 29.49it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2685/3000 [02:06<00:10, 29.13it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2688/3000 [02:06<00:10, 29.16it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2692/3000 [02:06<00:10, 29.42it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2695/3000 [02:06<00:10, 28.96it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 2699/3000 [02:06<00:10, 29.72it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2702/3000 [02:06<00:10, 29.70it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2705/3000 [02:07<00:10, 29.20it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2709/3000 [02:07<00:09, 29.72it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2712/3000 [02:07<00:09, 29.12it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2716/3000 [02:07<00:09, 29.36it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2719/3000 [02:07<00:09, 29.48it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2722/3000 [02:07<00:09, 28.94it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2725/3000 [02:07<00:09, 28.77it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2729/3000 [02:07<00:09, 29.15it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2732/3000 [02:07<00:09, 28.95it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2736/3000 [02:08<00:08, 29.43it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2739/3000 [02:08<00:08, 29.01it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2743/3000 [02:08<00:08, 29.33it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2746/3000 [02:08<00:08, 28.82it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2750/3000 [02:08<00:08, 29.60it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2753/3000 [02:08<00:08, 28.88it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2757/3000 [02:08<00:08, 29.49it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2760/3000 [02:08<00:08, 28.88it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2764/3000 [02:09<00:08, 29.27it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2767/3000 [02:09<00:08, 28.71it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2771/3000 [02:09<00:07, 29.05it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2774/3000 [02:09<00:07, 29.20it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2777/3000 [02:09<00:07, 28.70it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2781/3000 [02:09<00:07, 29.31it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2784/3000 [02:09<00:07, 29.31it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2787/3000 [02:09<00:07, 29.00it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2791/3000 [02:09<00:07, 29.42it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2794/3000 [02:10<00:07, 29.19it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2798/3000 [02:10<00:06, 29.38it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2801/3000 [02:13<01:00,  3.28it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2805/3000 [02:13<00:41,  4.73it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2810/3000 [02:13<00:26,  7.13it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2815/3000 [02:13<00:18, 10.05it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2820/3000 [02:13<00:13, 13.53it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2824/3000 [02:13<00:10, 16.48it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2829/3000 [02:14<00:08, 20.91it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2834/3000 [02:14<00:06, 24.95it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2839/3000 [02:14<00:05, 29.55it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2844/3000 [02:14<00:04, 31.81it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2850/3000 [02:14<00:04, 37.03it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2855/3000 [02:14<00:03, 37.61it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2861/3000 [02:14<00:03, 41.60it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2866/3000 [02:14<00:03, 40.56it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2872/3000 [02:14<00:02, 44.03it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2877/3000 [02:15<00:02, 42.96it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2882/3000 [02:15<00:02, 43.72it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2887/3000 [02:15<00:02, 43.11it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2892/3000 [02:15<00:02, 44.85it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2897/3000 [02:15<00:02, 44.25it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2902/3000 [02:15<00:02, 39.83it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2907/3000 [02:15<00:02, 36.16it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2911/3000 [02:15<00:02, 34.35it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2915/3000 [02:16<00:02, 32.94it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2919/3000 [02:16<00:02, 31.60it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2923/3000 [02:16<00:02, 30.57it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2927/3000 [02:16<00:02, 31.40it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2931/3000 [02:16<00:02, 30.60it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2935/3000 [02:16<00:02, 30.00it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2939/3000 [02:16<00:02, 30.34it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2943/3000 [02:17<00:01, 30.26it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2947/3000 [02:17<00:01, 29.95it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2951/3000 [02:17<00:01, 29.76it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2954/3000 [02:17<00:01, 29.44it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2958/3000 [02:17<00:01, 29.68it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2961/3000 [02:17<00:01, 29.40it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2964/3000 [02:17<00:01, 28.93it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2968/3000 [02:17<00:01, 29.42it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2971/3000 [02:18<00:01, 28.91it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2975/3000 [02:18<00:00, 29.50it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2978/3000 [02:18<00:00, 29.42it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2981/3000 [02:18<00:00, 29.48it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2984/3000 [02:18<00:00, 29.15it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2987/3000 [02:18<00:00, 28.74it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2991/3000 [02:18<00:00, 29.18it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2995/3000 [02:18<00:00, 29.43it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2998/3000 [02:18<00:00, 28.94it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [02:21<00:00, 21.17it/s]
wandb: uploading artifact comparative-1-iter1-CoNLL-2003_model; updating run metadata
wandb: uploading artifact comparative-1-iter1-CoNLL-2003_model; uploading wandb-summary.json; uploading config.yaml
wandb: uploading artifact comparative-1-iter1-CoNLL-2003_model; uploading config.yaml
wandb: uploading artifact comparative-1-iter1-CoNLL-2003_model
wandb: 
wandb: Run history:
wandb: eval_accuracy â–â–ƒâ–…â–‡â–‡â–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:       eval_f1 â–â–ƒâ–„â–‡â–‡â–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:    gpu_mem_mb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:     grad_norm â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–…â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–ƒâ–‚â–ˆâ–â–â–â–â–â–â–â–â–
wandb:            lr â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:          step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    train_loss â–…â–…â–„â–„â–ˆâ–ƒâ–…â–ƒâ–‚â–„â–‚â–‚â–ƒâ–‚â–„â–ƒâ–‚â–â–‚â–„â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–
wandb: 
wandb: Run summary:
wandb: adapter_param_MB 253.1807
wandb:    eval_accuracy 0.98725
wandb:          eval_f1 0.95015
wandb:   final_accuracy 0.98725
wandb:         final_f1 0.95015
wandb:       gpu_mem_mb 831.46045
wandb:        grad_norm 2864.40015
wandb:               lr 0
wandb:             step 3000
wandb:       train_loss 0.00051
wandb: 
wandb: ðŸš€ View run comparative-1-iter1-CoNLL-2003 at: https://wandb.ai/gengaru617-personal/251106-test/runs/comparative-1-iter1-CoNLL-2003
wandb: â­ï¸ View project at: https://wandb.ai/gengaru617-personal/251106-test
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./.research/iteration1/wandb/run-20251108_043925-comparative-1-iter1-CoNLL-2003/logs
