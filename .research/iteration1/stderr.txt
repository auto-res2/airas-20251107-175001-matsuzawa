Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 665ms
Installed 99 packages in 4.97s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 238784.84 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 260610.41 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 348256.23 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 2067.38 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 1645.30 examples/s]
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:303: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
Training:   0%|          | 0/2 [00:00<?, ?it/s]/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.92s/it]
Downloading builder script: 0.00B [00:00, ?B/s][ADownloading builder script: 6.34kB [00:00, 11.2MB/s]
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.90s/it]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.15s/it]
Using CPython 3.11.14
Creating virtual environment at: .venv
Resolved 108 packages in 589ms
Installed 99 packages in 7.12s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 164689.73 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 159079.10 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 207828.31 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 2186.83 examples/s]
Map:   0%|          | 0/64 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 7167.45 examples/s]
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:303: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
Training:   0%|          | 0/2 [00:00<?, ?it/s]/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.58it/s]
Downloading builder script: 0.00B [00:00, ?B/s][ADownloading builder script: 6.34kB [00:00, 8.21MB/s]
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.51s/it]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.99s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 164ms
Installed 99 packages in 5.08s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 196969.88 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 250293.56 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 321464.31 examples/s]
Map:   0%|          | 0/14041 [00:00<?, ? examples/s]Map:  21%|â–ˆâ–ˆâ–       | 3000/14041 [00:00<00:00, 21196.52 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 7000/14041 [00:00<00:00, 23902.72 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 11000/14041 [00:00<00:00, 22695.60 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 14000/14041 [00:00<00:00, 21556.35 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 21271.67 examples/s]
Map:   0%|          | 0/3250 [00:00<?, ? examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3000/3250 [00:00<00:00, 8640.46 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 8720.57 examples/s]
[I 2025-11-07 19:23:47,629] A new study created in memory with name: no-name-6e6072db-4519-4719-b44e-b681aa292ffb
[W 2025-11-07 19:23:47,630] Trial 0 failed with parameters: {} because of the following error: AttributeError("type object 'OmegaConf' has no attribute 'deepcopy'").
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 283, in <lambda>
    lambda trial: _objective(
                  ^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 184, in _objective
    cfg_tmp = OmegaConf.deepcopy(cfg)
              ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OmegaConf' has no attribute 'deepcopy'
[W 2025-11-07 19:23:47,630] Trial 0 failed with value None.
Error executing job with overrides: ['run=proposed-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 282, in main
    study.optimize(
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 63, in _optimize
    _optimize_sequential(
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 258, in _run_trial
    raise func_err
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 283, in <lambda>
    lambda trial: _objective(
                  ^^^^^^^^^^^
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 184, in _objective
    cfg_tmp = OmegaConf.deepcopy(cfg)
              ^^^^^^^^^^^^^^^^^^
AttributeError: type object 'OmegaConf' has no attribute 'deepcopy'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['run=proposed-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/main.py", line 38, in main
    subprocess.run(cmd, check=True, env=env)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=proposed-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 130ms
Installed 99 packages in 4.79s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 198800.36 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 254480.23 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 270628.07 examples/s]
Map:   0%|          | 0/14041 [00:00<?, ? examples/s]Map:  14%|â–ˆâ–        | 2000/14041 [00:00<00:00, 15649.07 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4000/14041 [00:00<00:00, 16578.04 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6000/14041 [00:00<00:00, 16931.33 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8000/14041 [00:00<00:00, 16061.48 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 10000/14041 [00:00<00:00, 15437.42 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12000/14041 [00:00<00:00, 14894.84 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 14000/14041 [00:00<00:00, 14364.15 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 14721.58 examples/s]
Map:   0%|          | 0/3250 [00:00<?, ? examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3250 [00:00<00:00, 13651.98 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 12921.75 examples/s]
[I 2025-11-07 19:26:25,584] A new study created in memory with name: no-name-0b666b05-63da-4058-9191-0b68e31314c6
[W 2025-11-07 19:26:25,589] Trial 0 failed with parameters: {'learning_rate': 2.149151856614601e-05} because of the following error: ConfigAttributeError("Key 'learning_rate' is not in struct\n    full_key: learning_rate\n    object_type=dict").
Traceback (most recent call last):
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 284, in <lambda>
    lambda trial: _objective(
                  ^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 186, in _objective
    _suggest_and_apply(trial, cfg_tmp)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 174, in _suggest_and_apply
    OmegaConf.update(cfg, path, value, merge=True)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/omegaconf.py", line 741, in update
    root.__setattr__(last_key, value)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/dictconfig.py", line 337, in __setattr__
    raise e
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/dictconfig.py", line 334, in __setattr__
    self.__set_impl(key, value)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/dictconfig.py", line 318, in __set_impl
    self._set_item_impl(key, value)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/basecontainer.py", line 549, in _set_item_impl
    self._validate_set(key, value)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/dictconfig.py", line 180, in _validate_set
    target = self._get_node(key) if key is not None else self
             ^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/dictconfig.py", line 475, in _get_node
    self._validate_get(key)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/dictconfig.py", line 164, in _validate_get
    self._format_and_raise(
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/base.py", line 231, in _format_and_raise
    format_and_raise(
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/_utils.py", line 899, in format_and_raise
    _raise(ex, cause)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/omegaconf/_utils.py", line 797, in _raise
    raise ex.with_traceback(sys.exc_info()[2])  # set env var OC_CAUSE=1 for full trace
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
omegaconf.errors.ConfigAttributeError: Key 'learning_rate' is not in struct
    full_key: learning_rate
    object_type=dict
[W 2025-11-07 19:26:25,598] Trial 0 failed with value None.
Error executing job with overrides: ['run=proposed-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 283, in main
    study.optimize(
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 63, in _optimize
    _optimize_sequential(
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 160, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 258, in _run_trial
    raise func_err
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/optuna/study/_optimize.py", line 201, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 284, in <lambda>
    lambda trial: _objective(
                  ^^^^^^^^^^^
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 186, in _objective
    _suggest_and_apply(trial, cfg_tmp)
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py", line 174, in _suggest_and_apply
    OmegaConf.update(cfg, path, value, merge=True)
omegaconf.errors.ConfigAttributeError: Key 'learning_rate' is not in struct
    full_key: learning_rate
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['run=proposed-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']
Traceback (most recent call last):
  File "/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/main.py", line 38, in main
    subprocess.run(cmd, check=True, env=env)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3', '-u', '-m', 'src.train', 'run=proposed-iter1-CoNLL-2003', 'results_dir=.research/iteration1', 'mode=full']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 108 packages in 607ms
Installed 99 packages in 5.01s
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.2
 + aiosignal==1.4.0
 + alembic==1.17.1
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.4.1
 + dill==0.4.0
 + evaluate==0.4.6
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.10.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + joblib==1.5.2
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.18
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + pyarrow==22.0.0
 + pydantic==2.12.4
 + pydantic-core==2.41.5
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.11.3
 + requests==2.32.5
 + safetensors==0.6.2
 + scikit-learn==1.7.2
 + scipy==1.16.3
 + seaborn==0.13.2
 + sentry-sdk==2.43.0
 + seqeval==1.2.2
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + threadpoolctl==3.6.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + unihyperlora-experiments==0.1.0 (from file:///home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa)
 + urllib3==2.5.0
 + wandb==0.22.3
 + xxhash==3.6.0
 + yarl==1.22.0
Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 262068.18 examples/s]
Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 251703.16 examples/s]
Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 274641.25 examples/s]
Map:   0%|          | 0/14041 [00:00<?, ? examples/s]Map:  14%|â–ˆâ–        | 2000/14041 [00:00<00:00, 15830.07 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 4000/14041 [00:00<00:00, 16771.49 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6000/14041 [00:00<00:00, 17212.38 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 8000/14041 [00:00<00:00, 16184.04 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 10000/14041 [00:00<00:00, 15550.06 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 12000/14041 [00:00<00:00, 15042.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 14000/14041 [00:00<00:00, 14497.52 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:00<00:00, 14873.35 examples/s]
Map:   0%|          | 0/3250 [00:00<?, ? examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2000/3250 [00:00<00:00, 13775.10 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 13060.77 examples/s]
[I 2025-11-07 19:29:12,248] A new study created in memory with name: no-name-b7c476fa-e40e-4159-9e32-2b4307e6ae97
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:194: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
[I 2025-11-07 19:30:02,276] Trial 0 finished with value: 0.0 and parameters: {'training.learning_rate': 3.564245840566074e-05, 'adapter.lambda_F': 0.03859739934723842, 'adapter.lambda_Q': 2.027546932193335e-06, 'adapter.router_top_k': 1}. Best is trial 0 with value: 0.0.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:30:44,183] Trial 1 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 0.00012878465214369208, 'adapter.lambda_F': 0.07538913943121849, 'adapter.lambda_Q': 2.5242238686854378e-06, 'adapter.router_top_k': 1}. Best is trial 1 with value: 0.0677983917590792.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:31:26,449] Trial 2 finished with value: 0.00468958867486249 and parameters: {'training.learning_rate': 0.00036747931619310694, 'adapter.lambda_F': 0.03520348132250096, 'adapter.lambda_Q': 1.4446638370388827e-06, 'adapter.router_top_k': 1}. Best is trial 1 with value: 0.0677983917590792.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:32:12,345] Trial 3 finished with value: 0.004688766679082404 and parameters: {'training.learning_rate': 1.4394720623656e-05, 'adapter.lambda_F': 0.0689506602079792, 'adapter.lambda_Q': 2.6001426648094675e-05, 'adapter.router_top_k': 2}. Best is trial 1 with value: 0.0677983917590792.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:32:54,633] Trial 4 finished with value: 0.07605695509309966 and parameters: {'training.learning_rate': 3.06140370254887e-05, 'adapter.lambda_F': 0.0893757085082207, 'adapter.lambda_Q': 1.1950351945571423e-05, 'adapter.router_top_k': 1}. Best is trial 4 with value: 0.07605695509309966.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:33:41,291] Trial 5 finished with value: 0.07603313028616504 and parameters: {'training.learning_rate': 0.00026353812580390246, 'adapter.lambda_F': 0.08581007404468874, 'adapter.lambda_Q': 4.108313829016581e-05, 'adapter.router_top_k': 2}. Best is trial 4 with value: 0.07605695509309966.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:34:27,079] Trial 6 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 0.00028916956050076286, 'adapter.lambda_F': 0.05086700454063619, 'adapter.lambda_Q': 5.427584384850126e-05, 'adapter.router_top_k': 2}. Best is trial 4 with value: 0.07605695509309966.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:34:35,732] Trial 7 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:35:17,617] Trial 8 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 8.543455239062686e-05, 'adapter.lambda_F': 0.019198183375699243, 'adapter.lambda_Q': 2.7283032393057634e-06, 'adapter.router_top_k': 1}. Best is trial 4 with value: 0.07605695509309966.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:36:00,225] Trial 9 finished with value: 0.07624350571059035 and parameters: {'training.learning_rate': 3.7895483806895786e-05, 'adapter.lambda_F': 0.030345877509192105, 'adapter.lambda_Q': 8.067949987291364e-05, 'adapter.router_top_k': 1}. Best is trial 9 with value: 0.07624350571059035.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:36:09,587] Trial 10 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:36:18,217] Trial 11 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:36:26,837] Trial 12 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:37:08,613] Trial 13 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 8.166664712274638e-05, 'adapter.lambda_F': 0.029633947433584804, 'adapter.lambda_Q': 2.2964638455599748e-05, 'adapter.router_top_k': 1}. Best is trial 9 with value: 0.07624350571059035.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:37:50,625] Trial 14 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 4.966017207073475e-05, 'adapter.lambda_F': 0.05071236991040044, 'adapter.lambda_Q': 5.139121035203179e-06, 'adapter.router_top_k': 1}. Best is trial 9 with value: 0.07624350571059035.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:37:59,252] Trial 15 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:38:07,884] Trial 16 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:38:16,474] Trial 17 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:39:02,262] Trial 18 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 3.325782204521967e-05, 'adapter.lambda_F': 0.022831558446673207, 'adapter.lambda_Q': 4.537095587202208e-05, 'adapter.router_top_k': 2}. Best is trial 9 with value: 0.07624350571059035.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:39:11,057] Trial 19 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:39:19,656] Trial 20 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:40:05,489] Trial 21 finished with value: 0.0677983917590792 and parameters: {'training.learning_rate': 0.00024727379593590784, 'adapter.lambda_F': 0.08609059543937071, 'adapter.lambda_Q': 4.0635566009480665e-05, 'adapter.router_top_k': 2}. Best is trial 9 with value: 0.07624350571059035.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:40:14,930] Trial 22 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:41:01,258] Trial 23 finished with value: 0.07632818490524701 and parameters: {'training.learning_rate': 0.00018716843924922904, 'adapter.lambda_F': 0.088301583692537, 'adapter.lambda_Q': 3.187974934174355e-05, 'adapter.router_top_k': 2}. Best is trial 23 with value: 0.07632818490524701.
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[I 2025-11-07 19:41:10,831] Trial 24 pruned. 
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:304: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None
wandb: Currently logged in as: gengaru617 (gengaru617-personal) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run proposed-iter1-CoNLL-2003
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.research/iteration1/wandb/run-20251107_194111-proposed-iter1-CoNLL-2003
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proposed-iter1-CoNLL-2003
wandb: â­ï¸ View project at https://wandb.ai/gengaru617-personal/251106-test
wandb: ðŸš€ View run at https://wandb.ai/gengaru617-personal/251106-test/runs/proposed-iter1-CoNLL-2003
Training:   0%|          | 0/500 [00:00<?, ?it/s]/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/src/train.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Training:   0%|          | 1/500 [00:00<01:09,  7.17it/s]Training:   0%|          | 2/500 [00:00<01:08,  7.29it/s]Training:   1%|          | 3/500 [00:00<01:07,  7.41it/s]Training:   1%|          | 4/500 [00:00<01:05,  7.55it/s]Training:   1%|          | 5/500 [00:00<01:06,  7.45it/s]Training:   1%|          | 6/500 [00:00<01:07,  7.34it/s]Training:   1%|â–         | 7/500 [00:00<01:08,  7.19it/s]Training:   2%|â–         | 8/500 [00:01<01:08,  7.17it/s]Training:   2%|â–         | 9/500 [00:01<01:08,  7.15it/s]Training:   2%|â–         | 10/500 [00:01<01:08,  7.14it/s]Training:   2%|â–         | 11/500 [00:01<01:08,  7.14it/s]Training:   2%|â–         | 12/500 [00:01<01:08,  7.12it/s]Training:   3%|â–Ž         | 13/500 [00:01<01:08,  7.11it/s]Training:   3%|â–Ž         | 14/500 [00:01<01:08,  7.12it/s]Training:   3%|â–Ž         | 15/500 [00:02<01:08,  7.11it/s]Training:   3%|â–Ž         | 16/500 [00:02<01:08,  7.10it/s]Training:   3%|â–Ž         | 17/500 [00:02<01:08,  7.10it/s]Training:   4%|â–Ž         | 18/500 [00:02<01:07,  7.10it/s]Training:   4%|â–         | 19/500 [00:02<01:07,  7.10it/s]Training:   4%|â–         | 20/500 [00:02<01:07,  7.10it/s]Training:   4%|â–         | 21/500 [00:02<01:07,  7.09it/s]Training:   4%|â–         | 22/500 [00:03<01:07,  7.09it/s]Training:   5%|â–         | 23/500 [00:03<01:07,  7.11it/s]Training:   5%|â–         | 24/500 [00:03<01:07,  7.09it/s]Training:   5%|â–Œ         | 25/500 [00:03<01:06,  7.10it/s]Training:   5%|â–Œ         | 26/500 [00:03<01:06,  7.10it/s]Training:   5%|â–Œ         | 27/500 [00:03<01:06,  7.10it/s]Training:   6%|â–Œ         | 28/500 [00:03<01:06,  7.11it/s]Training:   6%|â–Œ         | 29/500 [00:04<01:06,  7.11it/s]Training:   6%|â–Œ         | 30/500 [00:04<01:06,  7.09it/s]Training:   6%|â–Œ         | 31/500 [00:04<01:03,  7.42it/s]Training:   6%|â–‹         | 32/500 [00:04<00:59,  7.89it/s]Training:   7%|â–‹         | 33/500 [00:04<00:56,  8.31it/s]Training:   7%|â–‹         | 34/500 [00:04<00:54,  8.61it/s]Training:   7%|â–‹         | 35/500 [00:04<00:52,  8.87it/s]Training:   7%|â–‹         | 36/500 [00:04<00:51,  9.01it/s]Training:   7%|â–‹         | 37/500 [00:04<00:50,  9.10it/s]Training:   8%|â–Š         | 38/500 [00:05<00:50,  9.20it/s]Training:   8%|â–Š         | 39/500 [00:05<00:49,  9.28it/s]Training:   8%|â–Š         | 40/500 [00:05<00:49,  9.34it/s]Training:   8%|â–Š         | 41/500 [00:05<00:49,  9.34it/s]Training:   8%|â–Š         | 42/500 [00:05<00:48,  9.41it/s]Training:   9%|â–Š         | 43/500 [00:05<00:48,  9.44it/s]Training:   9%|â–‰         | 44/500 [00:05<00:48,  9.46it/s]Training:   9%|â–‰         | 45/500 [00:05<00:47,  9.48it/s]Training:   9%|â–‰         | 46/500 [00:05<00:47,  9.48it/s]Training:   9%|â–‰         | 47/500 [00:06<00:47,  9.49it/s]Training:  10%|â–‰         | 48/500 [00:06<00:48,  9.41it/s]Training:  10%|â–‰         | 49/500 [00:06<00:47,  9.41it/s]Training:  10%|â–ˆ         | 50/500 [00:06<00:47,  9.44it/s]Training:  10%|â–ˆ         | 51/500 [00:06<00:47,  9.43it/s]Training:  10%|â–ˆ         | 52/500 [00:06<00:47,  9.47it/s]Training:  11%|â–ˆ         | 53/500 [00:06<00:47,  9.51it/s]Training:  11%|â–ˆ         | 54/500 [00:06<00:46,  9.54it/s]Training:  11%|â–ˆ         | 55/500 [00:06<00:46,  9.56it/s]Training:  11%|â–ˆ         | 56/500 [00:06<00:46,  9.55it/s]Training:  11%|â–ˆâ–        | 57/500 [00:07<00:46,  9.55it/s]Training:  12%|â–ˆâ–        | 58/500 [00:07<00:46,  9.53it/s]Training:  12%|â–ˆâ–        | 59/500 [00:07<00:46,  9.55it/s]Training:  12%|â–ˆâ–        | 60/500 [00:07<00:45,  9.57it/s]Training:  12%|â–ˆâ–        | 61/500 [00:07<00:45,  9.56it/s]Training:  12%|â–ˆâ–        | 62/500 [00:07<00:45,  9.58it/s]Training:  13%|â–ˆâ–Ž        | 63/500 [00:07<00:45,  9.58it/s]Training:  13%|â–ˆâ–Ž        | 64/500 [00:07<00:45,  9.58it/s]Training:  13%|â–ˆâ–Ž        | 65/500 [00:07<00:45,  9.59it/s]Training:  13%|â–ˆâ–Ž        | 66/500 [00:08<00:45,  9.56it/s]Training:  13%|â–ˆâ–Ž        | 67/500 [00:08<00:45,  9.51it/s]Training:  14%|â–ˆâ–Ž        | 68/500 [00:08<00:45,  9.51it/s]Training:  14%|â–ˆâ–        | 69/500 [00:08<00:45,  9.51it/s]Training:  14%|â–ˆâ–        | 70/500 [00:08<00:45,  9.48it/s]Training:  14%|â–ˆâ–        | 71/500 [00:08<00:45,  9.48it/s]Training:  14%|â–ˆâ–        | 72/500 [00:08<00:45,  9.48it/s]Training:  15%|â–ˆâ–        | 73/500 [00:08<00:45,  9.47it/s]Training:  15%|â–ˆâ–        | 74/500 [00:08<00:45,  9.46it/s]Training:  15%|â–ˆâ–Œ        | 75/500 [00:08<00:44,  9.45it/s]Training:  15%|â–ˆâ–Œ        | 76/500 [00:09<00:44,  9.46it/s]Training:  15%|â–ˆâ–Œ        | 77/500 [00:09<00:44,  9.48it/s]Training:  16%|â–ˆâ–Œ        | 78/500 [00:09<00:44,  9.48it/s]Training:  16%|â–ˆâ–Œ        | 79/500 [00:09<00:44,  9.47it/s]Training:  16%|â–ˆâ–Œ        | 80/500 [00:09<00:44,  9.45it/s]Training:  16%|â–ˆâ–Œ        | 81/500 [00:09<00:44,  9.45it/s]Training:  16%|â–ˆâ–‹        | 82/500 [00:09<00:43,  9.50it/s]Training:  17%|â–ˆâ–‹        | 83/500 [00:09<00:43,  9.52it/s]Training:  17%|â–ˆâ–‹        | 84/500 [00:09<00:43,  9.53it/s]Training:  17%|â–ˆâ–‹        | 85/500 [00:10<00:44,  9.25it/s]Training:  17%|â–ˆâ–‹        | 86/500 [00:10<00:44,  9.23it/s]Training:  17%|â–ˆâ–‹        | 87/500 [00:10<00:44,  9.34it/s]Training:  18%|â–ˆâ–Š        | 88/500 [00:10<00:43,  9.38it/s]Training:  18%|â–ˆâ–Š        | 89/500 [00:10<00:43,  9.39it/s]Training:  18%|â–ˆâ–Š        | 90/500 [00:10<00:43,  9.42it/s]Training:  18%|â–ˆâ–Š        | 91/500 [00:10<00:43,  9.43it/s]Training:  18%|â–ˆâ–Š        | 92/500 [00:10<00:43,  9.42it/s]Training:  19%|â–ˆâ–Š        | 93/500 [00:10<00:43,  9.42it/s]Training:  19%|â–ˆâ–‰        | 94/500 [00:10<00:42,  9.44it/s]Training:  19%|â–ˆâ–‰        | 95/500 [00:11<00:42,  9.43it/s]Training:  19%|â–ˆâ–‰        | 96/500 [00:11<00:42,  9.44it/s]Training:  19%|â–ˆâ–‰        | 97/500 [00:11<00:42,  9.43it/s]Training:  20%|â–ˆâ–‰        | 98/500 [00:11<00:42,  9.45it/s]Training:  20%|â–ˆâ–‰        | 99/500 [00:11<00:42,  9.43it/s]Training:  20%|â–ˆâ–ˆ        | 100/500 [00:11<00:42,  9.48it/s]/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_5 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_7 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_8 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_6 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_4 seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Training:  20%|â–ˆâ–ˆ        | 101/500 [00:18<15:12,  2.29s/it]Training:  20%|â–ˆâ–ˆ        | 102/500 [00:19<10:49,  1.63s/it]Training:  21%|â–ˆâ–ˆ        | 103/500 [00:19<07:45,  1.17s/it]Training:  21%|â–ˆâ–ˆ        | 104/500 [00:19<05:37,  1.17it/s]Training:  21%|â–ˆâ–ˆ        | 105/500 [00:19<04:08,  1.59it/s]Training:  21%|â–ˆâ–ˆ        | 106/500 [00:19<03:06,  2.12it/s]Training:  21%|â–ˆâ–ˆâ–       | 107/500 [00:19<02:22,  2.76it/s]Training:  22%|â–ˆâ–ˆâ–       | 108/500 [00:19<01:52,  3.50it/s]Training:  22%|â–ˆâ–ˆâ–       | 109/500 [00:19<01:30,  4.32it/s]Training:  22%|â–ˆâ–ˆâ–       | 110/500 [00:19<01:15,  5.16it/s]Training:  22%|â–ˆâ–ˆâ–       | 111/500 [00:20<01:05,  5.95it/s]Training:  22%|â–ˆâ–ˆâ–       | 112/500 [00:20<00:57,  6.70it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 113/500 [00:20<00:52,  7.34it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 114/500 [00:20<00:49,  7.87it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 115/500 [00:20<00:46,  8.28it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 116/500 [00:20<00:45,  8.53it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 117/500 [00:20<00:43,  8.78it/s]Training:  24%|â–ˆâ–ˆâ–Ž       | 118/500 [00:20<00:42,  8.98it/s]Training:  24%|â–ˆâ–ˆâ–       | 119/500 [00:20<00:41,  9.12it/s]Training:  24%|â–ˆâ–ˆâ–       | 120/500 [00:20<00:41,  9.22it/s]Training:  24%|â–ˆâ–ˆâ–       | 121/500 [00:21<00:40,  9.29it/s]Training:  24%|â–ˆâ–ˆâ–       | 122/500 [00:21<00:40,  9.34it/s]Training:  25%|â–ˆâ–ˆâ–       | 123/500 [00:21<00:40,  9.38it/s]Training:  25%|â–ˆâ–ˆâ–       | 124/500 [00:21<00:40,  9.40it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 125/500 [00:21<00:39,  9.46it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 126/500 [00:21<00:39,  9.50it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 127/500 [00:21<00:39,  9.52it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 128/500 [00:21<00:38,  9.54it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 129/500 [00:21<00:38,  9.55it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 130/500 [00:22<00:38,  9.57it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 131/500 [00:22<00:38,  9.58it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 132/500 [00:22<00:38,  9.55it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 133/500 [00:22<00:38,  9.57it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 134/500 [00:22<00:38,  9.54it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 135/500 [00:22<00:38,  9.53it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 136/500 [00:22<00:38,  9.55it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 137/500 [00:22<00:38,  9.55it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 138/500 [00:22<00:37,  9.54it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 139/500 [00:22<00:37,  9.55it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 140/500 [00:23<00:37,  9.58it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 141/500 [00:23<00:37,  9.58it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 142/500 [00:23<00:37,  9.57it/s]Training:  29%|â–ˆâ–ˆâ–Š       | 143/500 [00:23<00:37,  9.56it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 144/500 [00:23<00:37,  9.55it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 145/500 [00:23<00:37,  9.51it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 146/500 [00:23<00:37,  9.50it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 147/500 [00:23<00:37,  9.48it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 148/500 [00:23<00:37,  9.48it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 149/500 [00:24<00:36,  9.49it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 150/500 [00:24<00:36,  9.50it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 151/500 [00:24<00:36,  9.46it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 152/500 [00:24<00:36,  9.47it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 153/500 [00:24<00:36,  9.46it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 154/500 [00:24<00:36,  9.45it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 155/500 [00:24<00:36,  9.44it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 156/500 [00:24<00:36,  9.40it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 157/500 [00:24<00:36,  9.46it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 158/500 [00:24<00:36,  9.46it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 159/500 [00:25<00:35,  9.49it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 160/500 [00:25<00:36,  9.40it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 161/500 [00:25<00:35,  9.42it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 162/500 [00:25<00:35,  9.43it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 163/500 [00:25<00:35,  9.44it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 164/500 [00:25<00:35,  9.45it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 165/500 [00:25<00:35,  9.44it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 166/500 [00:25<00:35,  9.44it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 167/500 [00:25<00:35,  9.46it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 168/500 [00:26<00:35,  9.46it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 169/500 [00:26<00:34,  9.48it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 170/500 [00:26<00:34,  9.45it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 171/500 [00:26<00:34,  9.44it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 172/500 [00:26<00:34,  9.45it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 173/500 [00:26<00:34,  9.48it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–      | 174/500 [00:26<00:34,  9.48it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 175/500 [00:26<00:34,  9.52it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 176/500 [00:26<00:34,  9.51it/s]Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 177/500 [00:26<00:33,  9.52it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 178/500 [00:27<00:33,  9.55it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 179/500 [00:27<00:33,  9.56it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/500 [00:27<00:33,  9.57it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 181/500 [00:27<00:33,  9.57it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 182/500 [00:27<00:33,  9.56it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 183/500 [00:27<00:33,  9.57it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 184/500 [00:27<00:32,  9.58it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 185/500 [00:27<00:32,  9.59it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 186/500 [00:27<00:32,  9.57it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 187/500 [00:28<00:32,  9.59it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 188/500 [00:28<00:32,  9.60it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 189/500 [00:28<00:32,  9.54it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 190/500 [00:28<00:32,  9.55it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 191/500 [00:28<00:32,  9.51it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 192/500 [00:28<00:32,  9.53it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 193/500 [00:28<00:32,  9.50it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 194/500 [00:28<00:32,  9.52it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 195/500 [00:28<00:32,  9.52it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 196/500 [00:28<00:31,  9.52it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 197/500 [00:29<00:31,  9.52it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 198/500 [00:29<00:31,  9.52it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 199/500 [00:29<00:31,  9.48it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [00:29<00:31,  9.47it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 201/500 [00:36<11:19,  2.27s/it]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 202/500 [00:36<08:03,  1.62s/it]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 203/500 [00:36<05:46,  1.17s/it]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 204/500 [00:37<04:10,  1.18it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 205/500 [00:37<03:04,  1.60it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 206/500 [00:37<02:17,  2.13it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 207/500 [00:37<01:45,  2.78it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 208/500 [00:37<01:22,  3.53it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 209/500 [00:37<01:06,  4.35it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 210/500 [00:37<00:55,  5.18it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 211/500 [00:37<00:48,  5.98it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 212/500 [00:37<00:42,  6.72it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 213/500 [00:37<00:38,  7.36it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 214/500 [00:38<00:36,  7.90it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 215/500 [00:38<00:34,  8.33it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 216/500 [00:38<00:32,  8.66it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 217/500 [00:38<00:31,  8.87it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 218/500 [00:38<00:31,  9.04it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 219/500 [00:38<00:30,  9.17it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 220/500 [00:38<00:30,  9.24it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 221/500 [00:38<00:29,  9.31it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 222/500 [00:38<00:29,  9.35it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 223/500 [00:39<00:29,  9.39it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 224/500 [00:39<00:29,  9.39it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 225/500 [00:39<00:29,  9.44it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 226/500 [00:39<00:29,  9.32it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 227/500 [00:39<00:28,  9.42it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 228/500 [00:39<00:28,  9.42it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 229/500 [00:39<00:28,  9.48it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 230/500 [00:39<00:28,  9.48it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 231/500 [00:39<00:28,  9.50it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 232/500 [00:40<00:29,  9.24it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 233/500 [00:40<00:28,  9.33it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 234/500 [00:40<00:28,  9.37it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 235/500 [00:40<00:28,  9.40it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 236/500 [00:40<00:27,  9.54it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 237/500 [00:40<00:27,  9.53it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 238/500 [00:40<00:27,  9.49it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 239/500 [00:40<00:27,  9.48it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 240/500 [00:40<00:27,  9.48it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 241/500 [00:40<00:27,  9.47it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 242/500 [00:41<00:27,  9.46it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 243/500 [00:41<00:27,  9.51it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 244/500 [00:41<00:26,  9.55it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 245/500 [00:41<00:26,  9.54it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 246/500 [00:41<00:26,  9.52it/s]Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 247/500 [00:41<00:26,  9.51it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 248/500 [00:41<00:26,  9.50it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 249/500 [00:41<00:26,  9.50it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 250/500 [00:41<00:26,  9.49it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 251/500 [00:42<00:26,  9.49it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 252/500 [00:42<00:26,  9.48it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 253/500 [00:42<00:26,  9.46it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 254/500 [00:42<00:25,  9.49it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 255/500 [00:42<00:25,  9.50it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 256/500 [00:42<00:25,  9.50it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 257/500 [00:42<00:25,  9.48it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 258/500 [00:42<00:25,  9.47it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 259/500 [00:42<00:25,  9.47it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 260/500 [00:42<00:25,  9.48it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261/500 [00:43<00:25,  9.48it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262/500 [00:43<00:25,  9.47it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 263/500 [00:43<00:25,  9.47it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 264/500 [00:43<00:24,  9.47it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 265/500 [00:43<00:24,  9.48it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 266/500 [00:43<00:24,  9.49it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 267/500 [00:43<00:24,  9.48it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 268/500 [00:43<00:24,  9.49it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 269/500 [00:43<00:24,  9.50it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 270/500 [00:44<00:24,  9.49it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 271/500 [00:44<00:24,  9.46it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 272/500 [00:44<00:24,  9.49it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 273/500 [00:44<00:23,  9.49it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 274/500 [00:44<00:23,  9.46it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 275/500 [00:44<00:23,  9.49it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 276/500 [00:44<00:23,  9.50it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 277/500 [00:44<00:23,  9.47it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 278/500 [00:44<00:23,  9.48it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 279/500 [00:44<00:23,  9.46it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 280/500 [00:45<00:23,  9.41it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 281/500 [00:45<00:23,  9.31it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 282/500 [00:45<00:23,  9.37it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 283/500 [00:45<00:22,  9.45it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 284/500 [00:45<00:22,  9.43it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 285/500 [00:45<00:22,  9.41it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 286/500 [00:45<00:22,  9.39it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 287/500 [00:45<00:22,  9.43it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 288/500 [00:45<00:22,  9.45it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 289/500 [00:46<00:22,  9.46it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 290/500 [00:46<00:22,  9.46it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 291/500 [00:46<00:22,  9.43it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 292/500 [00:46<00:22,  9.45it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 293/500 [00:46<00:21,  9.45it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 294/500 [00:46<00:21,  9.45it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 295/500 [00:46<00:21,  9.40it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 296/500 [00:46<00:21,  9.44it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 297/500 [00:46<00:21,  9.47it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 298/500 [00:46<00:21,  9.33it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 299/500 [00:47<00:21,  9.37it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [00:47<00:21,  9.42it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 301/500 [00:54<07:31,  2.27s/it]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 302/500 [00:54<05:20,  1.62s/it]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 303/500 [00:54<03:49,  1.16s/it]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 304/500 [00:54<02:45,  1.18it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 305/500 [00:54<02:01,  1.60it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 306/500 [00:55<01:30,  2.13it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 307/500 [00:55<01:09,  2.78it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 308/500 [00:55<00:54,  3.52it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309/500 [00:55<00:44,  4.34it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 310/500 [00:55<00:36,  5.18it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 311/500 [00:55<00:31,  5.97it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 312/500 [00:55<00:27,  6.74it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 313/500 [00:55<00:25,  7.37it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 314/500 [00:55<00:23,  7.92it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 315/500 [00:55<00:21,  8.42it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 316/500 [00:56<00:21,  8.72it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 317/500 [00:56<00:20,  8.94it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 318/500 [00:56<00:19,  9.13it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 319/500 [00:56<00:20,  9.05it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 320/500 [00:56<00:19,  9.16it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 321/500 [00:56<00:19,  9.25it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 322/500 [00:56<00:19,  9.31it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 323/500 [00:56<00:18,  9.38it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 324/500 [00:56<00:18,  9.42it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 325/500 [00:57<00:18,  9.48it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 326/500 [00:57<00:18,  9.52it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 327/500 [00:57<00:18,  9.55it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 328/500 [00:57<00:17,  9.57it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 329/500 [00:57<00:17,  9.50it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 330/500 [00:57<00:17,  9.56it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 331/500 [00:57<00:17,  9.55it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 332/500 [00:57<00:17,  9.50it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 333/500 [00:57<00:17,  9.51it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 334/500 [00:57<00:17,  9.49it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 335/500 [00:58<00:17,  9.51it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 336/500 [00:58<00:17,  9.48it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 337/500 [00:58<00:17,  9.46it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 338/500 [00:58<00:17,  9.45it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 339/500 [00:58<00:17,  9.40it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 340/500 [00:58<00:16,  9.43it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 341/500 [00:58<00:16,  9.45it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 342/500 [00:58<00:16,  9.43it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 343/500 [00:58<00:16,  9.48it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 344/500 [00:59<00:16,  9.45it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 345/500 [00:59<00:16,  9.51it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 346/500 [00:59<00:16,  9.55it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 347/500 [00:59<00:16,  9.55it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 348/500 [00:59<00:15,  9.56it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 349/500 [00:59<00:15,  9.58it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 350/500 [00:59<00:15,  9.57it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 351/500 [00:59<00:15,  9.52it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 352/500 [00:59<00:15,  9.49it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 353/500 [00:59<00:15,  9.40it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 354/500 [01:00<00:15,  9.41it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 355/500 [01:00<00:15,  9.43it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 356/500 [01:00<00:15,  9.40it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/500 [01:00<00:15,  9.43it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358/500 [01:00<00:15,  9.37it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 359/500 [01:00<00:15,  9.39it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 360/500 [01:00<00:14,  9.43it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 361/500 [01:00<00:14,  9.45it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 362/500 [01:00<00:14,  9.50it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 363/500 [01:01<00:14,  9.54it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 364/500 [01:01<00:14,  9.55it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 365/500 [01:01<00:14,  9.54it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 366/500 [01:01<00:14,  9.51it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 367/500 [01:01<00:13,  9.50it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 368/500 [01:01<00:13,  9.50it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 369/500 [01:01<00:13,  9.49it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 370/500 [01:01<00:13,  9.49it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 371/500 [01:01<00:13,  9.47it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 372/500 [01:01<00:13,  9.44it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 373/500 [01:02<00:13,  9.45it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 374/500 [01:02<00:13,  9.50it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 375/500 [01:02<00:13,  9.54it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 376/500 [01:02<00:13,  9.53it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 377/500 [01:02<00:12,  9.52it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 378/500 [01:02<00:12,  9.55it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 379/500 [01:02<00:12,  9.55it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 380/500 [01:02<00:12,  9.57it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 381/500 [01:02<00:12,  9.59it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 382/500 [01:03<00:12,  9.59it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 383/500 [01:03<00:12,  9.59it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 384/500 [01:03<00:12,  9.60it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 385/500 [01:03<00:12,  9.58it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 386/500 [01:03<00:11,  9.54it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 387/500 [01:03<00:11,  9.53it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 388/500 [01:03<00:11,  9.52it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 389/500 [01:03<00:11,  9.53it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 390/500 [01:03<00:11,  9.53it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 391/500 [01:03<00:11,  9.56it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 392/500 [01:04<00:11,  9.58it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 393/500 [01:04<00:11,  9.60it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 394/500 [01:04<00:11,  9.61it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 395/500 [01:04<00:10,  9.57it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 396/500 [01:04<00:10,  9.58it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 397/500 [01:04<00:10,  9.59it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 398/500 [01:04<00:10,  9.61it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 399/500 [01:04<00:10,  9.59it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [01:04<00:10,  9.57it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 401/500 [01:12<03:44,  2.27s/it]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 402/500 [01:12<02:38,  1.62s/it]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 403/500 [01:12<01:53,  1.17s/it]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 404/500 [01:12<01:21,  1.18it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 405/500 [01:12<00:59,  1.60it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 406/500 [01:12<00:44,  2.14it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 407/500 [01:12<00:33,  2.79it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 408/500 [01:12<00:25,  3.54it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 409/500 [01:13<00:20,  4.37it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 410/500 [01:13<00:17,  5.21it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 411/500 [01:13<00:14,  6.04it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 412/500 [01:13<00:12,  6.78it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 413/500 [01:13<00:11,  7.43it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 414/500 [01:13<00:10,  7.97it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 415/500 [01:13<00:10,  8.28it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 416/500 [01:13<00:09,  8.59it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 417/500 [01:13<00:09,  8.84it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 418/500 [01:14<00:09,  9.03it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 419/500 [01:14<00:08,  9.18it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 420/500 [01:14<00:08,  9.27it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 421/500 [01:14<00:08,  9.31it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 422/500 [01:14<00:08,  9.38it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 423/500 [01:14<00:08,  9.42it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 424/500 [01:14<00:08,  9.37it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 425/500 [01:14<00:07,  9.38it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 426/500 [01:14<00:07,  9.45it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 427/500 [01:14<00:07,  9.46it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 428/500 [01:15<00:07,  9.42it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 429/500 [01:15<00:07,  9.44it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 430/500 [01:15<00:07,  9.34it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 431/500 [01:15<00:07,  9.38it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 432/500 [01:15<00:07,  9.43it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 433/500 [01:15<00:07,  9.50it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 434/500 [01:15<00:06,  9.49it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 435/500 [01:15<00:06,  9.49it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 436/500 [01:15<00:06,  9.51it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 437/500 [01:16<00:06,  9.50it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 438/500 [01:16<00:06,  9.46it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 439/500 [01:16<00:06,  9.43it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 440/500 [01:16<00:06,  9.43it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 441/500 [01:16<00:06,  9.48it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 442/500 [01:16<00:06,  9.47it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 443/500 [01:16<00:06,  9.48it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 444/500 [01:16<00:05,  9.46it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 445/500 [01:16<00:05,  9.47it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 446/500 [01:16<00:05,  9.48it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 447/500 [01:17<00:05,  9.47it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 448/500 [01:17<00:05,  9.47it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 449/500 [01:17<00:05,  9.47it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 450/500 [01:17<00:05,  9.49it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 451/500 [01:17<00:05,  9.47it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 452/500 [01:17<00:05,  9.50it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 453/500 [01:17<00:04,  9.59it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 454/500 [01:17<00:04,  9.56it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 455/500 [01:17<00:04,  9.54it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 456/500 [01:18<00:04,  9.53it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 457/500 [01:18<00:04,  9.51it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 458/500 [01:18<00:04,  9.47it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 459/500 [01:18<00:04,  9.47it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 460/500 [01:18<00:04,  9.48it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 461/500 [01:18<00:04,  9.48it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 462/500 [01:18<00:04,  9.44it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 463/500 [01:18<00:03,  9.47it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 464/500 [01:18<00:03,  9.49it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 465/500 [01:18<00:03,  9.48it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 466/500 [01:19<00:03,  9.49it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 467/500 [01:19<00:03,  9.50it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 468/500 [01:19<00:03,  9.53it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 469/500 [01:19<00:03,  9.53it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 470/500 [01:19<00:03,  9.53it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 471/500 [01:19<00:03,  9.52it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 472/500 [01:19<00:02,  9.51it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 473/500 [01:19<00:02,  9.52it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 474/500 [01:19<00:02,  9.52it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 475/500 [01:20<00:02,  9.48it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 476/500 [01:20<00:02,  9.49it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 477/500 [01:20<00:02,  9.49it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 478/500 [01:20<00:02,  9.47it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 479/500 [01:20<00:02,  9.46it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 480/500 [01:20<00:02,  9.43it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 481/500 [01:20<00:02,  9.43it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 482/500 [01:20<00:01,  9.44it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 483/500 [01:20<00:01,  9.45it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 484/500 [01:20<00:01,  9.45it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 485/500 [01:21<00:01,  9.47it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 486/500 [01:21<00:01,  9.46it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 487/500 [01:21<00:01,  9.48it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 488/500 [01:21<00:01,  9.53it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 489/500 [01:21<00:01,  9.54it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 490/500 [01:21<00:01,  9.44it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 491/500 [01:21<00:00,  9.54it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 492/500 [01:21<00:00,  9.51it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 493/500 [01:21<00:00,  9.49it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 494/500 [01:22<00:00,  9.49it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 495/500 [01:22<00:00,  9.42it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 496/500 [01:22<00:00,  9.44it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 497/500 [01:22<00:00,  9.46it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 498/500 [01:22<00:00,  9.48it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 499/500 [01:22<00:00,  9.49it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:22<00:00,  9.45it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:29<00:00,  5.56it/s]
wandb: uploading artifact proposed-iter1-CoNLL-2003_model; updating run metadata
wandb: uploading artifact proposed-iter1-CoNLL-2003_model; uploading wandb-summary.json; uploading config.yaml
wandb: uploading artifact proposed-iter1-CoNLL-2003_model
wandb: uploading artifact proposed-iter1-CoNLL-2003_model; uploading history steps 499-499, summary, console lines 26-26
wandb: uploading artifact proposed-iter1-CoNLL-2003_model
wandb: 
wandb: Run history:
wandb: eval_accuracy â–â–â–â–â–
wandb:       eval_f1 â–â–â–â–â–
wandb:    gpu_mem_mb â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–
wandb:          step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:            +2 ...
wandb: 
wandb: Run summary:
wandb: adapter_param_MB 91.43799
wandb:    eval_accuracy 0.32427
wandb:          eval_f1 0.00469
wandb:   final_accuracy 0.32427
wandb:         final_f1 0.00469
wandb:       gpu_mem_mb 273.86963
wandb:        grad_norm nan
wandb:               lr 0
wandb:             step 500
wandb:       train_loss nan
wandb: 
wandb: ðŸš€ View run proposed-iter1-CoNLL-2003 at: https://wandb.ai/gengaru617-personal/251106-test/runs/proposed-iter1-CoNLL-2003
wandb: â­ï¸ View project at: https://wandb.ai/gengaru617-personal/251106-test
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./.research/iteration1/wandb/run-20251107_194111-proposed-iter1-CoNLL-2003/logs
