=== [UV SYNC] Start at Fri Nov  7 07:18:08 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Fri Nov  7 07:18:14 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-iter1-CoNLL-2003 at Fri Nov  7 07:18:14 PM UTC 2025 ===
ðŸš€ Launching: /home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3 -u -m src.train run=proposed-iter1-CoNLL-2003 results_dir=.research/iteration1 mode=trial
[2025-11-07 19:18:50,163][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Final metrics: {'f1': np.float64(0.002207505518763797), 'accuracy': 0.2632146709816613}
=== [TRIAL RUN] PASSED for proposed-iter1-CoNLL-2003 at Fri Nov  7 07:19:06 PM UTC 2025 ===

=== [UV SYNC] Start at Fri Nov  7 19:20:03 UTC 2025 ===
=== [UV SYNC] Finished successfully at Fri Nov  7 19:20:11 UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-iter1-CoNLL-2003 at Fri Nov  7 19:20:11 UTC 2025 ===
ðŸš€ Launching: /mnt/home/toma/KRK-039/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3 -u -m src.train run=comparative-1-iter1-CoNLL-2003 results_dir=.research/iteration1 mode=trial
[2025-11-08 04:21:14,744][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Final metrics: {'f1': np.float64(0.010050251256281407), 'accuracy': 0.07119741100323625}
=== [TRIAL RUN] PASSED for comparative-1-iter1-CoNLL-2003 at Fri Nov  7 19:21:25 UTC 2025 ===

=== [UV SYNC] Start at Fri Nov  7 07:23:09 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Fri Nov  7 07:23:14 PM UTC 2025 ===
=== [FULL EXPERIMENT] Start for proposed-iter1-CoNLL-2003 at Fri Nov  7 07:23:14 PM UTC 2025 ===
ðŸš€ Launching: /home/toma/t-80-8-b-03/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3 -u -m src.train run=proposed-iter1-CoNLL-2003 results_dir=.research/iteration1 mode=full
=== [UV SYNC] Start at Fri Nov  7 07:25:43 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Fri Nov  7 07:25:49 PM UTC 2025 ===
=== [FULL EXPERIMENT] Start for proposed-iter1-CoNLL-2003 at Fri Nov  7 07:25:49 PM UTC 2025 ===
ðŸš€ Launching: /home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3 -u -m src.train run=proposed-iter1-CoNLL-2003 results_dir=.research/iteration1 mode=full
=== [UV SYNC] Start at Fri Nov  7 07:28:27 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Fri Nov  7 07:28:33 PM UTC 2025 ===
=== [FULL EXPERIMENT] Start for proposed-iter1-CoNLL-2003 at Fri Nov  7 07:28:33 PM UTC 2025 ===
ðŸš€ Launching: /home/toma/pt80-1-a-29/_work/airas-20251107-175001-matsuzawa/airas-20251107-175001-matsuzawa/.venv/bin/python3 -u -m src.train run=proposed-iter1-CoNLL-2003 results_dir=.research/iteration1 mode=full
[2025-11-07 19:29:17,202][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:30:02,482][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:30:44,392][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:31:26,655][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:32:12,549][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:32:55,027][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:33:41,503][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:34:27,287][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:34:35,936][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:35:17,825][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:36:00,434][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:36:09,798][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:36:18,430][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:36:27,045][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:37:08,814][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:37:50,841][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:37:59,459][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:38:08,090][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:38:16,684][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:39:02,468][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:39:11,268][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:39:19,881][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:40:05,738][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:40:15,139][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-07 19:41:01,473][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Optuna best trial 23: value=0.0763 params={'training.learning_rate': 0.00018716843924922904, 'adapter.lambda_F': 0.088301583692537, 'adapter.lambda_Q': 3.187974934174355e-05, 'adapter.router_top_k': 2}
[2025-11-07 19:41:11,030][transformers.modeling_utils][WARNING] - Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb URL: https://wandb.ai/gengaru617-personal/251106-test/runs/proposed-iter1-CoNLL-2003
Final metrics: {'f1': np.float64(0.0046902053608608935), 'accuracy': 0.32426696779720415}
=== [FULL EXPERIMENT] PASSED for proposed-iter1-CoNLL-2003 at Fri Nov  7 07:43:08 PM UTC 2025 ===
