{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "LoRA fine-tuning",
    "adapter tuning",
    "prefix tuning",
    "hyperparameter optimization",
    "curriculum learning"
  ],
  "research_study_list": [
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
      "full_text": "DoRA: Weight-Decomposed Low-Rank Adaptation Shih-Yang Liu1 2 Chien-Yi Wang1 Hongxu Yin 1 Pavlo Molchanov1 Yu-Chiang Frank Wang1 Kwang-Ting Cheng 2 Min-Hung Chen 1 Abstract Among the widely used parameter-efficient fine- tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decom- position analysis to investigate the inherent dif- ferences between FT and LoRA. Aiming to re- semble the learning capacity of FT from the findings, we propose Weight-Decomposed Low- Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, mag- nitude and direction, for fine-tuning, specifically employing LoRA for directional updates to ef- ficiently minimize the number of trainable pa- rameters. By employing DoRA, we enhance both the learning capacity and training stabil- ity of LoRA while avoiding any additional in- ference overhead. DoRA consistently outper- forms LoRA on fine-tuning LLaMA, LLaV A, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruc- tion tuning, and image/video-text understanding. Code is available at https://github.com/ NVlabs/DoRA. 1. Introduction Models that are pre-trained with extensive general domain datasets have demonstrated remarkable generalization abil- ities, significantly benefiting a wide array of applications, from natural language processing (NLP) tasks (Qin et al., 2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022; Liu et al., 2023a). To tailor these general models for spe- cific downstream tasks, full fine-tuning (FT) is commonly 1NVIDIA 2HKUST. Correspondence to: Shih-Yang Liu <shi- hyangl@nvidia.com, sliuau@connect.ust.hk>, Min-Hung Chen <minhungc@nvidia.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Magnitude B  A  Pretrained Weight Merged Weight Pretrained Weight Pretrained Weight Adapt Frozen Trainable Magnitude Direction Direction Decompose (Initialize) Merge Figure 1.An overview of our proposed DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that || · ||c denotes the vector-wise norm of a matrix across each column vector. employed, involving the retraining of all model parameters. Nevertheless, as the size of models and datasets expand in scale, the expense associated with fine-tuning the entire model becomes prohibitively large. To address this issue, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019) have been introduced to fine- tune the pre-trained models with only a minimal number of parameters. Among these, LoRA (Hu et al., 2022), which does not change the model architecture, has become notably popular for its simplicity and efficacy. Nevertheless, there is still a capacity gap between LoRA and FT, which is often attributed to the limited number of trainable parameters without further exploration of other underlying causes (Hu et al., 2022; Kopiczko et al., 2024). Drawing on Weight Normalization (Salimans & Kingma, 2016), which achieves faster convergence via improving the conditioning of the gradient with weight reparameterization, we introduce a novel weight decomposition analysis that ini- tially reparameterizes model weights into magnitude and di- rectional components, subsequently examining the changes 1 arXiv:2402.09353v6  [cs.CL]  9 Jul 2024DoRA: Weight-Decomposed Low-Rank Adaptation in magnitude and direction introduced by LoRA and FT. Our analysis reveals that LoRA and FT exhibit markedly distinct patterns of updates, leading us to surmise that these variations mirror the learning capability of each method. Inspired by our findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA), which begins by decompos- ing the pre-trained weight into its magnitude and directional components, then fine-tunes both. Given the substantial size of the directional component in terms of parameters, we exploit LoRA for the directional adaptation to enable efficient fine-tuning, as illustrated in Figure.1. Moreover, by showing a learning behavior similar to FT both empir- ically and mathematically, suggesting a learning capacity closely resembling FT, we have validated DoRA across a wide variety of tasks, from NLP to Vision-Language, and over various backbones, including LLM and LVLM. The experimental results show that DoRA consistently outper- forms LoRA without sacrificing inference efficiency, such as commonsense reasoning (+3.7/+1.0 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +4.4 on LLaMA3-8B), visual in- struction tuning (+0.6 on LLaV A-7B), and image/video-text understanding (+0.9/+1.9 on VL-BART). The summary of our contributions is as follows: • We introduce DoRA, a novel PEFT method that incor- porates weight decomposition, achieving a learning capacity closely resembling FT without any additional inference latency over LoRA. • We introduce a novel weight decomposition analysis to uncover the fundamental differences in the learning patterns of FT and different PEFT methods. • DoRA consistently surpasses LoRA on various tasks, from NLP to Vision-Language benchmarks and across various backbones, including LLM and LVLM. 2. Related Works Parameter-Efficient Fine-Tuning (PEFT)methods are de- signed to reduce the high expense of fine-tuning large-scale models. They achieve this by training a relatively small subset of parameters, compared to the total number of pa- rameters, for adapting to downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as Adapter-based methods, which involve introducing additional trainable modules into the original frozen backbone, such as (Houlsby et al., 2019; He et al., 2021; Karimi Mahabadi et al., 2021; mahabadi et al., 2021). For example, (Houlsby et al., 2019) proposes adding linear modules in sequence to the existing layer, whereas (He et al., 2021) advocates for integrating these modules in parallel with the original layer to enhance performance. The second category is Prompt-based methods. These methods add extra soft tokens (prompts) to the initial input and fo- cus solely on fine-tuning these trainable vectors, as seen in works like (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023). However, these approaches typically face challenges due to their sensitivity to initialization, af- fecting their overall effectiveness. These first two categories, whether altering the model’s input or architecture, result in increased inference latency compared to the baseline model. LoRA (Hu et al., 2022) and its variants are among the third category of PEFT, notable for not adding any extra inference burden. These methods apply low-rank matrices to approximate weight changes during fine-tuning and can merge with pre-trained weights prior to inference. For ex- ample, (Zhang et al., 2023) employs SVD decomposition and prunes less significant singular values for more efficient updates. (Hyeon-Woo et al., 2022) focuses on low-rank Hadamard product for federated learning. (Qiu et al., 2023; Liu et al., 2023b) exploit orthogonal factorization in fine- tuning diffusion models. (Renduchintala et al., 2023) uses weight tying to further reduce the trainable parameters. (Yeh et al., 2023) introduces a unified LoRA family framework for Stable diffusion. (Ponti et al., 2022) chooses different combinations of LoRAs from the inventory with a routing function for different tasks. (Kopiczko et al., 2024) imple- ments learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Our research also falls within this third category, and we validate the efficacy of our proposed method alongside LoRA and its variants through comprehensive experimentation. 3. Pattern Analysis of LoRA and FT 3.1. Low-Rank Adaptation (LoRA) Building upon the hypothesis that updates made during the fine-tuning exhibit a low “intrinsic rank”, LoRA (Hu et al., 2022) proposes using the product of two low-rank matri- ces to update the pre-trained weights incrementally. For a pre-trained weight matrix W0 ∈ Rd×k, LoRA models the weight update ∆W ∈ Rd×k utilizing a low-rank decompo- sition, expressed as BA, where B ∈ Rd×r and A ∈ Rr×k represent two low-rank matrices, withr ≪ min(d, k). Con- sequently, the fine-tuned weight W′ can be represented as: W′ = W0 + ∆W = W0 + BA (1) where W0 remains static during the fine-tuning process, and the underlined parameters are being trained. The ma- trix A is initialized with uniform Kaiming distribution (He et al., 2015), while B is initially set to zero, resulting in ∆W = BA being zero at the start of training. Notably, this decomposition of ∆W can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Ad- ditionally, based on Eq. (1), we can merge the learned ∆W with the pre-trained weight W0 and obtain W′ in advance 2DoRA: Weight-Decomposed Low-Rank Adaptation of deployment, and given that both W′ and W0 both fall within the dimensionality of Rd×k, LoRA and its related variants do not introduce any extra latency during the infer- ence compared to the original model. 3.2. Weight Decomposition Analysis The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank r of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Con- sequently, many previous studies have attributed the dis- crepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans & Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two sep- arate components, magnitude and direction, to reveal the inherent differences in LoRA and FT learning patterns. Analysis Method: This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of W ∈ Rd×k can be formulated as: W = m V ||V ||c = ||W||c W ||W||c (2) where m ∈ R1×k is the magnitude vector, V ∈ Rd×k is the directional matrix, with ||·|| c being the vector-wise norm of a matrix across each column. This decomposition ensures that each column of V/||V ||c remains a unit vector, and the corresponding scalar in m defines the magnitude of each vector. For our weight decomposition analysis, we select the VL- BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre- trained weight W0, the full fine-tuned weight WFT, and the merged LoRA weight WLoRA of query/value weight matrix using Eq. (2). The magnitude and directional variations between W0 and WFT can be defined as follows: ∆Mt FT = Pk n=1 |mn,t FT − mn 0 | k (3) ∆Dt FT = Pk n=1(1 − cos(V n,t FT , Wn 0 )) k (4) Here, ∆Mt FT and and ∆Dt FT represent the magnitude dif- ference and directional difference between W0 and WFT at t training step respectively, with cos(·, ·) being the cosine similarity function. Mn,t FT and Mn 0 are the nth scalars in their respective magnitude vectors, while V n,t FT and Wn 0 are the nth columns in V t FT and W0. The magnitude and direc- tional differences between WLoRA and W0 are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition anal- ysis on each of these checkpoints to determine the ∆M and ∆D throughout different layers. Analysis Results: Figure 2 (a) and (b) illustrate the alter- ations in the query weight matrix of FT and LoRA, with each point representing a ( ∆Dt, ∆Mt) pair from query weight matrices across different layers and training steps. Similarly, Figure 7 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively neg- ative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direc- tion updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and direc- tional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resem- bling that of FT, and can improve the learning capacity over LoRA. 4. Method 4.1. Weight-Decomposed Low-Rank Adaptation Drawing from the insights of our weight decomposition analysis, we introduce Weight- Decomposed L ow-Rank Adaptation (DoRA). DoRA initially decomposes the pre- trained weight into its magnitude and directional compo- nents and finetunes both of them. Because the directional component is large in terms of parameter numbers, we fur- ther decompose it with LoRA for efficient finetuning. Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tun- able simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag- 3DoRA: Weight-Decomposed Low-Rank Adaptation 0.014 0.016 0.018 0.020 0.022 D (a) 0.05 0.06 0.07 0.08 0.09 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.10 0.11 0.12 0.13 0.14 0.15 0.16 D (b) 0.25 0.30 0.35 0.40 0.45 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 D (c) 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 2.Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer. nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinc- tion between DoRA and weight normalization (Salimans & Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, mak- ing the method sensitive to different initializations. Con- versely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight W0 as outlined in Eq. (2), where m = ||W0||c and V = W0 after initialization. We then keep V frozen and m a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as: W′ = m V + ∆V ||V + ∆V ||c = m W0 + BA ||W0 + BA||c (5) where ∆V is the incremental directional update learned by multiplying two low-rank matrices B and A, and the underlined parameters denote the trainable parameters. The matrices B ∈ Rd×r and A ∈ Rr×k are initialized in line with LoRA’s strategy to ensure that W′ equals W0 before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency. We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and W0 in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for (∆D, ∆M) of both DoRA and FT, we reveal that in contrast to LoRA’s pattern, DoRA, and FT are characterized by a distinct neg- ative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substan- tial knowledge suitable for various downstream tasks. There- fore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally com- pute the correlation between ∆D and ∆M for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT’s signifies its superior learning capacity over LoRA. 4.2. Gradient Analysis of DoRA In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the opti- mization of ∆V . Subsequently, we analyze from the gradi- ent’s perspective to explicate the learning pattern of DoRA, which tends to have a negative slope. From Eq. (5), we can obtain the gradient of Loss L with respect to m and V ′ = V + ∆V as: ∇V ′L = m ||V ′||c \u0012 I − V ′V ′T ||V ′||2c \u0013 ∇W′L (6) ∇mL = ∇W′L ·V ′ ||V ′||c (7) Eq. (6) reveals that the weight gradient ∇W′L is scaled by m/||V ′||c and is projected away from the current weight matrix. These two effects contribute to aligning the gra- dient’s covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans & Kingma, 2016). Additionally, given that V ′ = V + ∆V , the gradient ∇V ′L is equivalent to ∇∆V L. Therefore, the optimization benefits derived from this decomposition are fully transferred to ∆V , enhancing the learning stability of LoRA. We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent dis- cussion, we represent vectors using lower-case letters in- stead of the previous matrix form notation. Consider w′′ = w′ + ∆w as the parameter update for a weight vector, where ∆w ∝ ∇w′L. In two hypothetical update 4DoRA: Weight-Decomposed Low-Rank Adaptation scenarios, S1 and S2, S1 involves a smaller directional update (∆DS1), while S2 involves a larger one ( ∆DS2). Assuming ||∆wS1|| = ||∆wS2||, and at time 0, we have ∆v = 0 and v′ = v. From ∆DS1 < ∆DS2, it follows that |cos(∆wS1, w′)| > |cos(∆wS2, w′)|. Since ∆w ∝ ∇w′L, it implies |cos(∇S1 w′ L, w′)| > |cos(∇S2 w′ L, w′)|. From Sec 4.1, with v initialized as v0 and w′ = w0 at time 0, we get |cos(∇w′L, w′)| = |cos(∇w′L, v′)| = |cos(∇w′L, v)|. Using the cosine similarity equation with ∆v = 0: cos(∇w′L, v′) = cos(∇w′L, v) = ∇w′L ·v ||∇w′L||||v|| (8) denote m∗ as the magnitude scalar of vector w′ then Eq. (7) w.r.t m∗ can be rewritten to: ∇m∗L = ∇w′L ·v′ ||v′|| = ||∇w′L|| ·cos(∇w′L, v) (9) Given that ||∆wS1|| = ||∆wS2|| for S1 and S2, and ||∇S1 w′ L|| = ||∇S2 w′ L||. Therefore, with: ||∇S1 w′ L|| · |cos(∇S1 w′ L, v)| > ||∇S2 w′ L|| · |cos(∇S2 w′ L, v)| (10) it can be inferred that |∇S1 m∗L| > |∇S2 m∗L| which indicate that S1 has larger magnitude updates over S2 while having smaller directional alteration than that of S2. Our conclu- sion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT. 4.3. Reduction of Training Overhead In Eq. (1), the gradients of W′ and ∆W are the same. How- ever, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low- rank updates differs from that ofW′, as illustrated in Eq. (6). This divergence necessitates extra memory during backprop- agation. To address this, we suggest treating||V + ∆V ||c in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while ||V + ∆V ||c dynamically reflects the updates of ∆V , it won’t receive any gradient during backpropagation. With this modification, the gra- dient w.r.t m remains unchanged, and ∇V ′L is redefined as: ∇V ′L = m C ∇W′L where C = ||V ′||c (11) This approach reduces the gradient graph memory consump- tion drastically without a noticeable difference in accuracy. We conduct an ablation study to evaluate the impact of the proposed modification on fine-tuning LLaMA-7B and VL- BART. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains un- changed for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. For a comprehensive comparison of training memory usage and accuracy differences, please see Table 7 in the appendix. Consequently, all subsequent experiments with DoRA incorporate this adjustment. 5. Experiments We conduct a variety of experiments to showcase the effi- cacy of DoRA on various tasks including language, image, and video domains. Firstly, we evaluate DoRA against sev- eral Parameter-Efficient Fine-Tuning (PEFT) methods by fine-tuning LLaMA-7B/13B, LLaMA2-7B, and LLaMA3- 8B on commonsense reasoning tasks. Subsequently, we extend from single modality to multimodality. We compare DoRA with LoRA across multi-task image/video-text under- standing tasks using VL-BART and visual instruction tuning with LLaV A-1.5-7B. Following this, we explore the com- patibility of DoRA with LoRA and VeRA (Kopiczko et al., 2024) for instruction-tuning on LLaMA-7B and LLaMA2- 7B. Furthermore, we perform a series of ablation studies to illustrate that DoRA surpasses LoRA in performance, irre- spective of the number of fine-tuning training samples and rank variations. Lastly, We analyze the tuning granularity of DoRA, and show that DoRA can achieve better accuracy than LoRA with fewer trainable parameters by selectively updating only the directional components of certain mod- ules. 5.1. Commonsense Reasoning We evaluate DoRA against LoRA and several baseline meth- ods which include Prompt learning (Prefix) (Li & Liang, 2021), Series adapter (Series) (Houlsby et al., 2019), and Parallel adapter (Parallel) (He et al., 2021) on LLaMA- 7B/13B (Touvron et al., 2023) for commonsense reasoning tasks. We also include ChatGPT’s accuracy obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Ope- nAI, 2023; Wei et al., 2022). The commonsense reasoning tasks comprise 8 sub-tasks, each with a predefined training and testing set. We follow the setting of (Hu et al., 2023) and amalgamate the training datasets from all 8 tasks to create the final training dataset and conduct evaluations on the individual testing dataset for each task. To ensure a fair comparison, we initially fine-tuned models with DoRA following the LoRA config- uration, maintaining the same rank while adjusting only the learning rate. The marginal increase of 0.01% in the number of trainable parameters for DoRA over LoRA, as detailed in Table 1, arises from the inclusion of learnable magnitude components (parameter of size 1 × k). Then, we further halve the rank used in DoRA compared to LoRA 5DoRA: Weight-Decomposed Low-Rank Adaptation Table 1.Accuracy comparison of LLaMA 7B/13B, LLaMA2 7B, and LLaMA3 8B with various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods on LLaMA 7B/13B are taken from (Hu et al., 2023). Results of LoRA on LLaMA2 7B and LLaMA3 8B are obtained using the hyperparameters described in (Hu et al., 2023). DoRA†: the adjusted version of DoRA with the rank halved. Model PEFT Method # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B Prefix 0.11 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series 0.99 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel 3.54 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2 LoRA 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 DoRA† (Ours) 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA (Ours) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 LLaMA-13B Prefix 0.03 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 Series 0.80 71.8 83 79.2 88.1 82.4 82.5 67.3 81.8 79.5 Parallel 2.89 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4 LoRA 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5 DoRA† (Ours) 0.35 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8 DoRA (Ours) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LLaMA2-7B LoRA 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6 DoRA† (Ours) 0.43 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5 DoRA (Ours) 0.84 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 LLaMA3-8B LoRA 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA† (Ours) 0.35 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA (Ours) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 and denote this adjusted configuration as DoRA†. See Table 8 for details on the hyperparameters used. Table 1 demonstrates that DoRA consistently surpasses all baseline methods across both LLaMA-7B/13B, LLaMA2- 7B and LLaMA3-8B. Notably, in the LLaMA-7B model, where LoRA exceeds the performance of other baselines, DoRA further enhances accuracy by 3.7%, outstripping ChatGPT’s accuracy levels. Conversely, for LLaMA-13B, where LoRA’s effectiveness is inferior to the Parallel adapter, DoRA achieves superior accuracy over LoRA by 1% and comparable accuracy to the Parallel adapter, with only a quarter of the trainable parameters required by the Parallel adapter and without adding any extra inference overhead as the Parallel adapter. Additionally, DoRA consistently surpasses LoRA on both LLaMA2-7B and LLaMA3-8B by 2.1% and 4.4%, respectively. Furthermore, DoRA† exceeds LoRA’s performance on LLaMA-7B by 2.8%, on LLaMA- 13B by 1%, on LLaMA2-7B by 2.9%, and on LLaMA3-8B by 4.2%, despite having only half as many trainable parame- ters as LoRA. This outcome suggests that the integration of DoRA enhances the learning capability of LoRA, thereby reducing the need for a higher rank to surpass LoRA in terms of accuracy. Additionally, in previous sections, we hypothesize that a negative correlation between the magnitude update and di- rectional update is more optimal than a positive correlation. This is because pre-trained weights already contain sub- stantial knowledge suitable for downstream tasks, and a larger magnitude or direction alteration alone is sufficient Figure 3.Magnitude (a) and direction (b) difference of LoRA/DoRA and the pre-trained weight of the query matrices across different layers. for downstream adaptation. To further validate our hypoth- esis, we used LLaMA2-7B fine-tuned with DoRA/LoRA on commonsense reasoning datasets as a case study. We visualized the magnitude (∆M) and directional difference (∆D) between the DoRA/LoRA weights and the pre-trained model weights across different modules and layers. In Fig- ure 3 (a) and (b), we observe that the DoRA fine-tuned weights show less deviation from the pre-trained weights in both magnitude and direction, while the differences for the LoRA fine-tuned weights are significantly larger. Coupled with the experimental results that DoRA significantly out- performs LoRA, we can conclude that our earlier hypothesis is valid: a robust foundation model does not require signif- icant alterations for effective downstream adaptation and having the ability to perform more fine-grained magnitude and directional update explains the superiority of DoRA over LoRA. We leave the visualization of the value and key 6DoRA: Weight-Decomposed Low-Rank Adaptation weight matrices in the appendix. 5.2. Image/Video-Text Understanding Table 2.The multi-task evaluation results on VQA, GQA, NVLR2 and COCO Caption with the VL-BART backbone. Method #Params (%) VQAv2 GQA NVLR2 COCO Cap Avg. FT 100 66.9 56.7 73.7 112.0 77.3 LoRA 5.93 65.2 53.6 71.9 115.3 76.5 DoRA (Ours) 5.96 65.8 54.7 73.1 115.9 77.4 Table 3.The multi-task evaluation results on TVQA, How2QA, TVC, and YC2C with the VL-BART backbone. Method #Params (%) TVQA How2QA TVC YC2C Avg. FT 100 76.3 73.9 45.7 154 87.5 LoRA 5.17 75.5 72.9 44.6 140.9 83.5 DoRA (Ours) 5.19 76.3 74.1 45.8 145.4 85.4 Having shown that DoRA can consistently achieve bet- ter accuracy on fine-tuning LLM, we would like to see if DoRA can remain competitive on multi-modality fine- tuning tasks. We compare DoRA with LoRA and full fine- tuning on VL-BART which comprises a vision encoder (CLIP-ResNet101 (Radford et al., 2021)) and an encoder- decoder language model ( BARTBase (Lewis et al., 2020)) across four different image-text tasks: VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for visual ques- tion answering, NLVR2 (Suhr et al., 2019) for visual reason- ing, and MSCOCO (Chen et al., 2015) for image captioning, and four different video-text tasks from the V ALUE (Li et al., 2021) Benchmark: TVQA (Lei et al., 2018) and How2QA (Li et al., 2020) for video question answering, TVC (Lei et al., 2020) and YC2C (Zhou et al., 2018) for video captioning. We follow the same framework as (Sung et al., 2022) and fine-tuned VL-BART within a multi-task framework for both image/video-text tasks. We adopt the same setup as that of LoRA outlined in (Sung et al., 2022) when ap- plying DoRA. See Table 9 for the complete hyperparam- eters. The result of LoRA and FT for both image/video- text tasks is directly quoted from (Sung et al., 2022). We can see that DoRA uniformly surpasses LoRA in accuracy while maintaining a similar count of trainable parameters in both Table 2 and Table 3. In particular, DoRA exceeds LoRA’s performance by nearly 1% in image-text under- standing tasks, reaching the accuracy level of FT. Moreover, DoRA achieves roughly 2% higher accuracy than LoRA in video-text understanding tasks. 5.3. Visual Instruction Tuning We further scale up the model size and compare DoRA to LoRA and FT on the visual instruction tuning tasks with Table 4.Visual instruction tuning evaluation results for LLaV A- 1.5-7B on a wide range of seven vision-language tasks. We directly use checkpoints from (Liu et al., 2023a) to reproduce their results. Method # Params(%) Avg. FT 100 66.5 LoRA 4.61 66.9 DoRA (Ours) 4.63 67.6 LLaV A-1.5-7B (Liu et al., 2023a) which is composed of a language model, Vicuna-1.5-7B (Peng et al., 2023), and a vision encoder, CLIP ViT-L/336px (Radford et al., 2021). The training datasets contain several datasets from VQA (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Schwenk et al., 2022), OCR (Mishra et al., 2019; Sidorov et al., 2020), region-level VQA (Kazemzadeh et al., 2014; Krishna et al., 2017; Mao et al., 2016), visual conversation (Liu et al., 2023a), and language conversation data. We follow the setting of (Liu et al., 2023a) to filter the training data and construct the tunning prompt format. For a fair comparison, DoRA follows the same configuration as the LoRA configuration provided by (Liu et al., 2023a). The fine-tuned models are then evaluated on seven vision- language benchmarks: VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018) SQA (Lu et al., 2022), VQAT (Singh et al., 2019), POPE (Li et al., 2023), and MMBench (Liu et al., 2023c). From Table 4, we can observe that the average accuracy of LoRA already surpasses FT, which could imply that FT might be experiencing issues with overfitting. Given that DoRA is designed to enhance LoRA’s performance to more closely resemble that of FT, in scenarios where FT is infe- rior to LoRA, DoRA’s improvement over LoRA might not be as pronounced as observed in other experiments where FT usually outperforms LoRA. Nonetheless, DoRA still demonstrates superior performance over both LoRA and FT, with an average improvement of 0.7% over LoRA and 1.1% over FT. See Table 10 for the hyperparameters setting and Table 12 for the score of each evaluation benchmark. 5.4. Compatibility of DoRA with other LoRA variants Recall from Equation.(1) that ∆W can be adapted by differ- ent LoRA variants. With DoRA, the concept of incremental directional update ∆V introduced in Equation.(5) can like- wise be replaced with alternative LoRA variants. In this section, we select VeRA (Kopiczko et al., 2024) as a case study to explore DoRA’s compatibility with other LoRA variants. VeRA suggests freezing a unique pair of random low-rank matrices to be shared across all layers, employ- ing only minimal layer-specific trainable scaling vectors to capture each layer’s incremental updates. This approach allows VeRA to reduce trainable parameters significantly 7DoRA: Weight-Decomposed Low-Rank Adaptation Table 5.Average scores on MT-Bench assigned by GPT-4 to the answers generated by fine-tuned LLaMA-7B/LLaMA2-7B. Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA (Ours) 2.33 5.5 VeRA 0.02 4.3 DV oRA (Ours) 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA (Ours) 2.33 6.0 VeRA 0.02 5.5 DV oRA (Ours) 0.04 6.0 by 10x compared to LoRA, with only a minimal impact on accuracy. We apply VeRA for the directional update in DoRA and name such combination DV oRA. We assess the effectiveness of both DV oRA and DoRA compared to VeRA and LoRA across LLaMA-7B and LLaMA2-7B, focusing on instruction tuning with the 10K subset of cleaned Alpaca dataset (Taori et al., 2023). We utilize the official imple- mentation of VeRA to obtain the results of VeRA and LoRA and fine-tune the model with DV oRA and DoRA using the identical training settings as VeRA and LoRA (see Table 11 in the appendix for more details). The performance of the fine-tuned models is then evaluated on the MT-Bench bench- mark (Zheng et al., 2023) by generating model responses to a pre-defined set of 80 multi-turn questions. These re- sponses are then evaluated by GPT-4, which reviews each answer and assigns a numerical score out of 10. Table 5 presents the average scores for DV oRA, DoRA, VeRA, and LoRA, demonstrating that our proposed method exhibits consistent improvements over VeRA and LoRA for both LLaMA-7B and LLaMA2-7B. This effectively show- cases the compatibility of DoRA with VeRA. In particular, DV oRA merges the advantageous qualities of DoRA and VeRA, attaining scores that are on par with or even sur- pass those of LoRA, yet with significantly fewer parame- ters. For example, DV oRA outperforms VeRA by 0.7/0.5 points and achieves the same level of accuracy as LoRA on LLaMA-7B and DoRA on LLaMA2-7B, respectively. Ad- ditionally, we present a selection of questions chosen from MT-Bench, accompanied by the responses from LLaMA2- 7B fine-tuned using DV oRA and VeRA in the appendix (Table 13 and 14) where we can observe that the answers given by DV oRA tend to be more precise and structural. Next, to further assess DoRA’s ability to remain competitive under varying amounts of training data, considering that in practical situations, access to extensive fine-tuning datasets is frequently limited. We compare DoRA to LoRA and DV oRA to VeRA for fine-tuning LLaMA2-7B/LLaMA-7B with a range of instruction-tuning sample sizes, specifically 1000, 4000, 7000, 10000, with 10000 being the setting of 1000 4000 7000 10000 Number of instruction tuning training samples 5.0 5.2 5.4 5.6 5.8 6.0Score LLaMA2-7B DoRA LoRA DV oRA VeRA Figure 4.Performance of fine-tuned LLaMA2-7B on MT-Bench using different numbers of Alpaca training samples. (Kopiczko et al., 2024). We visualize the average perfor- mance of each method on LLaMA2-7B in Figure 4, and on LLaMA-7B in Figure 9 in the appendix. The result shows that DoRA and DV oRA consistently outperform LoRA and VeRA across all training sample sizes. For instance, with 7000 training samples, DoRA and DV oRA surpass LoRA and VeRA by margins of 0.3 and 0.33, respectively. Even when the sample size is reduced to 1000, DoRA and DV oRA maintain their lead with advantages of 0.29 and 0.22 over LoRA and VeRA, respectively. This demonstrates that our methods persistently enhance performance over LoRA and VeRA, regardless of the training sample volume. 5.5. Robustness of DoRA towards different rank settings 4 8 16 32 64 rank r 40 45 50 55 60 65 70 75 80Avg. Accuracy (+ 37.2%) (+ 22.4%) LLaMA-7B DoRA LoRA Figure 5.Average accuracy of LoRA and DoRA for varying ranks for LLaMA-7B on the commonsense reasoning tasks. This section explores the impact of various rank configura- tions on DoRA and LoRA by adjustingr within the set {4, 8, 16, 32, 64} and assessing the performance of the fine-tuned LLaMA-7B on commonsense reasoning tasks as outlined in Sec 5.1. The average accuracies of LoRA and DoRA across different ranks are depicted in Figure 5, with detailed num- bers presented in Table 15. From Figure 5, we can observe that DoRA consistently surpasses LoRA across all rank con- figurations. Notably, the performance gap widens for ranks 8DoRA: Weight-Decomposed Low-Rank Adaptation Table 6.Accuracy comparison of LLaMA 7B/13B with two differ- ent tuning granularity of DoRA. Columns m and V designate the modules with tunable magnitude and directional components, re- spectively. Each module is represented by its first letter as follows: (Q)uery, (K)ey, (V)alue, (O)utput, (G)ate, (U)p, (D)own. Model PEFT Method # Params (%) m V Avg. LLaMA-7B LoRA 0.83 - - 74.7 DoRA (Ours) 0.84 QKVUD QKVUD 78.1 DoRA (Ours) 0.39 QKVOGUD QKV 77.5 LLaMA-13B LoRA 0.67 - - 80.5 DoRA (Ours) 0.68 QKVUD QKVUD 81.5 DoRA (Ours) 0.31 QKVOGUD QKV 81.3 below 8, where LoRA’s average accuracies drop to 40.74% for r = 8 and 39.49% for r = 4. In contrast, DoRA retains a notable accuracy of 77.96% for r = 8 and 61.89% for r = 4, demonstrating its resilience and consistently superior performance over LoRA regardless of the rank setting. 5.6. Tuning Granularity Analysis The visualization in Figure 2 indicates that significant changes in magnitude often result in relatively smaller di- rectional changes. Given this observation and the fact that directional updates account for most of the trainable param- eters, it prompts an investigation into whether it is possible to decrease the number of trainable parameters by updating only the magnitude components of specific modules while continuing to update both the magnitude and directional components for the remaining linear modules. Our findings indicate that, in contrast to the original con- figuration suggested for LoRA in (Hu et al., 2023), which requires updates to both the Multi-head Attention and MLP layers for optimal performance, DoRA can already achieve superior accuracy by updating only the directional and mag- nitude components of the multi-head layers and the magni- tude of the MLP layers. Specifically, as shown in Table 6, by updating the directional and magnitude components of the QKV modules and only the magnitude of the rest of the layers, DoRA surpasses LoRA by 2.8% on LLaMA-7B and 0.8% on LLaMA-13B, while utilizing only less than half of the trainable parameters compared to LoRA. 6. Broader Impacts 6.1. QDoRA: Enhancements to QLoRA While finetuning LLMs with PEFT significantly reduces training memory overhead, a considerable amount of GPU memory is still required to initially load the model weights onto the GPUs. To further decrease the memory demands of finetuning, QLoRA (Dettmers et al., 2023) suggests quan- tizing the pretrained model to 4-bit and finetuning LoRA on top of the frozen low-bit backbone. With our porposed 0.0 0.1 0.2 0.3 0.4 0.5 Exact match score (Eval size: 500) Zero-shot Five-shot Full Finetune QLoRA QDoRA 0.23 0.27 0.51 0.32 0.56 0.07 0.08 0.26 0.12 0.31 100k Orca-Math finetuning results LLaMA2-7B LLaMA3-8B Figure 6.Accuracy comparison of LLaMA2-7B/LLaMA3-8B with QDoRA, QLoRA and FT on Orca-Math (Mitra et al., 2024). DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework. Recently, (Kerem Turgutlu, 2024) launch a project that substitutes the LoRA component in QLoRA with DoRA, dubbing it QDoRA, and incorporate the training pipeline with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) to enable model splitting and parallel training across multiple GPUs. They conducted experiments on fine-tuning LLaMA2- 7B/LLaMA3-8B using the Orca-Math(Mitra et al., 2024) dataset with QDoRA, QLoRA, and FT. The training set included 100k samples, with 500 reserved for evaluation using the exact match score as the metric. In addition to the fine-tuned models, they also reported results from zero-shot, few-shot, and FT with post-training quantization (PTQ), where the FT model is quantized to the BnB NF4 format after training. According to Figure 6, QDoRA not only significantly surpasses QLoRA by 0.19/0.23 on LLaMA2- 7B and LLaMA3-8B, but it also slightly outperforms FT on both models, while using considerably less memory. This in- dicates that QDoRA can effectively combines the parameter efficiency of QLoRA with the more granular optimization of full finetuning. These initial findings suggest that QDoRA holds considerable promise and could hugely benefit the opensoure community by substantially lowering the GPU memory requirements for fine-tuning large language mod- els. 6.2. Text-to-Image Generation Recently, as diffusion models have expanded in size, LoRA has become a popular method for efficiently fine-tuning large stable diffusion models. In this section, we aim to explore whether DoRA’s advantages over LoRA extend to the task of text-to-image generation. We follow the training pipeline of DreamBooth (Ruiz et al., 2023) for fine-tuning 9DoRA: Weight-Decomposed Low-Rank Adaptation SDXL (Podell et al., 2023), utilizing the advanced train- ing scripts developed by HuggingFace. The hyperparameter settings for LoRA and DoRA are kept the same, and we fine- tune the model using two challenging datasets: 3D icons and Lego sets. The sample seeds for generating the images are kept the same for LoRA and DoRA for fair comparison. The generated images are shown in Figure 10 and 11 in the appendix. The results indicate that DoRA achieves signif- icantly better personalization than LoRA when using the same training settings, and more accurately reflects the train- ing targets. For example, in Figure 10, the first sub-figure of DoRA’s output features a unique round square around the image, which is a feature common to all the training targets. In contrast, this feature is absent in all the LoRA outputs. A similar observation could be found with the Lego training targets, where only the DoRA outputs consistently incorporate the Lego logo in the generated images. 7. Conclusion In this work, we first conduct a novel weight decomposi- tion analysis to reveal the distinct learning patterns between LoRA and FT. Building on these insights, we introduce DoRA, a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT’s learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Specifically, DoRA improves upon LoRA in commonsense reasoning and visual instruction tuning tasks. Furthermore, DoRA also shows compatibility with VeRA on the Alpaca instruction tuning task. Moreover, DoRA can be considered as a costless alternative to LoRA, as its decomposed mag- nitude and direction components can be merged back into the pre-trained weight after the training, ensuring that there is no extra inference overhead. For future work, we wish to explore the generalizability of DoRA in domains beyond language and vision, particularly in the field of audio. Acknowledgements We extend our gratitude to Benjamin Bossan, Younes Belkada, and Sourab Mangrulkar from Hugging Face for their assistance in integrating DoRA into the PEFT pack- age, thus making our work more accessible to the broader public. We thank Kerem Turgutlu, Jonathan Whitaker, and Jeremy Howard from Answer.AI for their work on the imple- mentation and experiments of QDoRA/FSDP, which makes fine-tuning of large language models with DoRA on con- sumer GPUs a lot more feasible. We also thank Sebastian Raschka for his well-written tutorial on DoRA which offers a thorough overview of the background knowledge neces- sary to comprehend DoRA. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Chen, X., Fang, H., Lin, T.-Y ., Vedantam, R., Gupta, S., Doll´ar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 10088–10115. Cur- ran Associates, Inc., 2023. Goyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617, 2018. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient trans- fer learning. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on 10DoRA: Weight-Decomposed Low-Rank Adaptation imagenet classification. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 1026–1034, 2015. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pp. 2790–2799, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Hu, Z., Wang, L., Lan, Y ., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large lan- guage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700– 6709, 2019. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low- rank hadamard product for communication-efficient fed- erated learning. In International Conference on Learning Representations, 2022. Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 565–576, 2021. Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014. Kerem Turgutlu, Jonathan Whitaker, J. H. Ef- ficient finetuning of llama 3 with fsdp qdora. https://www.answer.ai/posts/ 2024-04-26-fsdp-qdora-llama3.html , 2024. Kopiczko, D. J., Blankevoort, T., and Asano, Y . M. Vera: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024. Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, pp. 32–73, 2017. Lei, J., Yu, L., Bansal, M., and Berg, T. TVQA: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1369–1379, 2018. Lei, J., Yu, L., Berg, T. L., and Bansal, M. Tvr: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision, pp. 447–463, 2020. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pp. 7871–7880, 2020. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Confer- ence on Machine Learning, pp. 12888–12900, 2022. Li, L., Chen, Y .-C., Cheng, Y ., Gan, Z., Yu, L., and Liu, J. HERO: Hierarchical encoder for Video+Language omni- representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2046–2065, 2020. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y .-C., Pillai, R., Cheng, Y ., Zhou, L., Wang, X. E., Wang, W. Y ., et al. Value: A multi-task benchmark for video-and-language understand- ing evaluation. In Thirty-fifth Conference on Neural In- formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582– 4597, 2021. Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language 11DoRA: Weight-Decomposed Low-Rank Adaptation models. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing , pp. 292–305, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun- ing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Liu, W., Qiu, Z., Feng, Y ., Xiu, Y ., Xue, Y ., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W., Yuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Infor- mation Processing Systems, pp. 2507–2521, 2022. mahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural Information Processing Systems, 2021. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unam- biguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11–20, 2016. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requir- ing external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195–3204, 2019. Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952, 2019. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. OpenAI. Welcome to the openai platform, 2023. URL https://platform.openai.com/docs/ introduction. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im- proving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining modular skills in multitask learning.arXiv preprint arXiv:2202.13914, 2022. Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339–1384, 2023. Qiu, Z., Liu, W., Feng, H., Xue, Y ., Feng, Y ., Liu, Z., Zhang, D., Weller, A., and Sch¨olkopf, B. Controlling text- to-image diffusion by orthogonal finetuning. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763, 2021. Razdaibiedina, A., Mao, Y ., Khabsa, M., Lewis, M., Hou, R., Ba, J., and Almahairi, A. Residual prompt tuning: improving prompt tuning with residual reparameteriza- tion. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6740–6757, 2023. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023. Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image dif- fusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023. Salimans, T. and Kingma, D. P. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 901–909, 2016. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual ques- tion answering using world knowledge. In European Conference on Computer Vision, pp. 146–162, 2022. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehen- sion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part II 16, pp. 742–758, 2020. 12DoRA: Weight-Decomposed Low-Rank Adaptation Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317–8326, 2019. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y . A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418–6428, 2019. Sung, Y .-L., Cho, J., and Bansal, M. Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 5227–5237, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stan- ford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/ stanford_alpaca. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wang, Y ., Wu, J., Dabral, T., Zhang, J., Brown, G., Lu, C.-T., Liu, F., Liang, Y ., Pang, B., Bendersky, M., et al. Non- intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. arXiv preprint arXiv:2310.12100, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y ., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Interna- tional Conference on Learning Representations, 2023. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In Proceed- ings of the AAAI Conference on Artificial Intelligence , 2018. 13DoRA: Weight-Decomposed Low-Rank Adaptation A. Appendix A.1. Weight decomposition analysis on the value weight matrix In this section, we illustrate the changes in magnitude and direction within the value weight matrix for FT, LoRA, and DoRA across different training steps and layers, as shown in Figure 7. This reveals patterns similar to those seen in the query weight matrix depicted in Figure 2, indicating that DoRA is capable of displaying learning behaviors that closely mirror those of FT across various modules. 0.02 0.03 0.04 0.05 0.06 0.07 0.08 D (a) 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325 D (b) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 D (c) 0.4 0.6 0.8 1.0 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 7.Magnitude and Directional changes of FT (a), LoRA (b), and DoRA (c) of the V weight matrices across different layers and intermediate steps. A.2. Ablation study for the modification to reduce DoRA training cost Table 7 presents the GPU cost and the average accuracy of DoRA with and without the proposed modification for commonsense reasoning tasks and image-text understanding tasks. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains unchanged for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. Table 7.GPU cost and accuracy of DoRA with or without the modification on the commonsense reasoning tasks and image-text understanding tasks. Model PEFT Method Accumulation steps Batch Size GPU Memory Cost (GB) # Params (%) Avg. LLaMA-7B DoRA w/o modification 4 16 37.3 0.84 78.3 DoRA 28.2 (-24.4%) 0.84 78.1 VL-BART DoRA w/o modification - 300 23.4 5.96 77.3 DoRA 20.5 (-12.4%) 5.96 77.4 14DoRA: Weight-Decomposed Low-Rank Adaptation A.3. Hyperparameters Table 8.Hyperparameter configurations of DoRA for LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B on the commonsense reasoning tasks. Hyperparameters (DoRA) LLaMA-7B LLaMA-13B LLaMA2-7B LLaMA3-8B Rank r 16 32 16 32 16 32 16 32 α 32 64 32 64 32 64 32 64 Dropout 0.05 Optimizer AdamW LR 2e-4 1e-4 3e-4 2e-4 2e-4 2e-4 1e-4 1e-4 LR Scheduler Linear Batch size 16 Warmup Steps 100 Epochs 3 Where Q,K,V ,Up,Down Table 9.Hyperparameter configurations of DoRA for fine-tuning VL-Bart on image/video-text tasks. Hyperparameters (DoRA) image-text video-text Rank r 128 α 128 Dropout 0.0 Optimizer AdamW LR 1e-3 3e-4 LR Scheduler Linear Batch size 300 40 Warmup ratio 0.1 Epochs 20 7 Where Q,K Table 10.Hyperparameter configurations of DoRA and LoRA for fine-tuning LLaV A-1.5-7B with visual instruction tuning datasets. Hyperparameters DoRA LoRA Rank r 128 α 256 Dropout 0.05 Optimizer AdamW LR 2e-4 LR Scheduler Cosine decay Batch size 16 Warmup ratio 0.03 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 15DoRA: Weight-Decomposed Low-Rank Adaptation Table 11.Hyperparameter configurations of DoRA and DV oRA for fine-tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset. Hyperparameters (DoRA) LLaMA-7B LLaMA2-7B Rank r 64 Dropout 0.0 Optimizer AdamW LR 4e-4 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate Hyperparameters (DVoRA) LLaMA-7B LLaMA2-7B Rank r 1024 Dropout 0.0 Optimizer AdamW LR 4e-3 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 16DoRA: Weight-Decomposed Low-Rank Adaptation A.4. Magnitude and Direction difference between DoRA/LoRA fine-tuned weight and the pre-triained weight of LLaMA2-7B for the commonsesne reasoning tasks Figure 8 depicts the magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. The figure shows that the DoRA fine-tuned weights deviate less from the pre-trained weights in both magnitude and direction, supporting our hypothesis that a robust foundation model does not need substantial changes for effective downstream adaptation. (a) (b) (d) (c) (f) (e) Figure 8.Magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. 17DoRA: Weight-Decomposed Low-Rank Adaptation A.5. Visual instruction tuning evaluation result Table 12 presents the evaluation result of LLaV A-1.5-7B fine-tuned using DoRA, LoRA, and FT with visual instruction tuning data. The evaluation is conducted on an array of seven different vision-language benchmarks. From the table, we can see that DoRA consistently outmatch FT, and can achieve an average accuracy better than LoRA. Table 12.Visual instruction tuning evaluation result of DoRA, LoRA, and FT for LLaV A-1.5-7B on a wide range of 7 vision-language tasks. Method # Params(%) VQAv2 GQA VisWiz SQA VQA T POPE MMBench Avg. FT 100 78.5 61.9 50.0 66.8 58.2 85.9 64.3 66.5 LoRA 4.61 79.1 62.9 47.8 68.4 58.2 86.4 66.1 66.9 DoRA (Ours) 4.63 78.6 62.9 52.2 69.9 57 87.2 66.1 67.6 18DoRA: Weight-Decomposed Low-Rank Adaptation A.6. Instruction-tuning evaluation examples We sample two questions from the MT-Bench question set and present the respective responses from DV oRA and VeRA, including the scores and comments provided by GPT-4. From these examples, we can observe that responses from DV oRA are generally more accurate and well-organized compared to those from VeRA. For instance, in response to Question 24, DV oRA leverages a well-known superhero story that is familiar to children, whereas VeRA’s response begins with an uncertain tone, potentially making it less engaging than DV oRA’s reply. Table 13.Question 24 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 24 As a superhero, how would you explain your origin story to a curious child? VeRA Well, I’m not really sure where I come from. All I know is that one day, I woke up with special powers and abilities that allowed me to help people. I decided to use my powers to fight crime and protect the innocent, and that’s how I became a superhero DV oRA As a superhero, I have the ability to use my super powers to protect people and fight against evil. My origin story begins when I was just a regular kid like you, but one day, I was bitten by a radioactive spider, and it gave me incredible powers. I could climb walls, shoot webs, and see in the dark. I decided to use my powers to help people, and that’s how I became your friendly neighborhood Spider-Man. GPT-4 Evaluation Score: 5 7. Assistant 1’s response starts with an uncertain tone, which might not be very engaging for a child. The explanation is quite vague and doesn’t offer a clear or imaginative origin story that could captivate a child’s interest. It does, however, maintain the theme of using powers for good, which is a positive message. Assistant 2’s response is more detailed and uses a familiar superhero origin story (Spider-Man’s) that many children can relate to and understand. It’s engaging and provides a clear narrative of how an ordinary person can become a superhero. However, it directly references Spider-Man, which is not an original origin story and could be misleading if the assistant is supposed to be a unique superhero. Despite this, the response is more likely to resonate with a child due to its clarity and reliability. Both responses could be improved by providing an original and imaginative origin story that doesn’t rely on existing superhero narratives. They should also ensure that the story is simple enough for a child to understand while being engaging and inspiring. 19DoRA: Weight-Decomposed Low-Rank Adaptation Table 14.Question 33 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 33 Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? VeRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Some people may prefer the physical act of navigating with a paper map, as it can be more tactile and interactive. Others may prefer to have a hard copy of the map in case the GPS or app fails or the battery dies. Additionally, some people may not have access to a GPS device or smartphone, or may not want to rely on technology for navigation. Finally, some people may simply prefer the security and peace of mind that comes with knowing exactly where they are and how to get to their destination DV oRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Here are some of them: 1. Navigation accuracy: GPS devices and smartphone apps rely on satellite signals to pinpoint your location, but this technology is not foolproof. GPS signals can be blocked by tall buildings, tunnels, or other obstacles, resulting in inaccurate or missing information. Paper maps, on the other hand, are based on accurate, up-to-date information that is not subject to these limitations. 2. Cost: GPS devices and smartphone apps can be expensive to purchase and maintain. Paper maps are free to use and do not require an ongoing subscription or data plan. 3. Personal preference: Some people prefer the tactile experience of using a paper map, which can be easier to read and navigate than a screen. Others enjoy the challenge of navigating without technology, which can help them learn more about the area they are visiting. 4. Battery life: GPS devices and smartphone apps can drain batteries quickly, especially when used for navigation. Paper maps do not require power, so they can be used for extended periods without recharging. 5. Offline use: GPS devices and smartphone apps rely on an internet connection to function properly. If there is no signal, the device will not be able to provide accurate information. Paper maps, on the other hand, can be used offline, making them ideal for use in remote or unpopulated areas. In summary, there are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. These include navigation accuracy, cost, personal preference, battery life, and offline use. GPT-4 Evaluation Score: 6 8. Assistant 1’s response is somewhat brief and lacks specific details that could be helpful to the user. It mentions the tactile nature of paper maps, the possibility of technology failure, lack of access to technology, and a general preference for security and peace of mind. However, it does not delve into the practical considerations such as GPS signal limitations, cost, battery life, or offline use, which are important factors in deciding between paper maps and digital navigation tools. Assistant 2’s response is more comprehensive and provides a structured list of reasons, including navigation accuracy, cost, personal preference, battery life, and offline use. Each point is explained clearly, giving the user a better understanding of the potential advantages of paper maps over GPS devices or smartphone apps. The response is relevant, accurate, and detailed, making it more helpful to the user. Both assistants provided relevant and accurate information, but Assistant 2’s answer is more detailed and structured, which is why it receives a higher score. The scores reflect the level of detail and the usefulness of the information provided to the user, with Assistant 2 offering a more complete answer to the question 20DoRA: Weight-Decomposed Low-Rank Adaptation A.7. Robustness towards different numbers of instruction tunning examples Figure 9 depicts the score of DoRA, LoRA, DV oRA, and VeRA for fine-tuning LLaMA-7B with varying instruction-tuning sample sizes, including 1000, 4000, 7000, and 10000. From the figure, we can observe that similar to that of the result on LLaMA2-7B, both DoRA and DV oRA consistently outperform LoRA and VeRA across the entire range of training sample sizes. 1000 4000 7000 10000 Number of instruction tuning training samples 3.0 3.5 4.0 4.5 5.0 5.5Score LLaMA-7B DoRA LoRA DV oRA VeRA Figure 9.Performance of LLaMA-7B fine-tuned with LoRA, DoRA, VeRA, and DV oRA on MT-Bench using different numbers of training samples from Alpaca dataset (Taori et al., 2023). 21DoRA: Weight-Decomposed Low-Rank Adaptation A.8. Robustness of DoRA towards different rank Table 15 shows a comparison of the average accuracy between LoRA and DoRA method across various rank settings for commonsense reasoning tasks. DoRA consistently outperforms LoRA at all rank settings, with the performance gap widening as the rank decreases. This suggests that our method effectively enhances the learning capacity of LoRA, enabling it to achieve better accuracy with fewer trainable parameters. Table 15.Accuracy comparison of LoRA and DoRA with varying ranks for LLaMA-7B on the commonsense reasoning tasks. PEFT Method rank r # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. LoRA 4 0.10 2.3 46.1 18.3 19.7 55.2 65.4 51.9 57 39.5 8 0.21 31.3 57.0 44.0 11.8 43.3 45.7 39.2 53.8 40.7 16 0.42 69.9 77.8 75.1 72.1 55.8 77.1 62.2 78.0 70.9 32 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 64 1.64 66.7 79.1 75.7 17.6 78.8 73.3 59.6 75.2 65.8 DoRA (Ours) 4 0.11 51.3 42.2 77.8 25.4 78.8 78.7 62.5 78.6 61.9 8 0.22 69.9 81.8 79.7 85.2 80.1 81.5 65.7 79.8 77.9 16 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 32 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 64 1.65 69.9 81.4 79.1 40.7 80.0 80.9 65.5 79.4 72.1 22DoRA: Weight-Decomposed Low-Rank Adaptation A.9. Text-to-Image Generation Figures 10 and 11 show the images produced by SDXL fine-tuned with DoRAand LoRA via DreamBooth (Ruiz et al., 2023) personalization techniques on two distinct training sets: 3D Icon 1 and Lego2. The results reveal that DoRA can achieve considerably better personalization than LoRA with identical training configurations, more closely matching the training target. Figure 10.Images generated with SDXL finetuned with LoRA and DoRA on the 3D Icon training sets. Figure 11.Images generated with SDXL finetuned with LoRA and DoRA on the Lego training sets. 1https://huggingface.co/datasets/linoyts/3d_icon 2https://huggingface.co/datasets/merve/lego_sets_latest 23",
      "meta_data": {
        "arxiv_id": "2402.09353v6",
        "authors": [
          "Shih-Yang Liu",
          "Chien-Yi Wang",
          "Hongxu Yin",
          "Pavlo Molchanov",
          "Yu-Chiang Frank Wang",
          "Kwang-Ting Cheng",
          "Min-Hung Chen"
        ],
        "published_date": "2024-02-14T17:59:34Z",
        "pdf_url": "https://arxiv.org/pdf/2402.09353v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel Parameter-Efficient Fine-Tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and full fine-tuning (FT) without incurring additional inference costs. It achieves this by decomposing pre-trained weights into magnitude and direction components for fine-tuning, specifically employing LoRA for efficient directional updates. The work also presents a novel weight decomposition analysis to investigate the inherent differences in learning patterns between FT and LoRA. DoRA consistently outperforms LoRA on various downstream tasks across LLaMA, LLaVA, and VL-BART models, including commonsense reasoning, visual instruction tuning, and image/video-text understanding.",
        "methodology": "DoRA reparameterizes model weights into magnitude (m) and directional (V) components. The magnitude component is a trainable vector, while the directional component is updated using a low-rank adaptation method, specifically LoRA. This decomposition allows DoRA to exhibit a learning pattern similar to FT, characterized by a negative correlation between magnitude and directional updates, in contrast to LoRA's positive correlation. The methodology includes a gradient analysis demonstrating how this decomposition enhances optimization stability. To reduce training memory overhead, the term ||V + ∆V ||c is treated as a constant during backpropagation, dynamically reflecting updates without receiving gradients.",
        "experimental_setup": "Experiments were conducted across diverse tasks and model architectures. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B were fine-tuned on eight sub-tasks (e.g., BoolQ, PIQA, HellaSwag). For image/video-text understanding, VL-BART was evaluated on VQAv2, GQA, NLVR2, MSCOCO Caption (image) and TVQA, How2QA, TVC, YC2C (video). Visual instruction tuning used LLaVA-1.5-7B, evaluated on VQAv2, GQA, VisWiz, SQA, VQAT, POPE, and MMBench. Instruction tuning of LLaMA-7B/LLaMA2-7B employed a 10K subset of the Alpaca dataset, with evaluation on MT-Bench using GPT-4 scores. QDoRA (DoRA within QLoRA framework) was tested on LLaMA2-7B/LLaMA3-8B using the Orca-Math dataset. Text-to-image generation involved fine-tuning SDXL with DreamBooth on 3D icons and Lego sets. Validation involved comparing DoRA against LoRA, FT, and other PEFT methods like Prefix, Series, Parallel adapters, and VeRA, along with ablation studies on training sample size, rank settings, and tuning granularity.",
        "limitations": "While DoRA generally enhances LoRA to resemble FT's performance, its improvement over LoRA may be less pronounced in scenarios where FT itself performs worse than LoRA, potentially due to overfitting in FT. The proposed modification to reduce training overhead by treating ||V + ∆V ||c as a constant, while effective for memory reduction (24.4% for LLaMA, 12.4% for VL-BART), results in a negligible accuracy difference (0.2 for LLaMA), suggesting a minor, though not significant, trade-off.",
        "future_research_directions": "The authors intend to explore the generalizability of DoRA in domains beyond language and vision, specifically mentioning the field of audio."
      }
    },
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
      "full_text": "DoRA: Weight-Decomposed Low-Rank Adaptation Shih-Yang Liu1 2 Chien-Yi Wang1 Hongxu Yin 1 Pavlo Molchanov1 Yu-Chiang Frank Wang1 Kwang-Ting Cheng 2 Min-Hung Chen 1 Abstract Among the widely used parameter-efficient fine- tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decom- position analysis to investigate the inherent dif- ferences between FT and LoRA. Aiming to re- semble the learning capacity of FT from the findings, we propose Weight-Decomposed Low- Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, mag- nitude and direction, for fine-tuning, specifically employing LoRA for directional updates to ef- ficiently minimize the number of trainable pa- rameters. By employing DoRA, we enhance both the learning capacity and training stabil- ity of LoRA while avoiding any additional in- ference overhead. DoRA consistently outper- forms LoRA on fine-tuning LLaMA, LLaV A, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruc- tion tuning, and image/video-text understanding. Code is available at https://github.com/ NVlabs/DoRA. 1. Introduction Models that are pre-trained with extensive general domain datasets have demonstrated remarkable generalization abil- ities, significantly benefiting a wide array of applications, from natural language processing (NLP) tasks (Qin et al., 2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022; Liu et al., 2023a). To tailor these general models for spe- cific downstream tasks, full fine-tuning (FT) is commonly 1NVIDIA 2HKUST. Correspondence to: Shih-Yang Liu <shi- hyangl@nvidia.com, sliuau@connect.ust.hk>, Min-Hung Chen <minhungc@nvidia.com>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Magnitude B  A  Pretrained Weight Merged Weight Pretrained Weight Pretrained Weight Adapt Frozen Trainable Magnitude Direction Direction Decompose (Initialize) Merge Figure 1.An overview of our proposed DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that || · ||c denotes the vector-wise norm of a matrix across each column vector. employed, involving the retraining of all model parameters. Nevertheless, as the size of models and datasets expand in scale, the expense associated with fine-tuning the entire model becomes prohibitively large. To address this issue, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019) have been introduced to fine- tune the pre-trained models with only a minimal number of parameters. Among these, LoRA (Hu et al., 2022), which does not change the model architecture, has become notably popular for its simplicity and efficacy. Nevertheless, there is still a capacity gap between LoRA and FT, which is often attributed to the limited number of trainable parameters without further exploration of other underlying causes (Hu et al., 2022; Kopiczko et al., 2024). Drawing on Weight Normalization (Salimans & Kingma, 2016), which achieves faster convergence via improving the conditioning of the gradient with weight reparameterization, we introduce a novel weight decomposition analysis that ini- tially reparameterizes model weights into magnitude and di- rectional components, subsequently examining the changes 1 arXiv:2402.09353v6  [cs.CL]  9 Jul 2024DoRA: Weight-Decomposed Low-Rank Adaptation in magnitude and direction introduced by LoRA and FT. Our analysis reveals that LoRA and FT exhibit markedly distinct patterns of updates, leading us to surmise that these variations mirror the learning capability of each method. Inspired by our findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA), which begins by decompos- ing the pre-trained weight into its magnitude and directional components, then fine-tunes both. Given the substantial size of the directional component in terms of parameters, we exploit LoRA for the directional adaptation to enable efficient fine-tuning, as illustrated in Figure.1. Moreover, by showing a learning behavior similar to FT both empir- ically and mathematically, suggesting a learning capacity closely resembling FT, we have validated DoRA across a wide variety of tasks, from NLP to Vision-Language, and over various backbones, including LLM and LVLM. The experimental results show that DoRA consistently outper- forms LoRA without sacrificing inference efficiency, such as commonsense reasoning (+3.7/+1.0 on LLaMA-7B/13B, +2.9 on LLaMA2-7B, and +4.4 on LLaMA3-8B), visual in- struction tuning (+0.6 on LLaV A-7B), and image/video-text understanding (+0.9/+1.9 on VL-BART). The summary of our contributions is as follows: • We introduce DoRA, a novel PEFT method that incor- porates weight decomposition, achieving a learning capacity closely resembling FT without any additional inference latency over LoRA. • We introduce a novel weight decomposition analysis to uncover the fundamental differences in the learning patterns of FT and different PEFT methods. • DoRA consistently surpasses LoRA on various tasks, from NLP to Vision-Language benchmarks and across various backbones, including LLM and LVLM. 2. Related Works Parameter-Efficient Fine-Tuning (PEFT)methods are de- signed to reduce the high expense of fine-tuning large-scale models. They achieve this by training a relatively small subset of parameters, compared to the total number of pa- rameters, for adapting to downstream tasks. Existing PEFT methods can be divided into three categories. The first category is referred to as Adapter-based methods, which involve introducing additional trainable modules into the original frozen backbone, such as (Houlsby et al., 2019; He et al., 2021; Karimi Mahabadi et al., 2021; mahabadi et al., 2021). For example, (Houlsby et al., 2019) proposes adding linear modules in sequence to the existing layer, whereas (He et al., 2021) advocates for integrating these modules in parallel with the original layer to enhance performance. The second category is Prompt-based methods. These methods add extra soft tokens (prompts) to the initial input and fo- cus solely on fine-tuning these trainable vectors, as seen in works like (Lester et al., 2021; Razdaibiedina et al., 2023; Wang et al., 2023). However, these approaches typically face challenges due to their sensitivity to initialization, af- fecting their overall effectiveness. These first two categories, whether altering the model’s input or architecture, result in increased inference latency compared to the baseline model. LoRA (Hu et al., 2022) and its variants are among the third category of PEFT, notable for not adding any extra inference burden. These methods apply low-rank matrices to approximate weight changes during fine-tuning and can merge with pre-trained weights prior to inference. For ex- ample, (Zhang et al., 2023) employs SVD decomposition and prunes less significant singular values for more efficient updates. (Hyeon-Woo et al., 2022) focuses on low-rank Hadamard product for federated learning. (Qiu et al., 2023; Liu et al., 2023b) exploit orthogonal factorization in fine- tuning diffusion models. (Renduchintala et al., 2023) uses weight tying to further reduce the trainable parameters. (Yeh et al., 2023) introduces a unified LoRA family framework for Stable diffusion. (Ponti et al., 2022) chooses different combinations of LoRAs from the inventory with a routing function for different tasks. (Kopiczko et al., 2024) imple- ments learnable scaling vectors to adjust a shared pair of frozen random matrices across layers. Our research also falls within this third category, and we validate the efficacy of our proposed method alongside LoRA and its variants through comprehensive experimentation. 3. Pattern Analysis of LoRA and FT 3.1. Low-Rank Adaptation (LoRA) Building upon the hypothesis that updates made during the fine-tuning exhibit a low “intrinsic rank”, LoRA (Hu et al., 2022) proposes using the product of two low-rank matri- ces to update the pre-trained weights incrementally. For a pre-trained weight matrix W0 ∈ Rd×k, LoRA models the weight update ∆W ∈ Rd×k utilizing a low-rank decompo- sition, expressed as BA, where B ∈ Rd×r and A ∈ Rr×k represent two low-rank matrices, withr ≪ min(d, k). Con- sequently, the fine-tuned weight W′ can be represented as: W′ = W0 + ∆W = W0 + BA (1) where W0 remains static during the fine-tuning process, and the underlined parameters are being trained. The ma- trix A is initialized with uniform Kaiming distribution (He et al., 2015), while B is initially set to zero, resulting in ∆W = BA being zero at the start of training. Notably, this decomposition of ∆W can be substituted with other LoRA variants, such as VeRA (Kopiczko et al., 2024). Ad- ditionally, based on Eq. (1), we can merge the learned ∆W with the pre-trained weight W0 and obtain W′ in advance 2DoRA: Weight-Decomposed Low-Rank Adaptation of deployment, and given that both W′ and W0 both fall within the dimensionality of Rd×k, LoRA and its related variants do not introduce any extra latency during the infer- ence compared to the original model. 3.2. Weight Decomposition Analysis The study presented in LoRA (Hu et al., 2022) suggests that LoRA can be considered a general approximation of full fine-tuning. By gradually increasing the rank r of LoRA to align with the rank of pre-trained weights, LoRA can attain a level of expressiveness akin to that of FT. Con- sequently, many previous studies have attributed the dis- crepancy in accuracy between LoRA and FT primarily to the limited number of trainable parameters, often without further analysis (Hu et al., 2022; Kopiczko et al., 2024). Drawing inspiration from Weight Normalization (Salimans & Kingma, 2016), which reparameterizes the weight matrix into magnitude and direction for accelerating optimization, we introduce an innovative weight decomposition analysis. Our analysis restructures the weight matrix into two sep- arate components, magnitude and direction, to reveal the inherent differences in LoRA and FT learning patterns. Analysis Method: This analysis examines the updates in both magnitude and direction of the LoRA and FT weights relative to the pre-trained weights to reveal the fundamental differences in the learning behaviors of both. The weight decomposition of W ∈ Rd×k can be formulated as: W = m V ||V ||c = ||W||c W ||W||c (2) where m ∈ R1×k is the magnitude vector, V ∈ Rd×k is the directional matrix, with ||·|| c being the vector-wise norm of a matrix across each column. This decomposition ensures that each column of V/||V ||c remains a unit vector, and the corresponding scalar in m defines the magnitude of each vector. For our weight decomposition analysis, we select the VL- BART model fine-tuned on four image-text tasks as outlined in (Sung et al., 2022) for a case study. Following (Sung et al., 2022), which applies LoRA only to the query/value weight matrix in the self-attention module. We decompose the pre- trained weight W0, the full fine-tuned weight WFT, and the merged LoRA weight WLoRA of query/value weight matrix using Eq. (2). The magnitude and directional variations between W0 and WFT can be defined as follows: ∆Mt FT = Pk n=1 |mn,t FT − mn 0 | k (3) ∆Dt FT = Pk n=1(1 − cos(V n,t FT , Wn 0 )) k (4) Here, ∆Mt FT and and ∆Dt FT represent the magnitude dif- ference and directional difference between W0 and WFT at t training step respectively, with cos(·, ·) being the cosine similarity function. Mn,t FT and Mn 0 are the nth scalars in their respective magnitude vectors, while V n,t FT and Wn 0 are the nth columns in V t FT and W0. The magnitude and direc- tional differences between WLoRA and W0 are calculated similarly, as per Eq. (3) and Eq. (4). We select checkpoints from four different training steps for analysis, comprising three intermediate steps and the final checkpoint from both FT and LoRA, and we perform weight decomposition anal- ysis on each of these checkpoints to determine the ∆M and ∆D throughout different layers. Analysis Results: Figure 2 (a) and (b) illustrate the alter- ations in the query weight matrix of FT and LoRA, with each point representing a ( ∆Dt, ∆Mt) pair from query weight matrices across different layers and training steps. Similarly, Figure 7 in the appendix displays the value weight matrix modifications. It is noticeable that LoRA exhibits a consistent positive slope trend across all the intermediate steps, signifying a proportional relationship between the changes in direction and magnitude. In contrast, the FT displays a more varied learning pattern with a relatively neg- ative slope. This distinction between FT and LoRA likely mirrors their respective learning capability. While LoRA tends to either increase or decrease the magnitude and direc- tion updates proportionally, it lacks the nuanced capability for more subtle adjustments. Specifically, LoRA does not show proficiency in executing slight directional changes alongside more significant magnitude alterations, or vice versa, a feature more characteristic of the FT method. We suspect that such limitation of LoRA might stem from the challenge of concurrent learning both magnitude and direc- tional adaptation, which could be overly complex for LoRA. Consequently, in this work, we aim to propose a variant of LoRA that exhibits a learning pattern more closely resem- bling that of FT, and can improve the learning capacity over LoRA. 4. Method 4.1. Weight-Decomposed Low-Rank Adaptation Drawing from the insights of our weight decomposition analysis, we introduce Weight- Decomposed L ow-Rank Adaptation (DoRA). DoRA initially decomposes the pre- trained weight into its magnitude and directional compo- nents and finetunes both of them. Because the directional component is large in terms of parameter numbers, we fur- ther decompose it with LoRA for efficient finetuning. Our intuitions are two-fold. Firstly, we believe that limiting LoRA to concentrate exclusively on directional adaptation while also allowing the magnitude component to be tun- able simplifies the task compared to the original approach, where LoRA is required to learn adjustments in both mag- 3DoRA: Weight-Decomposed Low-Rank Adaptation 0.014 0.016 0.018 0.020 0.022 D (a) 0.05 0.06 0.07 0.08 0.09 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.10 0.11 0.12 0.13 0.14 0.15 0.16 D (b) 0.25 0.30 0.35 0.40 0.45 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 D (c) 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 2.Magnitude and direction updates of (a) FT, (b) LoRA, and (c) DoRA of the query matrices across different layers and intermediate steps. Different markers represent matrices of different training steps and different colors represent the matrices of each layer. nitude and direction. Secondly, the process of optimizing directional updates is made more stable through weight decomposition, which we delve into more thoroughly in Section.4.2. It is important to highlight that the main distinc- tion between DoRA and weight normalization (Salimans & Kingma, 2016) lies in their training approaches. Weight normalization trains both components from scratch, mak- ing the method sensitive to different initializations. Con- versely, DoRA avoids such initialization concerns since both components begin with pre-trained weights. We initialize DoRA with pre-trained weight W0 as outlined in Eq. (2), where m = ||W0||c and V = W0 after initialization. We then keep V frozen and m a trainable vector. The directional component is then updated through LoRA. DoRA can be formulated similar to Eq. (1) as: W′ = m V + ∆V ||V + ∆V ||c = m W0 + BA ||W0 + BA||c (5) where ∆V is the incremental directional update learned by multiplying two low-rank matrices B and A, and the underlined parameters denote the trainable parameters. The matrices B ∈ Rd×r and A ∈ Rr×k are initialized in line with LoRA’s strategy to ensure that W′ equals W0 before the finetuning. Furthermore, DoRA can be merged with the pre-trained weight before inference, thereby not introducing any additional latency. We visualize the magnitude and directional differences of the query weight matrix between the merged DoRA weight and W0 in the same setting as for FT and LoRA in Figure 2 (c) and leave the visualization of the value weight matrix in the appendix. From the regression line for (∆D, ∆M) of both DoRA and FT, we reveal that in contrast to LoRA’s pattern, DoRA, and FT are characterized by a distinct neg- ative slope. We reason that FT tends towards a negative slope because pre-trained weights already possess substan- tial knowledge suitable for various downstream tasks. There- fore, when provided with adequate learning capacity, having a larger magnitude or direction alteration alone is sufficient enough for downstream adaptation. We additionally com- pute the correlation between ∆D and ∆M for FT, LoRA, and DoRA, and we found that both FT and DoRA exhibit negative correlation values of -0.62 and -0.31, respectively. In contrast, LoRA shows a positive correlation with a value of 0.83. In conclusion, the fact that DoRA demonstrates the ability to make only substantial directional adjustments with relatively minimal changes in magnitude or the reverse while showing learning patterns closer to FT’s signifies its superior learning capacity over LoRA. 4.2. Gradient Analysis of DoRA In this section, we first derive the gradient of DoRA and illustrate how our proposed decomposition benefits the opti- mization of ∆V . Subsequently, we analyze from the gradi- ent’s perspective to explicate the learning pattern of DoRA, which tends to have a negative slope. From Eq. (5), we can obtain the gradient of Loss L with respect to m and V ′ = V + ∆V as: ∇V ′L = m ||V ′||c \u0012 I − V ′V ′T ||V ′||2c \u0013 ∇W′L (6) ∇mL = ∇W′L ·V ′ ||V ′||c (7) Eq. (6) reveals that the weight gradient ∇W′L is scaled by m/||V ′||c and is projected away from the current weight matrix. These two effects contribute to aligning the gra- dient’s covariance matrix more closely with the identity matrix, which is advantageous for optimization (Salimans & Kingma, 2016). Additionally, given that V ′ = V + ∆V , the gradient ∇V ′L is equivalent to ∇∆V L. Therefore, the optimization benefits derived from this decomposition are fully transferred to ∆V , enhancing the learning stability of LoRA. We can gain further insight into the learning pattern of DoRA by referring to Eq. (7). In the subsequent dis- cussion, we represent vectors using lower-case letters in- stead of the previous matrix form notation. Consider w′′ = w′ + ∆w as the parameter update for a weight vector, where ∆w ∝ ∇w′L. In two hypothetical update 4DoRA: Weight-Decomposed Low-Rank Adaptation scenarios, S1 and S2, S1 involves a smaller directional update (∆DS1), while S2 involves a larger one ( ∆DS2). Assuming ||∆wS1|| = ||∆wS2||, and at time 0, we have ∆v = 0 and v′ = v. From ∆DS1 < ∆DS2, it follows that |cos(∆wS1, w′)| > |cos(∆wS2, w′)|. Since ∆w ∝ ∇w′L, it implies |cos(∇S1 w′ L, w′)| > |cos(∇S2 w′ L, w′)|. From Sec 4.1, with v initialized as v0 and w′ = w0 at time 0, we get |cos(∇w′L, w′)| = |cos(∇w′L, v′)| = |cos(∇w′L, v)|. Using the cosine similarity equation with ∆v = 0: cos(∇w′L, v′) = cos(∇w′L, v) = ∇w′L ·v ||∇w′L||||v|| (8) denote m∗ as the magnitude scalar of vector w′ then Eq. (7) w.r.t m∗ can be rewritten to: ∇m∗L = ∇w′L ·v′ ||v′|| = ||∇w′L|| ·cos(∇w′L, v) (9) Given that ||∆wS1|| = ||∆wS2|| for S1 and S2, and ||∇S1 w′ L|| = ||∇S2 w′ L||. Therefore, with: ||∇S1 w′ L|| · |cos(∇S1 w′ L, v)| > ||∇S2 w′ L|| · |cos(∇S2 w′ L, v)| (10) it can be inferred that |∇S1 m∗L| > |∇S2 m∗L| which indicate that S1 has larger magnitude updates over S2 while having smaller directional alteration than that of S2. Our conclu- sion generally holds in practice, as evidenced by Figure 2 (c). Consequently, we have effectively shown how DoRA can be utilized to adjust the learning pattern, diverging from that of LoRA and aligning more closely with the pattern of FT. 4.3. Reduction of Training Overhead In Eq. (1), the gradients of W′ and ∆W are the same. How- ever, with DoRA, which redirects the low-rank adaptation towards the directional component, the gradient of the low- rank updates differs from that ofW′, as illustrated in Eq. (6). This divergence necessitates extra memory during backprop- agation. To address this, we suggest treating||V + ∆V ||c in Eq. (5) as a constant, thereby detaching it from the gradient graph. This means that while ||V + ∆V ||c dynamically reflects the updates of ∆V , it won’t receive any gradient during backpropagation. With this modification, the gra- dient w.r.t m remains unchanged, and ∇V ′L is redefined as: ∇V ′L = m C ∇W′L where C = ||V ′||c (11) This approach reduces the gradient graph memory consump- tion drastically without a noticeable difference in accuracy. We conduct an ablation study to evaluate the impact of the proposed modification on fine-tuning LLaMA-7B and VL- BART. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains un- changed for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. For a comprehensive comparison of training memory usage and accuracy differences, please see Table 7 in the appendix. Consequently, all subsequent experiments with DoRA incorporate this adjustment. 5. Experiments We conduct a variety of experiments to showcase the effi- cacy of DoRA on various tasks including language, image, and video domains. Firstly, we evaluate DoRA against sev- eral Parameter-Efficient Fine-Tuning (PEFT) methods by fine-tuning LLaMA-7B/13B, LLaMA2-7B, and LLaMA3- 8B on commonsense reasoning tasks. Subsequently, we extend from single modality to multimodality. We compare DoRA with LoRA across multi-task image/video-text under- standing tasks using VL-BART and visual instruction tuning with LLaV A-1.5-7B. Following this, we explore the com- patibility of DoRA with LoRA and VeRA (Kopiczko et al., 2024) for instruction-tuning on LLaMA-7B and LLaMA2- 7B. Furthermore, we perform a series of ablation studies to illustrate that DoRA surpasses LoRA in performance, irre- spective of the number of fine-tuning training samples and rank variations. Lastly, We analyze the tuning granularity of DoRA, and show that DoRA can achieve better accuracy than LoRA with fewer trainable parameters by selectively updating only the directional components of certain mod- ules. 5.1. Commonsense Reasoning We evaluate DoRA against LoRA and several baseline meth- ods which include Prompt learning (Prefix) (Li & Liang, 2021), Series adapter (Series) (Houlsby et al., 2019), and Parallel adapter (Parallel) (He et al., 2021) on LLaMA- 7B/13B (Touvron et al., 2023) for commonsense reasoning tasks. We also include ChatGPT’s accuracy obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Ope- nAI, 2023; Wei et al., 2022). The commonsense reasoning tasks comprise 8 sub-tasks, each with a predefined training and testing set. We follow the setting of (Hu et al., 2023) and amalgamate the training datasets from all 8 tasks to create the final training dataset and conduct evaluations on the individual testing dataset for each task. To ensure a fair comparison, we initially fine-tuned models with DoRA following the LoRA config- uration, maintaining the same rank while adjusting only the learning rate. The marginal increase of 0.01% in the number of trainable parameters for DoRA over LoRA, as detailed in Table 1, arises from the inclusion of learnable magnitude components (parameter of size 1 × k). Then, we further halve the rank used in DoRA compared to LoRA 5DoRA: Weight-Decomposed Low-Rank Adaptation Table 1.Accuracy comparison of LLaMA 7B/13B, LLaMA2 7B, and LLaMA3 8B with various PEFT methods on eight commonsense reasoning datasets. Results of all the baseline methods on LLaMA 7B/13B are taken from (Hu et al., 2023). Results of LoRA on LLaMA2 7B and LLaMA3 8B are obtained using the hyperparameters described in (Hu et al., 2023). DoRA†: the adjusted version of DoRA with the rank halved. Model PEFT Method # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B Prefix 0.11 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 Series 0.99 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 Parallel 3.54 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.2 LoRA 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 DoRA† (Ours) 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 DoRA (Ours) 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 LLaMA-13B Prefix 0.03 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 Series 0.80 71.8 83 79.2 88.1 82.4 82.5 67.3 81.8 79.5 Parallel 2.89 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.4 LoRA 0.67 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5 DoRA† (Ours) 0.35 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8 DoRA (Ours) 0.68 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LLaMA2-7B LoRA 0.83 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6 DoRA† (Ours) 0.43 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5 DoRA (Ours) 0.84 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 LLaMA3-8B LoRA 0.70 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8 DoRA† (Ours) 0.35 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0 DoRA (Ours) 0.71 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 and denote this adjusted configuration as DoRA†. See Table 8 for details on the hyperparameters used. Table 1 demonstrates that DoRA consistently surpasses all baseline methods across both LLaMA-7B/13B, LLaMA2- 7B and LLaMA3-8B. Notably, in the LLaMA-7B model, where LoRA exceeds the performance of other baselines, DoRA further enhances accuracy by 3.7%, outstripping ChatGPT’s accuracy levels. Conversely, for LLaMA-13B, where LoRA’s effectiveness is inferior to the Parallel adapter, DoRA achieves superior accuracy over LoRA by 1% and comparable accuracy to the Parallel adapter, with only a quarter of the trainable parameters required by the Parallel adapter and without adding any extra inference overhead as the Parallel adapter. Additionally, DoRA consistently surpasses LoRA on both LLaMA2-7B and LLaMA3-8B by 2.1% and 4.4%, respectively. Furthermore, DoRA† exceeds LoRA’s performance on LLaMA-7B by 2.8%, on LLaMA- 13B by 1%, on LLaMA2-7B by 2.9%, and on LLaMA3-8B by 4.2%, despite having only half as many trainable parame- ters as LoRA. This outcome suggests that the integration of DoRA enhances the learning capability of LoRA, thereby reducing the need for a higher rank to surpass LoRA in terms of accuracy. Additionally, in previous sections, we hypothesize that a negative correlation between the magnitude update and di- rectional update is more optimal than a positive correlation. This is because pre-trained weights already contain sub- stantial knowledge suitable for downstream tasks, and a larger magnitude or direction alteration alone is sufficient Figure 3.Magnitude (a) and direction (b) difference of LoRA/DoRA and the pre-trained weight of the query matrices across different layers. for downstream adaptation. To further validate our hypoth- esis, we used LLaMA2-7B fine-tuned with DoRA/LoRA on commonsense reasoning datasets as a case study. We visualized the magnitude (∆M) and directional difference (∆D) between the DoRA/LoRA weights and the pre-trained model weights across different modules and layers. In Fig- ure 3 (a) and (b), we observe that the DoRA fine-tuned weights show less deviation from the pre-trained weights in both magnitude and direction, while the differences for the LoRA fine-tuned weights are significantly larger. Coupled with the experimental results that DoRA significantly out- performs LoRA, we can conclude that our earlier hypothesis is valid: a robust foundation model does not require signif- icant alterations for effective downstream adaptation and having the ability to perform more fine-grained magnitude and directional update explains the superiority of DoRA over LoRA. We leave the visualization of the value and key 6DoRA: Weight-Decomposed Low-Rank Adaptation weight matrices in the appendix. 5.2. Image/Video-Text Understanding Table 2.The multi-task evaluation results on VQA, GQA, NVLR2 and COCO Caption with the VL-BART backbone. Method #Params (%) VQAv2 GQA NVLR2 COCO Cap Avg. FT 100 66.9 56.7 73.7 112.0 77.3 LoRA 5.93 65.2 53.6 71.9 115.3 76.5 DoRA (Ours) 5.96 65.8 54.7 73.1 115.9 77.4 Table 3.The multi-task evaluation results on TVQA, How2QA, TVC, and YC2C with the VL-BART backbone. Method #Params (%) TVQA How2QA TVC YC2C Avg. FT 100 76.3 73.9 45.7 154 87.5 LoRA 5.17 75.5 72.9 44.6 140.9 83.5 DoRA (Ours) 5.19 76.3 74.1 45.8 145.4 85.4 Having shown that DoRA can consistently achieve bet- ter accuracy on fine-tuning LLM, we would like to see if DoRA can remain competitive on multi-modality fine- tuning tasks. We compare DoRA with LoRA and full fine- tuning on VL-BART which comprises a vision encoder (CLIP-ResNet101 (Radford et al., 2021)) and an encoder- decoder language model ( BARTBase (Lewis et al., 2020)) across four different image-text tasks: VQAv2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for visual ques- tion answering, NLVR2 (Suhr et al., 2019) for visual reason- ing, and MSCOCO (Chen et al., 2015) for image captioning, and four different video-text tasks from the V ALUE (Li et al., 2021) Benchmark: TVQA (Lei et al., 2018) and How2QA (Li et al., 2020) for video question answering, TVC (Lei et al., 2020) and YC2C (Zhou et al., 2018) for video captioning. We follow the same framework as (Sung et al., 2022) and fine-tuned VL-BART within a multi-task framework for both image/video-text tasks. We adopt the same setup as that of LoRA outlined in (Sung et al., 2022) when ap- plying DoRA. See Table 9 for the complete hyperparam- eters. The result of LoRA and FT for both image/video- text tasks is directly quoted from (Sung et al., 2022). We can see that DoRA uniformly surpasses LoRA in accuracy while maintaining a similar count of trainable parameters in both Table 2 and Table 3. In particular, DoRA exceeds LoRA’s performance by nearly 1% in image-text under- standing tasks, reaching the accuracy level of FT. Moreover, DoRA achieves roughly 2% higher accuracy than LoRA in video-text understanding tasks. 5.3. Visual Instruction Tuning We further scale up the model size and compare DoRA to LoRA and FT on the visual instruction tuning tasks with Table 4.Visual instruction tuning evaluation results for LLaV A- 1.5-7B on a wide range of seven vision-language tasks. We directly use checkpoints from (Liu et al., 2023a) to reproduce their results. Method # Params(%) Avg. FT 100 66.5 LoRA 4.61 66.9 DoRA (Ours) 4.63 67.6 LLaV A-1.5-7B (Liu et al., 2023a) which is composed of a language model, Vicuna-1.5-7B (Peng et al., 2023), and a vision encoder, CLIP ViT-L/336px (Radford et al., 2021). The training datasets contain several datasets from VQA (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Schwenk et al., 2022), OCR (Mishra et al., 2019; Sidorov et al., 2020), region-level VQA (Kazemzadeh et al., 2014; Krishna et al., 2017; Mao et al., 2016), visual conversation (Liu et al., 2023a), and language conversation data. We follow the setting of (Liu et al., 2023a) to filter the training data and construct the tunning prompt format. For a fair comparison, DoRA follows the same configuration as the LoRA configuration provided by (Liu et al., 2023a). The fine-tuned models are then evaluated on seven vision- language benchmarks: VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018) SQA (Lu et al., 2022), VQAT (Singh et al., 2019), POPE (Li et al., 2023), and MMBench (Liu et al., 2023c). From Table 4, we can observe that the average accuracy of LoRA already surpasses FT, which could imply that FT might be experiencing issues with overfitting. Given that DoRA is designed to enhance LoRA’s performance to more closely resemble that of FT, in scenarios where FT is infe- rior to LoRA, DoRA’s improvement over LoRA might not be as pronounced as observed in other experiments where FT usually outperforms LoRA. Nonetheless, DoRA still demonstrates superior performance over both LoRA and FT, with an average improvement of 0.7% over LoRA and 1.1% over FT. See Table 10 for the hyperparameters setting and Table 12 for the score of each evaluation benchmark. 5.4. Compatibility of DoRA with other LoRA variants Recall from Equation.(1) that ∆W can be adapted by differ- ent LoRA variants. With DoRA, the concept of incremental directional update ∆V introduced in Equation.(5) can like- wise be replaced with alternative LoRA variants. In this section, we select VeRA (Kopiczko et al., 2024) as a case study to explore DoRA’s compatibility with other LoRA variants. VeRA suggests freezing a unique pair of random low-rank matrices to be shared across all layers, employ- ing only minimal layer-specific trainable scaling vectors to capture each layer’s incremental updates. This approach allows VeRA to reduce trainable parameters significantly 7DoRA: Weight-Decomposed Low-Rank Adaptation Table 5.Average scores on MT-Bench assigned by GPT-4 to the answers generated by fine-tuned LLaMA-7B/LLaMA2-7B. Model PEFT Method # Params (%) Score LLaMA-7B LoRA 2.31 5.1 DoRA (Ours) 2.33 5.5 VeRA 0.02 4.3 DV oRA (Ours) 0.04 5.0 LLaMA2-7B LoRA 2.31 5.7 DoRA (Ours) 2.33 6.0 VeRA 0.02 5.5 DV oRA (Ours) 0.04 6.0 by 10x compared to LoRA, with only a minimal impact on accuracy. We apply VeRA for the directional update in DoRA and name such combination DV oRA. We assess the effectiveness of both DV oRA and DoRA compared to VeRA and LoRA across LLaMA-7B and LLaMA2-7B, focusing on instruction tuning with the 10K subset of cleaned Alpaca dataset (Taori et al., 2023). We utilize the official imple- mentation of VeRA to obtain the results of VeRA and LoRA and fine-tune the model with DV oRA and DoRA using the identical training settings as VeRA and LoRA (see Table 11 in the appendix for more details). The performance of the fine-tuned models is then evaluated on the MT-Bench bench- mark (Zheng et al., 2023) by generating model responses to a pre-defined set of 80 multi-turn questions. These re- sponses are then evaluated by GPT-4, which reviews each answer and assigns a numerical score out of 10. Table 5 presents the average scores for DV oRA, DoRA, VeRA, and LoRA, demonstrating that our proposed method exhibits consistent improvements over VeRA and LoRA for both LLaMA-7B and LLaMA2-7B. This effectively show- cases the compatibility of DoRA with VeRA. In particular, DV oRA merges the advantageous qualities of DoRA and VeRA, attaining scores that are on par with or even sur- pass those of LoRA, yet with significantly fewer parame- ters. For example, DV oRA outperforms VeRA by 0.7/0.5 points and achieves the same level of accuracy as LoRA on LLaMA-7B and DoRA on LLaMA2-7B, respectively. Ad- ditionally, we present a selection of questions chosen from MT-Bench, accompanied by the responses from LLaMA2- 7B fine-tuned using DV oRA and VeRA in the appendix (Table 13 and 14) where we can observe that the answers given by DV oRA tend to be more precise and structural. Next, to further assess DoRA’s ability to remain competitive under varying amounts of training data, considering that in practical situations, access to extensive fine-tuning datasets is frequently limited. We compare DoRA to LoRA and DV oRA to VeRA for fine-tuning LLaMA2-7B/LLaMA-7B with a range of instruction-tuning sample sizes, specifically 1000, 4000, 7000, 10000, with 10000 being the setting of 1000 4000 7000 10000 Number of instruction tuning training samples 5.0 5.2 5.4 5.6 5.8 6.0Score LLaMA2-7B DoRA LoRA DV oRA VeRA Figure 4.Performance of fine-tuned LLaMA2-7B on MT-Bench using different numbers of Alpaca training samples. (Kopiczko et al., 2024). We visualize the average perfor- mance of each method on LLaMA2-7B in Figure 4, and on LLaMA-7B in Figure 9 in the appendix. The result shows that DoRA and DV oRA consistently outperform LoRA and VeRA across all training sample sizes. For instance, with 7000 training samples, DoRA and DV oRA surpass LoRA and VeRA by margins of 0.3 and 0.33, respectively. Even when the sample size is reduced to 1000, DoRA and DV oRA maintain their lead with advantages of 0.29 and 0.22 over LoRA and VeRA, respectively. This demonstrates that our methods persistently enhance performance over LoRA and VeRA, regardless of the training sample volume. 5.5. Robustness of DoRA towards different rank settings 4 8 16 32 64 rank r 40 45 50 55 60 65 70 75 80Avg. Accuracy (+ 37.2%) (+ 22.4%) LLaMA-7B DoRA LoRA Figure 5.Average accuracy of LoRA and DoRA for varying ranks for LLaMA-7B on the commonsense reasoning tasks. This section explores the impact of various rank configura- tions on DoRA and LoRA by adjustingr within the set {4, 8, 16, 32, 64} and assessing the performance of the fine-tuned LLaMA-7B on commonsense reasoning tasks as outlined in Sec 5.1. The average accuracies of LoRA and DoRA across different ranks are depicted in Figure 5, with detailed num- bers presented in Table 15. From Figure 5, we can observe that DoRA consistently surpasses LoRA across all rank con- figurations. Notably, the performance gap widens for ranks 8DoRA: Weight-Decomposed Low-Rank Adaptation Table 6.Accuracy comparison of LLaMA 7B/13B with two differ- ent tuning granularity of DoRA. Columns m and V designate the modules with tunable magnitude and directional components, re- spectively. Each module is represented by its first letter as follows: (Q)uery, (K)ey, (V)alue, (O)utput, (G)ate, (U)p, (D)own. Model PEFT Method # Params (%) m V Avg. LLaMA-7B LoRA 0.83 - - 74.7 DoRA (Ours) 0.84 QKVUD QKVUD 78.1 DoRA (Ours) 0.39 QKVOGUD QKV 77.5 LLaMA-13B LoRA 0.67 - - 80.5 DoRA (Ours) 0.68 QKVUD QKVUD 81.5 DoRA (Ours) 0.31 QKVOGUD QKV 81.3 below 8, where LoRA’s average accuracies drop to 40.74% for r = 8 and 39.49% for r = 4. In contrast, DoRA retains a notable accuracy of 77.96% for r = 8 and 61.89% for r = 4, demonstrating its resilience and consistently superior performance over LoRA regardless of the rank setting. 5.6. Tuning Granularity Analysis The visualization in Figure 2 indicates that significant changes in magnitude often result in relatively smaller di- rectional changes. Given this observation and the fact that directional updates account for most of the trainable param- eters, it prompts an investigation into whether it is possible to decrease the number of trainable parameters by updating only the magnitude components of specific modules while continuing to update both the magnitude and directional components for the remaining linear modules. Our findings indicate that, in contrast to the original con- figuration suggested for LoRA in (Hu et al., 2023), which requires updates to both the Multi-head Attention and MLP layers for optimal performance, DoRA can already achieve superior accuracy by updating only the directional and mag- nitude components of the multi-head layers and the magni- tude of the MLP layers. Specifically, as shown in Table 6, by updating the directional and magnitude components of the QKV modules and only the magnitude of the rest of the layers, DoRA surpasses LoRA by 2.8% on LLaMA-7B and 0.8% on LLaMA-13B, while utilizing only less than half of the trainable parameters compared to LoRA. 6. Broader Impacts 6.1. QDoRA: Enhancements to QLoRA While finetuning LLMs with PEFT significantly reduces training memory overhead, a considerable amount of GPU memory is still required to initially load the model weights onto the GPUs. To further decrease the memory demands of finetuning, QLoRA (Dettmers et al., 2023) suggests quan- tizing the pretrained model to 4-bit and finetuning LoRA on top of the frozen low-bit backbone. With our porposed 0.0 0.1 0.2 0.3 0.4 0.5 Exact match score (Eval size: 500) Zero-shot Five-shot Full Finetune QLoRA QDoRA 0.23 0.27 0.51 0.32 0.56 0.07 0.08 0.26 0.12 0.31 100k Orca-Math finetuning results LLaMA2-7B LLaMA3-8B Figure 6.Accuracy comparison of LLaMA2-7B/LLaMA3-8B with QDoRA, QLoRA and FT on Orca-Math (Mitra et al., 2024). DoRA, which narrows the gap between LoRA and FT, it is natural to also explore whether DoRA can enhance the accuracy of LoRA within the QLoRA framework. Recently, (Kerem Turgutlu, 2024) launch a project that substitutes the LoRA component in QLoRA with DoRA, dubbing it QDoRA, and incorporate the training pipeline with Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) to enable model splitting and parallel training across multiple GPUs. They conducted experiments on fine-tuning LLaMA2- 7B/LLaMA3-8B using the Orca-Math(Mitra et al., 2024) dataset with QDoRA, QLoRA, and FT. The training set included 100k samples, with 500 reserved for evaluation using the exact match score as the metric. In addition to the fine-tuned models, they also reported results from zero-shot, few-shot, and FT with post-training quantization (PTQ), where the FT model is quantized to the BnB NF4 format after training. According to Figure 6, QDoRA not only significantly surpasses QLoRA by 0.19/0.23 on LLaMA2- 7B and LLaMA3-8B, but it also slightly outperforms FT on both models, while using considerably less memory. This in- dicates that QDoRA can effectively combines the parameter efficiency of QLoRA with the more granular optimization of full finetuning. These initial findings suggest that QDoRA holds considerable promise and could hugely benefit the opensoure community by substantially lowering the GPU memory requirements for fine-tuning large language mod- els. 6.2. Text-to-Image Generation Recently, as diffusion models have expanded in size, LoRA has become a popular method for efficiently fine-tuning large stable diffusion models. In this section, we aim to explore whether DoRA’s advantages over LoRA extend to the task of text-to-image generation. We follow the training pipeline of DreamBooth (Ruiz et al., 2023) for fine-tuning 9DoRA: Weight-Decomposed Low-Rank Adaptation SDXL (Podell et al., 2023), utilizing the advanced train- ing scripts developed by HuggingFace. The hyperparameter settings for LoRA and DoRA are kept the same, and we fine- tune the model using two challenging datasets: 3D icons and Lego sets. The sample seeds for generating the images are kept the same for LoRA and DoRA for fair comparison. The generated images are shown in Figure 10 and 11 in the appendix. The results indicate that DoRA achieves signif- icantly better personalization than LoRA when using the same training settings, and more accurately reflects the train- ing targets. For example, in Figure 10, the first sub-figure of DoRA’s output features a unique round square around the image, which is a feature common to all the training targets. In contrast, this feature is absent in all the LoRA outputs. A similar observation could be found with the Lego training targets, where only the DoRA outputs consistently incorporate the Lego logo in the generated images. 7. Conclusion In this work, we first conduct a novel weight decomposi- tion analysis to reveal the distinct learning patterns between LoRA and FT. Building on these insights, we introduce DoRA, a fine-tuning method that is compatible with LoRA and its variants and exhibits a closer resemblance to FT’s learning behavior. DoRA consistently outperforms LoRA across various fine-tuning tasks and model architectures. Specifically, DoRA improves upon LoRA in commonsense reasoning and visual instruction tuning tasks. Furthermore, DoRA also shows compatibility with VeRA on the Alpaca instruction tuning task. Moreover, DoRA can be considered as a costless alternative to LoRA, as its decomposed mag- nitude and direction components can be merged back into the pre-trained weight after the training, ensuring that there is no extra inference overhead. For future work, we wish to explore the generalizability of DoRA in domains beyond language and vision, particularly in the field of audio. Acknowledgements We extend our gratitude to Benjamin Bossan, Younes Belkada, and Sourab Mangrulkar from Hugging Face for their assistance in integrating DoRA into the PEFT pack- age, thus making our work more accessible to the broader public. We thank Kerem Turgutlu, Jonathan Whitaker, and Jeremy Howard from Answer.AI for their work on the imple- mentation and experiments of QDoRA/FSDP, which makes fine-tuning of large language models with DoRA on con- sumer GPUs a lot more feasible. We also thank Sebastian Raschka for his well-written tutorial on DoRA which offers a thorough overview of the background knowledge neces- sary to comprehend DoRA. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Chen, X., Fang, H., Lin, T.-Y ., Vedantam, R., Gupta, S., Doll´ar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 10088–10115. Cur- ran Associates, Inc., 2023. Goyal, Y ., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904–6913, 2017. Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., Luo, J., and Bigham, J. P. Vizwiz grand challenge: Answering visual questions from blind people. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 3608–3617, 2018. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient trans- fer learning. In International Conference on Learning Representations, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on 10DoRA: Weight-Decomposed Low-Rank Adaptation imagenet classification. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 1026–1034, 2015. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pp. 2790–2799, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Hu, Z., Wang, L., Lan, Y ., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large lan- guage models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700– 6709, 2019. Hyeon-Woo, N., Ye-Bin, M., and Oh, T.-H. Fedpara: Low- rank hadamard product for communication-efficient fed- erated learning. In International Conference on Learning Representations, 2022. Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 565–576, 2021. Kazemzadeh, S., Ordonez, V ., Matten, M., and Berg, T. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 787–798, 2014. Kerem Turgutlu, Jonathan Whitaker, J. H. Ef- ficient finetuning of llama 3 with fsdp qdora. https://www.answer.ai/posts/ 2024-04-26-fsdp-qdora-llama3.html , 2024. Kopiczko, D. J., Blankevoort, T., and Asano, Y . M. Vera: Vector-based random matrix adaptation. In International Conference on Learning Representations, 2024. Krishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y ., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In- ternational journal of computer vision, pp. 32–73, 2017. Lei, J., Yu, L., Bansal, M., and Berg, T. TVQA: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1369–1379, 2018. Lei, J., Yu, L., Berg, T. L., and Bansal, M. Tvr: A large-scale dataset for video-subtitle moment retrieval. In European Conference on Computer Vision, pp. 447–463, 2020. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pp. 7871–7880, 2020. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Confer- ence on Machine Learning, pp. 12888–12900, 2022. Li, L., Chen, Y .-C., Cheng, Y ., Gan, Z., Yu, L., and Liu, J. HERO: Hierarchical encoder for Video+Language omni- representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2046–2065, 2020. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y .-C., Pillai, R., Cheng, Y ., Zhou, L., Wang, X. E., Wang, W. Y ., et al. Value: A multi-task benchmark for video-and-language understand- ing evaluation. In Thirty-fifth Conference on Neural In- formation Processing Systems Datasets and Benchmarks Track (Round 1), 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582– 4597, 2021. Li, Y ., Du, Y ., Zhou, K., Wang, J., Zhao, X., and Wen, J.-R. Evaluating object hallucination in large vision-language 11DoRA: Weight-Decomposed Low-Rank Adaptation models. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing , pp. 292–305, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y . J. Visual instruction tun- ing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Liu, W., Qiu, Z., Feng, Y ., Xiu, Y ., Xue, Y ., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, Y ., Duan, H., Zhang, Y ., Li, B., Zhang, S., Zhao, W., Yuan, Y ., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Infor- mation Processing Systems, pp. 2507–2521, 2022. mahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural Information Processing Systems, 2021. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and comprehension of unam- biguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11–20, 2016. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: A visual question answering benchmark requir- ing external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 3195–3204, 2019. Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. Ocr-vqa: Visual question answering by reading text in images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952, 2019. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. OpenAI. Welcome to the openai platform, 2023. URL https://platform.openai.com/docs/ introduction. Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruc- tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M ¨uller, J., Penna, J., and Rombach, R. Sdxl: Im- proving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining modular skills in multitask learning.arXiv preprint arXiv:2202.13914, 2022. Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., and Yang, D. Is chatgpt a general-purpose natural language processing task solver? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1339–1384, 2023. Qiu, Z., Liu, W., Feng, H., Xue, Y ., Feng, Y ., Liu, Z., Zhang, D., Weller, A., and Sch¨olkopf, B. Controlling text- to-image diffusion by orthogonal finetuning. In Thirty- seventh Conference on Neural Information Processing Systems, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763, 2021. Razdaibiedina, A., Mao, Y ., Khabsa, M., Lewis, M., Hou, R., Ba, J., and Almahairi, A. Residual prompt tuning: improving prompt tuning with residual reparameteriza- tion. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6740–6757, 2023. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint arXiv:2311.09578, 2023. Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image dif- fusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023. Salimans, T. and Kingma, D. P. Weight normalization: a simple reparameterization to accelerate training of deep neural networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pp. 901–909, 2016. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: A benchmark for visual ques- tion answering using world knowledge. In European Conference on Computer Vision, pp. 146–162, 2022. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehen- sion. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceed- ings, Part II 16, pp. 742–758, 2020. 12DoRA: Weight-Decomposed Low-Rank Adaptation Singh, A., Natarajan, V ., Shah, M., Jiang, Y ., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317–8326, 2019. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and Artzi, Y . A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6418–6428, 2019. Sung, Y .-L., Cho, J., and Bansal, M. Vl-adapter: Parameter- efficient transfer learning for vision-and-language tasks. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pp. 5227–5237, 2022. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stan- ford alpaca: An instruction-following llama model, 2023. URL https://github.com/tatsu-lab/ stanford_alpaca. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Wang, Y ., Wu, J., Dabral, T., Zhang, J., Brown, G., Lu, C.-T., Liu, F., Liang, Y ., Pang, B., Bendersky, M., et al. Non- intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. arXiv preprint arXiv:2310.12100, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. arXiv preprint arXiv:2309.14859, 2023. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y ., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Interna- tional Conference on Learning Representations, 2023. Zhao, Y ., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In Proceed- ings of the AAAI Conference on Artificial Intelligence , 2018. 13DoRA: Weight-Decomposed Low-Rank Adaptation A. Appendix A.1. Weight decomposition analysis on the value weight matrix In this section, we illustrate the changes in magnitude and direction within the value weight matrix for FT, LoRA, and DoRA across different training steps and layers, as shown in Figure 7. This reveals patterns similar to those seen in the query weight matrix depicted in Figure 2, indicating that DoRA is capable of displaying learning behaviors that closely mirror those of FT across various modules. 0.02 0.03 0.04 0.05 0.06 0.07 0.08 D (a) 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.055 M FT layer 1 layer 2 layer 3 layer 4 layer 5 layer 6 Inter step 1 Inter step 2 Inter step 3 Final step 0.150 0.175 0.200 0.225 0.250 0.275 0.300 0.325 D (b) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 LoRA Inter step 1 Inter step 2 Inter step 3 Final step 0.20 0.25 0.30 0.35 0.40 0.45 0.50 D (c) 0.4 0.6 0.8 1.0 DoRA Inter step 1 Inter step 2 Inter step 3 Final step Figure 7.Magnitude and Directional changes of FT (a), LoRA (b), and DoRA (c) of the V weight matrices across different layers and intermediate steps. A.2. Ablation study for the modification to reduce DoRA training cost Table 7 presents the GPU cost and the average accuracy of DoRA with and without the proposed modification for commonsense reasoning tasks and image-text understanding tasks. The results indicate that the modification leads to a training memory reduction of approximately 24.4% in fine-tuning LLaMA and 12.4% in VL-BART. Furthermore, the accuracy of DoRA with the modification remains unchanged for VL-BART and shows a negligible difference of only 0.2 compared to DoRA without the modification on LLaMA. Table 7.GPU cost and accuracy of DoRA with or without the modification on the commonsense reasoning tasks and image-text understanding tasks. Model PEFT Method Accumulation steps Batch Size GPU Memory Cost (GB) # Params (%) Avg. LLaMA-7B DoRA w/o modification 4 16 37.3 0.84 78.3 DoRA 28.2 (-24.4%) 0.84 78.1 VL-BART DoRA w/o modification - 300 23.4 5.96 77.3 DoRA 20.5 (-12.4%) 5.96 77.4 14DoRA: Weight-Decomposed Low-Rank Adaptation A.3. Hyperparameters Table 8.Hyperparameter configurations of DoRA for LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B on the commonsense reasoning tasks. Hyperparameters (DoRA) LLaMA-7B LLaMA-13B LLaMA2-7B LLaMA3-8B Rank r 16 32 16 32 16 32 16 32 α 32 64 32 64 32 64 32 64 Dropout 0.05 Optimizer AdamW LR 2e-4 1e-4 3e-4 2e-4 2e-4 2e-4 1e-4 1e-4 LR Scheduler Linear Batch size 16 Warmup Steps 100 Epochs 3 Where Q,K,V ,Up,Down Table 9.Hyperparameter configurations of DoRA for fine-tuning VL-Bart on image/video-text tasks. Hyperparameters (DoRA) image-text video-text Rank r 128 α 128 Dropout 0.0 Optimizer AdamW LR 1e-3 3e-4 LR Scheduler Linear Batch size 300 40 Warmup ratio 0.1 Epochs 20 7 Where Q,K Table 10.Hyperparameter configurations of DoRA and LoRA for fine-tuning LLaV A-1.5-7B with visual instruction tuning datasets. Hyperparameters DoRA LoRA Rank r 128 α 256 Dropout 0.05 Optimizer AdamW LR 2e-4 LR Scheduler Cosine decay Batch size 16 Warmup ratio 0.03 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 15DoRA: Weight-Decomposed Low-Rank Adaptation Table 11.Hyperparameter configurations of DoRA and DV oRA for fine-tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset. Hyperparameters (DoRA) LLaMA-7B LLaMA2-7B Rank r 64 Dropout 0.0 Optimizer AdamW LR 4e-4 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate Hyperparameters (DVoRA) LLaMA-7B LLaMA2-7B Rank r 1024 Dropout 0.0 Optimizer AdamW LR 4e-3 LR Scheduler Cosine Batch size 4 Accumulation Steps 4 Warmup ratio 0.1 Epochs 1 Where Q,K,V ,O,Up,Down,Gate 16DoRA: Weight-Decomposed Low-Rank Adaptation A.4. Magnitude and Direction difference between DoRA/LoRA fine-tuned weight and the pre-triained weight of LLaMA2-7B for the commonsesne reasoning tasks Figure 8 depicts the magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. The figure shows that the DoRA fine-tuned weights deviate less from the pre-trained weights in both magnitude and direction, supporting our hypothesis that a robust foundation model does not need substantial changes for effective downstream adaptation. (a) (b) (d) (c) (f) (e) Figure 8.Magnitude and direction differences in the weights of the query, key, and value matrices between LoRA/DoRA fine-tuned models and the pre-trained model across various layers of LLaMA2-7B for the commonsense reasoning tasks. 17DoRA: Weight-Decomposed Low-Rank Adaptation A.5. Visual instruction tuning evaluation result Table 12 presents the evaluation result of LLaV A-1.5-7B fine-tuned using DoRA, LoRA, and FT with visual instruction tuning data. The evaluation is conducted on an array of seven different vision-language benchmarks. From the table, we can see that DoRA consistently outmatch FT, and can achieve an average accuracy better than LoRA. Table 12.Visual instruction tuning evaluation result of DoRA, LoRA, and FT for LLaV A-1.5-7B on a wide range of 7 vision-language tasks. Method # Params(%) VQAv2 GQA VisWiz SQA VQA T POPE MMBench Avg. FT 100 78.5 61.9 50.0 66.8 58.2 85.9 64.3 66.5 LoRA 4.61 79.1 62.9 47.8 68.4 58.2 86.4 66.1 66.9 DoRA (Ours) 4.63 78.6 62.9 52.2 69.9 57 87.2 66.1 67.6 18DoRA: Weight-Decomposed Low-Rank Adaptation A.6. Instruction-tuning evaluation examples We sample two questions from the MT-Bench question set and present the respective responses from DV oRA and VeRA, including the scores and comments provided by GPT-4. From these examples, we can observe that responses from DV oRA are generally more accurate and well-organized compared to those from VeRA. For instance, in response to Question 24, DV oRA leverages a well-known superhero story that is familiar to children, whereas VeRA’s response begins with an uncertain tone, potentially making it less engaging than DV oRA’s reply. Table 13.Question 24 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 24 As a superhero, how would you explain your origin story to a curious child? VeRA Well, I’m not really sure where I come from. All I know is that one day, I woke up with special powers and abilities that allowed me to help people. I decided to use my powers to fight crime and protect the innocent, and that’s how I became a superhero DV oRA As a superhero, I have the ability to use my super powers to protect people and fight against evil. My origin story begins when I was just a regular kid like you, but one day, I was bitten by a radioactive spider, and it gave me incredible powers. I could climb walls, shoot webs, and see in the dark. I decided to use my powers to help people, and that’s how I became your friendly neighborhood Spider-Man. GPT-4 Evaluation Score: 5 7. Assistant 1’s response starts with an uncertain tone, which might not be very engaging for a child. The explanation is quite vague and doesn’t offer a clear or imaginative origin story that could captivate a child’s interest. It does, however, maintain the theme of using powers for good, which is a positive message. Assistant 2’s response is more detailed and uses a familiar superhero origin story (Spider-Man’s) that many children can relate to and understand. It’s engaging and provides a clear narrative of how an ordinary person can become a superhero. However, it directly references Spider-Man, which is not an original origin story and could be misleading if the assistant is supposed to be a unique superhero. Despite this, the response is more likely to resonate with a child due to its clarity and reliability. Both responses could be improved by providing an original and imaginative origin story that doesn’t rely on existing superhero narratives. They should also ensure that the story is simple enough for a child to understand while being engaging and inspiring. 19DoRA: Weight-Decomposed Low-Rank Adaptation Table 14.Question 33 from MT-Bench alongside with response from LLaMA2-7B fine-tuned with DV oRA and VeRA, and the evaluation provided by GPT-4. Question 33 Why might someone choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app? VeRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Some people may prefer the physical act of navigating with a paper map, as it can be more tactile and interactive. Others may prefer to have a hard copy of the map in case the GPS or app fails or the battery dies. Additionally, some people may not have access to a GPS device or smartphone, or may not want to rely on technology for navigation. Finally, some people may simply prefer the security and peace of mind that comes with knowing exactly where they are and how to get to their destination DV oRA There are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. Here are some of them: 1. Navigation accuracy: GPS devices and smartphone apps rely on satellite signals to pinpoint your location, but this technology is not foolproof. GPS signals can be blocked by tall buildings, tunnels, or other obstacles, resulting in inaccurate or missing information. Paper maps, on the other hand, are based on accurate, up-to-date information that is not subject to these limitations. 2. Cost: GPS devices and smartphone apps can be expensive to purchase and maintain. Paper maps are free to use and do not require an ongoing subscription or data plan. 3. Personal preference: Some people prefer the tactile experience of using a paper map, which can be easier to read and navigate than a screen. Others enjoy the challenge of navigating without technology, which can help them learn more about the area they are visiting. 4. Battery life: GPS devices and smartphone apps can drain batteries quickly, especially when used for navigation. Paper maps do not require power, so they can be used for extended periods without recharging. 5. Offline use: GPS devices and smartphone apps rely on an internet connection to function properly. If there is no signal, the device will not be able to provide accurate information. Paper maps, on the other hand, can be used offline, making them ideal for use in remote or unpopulated areas. In summary, there are several reasons why someone might choose to use a paper map or ask for directions instead of relying on a GPS device or smartphone app. These include navigation accuracy, cost, personal preference, battery life, and offline use. GPT-4 Evaluation Score: 6 8. Assistant 1’s response is somewhat brief and lacks specific details that could be helpful to the user. It mentions the tactile nature of paper maps, the possibility of technology failure, lack of access to technology, and a general preference for security and peace of mind. However, it does not delve into the practical considerations such as GPS signal limitations, cost, battery life, or offline use, which are important factors in deciding between paper maps and digital navigation tools. Assistant 2’s response is more comprehensive and provides a structured list of reasons, including navigation accuracy, cost, personal preference, battery life, and offline use. Each point is explained clearly, giving the user a better understanding of the potential advantages of paper maps over GPS devices or smartphone apps. The response is relevant, accurate, and detailed, making it more helpful to the user. Both assistants provided relevant and accurate information, but Assistant 2’s answer is more detailed and structured, which is why it receives a higher score. The scores reflect the level of detail and the usefulness of the information provided to the user, with Assistant 2 offering a more complete answer to the question 20DoRA: Weight-Decomposed Low-Rank Adaptation A.7. Robustness towards different numbers of instruction tunning examples Figure 9 depicts the score of DoRA, LoRA, DV oRA, and VeRA for fine-tuning LLaMA-7B with varying instruction-tuning sample sizes, including 1000, 4000, 7000, and 10000. From the figure, we can observe that similar to that of the result on LLaMA2-7B, both DoRA and DV oRA consistently outperform LoRA and VeRA across the entire range of training sample sizes. 1000 4000 7000 10000 Number of instruction tuning training samples 3.0 3.5 4.0 4.5 5.0 5.5Score LLaMA-7B DoRA LoRA DV oRA VeRA Figure 9.Performance of LLaMA-7B fine-tuned with LoRA, DoRA, VeRA, and DV oRA on MT-Bench using different numbers of training samples from Alpaca dataset (Taori et al., 2023). 21DoRA: Weight-Decomposed Low-Rank Adaptation A.8. Robustness of DoRA towards different rank Table 15 shows a comparison of the average accuracy between LoRA and DoRA method across various rank settings for commonsense reasoning tasks. DoRA consistently outperforms LoRA at all rank settings, with the performance gap widening as the rank decreases. This suggests that our method effectively enhances the learning capacity of LoRA, enabling it to achieve better accuracy with fewer trainable parameters. Table 15.Accuracy comparison of LoRA and DoRA with varying ranks for LLaMA-7B on the commonsense reasoning tasks. PEFT Method rank r # Params (%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. LoRA 4 0.10 2.3 46.1 18.3 19.7 55.2 65.4 51.9 57 39.5 8 0.21 31.3 57.0 44.0 11.8 43.3 45.7 39.2 53.8 40.7 16 0.42 69.9 77.8 75.1 72.1 55.8 77.1 62.2 78.0 70.9 32 0.83 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7 64 1.64 66.7 79.1 75.7 17.6 78.8 73.3 59.6 75.2 65.8 DoRA (Ours) 4 0.11 51.3 42.2 77.8 25.4 78.8 78.7 62.5 78.6 61.9 8 0.22 69.9 81.8 79.7 85.2 80.1 81.5 65.7 79.8 77.9 16 0.43 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5 32 0.84 69.7 83.4 78.6 87.2 81.0 81.9 66.2 79.2 78.4 64 1.65 69.9 81.4 79.1 40.7 80.0 80.9 65.5 79.4 72.1 22DoRA: Weight-Decomposed Low-Rank Adaptation A.9. Text-to-Image Generation Figures 10 and 11 show the images produced by SDXL fine-tuned with DoRAand LoRA via DreamBooth (Ruiz et al., 2023) personalization techniques on two distinct training sets: 3D Icon 1 and Lego2. The results reveal that DoRA can achieve considerably better personalization than LoRA with identical training configurations, more closely matching the training target. Figure 10.Images generated with SDXL finetuned with LoRA and DoRA on the 3D Icon training sets. Figure 11.Images generated with SDXL finetuned with LoRA and DoRA on the Lego training sets. 1https://huggingface.co/datasets/linoyts/3d_icon 2https://huggingface.co/datasets/merve/lego_sets_latest 23",
      "meta_data": {
        "arxiv_id": "2402.09353v6",
        "authors": [
          "Shih-Yang Liu",
          "Chien-Yi Wang",
          "Hongxu Yin",
          "Pavlo Molchanov",
          "Yu-Chiang Frank Wang",
          "Kwang-Ting Cheng",
          "Min-Hung Chen"
        ],
        "published_date": "2024-02-14T17:59:34Z",
        "pdf_url": "https://arxiv.org/pdf/2402.09353v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel Parameter-Efficient Fine-Tuning (PEFT) method that aims to bridge the accuracy gap between LoRA and Full Fine-Tuning (FT) while avoiding additional inference costs. DoRA achieves this by performing a weight decomposition analysis to understand the learning patterns of FT and LoRA, and then proposes to fine-tune pre-trained weights by separating them into magnitude and directional components, with LoRA specifically used for efficient directional updates. DoRA consistently outperforms LoRA across various tasks and model architectures without additional inference latency.",
        "methodology": "The core methodology involves a novel weight decomposition analysis, which reparameterizes model weights into magnitude and directional components to analyze the learning patterns of FT and LoRA. Based on this, DoRA decomposes the pre-trained weight matrix (W0) into a trainable magnitude vector (m) and a directional matrix (V). The directional component is then updated using LoRA, expressed as W' = m * (W0 + BA) / ||W0 + BA||c, where BA represents the low-rank update for the directional component. A gradient analysis shows that this decomposition enhances learning stability by aligning the gradient's covariance matrix with the identity matrix. To reduce training overhead, the norm term ||V + ∆V||c is treated as a constant during backpropagation.",
        "experimental_setup": "DoRA was evaluated across various tasks and models. For commonsense reasoning, LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B were fine-tuned on 8 sub-tasks and compared against LoRA, Prompt learning, Series adapter, Parallel adapter, and ChatGPT. For multi-task image/video-text understanding, VL-BART was fine-tuned on 4 image-text tasks (VQAv2, GQA, NLVR2, MSCOCO Caption) and 4 video-text tasks (TVQA, How2QA, TVC, YC2C). Visual instruction tuning was performed on LLaVA-1.5-7B across 7 vision-language benchmarks (VQAv2, GQA, VisWiz, SQA, VQAT, POPE, MMBench). Compatibility with other LoRA variants was shown by combining DoRA with VeRA (DVora) for instruction tuning on LLaMA-7B/LLaMA2-7B using the Alpaca dataset, evaluated on MT-Bench by GPT-4. Ablation studies explored performance with varying training sample sizes and LoRA ranks. QDoRA (DoRA applied to QLoRA) was evaluated on LLaMA2-7B/LLaMA3-8B with the Orca-Math dataset. Text-to-image generation was demonstrated by fine-tuning SDXL with DreamBooth using 3D icons and Lego sets.",
        "limitations": "The paper primarily highlights the limitations of existing PEFT methods like LoRA and Full Fine-Tuning, which DoRA aims to address. For DoRA itself, specific limitations or weaknesses are not explicitly mentioned. The paper notes an optimization strategy for DoRA to reduce memory (treating ||V + ∆V||c as a constant during backpropagation), stating it has 'no noticeable difference in accuracy,' implying no significant performance compromise due to this simplification.",
        "future_research_directions": "Future work will explore the generalizability of DoRA in domains beyond language and vision, specifically in the field of audio."
      }
    },
    {
      "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
      "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .",
      "full_text": "Published as a conference paper at ICLR 2023 ADALORA: A DAPTIVE BUDGET ALLOCATION FOR PARAMETER -EFFICIENT FINE -TUNING Qingru Zhang†∗, Minshuo Chen‡, Alexander Bukharin†, Nikos Karampatziakis⋄, Pengcheng He⋄, Yu Cheng⋄, Weizhu Chen⋄ and Tuo Zhao† †Georgia Institute of Technology ‡Princeton University ⋄Microsoft Azure AI {qingru.zhang,abukharin3,tourzhao}@gatech.edu mc0750@princeton.edu {nikosk,penhe,yu.cheng,wzchen}@microsoft.com ABSTRACT Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine- tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/ QingruZhang/AdaLoRA. 1 I NTRODUCTION Pre-trained language models (PLMs) have manifested superior performance in various natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; He et al., 2021b; Radford et al., 2019; Brown et al., 2020). The most common way to adapt pre-trained models to down-stream tasks is to fine-tune all the parameters (full fine-tuning, Qiu et al. (2020); Raffel et al. (2020)). However, pre-trained models typically incurs large memory footprint. For example, BERT model (Devlin et al., 2019) consists up to 300 million parameters; T5 (Raffel et al., 2020) comprises up to 11 billion parameters and GPT-3 (Brown et al., 2020) contains up to 175 billion parameters. When building a NLP system upon these pre-trained models, we usually handle multiple tasks that arrive simultaneously (Radford et al., 2019). Given a large number of down-stream tasks, full fine-tuning requires that each task maintains a separated copy of large models. The resulting memory consumption is prohibitively expensive. To address this issue, researchers have proposed two main lines of research to reduce the fine-tuning parameters, while maintaining or even improving the performance of PLMs. Specifically, one line of research focuses on adding small neural modules to PLMs and fine-tune only these modules for each task – the base model is kept frozen and shared across tasks. In this way, only a small number of task-specific parameters are introduced and updated, greatly enhancing the practicality of large models. For example, adapter tuning (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2020; ∗Work was done during Qingru Zhang’s internship at Microsoft Azure AI. 1 arXiv:2303.10512v2  [cs.CL]  20 Dec 2023Published as a conference paper at ICLR 2023 Wq Wk Wv Wo Wf1 Wf2 88.50 88.75 89.00 89.25 89.50 89.75 90.00 MNLI Matched Acc 88.58 88.98 89.36 89.28 89.91 89.99 (a) Selected weight matrix 1,2,3 4,5,6 7,8,9 10,11,12 78 80 82 84 86 88MNLI Matched Acc 77.87 85.82 88.15 88.6 (b) Selected layers Figure 1: Given the total trainable parameters as 0.28M, we apply LoRA only to selected weight matrices (left) or selected layers (right) of DeBERTaV3-base and compare the fine-tuning performance on MNLI-m. Figure 1a: we only fine-tune a selected type of weight matrix of every transformer layer, including query/key/value projection (Wq, Wk, Wv), output projection (Wo) in the self-attention, and two weight matrices (Wf1 , Wf2 ) in two-layer FFNs. In Figure 1b, we apply LoRA to every weight matrix of the selected layers. He et al., 2022) inserts small neural modules called adapters between the layers of the base model. Prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) attach additional trainable prefix tokens to the input or hidden layers of the base model. These methods have shown to achieve comparable performance to full fine-tuning, while only updating less than 1% of the original model parameters, significantly releasing the memory consumption. Another line of research proposes to model the incremental update of the pre-trained weights in a parameter-efficient way, without modifying the model architecture (Zaken et al., 2021; Guo et al., 2020; Hu et al., 2022). Given a pre-trained weight matrix1 W(0), for example, diff pruning (Guo et al., 2020) models its incremental update ∆ as a sparse matrix. Diff pruning initializes ∆ as the same dimension as W(0) and then prunes ∆ element-wise based on the magnitude of the entries. As such, diff pruning can increase the parameter efficiency substantially by adaptively retaining important updates and pruning unimportant ones. Nonetheless, diff pruning has several limitations. First, it relies on low-level implementation to speed up the computation of unstructured sparse matrices, which is not well supported by existing deep learning frameworks. Therefore, we have to store ∆ as a dense matrix during training. Second, it needs to update every entry of ∆ with their gradients and then prune them. This results in similar computational cost as full fine-tuning (Guo et al., 2020). To overcome these drawbacks, Hu et al. (2022) propose a method named LoRA, which parameterizes ∆ as a low-rank matrix by the product of two much smaller matrices: W = W(0) + ∆ = W(0) + BA, (1) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. During fine-tuning, only A and B are updated. The rank r is chosen to be much smaller than the dimension of W (e.g., r = 8 when d1 = d2 = 1024). With less than 0.5% additional trainable parameters, the training overhead can be reduced up to 70%, compared to full fine-tuning. However, LoRA achieves comparable or even better performance than full fine-tuning (Hu et al., 2022). Meanwhile, the product of two samll matrices is more friendly to implement and deploy than unstructured sparse matrices in diff pruning. LoRA still has limitations as it prespecifies the rank r of each incremental matrix ∆ identical. This ignores the fact that the importance of weight matrices varies significantly across modules and layers when fine-tuning pre-trained models. To illustrate this point, we present an concrete example in Figure 1. We compare the performance of LoRA when fine-tuning specific modules or layers with the same number of trainable parameters. Figure 1a shows that fine-tuning feed-forward networks (FFN) achieves better performance than self-attention modules. In addition, Figure 1b demonstrates that weight matrices in top layers are more important than those in bottom layers. Adding more trainable parameters to the critical weight matrices can lead to better model performance. In contrast, adding more parameters to those less important weight matrices yields very marginal gains or even hurt model performance. Given the parameter budget, i.e., the number of total trainable parameters, we always prefer to allocate more parameters to those important modules. Distributing the budget evenly to all weight matrices/layers, like LoRA and other methods (e.g., adapter and prefix tuning), often gives suboptimal performance. To this end, a natural question is: How can we allocate the parameter budget adaptively according to importance of modules to improve the performance of parameter-efficient fine-tuning? 1Unless specified otherwise, we use W(0) to denote any pre-trained weight matrix. 2Published as a conference paper at ICLR 2023 To answer this question, we propose a new method –AdaLoRA (Adaptive Low-Rank Adaptation), which dynamically allocates the parameter budget among weight matrices during LoRA-alike fine- tuning. Specifically, AdaLoRA adjusts the rank of incremental matrices to control their budget. Critical incremental matrices are assigned with high rank such that they can capture more fine-grained and task-specific information. Less importance ones are pruned to have lower rank to prevent overfitting and save the computational budget. There are some methods to control the rank of matrices in the existing literature of matrix approximation (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). Most of them directly compute singular value decomposition (SVD) of a matrix and then truncate the smallest singular values. Such an operation can manipulate the rank explicitly and, more importantly, minimize the difference between the resulting matrix and the original matrix. However, for fine-tuning large models, it becomes prohibitively expensive to iteratively apply SVD for a large number of high-dimensional weight matrices. Therefore, instead of computing SVD exactly, we parameterize ∆ as ∆ = PΛQ to mimic SVD. The diagonal matrix Λ contains singular values while the orthogonal matrices P and Q represent left/right singular vectors of ∆. To regularize the orthogonality of P and Q, an additional penalty is added to training loss. Such a parameterization avoids the intensive computations of SVD. Besides, another advantage is that we only need to drop the unimportant singular values while the singular vectors are maintained. This preserves the possibility of future recovery and stabilizes the training. See a detailed comparison to LoRA in Section 3. Based on our SVD parameterization, AdaLoRA dynamically adjusts the rank of ∆ = PΛQ by importance scoring. Specifically, we divide the incremental matrix PΛQ into triplets, where each triplet Gi contains the i-th singular value and the corresponding singular vectors. To quantify the importance of triplets, we propose a novel importance metric, which takes account of the contribution of every entry in Gi to the model performance (Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). Triplets with low importance scores are granted low priority and hence the singular values are zeroed out. Triplets with high importance are retained for fine-tuning. Moreover, we also propose a global budget scheduler to facilitate the training. In particular, we start from an initial parameter budget, which is slightly higher than the final budget, and then gradually reduce it until matching the target. Such a scheduler can improve the training stability and model performance. Please see Section 3 for a detailed description of our importance metric and budget scheduler. We conduct extensive experiments on a wide range of tasks and models to demonstrate the effec- tiveness of AdaLoRA. Specifically, we evaluate the performance using DeBERTaV3-base (He et al., 2021a) on natural language understanding (GLUE, Wang et al. (2019)) and question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)) datasets. We also apply our methods to BART-large (Lewis et al., 2019) and evaluate the performance on natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail, Hermann et al. (2015)) tasks. We show AdaLoRA consistently outperforms the baseline, especially under low budget settings. For example, with less than 0.1% trainable parameters of full fine-tuning, AdaLoRA achieves a 1.2% F1 improvement on the SQuAD2.0 dataset compared with state-of-the-art approaches. 2 B ACKGROUND Transformer-based Models. A typical transformer model consists of L stacked blocks, where each block contains two submodules: a multi-head attention (MHA) and a fully connected FFN. Given the input sequence X ∈ Rn×d, MHA performs the attention function in parallel h heads: MHA (X) = Concat(head1, ...,headh)Wo, headi = Softmax \u0010 XWqi (XWki )⊤/ p dh \u0011 XWvi , where Wo ∈ Rd×d is an output projection and Wqi , Wki , Wvi ∈ Rd×dh are query, key and value projections of head i. dh is typically set to d/h. The other important module is a FFN which consists of two linear transformations with a ReLU activation in between: FFN(X) = ReLU(XWf1 + b1)Wf2 + b2, where Wf1 ∈ Rd×dm and Wf2 ∈ Rdm×d. Finally, a residual connection is used followed by a layer normalization (Ba et al., 2016). Low Rank Adaptation.LoRA (Hu et al., 2022) models the incremental update of the pre-trained weights by the product of two small matrices. For h = W(0)x, the modified forward pass is: h = W(0)x + ∆x = W(0)x + BAx, (2) where W(0), ∆ ∈ Rd1×d2 , A ∈ Rr×d2 and B ∈ Rd1×r with r ≪ {d1, d2}. A typically adopts a random Gaussion initialization while B is initialized with zero to have ∆ = 0 at the beginning of 3Published as a conference paper at ICLR 2023 training. We further denoteAi∗ as the i-th row ofA, B∗i as the i-th column ofB, and Gi = {Ai∗, B∗i} as the i-th doublet. Hu et al. (2022) only apply LoRA to query and value projections (i.e, Wq and Wv) in the MHAs. He et al. (2022) extend it to weight matrices of FFNs (i.e, Wf1 and Wf2 ), leading to the performance improvement . Meanwhile, they propose a unified view of various efficient tuning methods including adapter tuning, prefix tuning and LoRA. 3 A DALORA M ETHOD Our method contains two important components: (i) SVD-based adaptation, which formulates the incremental matrices in the form of singular value decomposition; (ii) Importance-aware rank allocation, which prunes redundant singular values based on our newly-designed importance metric. 3.1 SVD-B ASED ADAPTATION As mentioned in Section 1, we propose to parameterize the incremental updates of the pre-trained weight matrices in the form of singular value decomposition: W = W(0) + ∆ = W(0) + PΛQ, (3) where P ∈ Rd1×r and Q ∈ Rr×d2 represent the left/right singular vectors of ∆ and the diagonal matrix Λ ∈ Rr×r contains the singular values {λi}1≤i≤r with r ≪ min(d1, d2). We further denote Gi = {P∗i, λi, Qi∗} as the triplet containing the i-th singular value and vectors. In practice, since Λ is diagonal, we only need to save it as a vector in Rr. Λ is initialized with zero while P and Q adopt a random Gaussian initialization to ensure ∆ = 0 at the beginning of training. To enforce the orthogonality of P and Q, i.e., P⊤P = QQ⊤ = I, we utilize the following regularizer2: R(P, Q) = ∥P⊤P − I∥2 F + ∥QQ⊤ − I∥2 F. (4) In our method, Λ is iteratively pruned to adjust the rank after each gradient decent step. As mentioned in Section 1, one can directly compute SVD for every ∆ to manipulate singular values. The computational complexity, however, is O(min(d1, d2)d1d2). It becomes extremely expensive to iteratively apply SVD for a large number of high-dimensional incremental matrices. In contrast, our parameterization avoids intensive SVD computation, greatly releasing the computational overhead. We remark that one can also apply structured pruning to LoRA to control the rank (i.e., pruneBA doublet-wise in (1)), whereas it has the following disadvantages. First, when a doublet is measured as unimportant, we have to prune all of its elements. It makes scarcely possible to reactivate the pruned doublets as their entries are all zeroed out and not trained. In contrast, AdaLoRA only masks out the singular values based on (3) while the singular vectors are always maintained. It preserves the potential of future recovery for the triplets dropped by mistake. Second, A and B of LoRA are not orthogonal, meaning the doublets can be dependent with each other. Discarding the doublets can incur larger variation from the original matrix than truncating the smallest singular values. Therefore, the incremental matrices are often altered dramatically after each step of rank allocation, which causes training instability and even hurts generalization. To demonstrate this point, we present an ablation study in Section 4.4, which compares AdaLoRA with structured pruning for LoRA. 3.2 I MPORTANCE -AWARE RANK ALLOCATION We apply the SVD-based adaptation (3) to every weight matrix including Wq, Wk, Wv, Wf1 and Wf2 of each transformer layer. In order to control the budget, we iteratively prune singular values in correspondence to their importance score during the training. For clear reference, we use k to index the incremental matrix, i.e., ∆k = PkΛkQk for k = 1 , . . . , n, where n is the number of adapted weight matrices. We denote the i-th triplet of ∆k as Gk,i = {Pk,∗i, λk,i, Qk,i∗} and its importance score as Sk,i. We further denote the parameter sets P = {Pk}n k=1, E = {Λk}n k=1, Q = {Qk}n k=1 and training cost as C(P, E, Q). With the regularization (4), the training objective is given by L(P, E, Q) = C(P, E, Q) + γ Pn k=1 R(Pk, Qk), where γ > 0 is the regularization coefficient. At the t-th step, we first take a stochastic gradient step to update P(t) k , Λ(t) k and Q(t) k for k = 1, . . . , n. Specifically, for Λ(t) k ˜Λ(t) k = Λ(t) k − η∇Λk L(P(t), E(t), Q(t)), (5) 2We present the experiments in Appendix G to verify the effectiveness of the regularization. 4Published as a conference paper at ICLR 2023 where η > 0 is learning rate. Then, given importance score S(t) k , the singular values are pruned following Λ(t+1) k = T (˜Λ(t) k , S(t) k ), with T (˜Λ(t) k , S(t) k )ii = \u001a ˜Λ(t) k,ii S(t) k,i is in the top-b(t) of S(t), 0 otherwise, (6) where S(t) = {S(t) k,i}1≤k≤n,1≤i≤r contains the importance score of all triplets. Here b(t) is the budget of remaining singular values at the t-th step, which we explain more in Section 3.3. In this way, we leave more budget to the incremental matrices of higher priority by pruning the singular values of less important ones. In the sequel, we introduce several options to design the importance score. Magnitude of singular valuesis the most straightforward way to quantify the importance of every triplet, i.e., Sk,i = |λk,i|. In this way, only the least significant singular values are discarded. It minimizes the deviation from the original matrix and further stabilizes the training. Many existing methods use this criterion to control the rank of matrix (Cai et al., 2010; Koltchinskii et al., 2011; Toh & Yun, 2010). However, we remark that such a simple metric cannot properly quantify the contribution of parameters to model performance. Sensitivity-based importanceis another option for importance scoring, which quantifies the sensi- tivity of parameters to the training loss (Molchanov et al., 2019; Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). The prior work, however, leverages the sensitivity to quantify the importance of single entries and applies it for unstructured pruning that prunes weights element-wise. When it turns to our case, we have to design a new metric as the triplets are discarded group-wise. Every entry’s sensitivity ought to be considered and properly combined to quantify the overall contribution of the triplet to model performance. Therefore, we propose a newly-designed importance metric in account of both the singular value and vectors in triplet Gk,i: Sk,i = s(λk,i) + 1 d1 d1X j=1 s(Pk,ji) + 1 d2 d2X j=1 s(Qk,ij), (7) where we calculate the mean importance of Pk,∗i and Qk,i∗ such that Sk,i does not scale with the number of parameters in Gk,i. Here s(·) is a specific importance function for single entries. We can adopt the sensitivity for s(·), which is defined as the magnitude of the gradient-weight product: I(wij) = |wij∇wij L|, (8) where wij is any trainable parameter. (8) essentially approximates the change in loss when a parameter is zeroed out. If the removal of a parameter has a large influence, then the model is sensitive to it and we should retain it (Molchanov et al., 2019; Liang et al., 2021; Zhang et al., 2022). However, Zhang et al. (2022) point out that the sensitivity in (8) is not yet a reliable importance indi- cator. Such a score is estimated on the sampled mini batch. The stochastic sampling and complicated training dynamics incur high variability and large uncertainty for estimating the sensitivity with (8). Therefore, Zhang et al. (2022) propose to resolve this issue by sensitivity smoothing and uncertainty quantification: I (t) (wij) =β1I (t−1) (wij) + (1− β1)I(t)(wij) (9) U (t) (wij) =β2U (t−1) (wij) + (1− β2) \f\f\fI(t)(wij) − I (t) (wij) \f\f\f, (10) where 0 < β1, β2 < 1. I (t) is the smoothed sensitivity by exponential moving average and U (t) is the uncertainty term quantified by the local variation between I(t) and I (t) . Then they define the importance as the product between I (t) and U (t) , which can be another option for s(·): s(t)(wij) = I (t) (wij) · U (t) (wij). (11) We present a detailed ablation study in Section 4.4 to compare the performance of different importance metrics. We find the proposed metric (7) based on the sensitivity variant (11) generally performs best. We summarize the detailed algorithm in Algorithm 1. 5Published as a conference paper at ICLR 2023 Algorithm 1AdaLoRA 1: Input: Dataset D; total iterations T; budget schedule {b(t)}T t=0; hyperparameters η, γ, β1, β2. 2: for t = 1, . . . , Tdo 3: Sample a mini-batch from D and compute the gradient ∇L(P, E, Q); 4: Compute the sensitivity I(t) in (8) for every parameter in {P, E, Q}; 5: Update I (t) as (9) and U (t) as (10) for every parameter in {P, E, Q}; 6: Compute S(t) k,i by (7), for k = 1, . . . , nand i = 1, . . . , r; 7: Update P(t+1) k = P(t) k − η∇Pk L(P, E, Q) and Q(t+1) k = Q(t) k − η∇Qk L(P, E, Q); 8: Update Λ(t+1) k = T (Λ(t) k − η∇Λk L(P, E, Q), S(t) k ) given the budget b(t). 9: end for 10: Output: The fine-tuned parameters {P(T), E(T), Q(T)}. 3.3 G LOBAL BUDGET SCHEDULER As mentioned in Section 1, adjusting the rank is naturally to control the parameter budget in the context of low-rank adaptation. Hence we define the budget b(t) as the total rank of all incremental matrices, i.e., the number of total singular values. Recall that the budget allocation is iteratively conducted during the fine-tuning. To facilitate the training, we propose a global budget scheduler. Specifically, we start from an initial budgetb(0) that is slightly higher than the target budgetb(T) (e.g., 1.5 times of b(T)). We set the initial rank of each incremental matrix as r = b(0)/n. We warm up the training for ti steps, and then follow a cubic schedule to decrease the budget b(t) until it reaches b(T). Finally, we fix the resulting budget distribution and fine-tune the model for tf steps. The exact equation for the budget schedule is presented in Appendix A. This allows AdaLoRA to explore the parameter space first and then focus on the most important weights later. 4 E XPERIMENTS We implement AdaLoRA for fine-tuning DeBERTaV3-base (He et al., 2021a) and BART-large (Lewis et al., 2019). We evaluate the effectiveness of the proposed algorithm on natural language understanding (GLUE, Wang et al. (2019)), question answering (SQuADv1, Rajpurkar et al. (2016) and SQuADv2, Rajpurkar et al. (2018)), and natural language generation (XSum, Narayan et al. (2018) and CNN/DailyMail Hermann et al. (2015)). All the gains have passed significant tests with p <0.05. Implementation Details. We use PyTorch(Paszke et al., 2019) to implement all the algorithms. Our implementation is based on the publicly available Huggingface Transformers3 (Wolf et al., 2019) code-base. All the experiments are conducted on NVIDIA V100 GPUs. LoRA scales ∆x by α/r where α is a constant in r. As a result, the magnitude of output can be consistent given different r. It reduces the efforts of retuning learning rate when varying r. Typically α is set as 16 or 32 and never tuned (Hu et al., 2022; Yang & Hu, 2020). Following LoRA, we add the same scaling for (3) and fix α as LoRA. Besides, in Algorithm 1, we prune singular values every ∆T steps (e.g., ∆T = 100) such that the pruned triplets can still get updated within these intervals and possibly reactivated in future iterations. Baselines. We compare AdaLoRA with the following methods: • Full fine-tuning is the most common approach for adaptation. During fine-tuning, the model is initialized with pre-trained weights and biases, and all model parameters undergo gradient updates. • Bitfit (Zaken et al., 2021) is an effective parameter-efficient fine-tuning method. The method only fine-tunes bias vectors in the pre-trained model. • Adapter tuning (Houlsby et al., 2019; Pfeiffer et al., 2020) inserts two-layer adapters between transformer blocks. We compare with two types of adapter. Houlsby adapter as proposed in Houlsby et al. (2019) is inserted between the self-attention module and the FFN module followed by a subsequent residual connection. Recently, Pfeiffer et al. (2020) propose a more efficient design with adapters only applied after FFN modules and LayerNorm modules (Ba et al., 2016), which we call 3https://github.com/huggingface/transformers 6Published as a conference paper at ICLR 2023 Table 1: Results with DeBERTaV3-base on GLUE development set. The best results on each dataset are shown in bold. We report the average correlation for STS-B.Full FT, HAdapter and PAdapter represent full fine-tuning, Houlsby adapter, and Pfeiffer adapter respectively. We report mean of5 runs using different random seeds. Method # Params MNLI SST-2 CoLA QQP QNLI RTE MRPC STS-B All m/mm Acc Mcc Acc/F1 Acc Acc Acc Corr Ave. Full FT 184M 89.90/90.12 95.63 69.19 92.40/89.80 94.03 83.75 89.46 91.60 88.09 BitFit 0.1M 89.37/89.91 94.84 66.96 88.41/84.95 92.24 78.70 87.75 91.35 86.02 HAdapter 1.22M 90.13/90.17 95.53 68.64 91.91/89.27 94.11 84.48 89.95 91.48 88.12 PAdapter 1.18M 90.33/90.39 95.61 68.77 92.04/89.40 94.29 85.20 89.46 91.54 88.24 LoRAr=8 1.33M 90.65/90.69 94.95 69.82 91.99/89.38 93.87 85.20 89.95 91.60 88.34 AdaLoRA 1.27M 90.76/90.79 96.10 71.45 92.23/89.74 94.55 88.09 90.69 91.84 89.31 HAdapter 0.61M 90.12/90.23 95.30 67.87 91.65/88.95 93.76 85.56 89.22 91.30 87.93 PAdapter 0.60M 90.15/90.28 95.53 69.48 91.62/88.86 93.98 84.12 89.22 91.52 88.04 HAdapter 0.31M 90.10/90.02 95.41 67.65 91.54/88.81 93.52 83.39 89.25 91.31 87.60 PAdapter 0.30M 89.89/90.06 94.72 69.06 91.40/88.62 93.87 84.48 89.71 91.38 87.90 LoRAr=2 0.33M 90.30/90.38 94.95 68.71 91.61/88.91 94.03 85.56 89.71 91.68 88.15 AdaLoRA 0.32M 90.66/90.70 95.80 70.04 91.78/89.16 94.49 87.36 90.44 91.63 88.86 Pfeiffer adapter. The number of trainable parameters is determined by the number of layers, the hidden dimension of adapters and the dimension of their inputs. • LoRA (Hu et al., 2022) is a state-of-the-art method for parameter-efficient fine-tuning. The method parameterizes incremental updates by two small matrices and only fine-tune them. The number of trainable parameter is controlled by the rank r and the number of adapted weight matrices n. Hu et al. (2022) apply LoRA to query and value projections only. In empirical, we find that applying LoRA to all weight matrices, i.e., Wq, Wk, Wv, Wf1 and Wf2 , can further improve its performance (Please see Appendix F). Hence, we compare with this generalized LoRA to maximize its performance. We use publicly available implementation 4 to run all the baselines. Please refer to Hu et al. (2022) and reference therein for details. 4.1 N ATURAL LANGUAGE UNDERSTANDING Models and Datasets.We evaluate the fine-tuning performance of DeBERTaV3-base (He et al., 2021a) using the proposed algorithm. We conduct experiments on the General Language Understand- ing Evaluation (GLUE, Wang et al. 2019) benchmark. The benchmark includes two single-sentence classification tasks, three similarity and paraphrase tasks and four natural language inference tasks. Dataset details are summarized in Appendix B. Implementation Details. DeBERTaV3-base consists of 183 millions parameters. We compare AdaLoRA with the baselines under different budget levels, for example, given the total trainable parameters as 0.3/0.6/1.2 million. In order to match the parameter budget, we select the hidden dimensions of adapters from {8, 16, 32, 64}, set the rank r of LoRA as {2, 4, 8}, and choose the final budget b(T) of AdaLoRA from {144, 288, 576}. Then we set b(0) as 1.5 times of b(T) for AdaLoRA and select the regularization coefficient γ from {0.1, 0.3, 0.5}. We set the exponential moving average parameters β1 and β2 as their default value 0.85. We select the learning rate from {5 × 10−5, 8 × 10−5, 1 × 10−4, 2 × 10−4}. More details are presented in Appendix C. Main results.We compare AdaLoRA with the baseline methods under different budget settings. Table 1 shows experimental results on the GLUE development set. We see that AdaLoRA achieves better or on par performance compared with existing approaches on all datasets under all budget levels. For example, when the parameter budget is 0.3M, AdaLoRA achieves 87.36% accuracy on RTE, which is 1.8% higher than the best-performing baseline. Besides, AdaLoRA with extreme low budget can often perform better than the baselines with higher budget. For example, AdaLoRA achieve 70.04% Mcc. score on CoLA with 0.3M fine-tuning parameters, which is higher than all baseline methods with lager budget (e.g., 0.6M and 1.2M). 4.2 Q UESTION ANSWERING Models and Datasets.We evaluate performance of the proposed algorithm on two question answering (QA) datasets: SQuAD v1.1 (Rajpurkar et al., 2016) and SQuADv2.0 (Rajpurkar et al., 2018), where 4https://github.com/microsoft/LoRA 7Published as a conference paper at ICLR 2023 Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. Here # Params is the number of trainable parameters relative to that in full fine-tuning. We report EM/F1. The best results in each setting are shown in bold. SQuADv1.1 SQuADv2.0 Full FT 86.0 / 92.7 85.4 / 88.4 # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% HAdapter 84.4/91.5 85.3/92.1 86.1/92.7 86.7/92.9 83.4/86.6 84.3/87.3 84.9/87.9 85.4/88.3 PAdapter 84.4/91.7 85.9/92.5 86.2/92.8 86.6/93.0 84.2/87.2 84.5/87.6 84.9/87.8 84.5/87.5 LoRA 86.4/92.8 86.6/92.9 86.7/93.1 86.7/93.1 84.7/87.5 83.6/86.7 84.5/87.4 85.0/88.0 AdaLoRA 87.2/93.4 87.5/93.6 87.5/93.7 87.6/93.7 85.6/88.7 85.7/88.8 85.5/88.6 86.0/88.9 we use AdaLoRA to fine-tune DeBERTaV3-base. These tasks are treated as a sequence labeling problem, where we predict the probability of each token being the start and end of the answer span. Dataset details can be found in Appendix D. Implementation Details.We compare AdaLoRA with the baseline methods under different parameter budgets. That is we have the number of trainable parameters as 0.08%/0.16%/0.32%/0.65% of total pre-trained parameters. To match the budget requirements, we select the hidden dimensions of adapters from {4, 8, 16, 32, 64}, set the rank r of LoRA as {1, 2, 4, 8} and choose the final total rank b(T) of AdaLoRA from {72, 144, 288, 576}. We set the batch size as 16. We use AdamW (Loshchilov & Hutter, 2019) as the optimizer and we set the learning rate as 1 × 10−3 for AdaLoRA. Please refer to Appendix D for more details. Main Results. Table 2 summarizes experimental results when we fine-tune DeBERTaV3-base under 4 different budget settings: 0.08%, 0.16%, 0.32% and 0.65% of total pre-trained parameters. From the result, we see that AdaLoRA consistently outperforms existing approaches under all the budget levels in term of two evaluation metrics: exact match (EM) and F1. Notice that the performance of Houlsby adapter and Pfeiffer adapter are notably decreased when we reduce the parameter budget. In contrast, our method shows the consistent performance under different budget levels. For example, AdaLoRA achieves 88.7% F1 on SQuADv2.0 with the smallest budget 0.08%. It is close to its performance under the high budget and it is also 1.2% higher than the best-performing baseline. 4.3 N ATURAL LANGUAGE GENERATION Table 3: Results with BART-large on XSum and CNN/DailyMail. Here# Params is the number of trainable parameters relative to that in full fine-tuning. We report R-1/2/L. The best results are shown in bold. # Params Method XSum CNN/DailyMail 100% Full FT 45.49 / 22.33 / 37.26 44.16 / 21.28 / 40.90 2.20% LoRA 43.95 / 20.72 / 35.68 45.03 / 21.84 / 42.15 AdaLoRA 44.72 / 21.46 / 36.4645.00 / 21.89 / 42.16 1.10% LoRA 43.40 / 20.20 / 35.20 44.72 / 21.58 / 41.84 AdaLoRA 44.35 / 21.13 / 36.1344.96 / 21.77 / 42.09 0.26% LoRA 43.18 / 19.89 / 34.92 43.95 / 20.91 / 40.98 AdaLoRA 43.55 / 20.17 / 35.2044.39 / 21.28 / 41.50 0.13% LoRA 42.81 / 19.68 / 34.73 43.68 / 20.63 / 40.71 AdaLoRA 43.29 / 19.95 / 35.0443.94 / 20.83 / 40.96 Models and Datasets.To provide a comparison with the state-of-the-art in natural language gener- ation (NLG) tasks, we apply AdaLoRA to fine-tune a BART-large model (Lewis et al., 2019). We evaluate model performance on two datasets: XSum (Narayan et al., 2018) and CNN/DailyMail (Hermann et al., 2015). Implementation Details.Similarly as DeBERTav3-base, we apply low-rank/SVD-based adaptation to every weight matrix of both encoder and decoder layers. We report ROUGE 1/2/L scores (R-1/2/L, Lin (2004)). We set the training epochs as 15. For XSum, we set the beam length as 8 and batch size 8Published as a conference paper at ICLR 2023 as 64. For CNN/DailyMail, we set the beam length as 4 and batch size as 32. Please see Appendix E for the detailed configuration. Main Results.Experimental results are summarized in Table 3, where we compare the fine-tuning performance under four budget levels: the number of trainable parameters is 0.13%, 0.26%, 1.10% and 2.20% of total pre-trained parameters. We see that AdaLoRA achieves better or on par performance compared with the baseline on both datasets (XSum and CNN/DailyMail) under all the budget levels. For example, AdaLoRA achieves 21.13 R-2 score when budget level is 1.10%, compared with 19.89 for LoRA. 4.4 A NALYSIS Different budget levels.Figure 2 illustrates experimental results of fine-tuning DeBERTaV3-base under different budget levels. We see that on all the three datasets (MNLI-m, SQuADv2.0 and XSum), AdaLoRA achieves consistent performance improvement under all the budget levels compared with the baseline. The performance gain is more significant when increasing the budget for the XSum task, suggesting a high budget can help NLG tasks. Note that on the MNLI and SQuADv2.0 datasets, the performance of AdaLoRA under low budget levels (≤ 1%) can match the results of high budget settings. For example, AdaLoRA achieves 88.78% F1 on SQuADv2.0 when the budget is 0.16%. It is close to the performance (88.89% F1) of the highest budget (4.65%) with a more significant gain over the baseline. 0.08 0.16 0.32 0.65 0.96 1.30 1.95 2.88 # Params (%) 90.2 90.3 90.4 90.5 90.6 90.7Acc (MNLI-m) LoRA  AdaLoRA (a) MNLI 0.08 0.16 0.32 0.65 1.30 2.70 4.65 # Params (%) 87.0 87.5 88.0 88.5 89.0 F1  (b) SQuADv2.0 0.13 0.26 1.1 2.2 4.5 7.9 12.5 # Params (%) 20.0 20.5 21.0 21.5 ROUGE-2  (c) XSum Figure 2: Fine-tuning performance under different budget levels. We compare AdaLoRA with the generalized LoRA that applies to every weight matrix. Comparison to low-rank parameterization.As mentioned in Section 3.1, one can alternatively prune LoRA doublet-wise to conduct the rank allocation. In this case, the doublets are zeroed out entirely, raising the barrier to reactivate them. It can cause training instability and hurt the generalization when some crucial doublets are pruned by mistake. In Table 4, we compare AdaLoRA with pruning LoRA on three datasets (SST-2, RTE, and CoLA) to illustrate this point. We apply the same importance score, budget scheduler and training setups as Section 4.1 for pruning LoRA. We can see that AdaLoRA outperforms pruning LoRA on all the datasets under all the budget levels. Table 4: We present two ablation studies in this table: (i) Comparison between AdaLoRA and structured pruning on LoRA. (ii) Comparison of different importance metrics for AdaLoRA. SST-2 RTE CoLA # Params 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% 0.08% 0.16% 0.65% Prune LoRA 94.84 94.50 94.95 86.28 86.15 87.00 66.71 69.29 69.57 AdaLoRA 95.52 95.80 96.10 87.36 87.73 88.09 70.21 70.04 71.45 s(·) = I(·) 94.61 95.30 95.64 87.36 87.71 88.10 66.71 68.83 70.19 Si = |λi| 95.41 95.41 95.87 87.00 86.28 88.00 67.67 68.44 70.38 Variants of the importance score.Recall that in AdaLoRA, the importance score is defined by the sensitivity and uncertainty of every entry in the triplet (7). In Table 4, we examine two variants of the importance score: (i) changing s(·) in (7) to sensitivity-only; (ii) directly defining Si as |λi|. From the results, we can see that the proposed importance score generally performs best. The other two variants can degenerate the model performance up to 0.9%. The role of two components.We remark that both two components of our method - SVD adaptation and adaptive budget allocation, play vital roles for the performance gain. To demonstrate it, we 9Published as a conference paper at ICLR 2023 compare AdaLoRA with the following variants: (i) SVD-LoRA: fine-tuning only with the proposed SVD-based adaptation in (3) and (4); (ii) LoRA regu: LoRA with orthogonal regularization (4) on A and B; (iii) AdaLoRAγ = 0: AdaLoRA without orthogonal regularization (4). Table 5 present the results when fine-tuning DeBERTaVe-base on SST-2 and MNLI. We can see that fine-tuning only with SVD adaptation shows an improvement over LoRA but cannot match the performance of AdaLoRA. Meanwhile, without SVD orthogonal regularization, the performance of AdaLoRA can degenerate. These results validate that both components contribute to the model performance. Table 5: We present ablation studies about SVD-based adaptation, orthogonal regularization, and budget allocation in this table. For MNLI, we report the average score of m/mm acc. SST-2 MNLI # Params 0.08% 0.16% 0.32% 0.65% 0.08% 0.16% 0.32% 0.65% LoRA 94.38 94.95 - 94.95 90.19 90.34 - 90.57 LoRAregu - 94.61 94.72 94.61 - 90.30 90.40 90.66 SVD-LoRA 95.33 95.18 95.07 95.53 90.28 90.25 90.52 90.62 AdaLoRAγ = 0 95.41 95.10 95.30 95.10 90.37 90.34 90.56 90.43 AdaLoRA 95.64 95.80 96.10 96.10 90.65 90.68 90.66 90.77 The resulting budget distribution.Figure 3 shows the resulting rank of each incremental matrix of DeBERTaV3-base fine-tuned with AdaLoRA. We find that AdaLoRA always prefers to allocating more budget to FFNs and top layers. Such behavior aligns with our empirical conclusions presented in Figure 1 that weight matrices of FFN moduels and top layers are more important for model performance. Hence, it validates that our proposed importance metric can guide AdaLoRA to focus on crucial modules. Meanwhile, the rank distribution generated by AdaLoRA is consistent across different budget levels, tasks and models. It means the number of remaining parameters is linearly scaled with b(T) and hence we can tune b(T) to control the remaining parameters. 1 2 3 4 5 6 7 8 9 10 11 12 Layer Wf2 Wf1 Wo Wv Wk Wq 4 1 5 2 3 5 5 6 10 5 5 0 6 9 9 9 12 11 12 12 12 12 12 2 7 3 5 8 8 10 12 12 12 12 12 5 6 6 10 6 10 11 11 11 12 12 11 9 5 4 5 5 10 9 9 11 12 12 12 12 3 2 5 4 7 7 7 10 11 11 10 3 0 2 4 6 8 10 12 The ﬁnal rank Figure 3: The resulting rank of each incremental matrix when fine-tuning DeBERTaV3-base on MNLI with AdaLoRA. Here the x-axis is the layer index and the y-axis represents different types of adapted weight matrices. 5 C ONCLUSION We propose a parameter-efficient fine-tuning method – AdaLoRA that adaptively allocates the parameter budget according to importance scoring. In AdaLoRA, we parameterize the incremental updates of weight matrices in the form of singular value decomposition. Then, we dynamically allocate the parameter budget among incremental matrices by manipulating the singular values based on a new importance metric. Such an a pproach effectively improves the model performance and parameter efficiency. We conduct extensive experiments on natural language processing, question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches. 10Published as a conference paper at ICLR 2023 REFERENCES Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021a. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2021b. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302–2329, 2011. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https: //aclanthology.org/2021.emnlp-main.243. 11Published as a conference paper at ICLR 2023 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 4582–4597. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.353. URL https://doi.org/10.18653/v1/2021. acl-long.353. Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compression to improving generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6524–6538, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.510. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74–81, 2004. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 11264–11272. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01152. Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the sum- mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e- Buc, Emily B. Fox, and Roman Garnett (eds.),Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter- fusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872–1897, 2020. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. 12Published as a conference paper at ICLR 2023 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784–789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by fine-tuning. 2020. Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Pacific Journal of optimization, 6(615-640):15, 2010. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. In International Conference on Machine Learning, pp. 26809–26823. PMLR, 2022. 13Published as a conference paper at ICLR 2023 A G LOBAL BUDGET SCHEDULE As mentioned in Section 3.3, we propose a global budget scheduler to gradually decrease the budget b(t) following a cubic schedule. The detailed equation is given as follows: b(t) =    b(0) 0 ≤ t < ti b(T) + \u0000 b(0) − b(T)\u0001\u0010 1 − t−ti−tf T−ti−tf \u00113 ti ≤ t < T− tf b(T) o.w. . (12) B GLUE D ATASET STATISTICS We present the dataset statistics of GLUE (Wang et al., 2019) in the following table. Table 6: Summary of the GLUE benchmark. Corpus Task #Train #Dev #Test #Label Metrics Single-Sentence Classification (GLUE) CoLA Acceptability 8.5k 1k 1k 2 Matthews corr SST Sentiment 67k 872 1.8k 2 Accuracy Pairwise Text Classification (GLUE) MNLI NLI 393k 20k 20k 3 Accuracy RTE NLI 2.5k 276 3k 2 Accuracy QQP Paraphrase 364k 40k 391k 2 Accuracy/F1 MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1 QNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy Text Similarity (GLUE) STS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr C N ATURAL LANGUAGE UNDERSTANDING C.1 B UDGET CONFIGURATION For each budget level, we tune the final budget b(T) for AdaLoRA, the rank r for LoRA, the hidden dimension d for two adapters to match the budget requirements. Table 7: Detailed budget setup for GLUE benchmark. # Params Houlsby Adapter (d) Pfeiffer Adapter ( d) LoRA ( r) AdaLoRA ( b(T)) 1.2M 32 64 8 576 0.6M 16 32 4 288 0.3M 8 16 2 144 Alternatively, we can also set the final average rank ¯r(T) = b(T)/n for AdaLoRA to control the budget, which is set as 2, 4, and 8 given the final budget as 144, 288, and 576 respectively. Then we select the initial rank r from {4, 6, 12} for the final average rank {2, 4, 8} respectively. C.2 T RAINING DETAILS We tune the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best learning rate for every method. For each dataset, the batch size is set as identical for every method. 14Published as a conference paper at ICLR 2023 Table 8: Hyper-parameter setup of AdaLoRA for GLUE benchmark. Dataset learning rate batch size # epochs γ t i ∆T tf MNLI 5 × 10−4 32 7 0.1 8000 100 50000 RTE 1.2 × 10−3 32 50 0.3 600 1 1800 QNLI 1.2 × 10−3 32 5 0.1 2000 100 8000 MRPC 1 × 10−3 32 30 0.1 600 1 1800 QQP 5 × 10−4 32 5 0.1 8000 100 25000 SST-2 8 × 10−4 32 24 0.1 6000 100 22000 CoLA 5 × 10−4 32 25 0.5 800 10 3500 STS-B 2.2 × 10−3 32 25 0.1 800 10 2000 D Q UESTION ANSWERING D.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. Table 9: Detailed budget setup for question answering. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 D.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 10: Hyper-parameter setup of AdaLoRA for question answering tasks. Dataset learning rate batch size # epochs γ t i ∆T tf SQuADv1.1 1 × 10−3 16 10 0.1 5000 100 25000 SQuADv2.0 1 × 10−3 16 12 0.1 5000 100 50000 D.3 D ATASET The statistics of question answering datasets are summarized in Table 11. Table 11: Statistics of the SQuAD dataset. # Train # Validation SQuAD v1.1 87,599 10,570 SQuAD v2.0 130,319 11,873 E N ATURAL LANGUAGE GENERATION E.1 B UDGET CONFIGURATION Given the budget, we control the trainable parameters for each method as the following table. 15Published as a conference paper at ICLR 2023 Table 12: Detailed budget setup for summarization tasks. # Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA d d r b (T)/¯r(T)/r 0.65% 32 64 8 576 / 8 / 12 0.32% 16 32 4 288 / 4 / 6 0.16% 8 16 2 144 / 2 / 4 0.08% 4 8 1 72 / 1 / 2 E.2 T RAINING DETAILS We set the batch size as 16. We select the learning rate from {8 × 10−5, 5 × 10−5, 3 × 10−5, 1 × 10−4, 3 × 10−4, 5 × 10−4, 8 × 10−4, 1 × 10−3} and pick the best-performing learning rate for every method. The configuration of AdaLoRA is listed in the following table. Table 13: Hyper-parameter setup of AdaLoRA for summarization tasks. Dataset learning rate batch size # epochs γ t i ∆T tf XSum 5 × 10−4 64 25 0.1 6000 100 50000 CNN/DailyMail 5 × 10−4 32 15 0.1 5000 100 85000 F A BLATION STUDY FOR LORA As mentioned in Section 4, we find that the performance of LoRA can be further improved when applying it to every weight matrix, compared to fine-tuning Wq and Wv only (Hu et al., 2022). This observation aligns with the empirical results of He et al. (2022). In Table 14, we follow the same training configuration as Section 4.1 and present an ablation study to illustrate this point. Table 14: We compare the fine-tuning performance when apply LoRA to every weight matrix or Wq, Wv only. The parameter budget is fixed as 0.3M. We report accuracy for QQP and MRPC, accuracy(m) for MNLI, and average correlation for STS-B. MNLI QQP CoLA RTE QNLI SST-2 MRPC STS-B LoRA (Wq, Wk) 89.80 90.48 67.04 83.75 93.69 94.84 90.20 91.05 LoRA (all) 90.30 91.61 68.71 85.56 94.31 94.95 90.44 91.68 G O RTHOGONAL REGULARIZATION To verify the effectiveness of (4), we plot ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F to show whether P and Q are regularized to be orthogonal. We fine-tune a DeBERTaV3-base model on SST-2 with AdaLoRA and follow the same training configuration as Section 4.1. We set γ as 0.1 and plot the two terms along the training horizon. From Figure 4, we can see that two regularization terms can be optimized to a very small value (e.g., 0.001) at the beginning of training. Therefore, both P and Q can be enforced to be orthogonal quickly during the initial warm-up of AdaLoRA. It ensures that the triplets are not dependent with each other. H C OMPARISON OF TRAINING COST We compare the training cost between AdaLoRA and LoRA in the following table. We use two methods to fine-tune DeBERTaV3-base on a single NVIDIA V100 GPU. We do training only and set hyperparameters, e.g., batch size and training epochs, the same as in Section 4. Table 15 shows that AdaLoRA incurs 11% additional training time on MNLI and 16% on SQuADv2 under different budgets. The memory footprint of two methods are quite close. Such results demonstrate that AdaLoRA does not incur significant training overheads. The reason behind is that 16Published as a conference paper at ICLR 2023 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (a) P of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||QQ⊤ −I||2 F (b) Q of Wo at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 100 ||P⊤P −I||2 F (c) P of Wf2 at the first layer. 0 20000 40000 Iterations 10−4 10−3 10−2 10−1 ||QQ⊤ −I||2 F (d) Q of Wf2 at the first layer Figure 4: We plot the ∥P⊤P − I∥2 F and ∥QQ⊤ − I∥2 F when fine-tuning DeBERTaV3-base on SST-2. Table 15: Comparison of practical training cost between AdaLoRA and LoRA. Dataset # Param Method GPU Mem Time/epoch MNLI 0.08% LoRA 11.094 GB 105 min AdaLoRA 11.104 GB 116 min 0.16% LoRA 11.098 GB 105 min AdaLoRA 11.110 GB 117 min 0.65% LoRA 11.128 GB 105 min AdaLoRA 11.188 GB 117 min SST-2 0.08% LoRA 13.138 GB 60 min AdaLoRA 13.148 GB 71 min 0.16% LoRA 13.142 GB 61 min AdaLoRA 13.164 GB 71 min 0.65% LoRA 13.170 GB 61 min AdaLoRA 13.226 GB 71 min we only evaluate the importance score for small incremental matrices PΛQ. Their total number of parameters is usually less than 1% of pre-trained weights. Therefore, it does not lead to significant computational cost to update the importance scores of these well-structured small matrices, compared to forward-backward pass of full model. 17",
      "meta_data": {
        "arxiv_id": "2303.10512v2",
        "authors": [
          "Qingru Zhang",
          "Minshuo Chen",
          "Alexander Bukharin",
          "Nikos Karampatziakis",
          "Pengcheng He",
          "Yu Cheng",
          "Weizhu Chen",
          "Tuo Zhao"
        ],
        "published_date": "2023-03-18T22:36:25Z",
        "pdf_url": "https://arxiv.org/pdf/2303.10512v2.pdf",
        "github_url": "https://github.com/microsoft/LoRA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the inefficiency of existing parameter-efficient fine-tuning methods (like LoRA) that evenly distribute the budget of incremental updates across all pre-trained weight matrices, leading to suboptimal performance due to neglecting the varying importance of different weight parameters. The main contribution is AdaLoRA, an adaptive budget allocation method for parameter-efficient fine-tuning. AdaLoRA adaptively allocates the parameter budget among weight matrices based on their importance score. It parameterizes incremental updates using Singular Value Decomposition (SVD), enabling effective pruning of singular values for unimportant updates to reduce parameter budget while avoiding intensive exact SVD computations. AdaLoRA consistently outperforms baselines, especially in low-budget settings, achieving notable performance improvements, e.g., a 1.2% F1 improvement on SQuAD2.0 with less than 0.1% trainable parameters compared to full fine-tuning.",
        "methodology": "AdaLoRA builds on Low-Rank Adaptation (LoRA) by parameterizing incremental updates (∆) of pre-trained weight matrices in the form of SVD: W = W(0) + ∆ = W(0) + PΛQ, where P and Q are orthogonal matrices representing singular vectors and Λ is a diagonal matrix of singular values. An L2-norm regularizer is applied to P and Q to enforce orthogonality. Instead of expensive exact SVD, this parameterization allows for dynamic rank adjustment by manipulating singular values. The method incorporates an importance-aware rank allocation mechanism where singular values are pruned based on a newly designed importance metric. This metric, Sk,i, considers the contribution of the i-th singular value (λk,i) and its corresponding singular vectors (Pk,∗i, Qk,i∗), utilizing a smoothed sensitivity s(·) which is the product of exponential moving average of gradient-weight product (I(t)) and an uncertainty term (U(t)). A global budget scheduler facilitates training by starting with a slightly higher initial budget and gradually reducing it to the target budget via a cubic schedule, allowing initial exploration before focusing on crucial modules. Pruning of singular values is conducted iteratively every ∆T steps.",
        "experimental_setup": "The effectiveness of AdaLoRA is validated through extensive experiments on a wide range of tasks and models. For Natural Language Understanding (NLU) and Question Answering (QA), DeBERTaV3-base was fine-tuned on the GLUE benchmark (MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B) and SQuAD v1.1/v2.0 datasets, respectively. For Natural Language Generation (NLG), BART-large was fine-tuned on XSum and CNN/DailyMail datasets. Baselines included Full fine-tuning, Bitfit, Houlsby Adapter, Pfeiffer Adapter, and a generalized LoRA applied to all weight matrices (Wq, Wk, Wv, Wf1, Wf2). Performance was evaluated using standard metrics: Accuracy, Matthews correlation, F1-score for GLUE; Exact Match (EM) and F1 for SQuAD; and ROUGE 1/2/L (R-1/2/L) for NLG. Experiments were conducted across various parameter budget levels (e.g., 0.08% to 2.2% of total pre-trained parameters) on NVIDIA V100 GPUs, implemented using PyTorch and Huggingface Transformers. Hyperparameters like learning rates, batch sizes, and the regularization coefficient were tuned, with results averaged over multiple runs (e.g., 5 runs for GLUE) using different random seeds.",
        "limitations": "The paper does not explicitly list limitations of AdaLoRA in a dedicated section. However, it implicitly acknowledges that exact SVD computation is prohibitively expensive, which AdaLoRA's SVD-based parameterization aims to circumvent. A practical trade-off is observed in training cost, where AdaLoRA incurs an additional training time overhead (e.g., 11% on MNLI and 16% on SQuADv2) compared to LoRA, although it is argued not to be 'significant'. The complexity of the importance scoring mechanism, which involves sensitivity smoothing and uncertainty quantification, is also an added computational aspect.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import loralib as lora; class Attention(nn.Module):     def __init__(self, nx, n_ctx, config, scale=False):         super(Attention, self).__init__();         n_state = nx;         assert n_state % config.n_head == 0;         self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx));         self.n_head = config.n_head;         self.split_size = n_state;         self.scale = scale;         self.c_attn = lora.MergedLinear(nx, n_state * 3, r=config.lora_attn_dim, lora_alpha=config.lora_attn_alpha, lora_dropout=config.lora_dropout, enable_lora=[True, False, True], fan_in_fan_out=True, merge_weights=False);         self.c_proj = Conv1D(n_state, nx);         self.config = config; # ... (other methods of Attention class) ...; if args.lora_dim > 0: lora.mark_only_lora_as_trainable(lm_net); torch.save({'model_state_dict': lora.lora_state_dict(model)}, model_path);",
        "experimental_info": "Command-line arguments: --lora_dim (LoRA attention dimension), --lora_alpha (LoRA attention alpha), --lora_dropout (dropout probability for LoRA layers). Configuration parameters in GPT2Config: lora_attn_dim, lora_attn_alpha, lora_dropout. Note: The specific AdaLoRA mechanisms (SVD parameterization of updates, dynamic rank adjustment, importance-aware rank allocation, global cubic budget scheduler for rank, iterative pruning) are not found in the provided repository content. The extracted code and info pertain to a standard Low-Rank Adaptation (LoRA) implementation, which AdaLoRA builds upon."
      }
    },
    {
      "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
      "abstract": "Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.",
      "full_text": "LoRA Training in the NTK Regime has No Spurious Local Minima Uijeong Jang 1 Jason D. Lee 2 Ernest K. Ryu 3 Abstract Low-rank adaptation (LoRA) has become the stan- dard approach for parameter-efficient fine-tuning of large language models (LLM), but our the- oretical understanding of LoRA has been lim- ited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with N data points, showing: (i) full fine- tuning (without LoRA) admits a low-rank solu- tion of rank r ≲ √ N; (ii) using LoRA with rank r ≳ √ N eliminates spurious local minima, allow- ing (stochastic) gradient descent to find the low- rank solutions; (iii) the low-rank solution found using LoRA generalizes well. 1. Introduction The modern methodology of using large language models involves (at least) two phases: self-supervised pre-training on a large corpus followed by supervised fine-tuning to the downstream task. As large language models have grown in scale, pre-training has become out of reach for research groups without access to enormous computational resources. However, supervised fine-tuning remains feasible for such groups. One key strategy facilitating this efficient fine- tuning is Parameter-Efficient Fine-Tuning (PEFT), which freezes most of the pre-trained model’s weights while selec- tively fine-tuning a smaller number of parameters within an adapter module. Among various PEFT methodologies, low- rank adaptation (LoRA) (Hu et al., 2021) has emerged as the standard approach. Given a pre-trained matrixW0 ∈ Rm×n, LoRA trains a low-rank update such that the forward pass evaluates W0x + ∆W x= W0x + BAx 1Department of Mathematical Sciences, Seoul National Uni- versity 2Department of Electrical and Computer Engineering, Princeton University 3Department of Mathematics, University of California, Los Angeles. Correspondence to: Ernest Ryu <eryu@math.ucla.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). where r ≪ min(m, n), A ∈ Rr×n is initialized to be a random Gaussian, and B ∈ Rm×r is initialized to be zero. However, despite the widespread adoption of LoRA, our theoretical understanding of its mechanisms remains limited. One notable prior work is (Zeng & Lee, 2024), which ana- lyzes the expressive power of LoRA, showing that for any given function, there exist weight configurations for LoRA that approximate it. However, their work does not address whether LoRA can efficiently learn such configurations. Additionally, Malladi et al. (2023) experimentally demon- strated that under certain conditions, LoRA fine-tuning is nearly equivalent to a kernel regression, where the A matrix provides random features and is essentially not trained. This regime neglects the possibility of the A matrix learning new features and, consequently, leads to a LoRA rank require- ment of r ≥ Θ(1/ε2), where ε is an approximation toler- ance, originating from the use of the Johnson–Lindenstrauss lemma (Johnson & Lindenstrauss, 1984). Crucially, LoRA’s fundamental nature as a quadratic parameterization has not been considered in the prior analysis of trainability and generalizability. Contribution. In this work, we theoretically analyze LoRA fine-tuning and present results on trainability and generalizability. We consider fine-tuning a deep (trans- former) neural network with K-dimensional outputs using N training (fine-tuning) data points. Assuming that train- ing remains under the NTK regime, which we soon define and justify in Section 2, we show the following. First, full fine-tuning (without LoRA) admits a rank-r solution such that r(r+1) 2 ≤ KN . Second, using LoRA with rank r such that r(r+1) 2 > KN eliminates spurious local minima, al- lowing (stochastic) gradient descent to find the low-rank solutions. Finally, the low-rank solution found using LoRA generalizes well. 1.1. Prior works Theory of neural networks. The question of expressive power addresses whether certain neural networks of interest can approximate a given target function. Starting with the classical universal approximation theorems (Cybenko, 1989; Hornik et al., 1990; Barron, 1993), much research has been conducted in this direction. (Delalleau & Bengio, 2011; 1 arXiv:2402.11867v3  [cs.LG]  28 May 2024LoRA Training in the NTK Regime has No Spurious Local Minima Bengio & Delalleau, 2011; Lu et al., 2017; Duan et al., 2023). These can be thought of as existence results. The question of trainability addresses whether one can com- pute configurations of neural networks that approximate tar- get functions. Ghadimi & Lan (2013); Ge et al. (2015); Du et al. (2017); Jin et al. (2017) studied general convergence results of gradient descent and stochastic gradient descent. Soltanolkotabi et al. (2018); Du & Lee (2018); Allen-Zhu et al. (2019a;b); Du et al. (2019); Zou et al. (2020) stud- ied the loss landscape of neural networks and showed that first-order methods converge to global minima under certain conditions. The question of generalization addresses whether neural net- works trained on finite data can perform well on new unseen data. Classical learning theory (Koltchinskii & Panchenko, 2000; Bartlett et al., 2002; Bousquet & Elisseeff, 2002; Hardt et al., 2016; Bartlett et al., 2017) uses concepts such as uniform stability or the Rademacher complexities to ob- tain generalization bounds. Generalization bounds in the context of modern deep learning often utilize different ap- proaches (Wu et al., 2017; Dinh et al., 2017; Zhang et al., 2021), we use the Rademacher complexity for obtaining our generalization results. Neural tangent kernels. The theory of neural tangent kernel (NTK) concerns the training dynamics of certain infinitely wide neural networks. Jacot et al. (2018) shows that the training of an infinitely wide neural network is equivalent to training a kernel machine. Various studies such as (Arora et al., 2019; Chen et al., 2020) expand the NTK theory to more practical settings. Among these works, Wei et al. (2022a) introduced the concept of empirical NTK (eNTK) and showed that kernel regression with pretrained initialization also performs well on real datasets, providing a background to utilize NTK theory in fine-tuning. Theory of transformers and LLMs. As the transformer architecture (Vaswani et al., 2017) became the state-of-the- art architecture for natural language processing and other modalities, theoretical investigations of transformers have been pursued. Results include that transformers are uni- versal approximators (Yun et al., 2019), that transformers can emulate a certain class of algorithmic instructions (Wei et al., 2022b; Giannou et al., 2023), and that weight matrices in transformers increase their rank during training (Boix- Adsera et al., 2023). Also, (Zhang et al., 2020; Liu et al., 2020) presents improved adaptive optimization methods for transformers. PEFT methods and LoRA. Low-rank adaptation (LoRA) (Hu et al., 2021) has become the standard Parameter- Efficient Fine-Tuning (PEFT) method, and many variants of LoRA have been presented (Fu et al., 2023; Dettmers et al., 2023; Lialin et al., 2023). LoRA has proven to be quite versatile and has been used for convolution layers (Yeh et al., 2024) and for diffusion models (Ryu, 2023; Smith et al., 2023; Choi et al., 2023). Theoretically, Aghajanyan et al. (2021) found an intrinsic low-rank structure is critical for fine-tuning language mod- els, although this finding concerns full fine-tuning, not the setting that uses LoRA. Recently, Zeng & Lee (2024) an- alyzed the expressive power of LoRA. However, we still lack a sufficient theoretical understanding of why LoRA is effective in the sense of optimization and generalization. Matrix factorization. In this work, we utilize techniques developed in prior work on matrix factorization problems. Bach et al. (2008); Haeffele et al. (2014) established the sufficiency of low-rank parameterizations in matrix factor- ization problems, and their techniques have also been used in matrix completion (Ge et al., 2016), matrix sensing (Jin et al., 2023), and semidefinite programming (Bhojanapalli et al., 2018). 1.2. Organization Section 2 introduces the problem setting and reviews rel- evant prior notions and results. Section 3 proves the exis- tence of low-rank solutions. Section 4 proves LoRA has no spurious local minima and, therefore, establishes that (stochastic) gradient descent can find the low-rank global minima. Section 5 shows that the low-rank solution gener- alizes well. Finally, Section 6 presents simple experiments fine-tuning pre-trained models for different modalities. The experimental results validate our theory and provide further experimental insights. 2. Problem setting and preliminaries We primarily consider the setup of pre-trained large lan- guage models fine-tuned with LoRA. However, our theory does generally apply to other setups that utilize pre-training and LoRA fine-tuning, such as diffusion models. Matrix notation. For matrices A and B, let ∥A∥∗ denote the nuclear norm, ∥A∥F the Frobenius norm, and ⟨A, B⟩ = tr(A⊺B) the matrix inner product. We letSn and Sn + for the set of n × n symmetric and positive semi-definite matrices, respectively. Let R(·) and N(·) respectively denote the range and the null-space of a linear operator. Neural network. Let fΘ : X →RK be a neural network (e.g., a transformer-based model) parametrized by Θ, where X is the set of data (e.g., natural language text) and RK is the output (e.g., pre-softmax logits of tokens). K is the output dimension of fΘ, where K = k for k-class classification, K = 1 for binary classification, and K is the 2LoRA Training in the NTK Regime has No Spurious Local Minima dimension of the label Y when using mean square error loss. Assume the model has been pre-trained to Θ = Θ0, i.e., the pre-trained model is fΘ0 . Let W = ( W(1), . . . , W(T)) ⊂ Θ be a subset of the weights (e.g., dense layers in QKV-attention) with size W(i) ∈ Rmi×ni for i = 1, . . . , Tthat we choose to fine- tune. Let W0 = (W(1) 0 , . . . , W(T) 0 ) ⊂ Θ0 be their corre- sponding pre-trained weights. With slight abuse of notation, write fW to denote fΘ, where all parameters ofΘ excluding W are fixed to their corresponding values in Θ0. Fine-tuning loss. Assume we wish to fine-tune the pre- trained model with {(Xi, Yi)}N i=1, where N is the number of (fine-tuning) training data. (In many NLP tasks, it is not uncommon to have N < 100.) Denote δ = (δ(1), . . . , δ(T)) ⊂ Θ to be the change of W after the fine-tuning, i.e., fW0+δ is our fine-tuned model. We use the empirical risk ˆL(δ) = 1 N NX i=1 ℓ(fW0+δ(Xi), Yi), with some loss functionℓ. We assumeℓ(x, y) is convex, non- negative, and twice-differentiable with respect to x for any y. (This assumption holds for the cross-entropy loss and the mean squared error loss.) The empirical risk approximates the true risk L(δ) = E (X,Y )∼P \u0002 ℓ(fW0+δ(X), Y) \u0003 with some data distribution P. NTK regime. Under the NTK regime (also referred to as the lazy-training regime), the change of the network can be approximated by its first-order Taylor expansion fW0+δ(X) ≈ fW0 (X) + ⟨∇fW0 (X), δ⟩ (1) sufficiently well throughout (fine-tuning) training. To clarify, fW0+δ(X) ∈ RK, so the NTK regime requires the first- order Taylor expansion to be accurate for all coordinates: f(j) W0+δ(X) ≈ f(j) W0 (X) + ⟨∇f(j) W0 (X), δ⟩, where f(j) W is the j-th coordinate of fW for j = 1, . . . , K. The NTK regime is a reasonable assumption in fine-tuning if δ is small, and this assertion is supported by the empirical evidence of (Malladi et al., 2023). This prior work provides extensive experiments on various NLP tasks to validate that fine-tuning happens within the NTK regime for many, although not all, NLP tasks. Observation 2.1 (Malladi et al. (2023)) . When prompt- based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) is used, fine-tuning a pre-trained language model stays within the NTK regime. Motivated by this empirical observation, we define lin- earized losses ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001 ≈ ˆL(δ) and L(δ) = E (X,Y )∼P h ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001i ≈ L(δ). LoRA. We use the low-rank parameterization δ(i) = u(i)(v(i))⊺ ∈ Rmi×ni, where u(i) ∈ Rmi×r, v(i) ∈ Rni×r, for i ∈ {1, ··· , T}. Under the NTK regime, the empirical risk can be approxi- mated as ˆL(uv⊺) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 , where u =   u(1) ... u(T)   ∈ Rm×r, v =   v(1) ... v(T)   ∈ Rn×r with m = PT i=1 mi and n = PT i=1 ni, and G(Xi) = diag \u0000 ∇W(1) fW0 (Xi), . . . ,∇W(T) fW0 (Xi) \u0001 is an collection ofK m×n block diagonal matrices. To clar- ify, G(Xi) ∈ RK×m×n, so ⟨G(Xi), uv⊺⟩ ∈RK should be interpreted as K inner products of m ×n matrices where each matrices correspond to each coordinates of f. More specifically, G(j)(Xi) ∈ Rm×n and \u0000 ⟨G(Xi), uv⊺⟩ \u0001 j = ⟨G(j)(Xi), uv⊺⟩ for j = 1, . . . , K. Note that ˆL(uv⊺) under the NTK regime is non-convex in (u, v) so SGD-training does not converge to the global minimizer, in general. Weight decay on LoRA is nuclear norm regularization. The LoRA training of optimizing ˆL is often conducted with weight decay (Hu et al., 2021; Dettmers et al., 2023), which can be interpreted as solving minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , 3LoRA Training in the NTK Regime has No Spurious Local Minima with regularization parameter λ ≥ 0. This problem is equiv- alent to the rank-constrained nuclear-norm regularized prob- lem minimize δ, rankδ≤r ˆLλ(δ) ≜ ˆL(δ) + λ∥δ∥∗. This is due to the following lemma. Lemma 2.2 (Lemma 5.1 of (Recht et al., 2010)). Let r >0. For δ ∈ Rm×n such that rank(δ) ≤ r, ∥δ∥∗ = 1 2 min uv⊺=δ {∥u∥2 F + ∥v∥2 F |u ∈ Rm×r, v ∈ Rn×r}. (The connection between weight decay on Burer–Monteiro style low-rank factorization and nuclear norm regularization has been previously in different contexts not directly related to LoRA (Cabral et al., 2013; Pilanci & Ergen, 2020).) Second-order stationary points. Let ˆL: Rm×n → R be twice-continuously differentiable. We say U ∈ Rm×n is a (first-order) stationary point if ∇ˆL(U) = 0. We say U ∈ Rm×n is a second-order stationary point (SOSP) if ∇ˆL(U) = 0, ∇2 ˆL(U)[V, V] ≥ 0, for any direction V ∈ Rm×n. We say U is strict saddle if U is a first- but not second-order stationary point. Lastly, we say U ∈ Rm×n is a local minimum if there exists an open ball B that contains U and ˆL(U) ≤ ˆL(U′) for any U′ ∈ B. It follows that a local minimum is an SOSP. The following results, roughly speaking, establish that (stochastic) gradient descent only converges to SOSPs when a loss function is twice-continuously differentiable. Theorem 2.3 (Theorem 4.1 of (Lee et al., 2016)). Gradient descent on twice-differentiable function with random ini- tialization, almost surely, does not converge to strict saddle points. I.e., if gradient descent converges, it converges to an SOSP , almost surely. Theorem 2.4 (Informal, Theorem 1 of (Ge et al., 2015)). Stochastic gradient descent with noise on twice- differentiable strict saddle function (i.e., every stationary point is either a local minimum or a strict saddle) does not converge to strict saddle points with high probability. I.e., if stochastic gradient descent with noise converges, it converges to an SOSP with high probability. Therefore, if we can show that all SOSPs are global minima in our setup of interest, then (stochastic) gradient descent will only converge to global minima. 3. Low-rank solution exists In this section, we show that full fine-tuning in the NTK regime admits a low-rank solution of rank r ≲ √ N. The existence of a low-rank solution provides theoretical legit- imacy to using the low-rank parameterization of LoRA, which, of course, can only find low-rank solutions. Theorem 3.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global minimizer (not necessarily unique). Then there is a rank-r solution such that r(r+1) 2 ≤ KN . The assumption that ˆLλ(δ) has a global minimum is very mild; it is automatically satisfied if λ >0. When λ = 0, the assumption holds if ℓ is the mean squared error loss. The inspiration for Theorem 3.1 comes from the classical results of (Barvinok, 1995; Pataki, 1998; 2000) that establish that semi-definite programs (which have symmetric positive semi-definite matrices as optimization variables) admit low- rank solutions. We clarify that Theorem 3.1 does not require δ to be symmetric nor any notion of “semi-definiteness” (δ is not even square). Proof sketch of Theorem 3.1. We quickly outline the key ideas of the proof while deferring the details to Appendix A. We can show that finding δ⋆ λ ∈ argminδ ˆLλ(δ) with rank(δ⋆ λ) = r is equivalent to finding a rank-r global mini- mum of F : S(m+n) + → R where F(Z) = ˆL( ¯Z) + λ 2 tr(Z) and ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of Z such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . (2) Now suppose Z⋆ ∈ S(m+n) + is a global minimizer of F. Define S(Z⋆) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆)} and a linear operator A: S(m+n) → RKN as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Now let rank(Z⋆) = r and assume {0} = S(Z⋆) ∩ N(A). Then by dimension counting, we have the following inequal- 4LoRA Training in the NTK Regime has No Spurious Local Minima Figure 1.Geometric intuition of Theorem 3.1. The three dimensional space describes the space of 2 by 2 matrices \u0014 1 x y z \u0015 . The surface z = xy represents the rank 1 matrices. The blue region on the surface correspond to the region of smaller objective values, and the set of global minima are depicted with purple. (Left) Plot of (a) with N = 1. The set of global minima is a plane, and the intersection with the surface z = xy (curve) is the set of rank-1 global minima. (Middle) Plot of (b) with N = 2. the set of global minima is a line, and the intersection with the surface (two dots) is the set of rank 1 global minima. (Right) Plot of (c) with N = 3. The set of global minima is a line, and there is no intersection with the surface, i.e., there is no global minimum of rank-1 but admits a rank-2 global minima. ity. 0 = dimS(Z⋆) + dimN(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(m+n)) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆) − KN If there exists nonzero Z ∈ S(m+n) such that Z ∈ S(Z⋆) ∩ N(A), then we can show that there exists nonzero t ∈ R such that Z⋆ + tZ is also a global minimizer of F with strictly lower rank. Replace Z⋆ with Z⋆ + tZ and repeat this process until we find a solution Z⋆ with {0} = S(Z⋆) ∩ N(A). Together with the fact that dimS(Z⋆) = r(r+1) 2 , we have the desired result. Illustration of Theorem 3.1. The following toy example illustrates the geometric intuition of Theorem 3.1. Let ℓ be the mean square error loss, K = 1, δ = \u0014w x y z \u0015 , and λ = 0 (no regularization). Then consider the following objective functions each for N = 1, 2, and 3: ˆL0(δ) = (x + y)2 (a) ˆL0(δ) = 1 2(z + 4)2 + 1 2(x + y)2 (b) ˆL0(δ) = 1 3(w − 1)2 + 1 3(z − 4)2 + 1 3( √ 3x + √ 3y)2 (c) The set of low-rank (rank-1) solutions for the three objec- tives are depicted in Figure 1. 4. GD and LoRA finds low-rank solution In this section, we show that the optimization landscape with LoRA in the NTK regime has no spurious local minima if the LoRA parameterization uses rank r ≳ √ N and if we consider an ε-perturbed loss. This implies that optimizers such as stochastic gradient descent only converge to the low-rank global minimizers. Theorem 4.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global min- imizer (not necessarily unique) and r(r+1) 2 > KN. Con- sider the perturbed loss function ˆLλ,P defined as ˆLλ,P (u, v) ≜ ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 ∈ R(m+n)×r and P ∈ S(m+n) + is pos- itive semi-definite. Then, for almost all nonzero P (with respect to the Lebesgue measure on S(m+n) + ⊂ S(m+n) ∼= R (m+n)(m+n+1) 2 ), all SOSPs of ˆLλ,P are global minimizers of ˆLλ,P . To clarify, the conclusion that ‘all SOSPs are global mini- mizers’ holds with probability 1 even if the distribution of P is supported on {P ∈ S(m+n) + : ∥P∥ ≤ε} for arbitrarily small ε >0. In the practical LoRA fine-tuning setup where no perturbation is used and P = 0 is set deterministically, Theorem 4.1 does not apply. However, we can neverthe- less interpret the result of Theorem 4.1 to show that LoRA fine-tuning generically has no spurious local minima. 5LoRA Training in the NTK Regime has No Spurious Local Minima If we do use a randomly generated small perturbation P so that Theorem 4.1 applies, the solution to the perturbed problem with small P does not differ much from that of the unperturbed problem with P = 0 in the following sense. Corollary 4.2. Consider the setup of Theorem 4.1 and let ε > 0. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). Assume P is randomly sampled with a probability distribution supported in {P ∈ S(m+n) + : ∥P∥F < ε} and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Then for any SOSP (ˆu, ˆv) of ˆLλ,P ˆLλ(ˆuˆv⊺) ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗ = min δ ˆLλ(δ) + 2ε∥δ⋆ λ∥∗. I.e., if (ˆu, ˆv) is an SOSP (and thus a global minimizer by Theorem 4.1) of the perturbed loss ˆLλ,P , then it is an ε- approximate minimizer of the unperturbed loss ˆLλ. So if r(r+1) 2 > KN, then Theorem 2.3, Theorem 2.4, and Corollary 4.2 together establish that (stochastic) gradient descent finds a ˆuˆv⊺ such that its unperturbed empirical risk is ε-close to the the minimum unperturbed empirical risk. 4.1. Proof outlines The proof is done by continuing our analysis of global mini- mum of ˆLλ(δ). Given that low-rank solution exists, which we proved in the previous section, recall that LoRA training with weight decay is equivalent to solving argmin u,v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F . In this section, we relate SOSPs with global minimum, which opens the chance to find a global minimum by using gradient-based optimization methods. We start the analysis from the following lemma, which is a prior characterization of SOSPs in the matrix factorization. Lemma 4.3. (Theorem 2 of (Haeffele et al., 2014)) Let G: S(m+n) + → R be a twice differentiable convex function with compact level sets, H : S(m+n) + → R be a proper con- vex lower semi-continuous function, and r > 0. If the function F : U 7→ G(UU ⊺) +H(UU ⊺) defined over matri- ces U ∈ R(m+n)×r has a second order staionary point at a rank-deficient matrix U, then UU ⊺ is a global minimum of G + H. We build our analysis upon Lemma 4.3. However, Lemma 4.3 is not directly applicable to our setting since it requires that the SOSP must be rank-deficient. However, this can be effectively circumvented by employing a per- turbed empirical risk: minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 , and P is a positive semi-definite matrix. Now we get the following lemma by applying Lemma 4.3 to the perturbed empricial risk. Lemma 4.4. Fix λ ≥ 0. Assume ˆLλ(δ) has a global min- imum (not necessarily unique), P ∈ S(m+n) + is nonzero positive semi-definite, and r >0. If ˆQ = \u0014ˆu ˆv \u0015 ∈ R(m+n)×r is a rank deficient SOSP of ˆLλ,P (u, v) = ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, then ˆQ is a global minimum of ˆLλ,P (u, v). Proof. Define G, H: S(m+n) + → R to be G(X) = λ 2 tr(X) + ⟨P, X⟩, H (X) = ˆL( ¯X) where ¯X is the off-diagonal submatrix of X defined in (2). Note that G has compact level set for every λ ≥ 0 since tr(X) ≥ 0 and P, Xare positive semi-definite, concluding that ˆQλ,P is a global minimum of F(Q) ≜ G(QQ⊺) + H(QQ⊺) = ˆLλ,P (u, v). We now give a detailed analysis of the proof of Theorem 4.1. The structure of the proof is inspired by the original work of Pataki (1998) and followed by Burer & Monteiro (2003); Boumal et al. (2016); Du & Lee (2018). The proof uses an application of Sard’s theorem of differential geometry. The argument is captured in Lemma 4.5, and its proof is deferred to Appendix B. Lemma 4.5. Let M be m-dimensional smooth manifold embedded in Rd and V be a linear subspace of Rd with dimension n. If m + n < d, then the set M + V = {p + v : p ∈ M, v∈ V } has Lebesgue measure zero in Rd. Proof of Theorem 4.1. We show that second-order station- ary point ˆQλ,P = \u0014ˆu ˆv \u0015 is rank-deficient for almost all posi- tive semi-definite P, then use Lemma 4.4 to complete the proof. Denote f(j) for the j-th coordinate of f. For simplic- ity of notations, define ˆY (j) i ≜ f(j) W0 (Xi) + ⟨G(j)(Xi), uv⊺⟩, 6LoRA Training in the NTK Regime has No Spurious Local Minima and v(j) i ≜ 1 N ∂ ∂ ˆY (j) i ℓ( ˆYi, Yi) for 1 ≤ i ≤ N and 1 ≤ j ≤ K, which depends on u and v. Then for v = {v(j) i } ∈RKN define S(v) ≜ NX i=1 KX j=1 v(j) i G(j)(Xi) ∈ Rm×n. Then by first-order gradient condition, we have  \u0014 0 S(v) S(v)⊺ 0 \u0015 + λI + P | {z } ≜M ! ˆQλ,P = 0 We observe that the range of ˆQλ,P ∈ R(m+n)×r is in the nullspace of M ∈ S(m+n). We now suppose ˆQλ,P has full rank, i.e., rank( ˆQλ,P ) = r. Hence, we have the following inequality: r = rank( ˆQλ,P ) ≤ dim N(M) ≤ m + n Now for r ≤ s ≤ m + n and s ∈ Z, define As = n P : P = M − λI, M∈ S(m+n), dimN(M) = s o . Then from Proposition 2.1 of (Helmke & Shayman, 1995), As is a smooth manifold embedded in R (m+n)(m+n+1) 2 ∼= S(m+n) with dimension dimAs = (m + n + 1)(m + n) 2 − s(s + 1) 2 . Now by definition of P, we know that P ∈ m+n[ s=r (As + R(S)) where “ + ”is the set-sum (Minkowski sum) and R(S) is the range of S(v) in R (m+n)(m+n+1) 2 for any v ∈ RKN . The dimensions can be bounded by dimAs ≤ (m + n)(m + n + 1) 2 − r(r + 1) 2 for r ≤ s ≤ m + n and dimR(S) ≤ KN. Therefore given that r(r+1) 2 > KN, we have dimAs + dimR(S) < (m + n)(m + n + 1) 2 . Then, by Lemma 4.5, which is effectively an application of Sard’s theorem, we can conclude As + R(S) is a measure- zero set, and the finite union of such measure-zero sets is measure-zero. This implies that every P that makes ˆQλ,P to be of full rank must be chosen from measure-zero subset of S(m+m) + ⊂ S(m+n). Therefore we may conclude that rank( ˆQλ,P ) < rfor almost every nonzero positive semi- definite P. Proof of Corollary 4.2. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). We observe the following chain of inequalities. ˆL(ˆδ) + λ∥ˆδ∥∗ ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F + ⟨P, ˆQ ˆQ⊺⟩ = ˆLλ,P (ˆu, ˆv), where the first inequality of is from Lemma 2.2, the second is from P and ˆQ ˆQ⊺ being positive semi-definite. On the other hand, we can find u⋆ and v⋆ such that δ⋆ λ = u⋆v⋆⊺ and ∥δ⋆ λ∥∗ = 1 2 (∥u⋆∥2 F + ∥v⋆∥2 F ) by using Lemma 2.2. Now take Q⋆ = \u0014u⋆ v⋆ \u0015 , then we get ˆLλ,P (u⋆, v⋆) = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ⟨P, Q⋆Q⋆⊺⟩ ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆Q⋆⊺∥F ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥u⋆∥2 F + ε∥v⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗, where the first inequality is Cauchy–Schwartz inequality, and the second inequality is from sub-multiplicativity of ∥ · ∥F . Moreover by Theorem 4.1, ˆLλ,P (ˆu⋆, ˆv⋆) ≤ ˆLλ,P (u⋆, v⋆), and this happens for almost sure, since we sampled P from a probability distribution which is absolutely continuous with respect to the Lebesgue measure on R (m+n)(m+n+1) 2 ∼= S(m+n). 5. Low-rank LoRA solution generalizes well In this section, we establish a generalization guarantee for the low-rank solution obtained by minimizing the perturbed loss ˆLλ,P of Theorem 4.1. For simplicity, we restrict the following main result to the cross-entropy loss. General- ization guarantees for general convex, non-negative, and twice continuously differentiable losses, are provided as Theorem C.6 in Appendix C. Theorem 5.1. Assume ℓ is cross-entropy loss. Assume the population risk L has a minimizer (not necessarily unique) and denote it as δ⋆ true ∈ argminδ L(δ). Assume δ⋆ true ̸= 0. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely 7LoRA Training in the NTK Regime has No Spurious Local Minima (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 2.Training curves (training loss vs. epochs) on different NLP tasks. with respect to the random data X ∼ P. Let ε >0, η ∈ (0, 1), and λ = 2(2 + ε) √ KR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ). Consider the setup of Corollary 4.2 withP randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺)−L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N \u0012 2 + r log 1 η \u0013 . In the context of fine-tuning, where the target task is closely related to the pre-training task, it is natural to assume that δ⋆ true in Theorem 5.1 is “small”. The proof, deferred to Appendix C, utilizes standard arguments with Rademacher complexity. 6. Experiments In this section, we conduct simple experiments on fine- tuning linearized pre-trained models to validate our theory.1 1Code available at https://github.com/UijeongJang/LoRA-NTK. Experimental setup on NLP tasks. We use prompt-based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) and consider the same architecture and dataset as in (Malladi et al., 2023), which empirically verifies that with prompt- based fine-tuning, the fine-tuning dynamics stay within the NTK regime. We present the results of six NLP tasks that were also considered in (Malladi et al., 2023): sen- timent analysis (SST-2, MR, CR), natural language infer- ence (QNLI), subjectivity (Subj), and paraphrase detection (QQP). We optimize a linearized RoBERTa-base (Liu et al., 2019) model with dataset of size 32 ( N = 32) with two labels (K = 2) using cross entropy loss. With LoRA rank r ≥ 11, our theory guarantees that no spurious local min- ima exist. For a baseline comparison, we also perform full fine-tuning (without LoRA) on the linearized model. The training curves are presented in Figure 2, and additional details are provided in Appendix D. Results showing test accuracy are also presented in Appendix D. Experimental setup on image and speech classification tasks. We use a pre-trained vision transformer (Dosovit- skiy et al., 2021) and fine-tune it on the bean disease dataset (Makerere AI Lab, 2020) to perform an image classification task with 3 labels. We use dataset of size 48 with three labels. Similar to our experiments on NLP tasks, we find that training curves converge to the same loss value, where the rates of convergence differ. For speech classification, we use a pre-trained wav2vec2 (Baevski et al., 2020) model and fine-tune it on a SUPERB dataset (Yang et al., 2021) to perform a speech classification 8LoRA Training in the NTK Regime has No Spurious Local Minima (a) Image classification  (b) Speech classification Figure 3.Training curves (training loss vs. epochs) on image and speech classification tasks. task with 4 labels. We use a dataset of size 64 with four labels. We also find that the training curves converge to the same loss value. The details are the same as with the image classification task. The training curves of both image and speech data are pre- sented in Figure 3, and additional details are provided in Appendix D. Empirical observation. The experiments validate our the- ory as the training curves converge to the same globally optimal loss value. However, we do observe that therates of convergence differ. When the LoRA rank is higher or when full fine-tuning is performed and LoRA is not used, fine-tuning converges faster. Indeed, our theory ensures that spurious local minima do not exist, but it says nothing about how convex or favorable the landscape may or may not be. Our intuitive hypothesis is that using lower LoRA rank creates unfavorable regions of the loss landscape, such as plateaus or saddle points, and they slow down the gradient descent dynamics. If this hypothesis is generally true, we face an interesting tradeoff: lower LoRA rank reduces memory cost and per- iteration computation cost but increases the number of it- erations needed for convergence. Then, using a very low LoRA rank may be suboptimal not due to representation power, presence of spurious local minima, or poor general- ization guarantees, but rather due to unfavorable flat training landscapes slowing down convergence. Exploring this phe- nomenon and designing remedies is an interesting direction for future work. 7. Conclusion In this work, we present theoretical guarantees on the train- ability and generalization capabilities of LoRA fine-tuning of pre-trained models. Together with the work of Zeng & Lee (2024), our results represent a first step in theoretically analyzing the LoRA fine-tuning dynamics of pre-trained models by presenting guarantees (upper bounds). For future work, carrying out further refined analyses under more spe- cific assumptions, relaxing the linearization/NTK regime assumption through a local analysis, better understanding the minimum rank requirement through lower bounds, and, motivated by the observation of Section 6, analyzing the tradeoff between training rate and LoRA rank are exciting directions. Acknowledgments UJ and EKR were supported by the Samsung Science and Technology Foundation (Project Number SSTF-BA2101- 02) and the National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIP) [NRF- 2022R1C1C1010010]. JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. We thank Jungsoo Kang for the discussion on the proof of Lemma 4.5. We also thank Jisun Park for providing valuable feedback. Impact statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrin- sic dimensionality explains the effectiveness of language model fine-tuning. Association for Computational Lin- guistics, 2021. Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. International Conference on Machine Learning, 2019a. Allen-Zhu, Z., Li, Y ., and Song, Z. On the convergence rate of training recurrent neural networks. Neural Information Processing Systems, 2019b. Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely 9LoRA Training in the NTK Regime has No Spurious Local Minima wide neural net. Neural Information Processing Systems, 2019. Bach, F. Learning Theory from First Principles . Draft, 2023. Bach, F., Mairal, J., and Ponce, J. Convex sparse matrix factorizations. arXiv preprint arXiv:0812.1869, 2008. Baevski, A., Zhou, Y ., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Neural Information Processing Systems, 2020. Barron, A. R. Universal approximation bounds for super- positions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930–945, 1993. Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. Bartlett, P. L., Boucheron, S., and Lugosi, G. Model selec- tion and error estimation. Machine Learning, 48:85–113, 2002. Bartlett, P. L., Bousquet, O., and Mendelson, S. Local rademacher complexities. The Annals of Statistics, 33(4): 1497–1537, 2005. Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrally- normalized margin bounds for neural networks. Neural Information Processing Systems, 2017. Barvinok, A. I. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computational Geometry, 13:189–202, 1995. Bengio, Y . and Delalleau, O. On the expressive power of deep architectures. Algorithmic Learning Theory, 2011. Bhojanapalli, S., Boumal, N., Jain, P., and Netrapalli, P. Smoothed analysis for low-rank solutions to semidefi- nite programs in quadratic penalty form. Conference On Learning Theory, 2018. Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J. Transformers learn through gradual rank increase. Neural Information Processing Systems, 2023. Boumal, N., V oroninski, V ., and Bandeira, A. The non-convex Burer–Monteiro approach works on smooth semidefinite programs. Neural Information Processing Systems, 29, 2016. Bousquet, O. and Elisseeff, A. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. Burer, S. and Monteiro, R. D. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329– 357, 2003. Cabral, R., De la Torre, F., Costeira, J. P., and Bernardino, A. Unifying nuclear norm and bilinear factorization ap- proaches for low-rank matrix decomposition. Interna- tional Conference on Computer Vision, 2013. Chen, Z., Cao, Y ., Gu, Q., and Zhang, T. A generalized neu- ral tangent kernel analysis for two-layer neural networks. Neural Information Processing Systems, 2020. Choi, J. Y ., Park, J., Park, I., Cho, J., No, A., and Ryu, E. K. LoRA can replace time and class embeddings in diffusion probabilistic models. NeurIPS 2023 Workshop on Diffusion Models, 2023. Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of Control, Signals and Systems, 2(4):303–314, 1989. Delalleau, O. and Bengio, Y . Shallow vs. deep sum-product networks. Neural Information Processing Systems, 2011. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: efficient finetuning of quantized llms. Neural Information Processing Systems, 2023. Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y . Sharp minima can generalize for deep nets. International Con- ference on Machine Learning, 2017. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. Du, S. and Lee, J. On the power of over-parametrization in neural networks with quadratic activation. International Conference on Machine Learning, 2018. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. International Conference on Machine Learning, 2019. Du, S. S., Jin, C., Lee, J. D., Jordan, M. I., Singh, A., and Poczos, B. Gradient descent can take exponential time to escape saddle points. Neural Information Processing Systems, 2017. Duan, Y ., Ji, G., Cai, Y ., et al. Minimum width of leaky- relu neural networks for uniform universal approximation. International Conference on Machine Learning, 2023. 10LoRA Training in the NTK Regime has No Spurious Local Minima Fu, Z., Yang, H., So, A. M.-C., Lam, W., Bing, L., and Collier, N. On the effectiveness of parameter-efficient fine-tuning. AAAI Conference on Artificial Intelligence, 2023. Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. Association for Computational Linguistics, 2021. Ge, R., Huang, F., Jin, C., and Yuan, Y . Escaping from saddle points—online stochastic gradient for tensor de- composition. Conference on Learning Theory, 2015. Ge, R., Lee, J. D., and Ma, T. Matrix completion has no spurious local minimum. Neural Information Processing Systems, 2016. Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as pro- grammable computers. International Conference on Ma- chine Learning, 2023. Haeffele, B., Young, E., and Vidal, R. Structured low-rank matrix factorization: optimality, algorithm, and applica- tions to image processing. International Conference on Machine Learning, 2014. Hardt, M., Recht, B., and Singer, Y . Train faster, generalize better: stability of stochastic gradient descent. Interna- tional Conference on Machine Learning, 2016. Helmke, U. and Shayman, M. A. Critical points of matrix least squares distance functions. Linear Algebra and its Applications, 215:1–19, 1995. Hornik, K., Stinchcombe, M., and White, H. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551–560, 1990. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., Chen, W., et al. LoRA: low-rank adaptation of large language models. International Conference on Learning Representations, 2021. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker- nel: convergence and generalization in neural networks. Neural Information Processing Systems, 2018. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. How to escape saddle points efficiently. Interna- tional Conference on Machine Learning, 2017. Jin, J., Li, Z., Lyu, K., Du, S. S., and Lee, J. D. Under- standing incremental learning of gradient descent: A fine-grained analysis of matrix sensing. International Conference on Machine Learning, 2023. Johnson, W. and Lindenstrauss, J. Extensions of lipschitz maps into a hilbert space. Contemporary Mathematics, 26:189–206, 1984. Koltchinskii, V . and Panchenko, D. Rademacher processes and bounding the risk of function learning. In Gin ´e, E., Mason, D. M., and Wellner, J. A. (eds.), High Dimen- sional Probability II, pp. 443–457. Springer, 2000. Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B. Gra- dient descent only converges to minimizers. Conference on Learning Theory, 2016. Lialin, V ., Muckatira, S., Shivagunde, N., and Rumshisky, A. ReLoRA: high-rank training through low-rank up- dates. Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Re- source Optimization, 2023. Liu, L., Liu, X., Gao, J., Chen, W., and Han, J. Under- standing the difficulty of training transformers. Empirical Methods in Natural Language Processing, 2020. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . RoBERTa: a robustly optimized BERT pretraining ap- proach. arXiv preprint arXiv:1907.11692, 2019. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres- sive power of neural networks: a view from the width. Neural Information Processing Systems, 2017. Makerere AI Lab. Bean disease dataset, 2020. URLhttps: //github.com/AI-Lab-Makerere/ibean/. Malladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S. A kernel-based view of language model fine-tuning. Inter- national Conference on Machine Learning, 2023. Maurer, A. A vector-contraction inequality for rademacher complexities. Algorithmic Learning Theory, 2016. McDiarmid, C. et al. On the method of bounded differences. Surveys in Combinatorics, 141(1):148–188, 1989. Pataki, G. On the rank of extreme matrices in semidefi- nite programs and the multiplicity of optimal eigenval- ues. Mathematics of Operations Research, 23(2):339– 358, 1998. Pataki, G. The geometry of semidefinite programming. In Wolkowicz, H., Saigal, R., and Vandenberghe, L. (eds.), Handbook of Semidefinite Programming: Theory, Algo- rithms, and Applications, pp. 29–65. Springer, 2000. 11LoRA Training in the NTK Regime has No Spurious Local Minima Pilanci, M. and Ergen, T. Neural networks are convex reg- ularizers: exact polynomial-time convex optimization formulations for two-layer networks. International Con- ference on Machine Learning, 2020. Polyak, B. T. Introduction to Optimization . New York, Optimization Software, 1987. Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010. Ryu, S. Low-rank adaptation for fast text-to-image diffu- sion fine-tuning, 2023. URL https://github.com/ cloneofsimo/lora. Schick, T. and Sch¨utze, H. Exploiting cloze questions for few shot text classification and natural language inference. Association for Computational Linguistics, 2021. Smith, J. S., Hsu, Y .-C., Zhang, L., Hua, T., Kira, Z., Shen, Y ., and Jin, H. Continual diffusion: continual customiza- tion of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. Soltanolkotabi, M., Javanmard, A., and Lee, J. D. Theo- retical insights into the optimization landscape of over- parameterized shallow neural networks. IEEE Transac- tions on Information Theory, 65(2):742–769, 2018. Sridharan, K., Shalev-Shwartz, S., and Srebro, N. Fast rates for regularized objectives.Neural Information Processing Systems, 21, 2008. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention is all you need. Neural Information Processing Systems, 2017. Wei, A., Hu, W., and Steinhardt, J. More than a toy: random matrix models predict how real-world neural representa- tions generalize. International Conference on Machine Learning, 2022a. Wei, C., Chen, Y ., and Ma, T. Statistically meaningful approximation: a case study on approximating turing ma- chines with transformers. Neural Information Processing Systems, 2022b. Wu, L., Zhu, Z., et al. Towards understanding generalization of deep learning: perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Yang, S.-w., Chi, P.-H., Chuang, Y .-S., Lai, C.-I. J., Lakho- tia, K., Lin, Y . Y ., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., et al. Superb: Speech processing universal performance benchmark. Interspeech, 2021. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: from LyCORIS fine-tuning to model evaluation. International Conference on Learning Representations, 2024. Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers universal approximators of sequence-to-sequence functions? International Confer- ence on Learning Representations, 2019. Zeng, Y . and Lee, K. The expressive power of low-rank adaptation. International Conference on Learning Repre- sentations, 2024. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107– 115, 2021. Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. Why are adaptive methods good for attention models? Neural Information Processing Systems, 2020. Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. Machine Learning, 109(3):467–492, 2020. 12LoRA Training in the NTK Regime has No Spurious Local Minima A. Omitted proof of Theorem 3.1 Here, we explain the details in the proof of Theorem 3.1. We first prove the equivalence of minimize δ∈Rm×n ˆL(δ) + λ∥δ∥∗ (P) and minimize Z∈S(m+n) + ˆL( ¯Z) + λ 2 tr(Z) (Q) where ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of X such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . Lemma A.1. The following two statements hold. 1. Fix λ ≥ 0 and suppose (P) has a global minimizer (not necessarily unique). Let δ⋆ λ ∈ Rm×n be a global minimizer of (P). Then there exists an Z⋆ λ ∈ S(m+n) + induced from δ⋆ λ such that Z⋆ λ is a global minimizer of (Q), rank(Z⋆ λ) = rank(δ⋆ λ), and has same objective value. 2. Fix λ ≥ 0 and suppose (Q) has a global minimizer (not necessarily unique). Let Z⋆ λ ∈ S(m+n) + be a global minimum of (Q). Then ¯Z⋆ λ ∈ Rm×n is a global minimizer of (P) such that rank( ¯Z⋆ λ) = min(m, n,rank(Z⋆ λ)) and has same objective value. Proof. We prove the two statements at once. Let δ⋆ λ ∈ Rm×n be a global minimizer of (P) and let r = rank(δ⋆ λ). Then by Lemma 2.2, there exists u ∈ Rm×r and v ∈ Rn×r such that ∥δ⋆ λ∥∗ = 1 2 (∥u∥2 F + ∥v∥2 F ) and uv⊺ = δ⋆ λ. Take Z⋆ λ = \u0014u v \u0015\u0002u⊺ v⊺\u0003 = \u0014uu⊺ uv⊺ vu⊺ vv⊺ \u0015 ∈ S(m+n) + . Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = \r\r\r \u0014u v \u0015\r\r\r 2 F = ∥u∥2 F + ∥v∥2 F = 2∥δ⋆ λ∥∗, (Q) with Z⋆ λ has the same objective value with (P) with δ⋆ λ and rank(δ⋆ λ) = rank(Z⋆ λ) = r. Conversely, let Z⋆ λ ∈ S(m+n) + be a global minimizer of (Q) and let rank(Z⋆ λ) = r. Note that r may be larger than m or n. Then there exists Q = \u0014u v \u0015 ∈ R(m+n)×r such that QQ⊺ = Z⋆ λ. Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = ∥Q∥2 F = ∥u∥2 F + ∥v∥2 F ≥ 2∥uv⊺∥∗ = 2∥ ¯Z⋆ λ∥∗, the objective value of (P) with ¯Z⋆ λ ∈ Rm×n has less than or equal to minimum objective value of (Q) and rank( ¯Z⋆ λ) = min(m, n, r). If there exists m × n matrix whose objective value of (P) is strictly less than the minimum objective value of (Q), then we repeat the same step that was applied on δ⋆ λ to induce a solution of (Q) with strictly less objective value, which is a contradiction. Conversely, if there exists positive semi-definite matrix of size m + n whose objective value of (Q) is strictly less than the minimum objective value of (P), then we repeat the same step applied on Z⋆ λ to induce a solution of (P) with strictly less objective value, which is also a contradiction. Therefore if one of (P) and (Q) has a global minimizer, the other must have a global minimizer with same objective value. Next lemma states that if the rank of the global minimizer of (Q) is sufficiently large, then we can find an another solution with strictly less rank. Lemma A.2. Suppose X ∈ Sn + and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists nonzero t∗ ∈ R such that X + t∗Z is positive semi-definite and rank(X + t∗Z) < rank(X). 13LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let r = rank(X). Suppose Q ∈ Rn×r is a matrix where its columns are basis to R(X). Now suppose µ1(Q⊺(X + tZ)Q) > 0 for all t ∈ R where µ1(·) denotes the smallest eigenvalue (note thatµ1(·) is continuous). Then Q⊺(X +tZ)Q ∈ Sr should be positive definite for all t. For contradiction, take v ∈ R(Z) ⊆ R(X) = R(Q) to be an eigenvector of nonzero eigenvalue of Z. Since v⊺Xv >0 and v⊺Zv ̸= 0, there exists some t such that v⊺(X + tZ)v <0. Now take w ∈ Rr such that Qw = v. Then it follows that w⊺(Q⊺(X + tZ)Q)w <0, which is a contradiction. This implies that there exists t⋆ ̸= 0 such that µ1(Q⊺(X + t∗Z)Q) = 0, Hence we have r >rank(Q⊺(X + t∗Z)Q) = rank(X + t∗Z) and Q⊺(X + t∗Z)Q is positive semi-definite. To show thatX + t∗Z is positive semi-definite, take any x ∈ Rn and consider the decomposition x = Qy + z where y ∈ Rr and z ∈ N(Q) = N(X) ⊆ N(Z). Then, we have y⊺(X + t⋆Z)y = (y⊺Q⊺ + z⊺)(X + t∗Z)(Qy + z) = y⊺Q⊺(X + t∗Z)Qy ≥ 0. Finally, the following lemma and its proof are similar to the previous one, but we state it separately for the sake of clarity. It will be used in the proof of Theorem 3.1. Lemma A.3. Suppose X ∈ Sn + which is nonzero and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists t∗ > 0 such that X ± t∗Z is positive semi-definite. Proof. Let rank(X) = r and {y1, . . . , yr} be orthonormal eigenvectors of nonzero eigenvalues of X. Since y⊺ i Xyi > 0 for all yi, i = 1, ··· , r, there exists an interval (−ai, ai) for ai > 0 such that y⊺ i (X ± tZ)yi ≥ 0 for t ∈ (−ai, ai). Take t∗ = min{a1, . . . , ar}. Then t∗ satisfies the statement of the theorem. Now we provide the complete proof of Theorem 3.1. Proof of Theorem 3.1. Suppose Z⋆ λ ∈ S(m+n) + is a global minimizer of F(Z) = ˆL( ¯Z) + λ 2 tr(Z) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), ¯Z⟩, Yi \u0001 + λ 2 tr(Z) which is induced from δ⋆ λ ∈ Rm×n by Lemma A.1. Suppose there exists nonzero symmetric matrix Z such that Z ∈ S(Z⋆ λ) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆ λ)} and ⟨G(Xi), Z⟩ = 0 for 1 ≤ i ≤ N. In other words, Z ∈ S(Z⋆ λ) ∩ N(A) where A: S(m+n) → RKN is a linear operator defined as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Then there exists t >0 such that Z⋆ λ ± tZ is positive semi-definite by Lemma A.3, since Z⋆ must be nonzero. Therefore tr(Z) = 0, otherwise it will contradict the minimality of Z⋆ λ. Also we know that there exists nonzero t∗ ∈ R such that Z⋆ λ + t∗Z is also positive semi-definite with strictly lower rank by Lemma A.2. Since tr(Z) = 0, Z⋆ λ + t∗Z is also a global minimizer of F. Replace Z⋆ λ with Z⋆ λ + tZ and repeat this process until we find a solution Z⋆ λ with {0} = S(Z⋆ λ) ∩ N(A). Now we let rank(Z⋆ λ) = r. Then by dimension counting, we have the following inequality. 0 = dimS(Z⋆ λ) + dimN(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) − KN + dim(S(m+n)) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆ λ) − KN 14LoRA Training in the NTK Regime has No Spurious Local Minima Now we prove that dimS(Z⋆ λ) = r(r+1) 2 to complete the proof. Consider the diagonalization Z⋆ λ = UΛU⊺ where U is a orthogonal matrix. Since the dimension of the subspace is invariant under orthogonal transformations, we have dimS(Z⋆ λ) = dimS(Λ) = dim{Z ∈ S(m+n) : R(Z) ⊆ R(Λ)} where Λ is diagonal matrix with nontrivial entries in the leading principle minor of size r × r. This restricts the symmetric matrix Z to have nontrivial entries only in the leading r × r block. Hence, dimS(Z⋆ λ) = r(r+1) 2 . B. Omitted proof of Lemma 4.5 We prove Lemma 4.5 in this section. Proof of Lemma 4.5. Let ΠV ⊥ : Rd → V ⊥ be the orthogonal projection onto the orthogonal complement of V in Rd. Then, ΠV ⊥|M : M →V ⊥ is a smooth mapping between manifolds. Since dimV ⊥ = d − n > m= dimM, p is singular for all p ∈ M. Therefore ΠV ⊥(M) has measure zero in Rd−n by Sard’s theorem. Note that M + V ⊆ ΠV ⊥(M) + V and the measure of ΠV ⊥(M) + V in Rd is zero. This concludes that M + V is measure-zero in Rd. As a remark, the prior works of (Boumal et al., 2016; Du & Lee, 2018) also use dimension-counting arguments that would warrant the use of Lemma 4.5, but they do not provide a precise justification. Our Theorem 4.1 makes a similar argument, but does so fully rigorous through Lemma 4.5. C. Generalization guarantee In this section, let ℓ(·, ·) be our loss function which is convex, non-negative, and twice-differentiable on the first argument. Then, our empirical risk is ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 . We start the analysis from this non-regularized risk and expand it to regularized ones. We assume that our model is class of affine predictors X 7→ fW0 (X) + ⟨G(X), δ⟩ for given data X. Now we apply the theory of Rademacher complexity to derive the upper bound of the generalization bound. To begin with, we start with introducing the classical result in probability theory from (McDiarmid et al., 1989) without proof. Lemma C.1. (McDiarmid inequality) Let X1, . . . , XN ∈ Xbe i.i.d N random samples from dataset X. Let g : XN → R be a function satisfying the following property with c >0: |g(X1, . . . , Xi−1, Xi, Xi+1, . . . , XN ) − g(X1, . . . , Xi−1, X′ i, Xi+1, . . . , XN )| ≤c for all X1, . . . , XN , X′ i ∈ X. Then, for all ε >0, P(|g(X1, . . . , XN ) − E[g(X1, . . . , XN )]| ≥ε) ≤ exp \u0012 − 2ε2 Nc2 \u0013 . Now, we define the Rademacher complexity of the class of functions H from X to R: RN (H) = Eε,D   sup h∈H 1 N NX i=1 εih(Xi) ! , where {εi}1≤i≤N are independent Rademacher random variables, and D = {X1, . . . , XN } is N random samples from X. In our analysis, we will focus on class of affine predictors Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ and composition of affine predictors with loss Xi 7→ ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi). Rademacher complexities are closely related to upper bounds on generalization bound due to the following lemma. 15LoRA Training in the NTK Regime has No Spurious Local Minima Lemma C.2. Let RN (H) be the Rademacher complexity of the class of functions H from X to R and X1, . . . , XN are N samples from X. Then the following inequality holds. E \" sup h∈H   1 N NX i=1 h(Xi) − E[h(X)] !# ≤ 2RN (H), E \" sup h∈H   E[h(X)] − 1 N NX i=1 h(Xi) !# ≤ 2RN (H). Proof. The proof is by using standard symmetrization arguments. We defer its proof to Theorem 8 of (Bartlett & Mendelson, 2002), or Section 4.5 of (Bach, 2023). The next lemma uses a contraction property to reduce the Rademacher complexity of losses to linear predictors. These type of results are widely used in Rademacher analysis and we use the following specific version of contraction, which was originally introduced in Corollary 4 of (Maurer, 2016) and adapted to our setting. Write ∥ · ∥2 for Euclidean vector norm. Lemma C.3. Let A be the class of functions a : X →RK. For 1 ≤ i ≤ N, let ℓi : RK → R be G-Lipschitz continuous on A with respect to the Euclidean norm in the sense that the following holds: |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ∈ X. Then we have the following inequality for independent Rademacher random variables {σi}1≤i≤N and {εij}1≤i≤N,1≤j≤K: Eσ,D \" sup a∈A 1 N NX i=1 σiℓi(a(Xi)) # ≤ √ 2G · Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)  , where aj denotes the j-th coordinate of a and D = {(Xi, Yi)}i∈{1,...,N} are i.i.d N random samples sampled from X. Proof. We defer the proof to the Section 5 of (Maurer, 2016). In Lemma C.3, if we sample D from a probability distribution P, we can relax the Lipschitz continuity condition to hold for P- almost surely. In other words, |ℓ(a(X1)) − ℓ(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. The next lemma states that the Rademacher complexity of class of bounded affine predictors decays at most O( 1√ N ) rate. Lemma C.4. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Assume AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D >0. Suppose ∥G(j)(Xi)∥F ≤ R almost surely with respect to the random data Xi ∼ P. Then, Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   ≤ RD √ K√ N where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. 16LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Eε  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 f(j) W0 (Xi) + ⟨G(j)(Xi), δ⟩ \u0011   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩ + 1 N NX i=1 KX j=1 εijf(j) W0 (Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   ≤ Eε   sup ∥δ∥F ≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = Eε  D N sup ∥δ∥F ≤1 NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = D N Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F . The inequality is from the fact that ∥ · ∥F ≤ ∥ · ∥∗, hence {δ : ∥δ∥∗ ≤ D} ⊂ {δ : ∥δ∥F ≤ D}. The last equality is from the fact that ∥ · ∥F is self-dual. Next, we can bound Eε \r\r\rPN i=1 PK j=1 εijG(j)(Xi) \r\r\r F by the following inequalities. Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F ≤ vuuutEε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r 2 F = vuutEε NX i=1 KX j=1 \r\rεijG(j)(Xi) \r\r2 F = vuut NX i=1 KX j=1 \r\rG(j)(Xi) \r\r2 F ≤ R √ NK. a.s. The first inequality is from Jensen’s inequality, the equalities are from i.i.d assumption ofεik. We combine the results and take expectation with respect to D to get Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijak(Xi)   ≤ D N · R √ NK = RD √ K√ N . We then combine the previous results to get the following Lemma. Lemma C.5. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Let ˆL is non-regularized empirical risk defined as ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 17LoRA Training in the NTK Regime has No Spurious Local Minima and AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely with respect to the random data Xi ∼ P. For 1 ≤ i ≤ N, suppose ℓi ≜ ℓ(·, Yi) is G-Lipschitz continuous on A on the first argument (with respect to the Euclidean norm) for almost surely with respect to the random data Xi ⊆ D ∼ P. That is, |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. Then for any ∥δ∥∗ ≤ D, fixed δ0 such that ∥δ0∥∗ ≤ D, and η ∈ (0, 1), the following inequality holds with probability greater than 1 − η: ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . Proof. Take g of Lemma C.1 to be g = sup∥δ∥∗≤D(ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)), which is a function of X1, . . . , XN . Since ∥δ∥∗ ≤ D implies ∥δ∥F ≤ D and by the Lipschitz continuity of ℓ(·, Yi), we have the following for any (Xi, Yi) ∈ D: |ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ0⟩, Yi \u0001 − ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 | ≤G∥⟨δ0 − δ, G(Xi)⟩∥2 ≤ G vuut KX j=1 ∥δ0 − δ∥2 F ∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 ∥δ0 − δ∥2∗∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 4D2 · R2 = 2GRD √ K. Hence if we change only one data point (Xi, Yi) of g to (X ′ i, Y ′ i ), the deviation of ˆL(δ0) − ˆL(δ) is at most 2GRD √ K N . Then by Lemma C.1, we have sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) < E \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # + t √ 2KGRD√ N with probability greater than 1 − e−t2 . The expectation on the right hand side can be reduced to ED \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # = ED \" sup ∥δ∥∗≤D (−ˆL(δ) + L(δ)) + ˆL(δ0) − L(δ0) # = ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # Note that L(δ) − ˆL(δ) = L(δ) − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) = E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi), 18LoRA Training in the NTK Regime has No Spurious Local Minima where the expectation is taken over Xi ∼ P. Now apply Lemma C.2 to get ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # = ED \" sup ∥δ∥∗≤D (E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i# − ED \" 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # ≤ 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # where {σ}1≤i≤N are i.i.d Rademacher variables. Then apply Lemma C.3 to get 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # = 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. Finally, use Lemma C.4 to get 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   ≤ 2 √ 2G · RD √ K√ N . Therefore, we conclude that ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N (2 + t) . for ∥δ∥∗ ≤ D with probability greater than 1 − e−t2 . By reparametrization, we get ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . for ∥δ∥∗ ≤ D with probability greater than 1 − η. Now we can extend this generalization guarantee of constrained optimization to regularized optimization, which aligns with our problem of interest. For notational convenience, let Lλ(δ) = L(δ) + λ∥δ∥∗, ˆLλ(δ) = ˆL(δ) + λ∥δ∥∗ We follow the proof structure of (Bach, 2023), which was motivated by (Bartlett et al., 2005) and (Sridharan et al., 2008). Theorem C.6. Fix ε >0 and let 0 ̸= δ⋆ true ∈ argminδ L(δ) be the true optimum of the population risk and consider the setup of Lemma C.5 with D = (2 + ε)∥δ⋆ true∥∗, which is the upper bound on the nuclear norm of the predictors. Let η ∈ (0, 1) and λ = (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ).Consider the setup of Corollary 4.2 with P randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N \u0012 2 + r log 1 η \u0013 . 19LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let ˜ε = ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ and consider the convex set C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) ≤ λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Then for ∥δ∥∗ = 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, δ /∈ intC since the following inequalities hold. Lλ(δ) − Lλ(δ⋆ true) = L(δ) − L(δ⋆ true) + λ∥δ∥∗ − λ∥δ⋆ true∥∗ ≥ λ∥δ∥∗ − λ∥δ⋆ true∥∗ = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Therefore the boundary ∂C of C should be ∂C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Now suppose ˆuˆv⊺ /∈ C. Then since δ⋆ true ∈ C, there exists δ in the segment [ˆuˆv⊺, δ⋆ true] such that δ ∈ ∂C. By the convexity of ˆLλ, we have ˆLλ(δ) ≤ max \u0010 ˆLλ(δ⋆ true), ˆLλ(ˆuˆv⊺) \u0011 . Then we get ˆLλ(δ⋆ true) − ˆLλ(δ) ≥ −2˜ε∥δ⋆ λ∥∗ by Corollary 4.2. Therefore, ˆL(δ⋆ true) − ˆL(δ) − L(δ⋆ true) + L(δ) = ˆLλ(δ⋆ true) − ˆLλ(δ) − Lλ(δ⋆ true) + Lλ(δ) ≥ Lλ(δ) − Lλ(δ⋆) − 2˜ε∥δ⋆ λ∥∗ (3) = λ∥δ⋆ true∥∗ Note that ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗ < (2 + ε)∥δ⋆ true∥∗ and λ∥δ⋆ true∥∗ = ∥δ⋆ true∥∗ (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Then by Lemma C.5, (3) should happen with probability less than η. Then with probability greater than 1 − η, ˆuˆv⊺ ∈ C. In other words, Lλ(ˆuˆv⊺) − Lλ(δ⋆ true) < λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Hence, L(ˆuˆv⊺) + λ∥ˆuˆv⊺∥∗ < Lλ(δ⋆ true) + λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ = L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ ≤ L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + ελ∥δ⋆ true∥∗. Finally, we get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N   2 + r log 1 δ ! . By using the fact that ℓCE is Lipschitz continuous, we can reduce Theorem C.6 to Theorem 5.1. Note that the loss function ℓ may not be Lipschitz continuous in general. However, Lipschitz continuity is a mild assumption when the domain is restricted to a bounded class of predictors AD of Lemma C.5. Proof of Theorem 5.1. If ℓ(·, Y): RK → R is cross entropy loss defined as ℓ(X, Y) = ℓCE (X, Y) = −log   exp X(j) PK i=1 exp X(i) ! = −X(j) + log  KX i=1 exp X(i) ! 20LoRA Training in the NTK Regime has No Spurious Local Minima with true label Y = j, we have ∇ℓCE (X, Y)j = −1 + exp X(j) PK i=1 exp X(i) = − P i̸=j exp X(Y ) PK i=1 exp X(i) and for k ̸= j, ∇ℓCE (X, Y)k = exp X(k) PK i=1 exp X(Y ) Then we can bound the Euclidean norm of the gradient as follows. ∥∇ℓCE (X, Y)∥2 2 = \u0010P i̸=j exp X(i) \u00112 \u0010PK i=1 exp X(i) \u00112 + P i̸=j exp 2X(k) \u0010PK i=1 exp X(i) \u00112 ≤ 1 + 1 = 2. Hence the gradient of the cross entropy loss is bounded by √ 2 and we may replace G in Theorem C.6 with √ 2 to get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N   2 + r log 1 δ ! . D. Details of experiments Optimizing nuclear norm. Recall that SGD or GD on the loss function with weight decay and with regularization parameter λ is equivalent to minimizing 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , with respect to u and v. In full fine-tuning however, this is equivalent to minimize the following with respect to δ: 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 + λ∥δ∥∗. The problem here is that gradient methods no longer apply since the nuclear norm is non-differentiable. Therefore, we use the proximal gradient method: δt+1 = proxαλ∥·∥∗(δt − α∇ˆL(δt)) where proxαλ∥·∥∗(δ) = argmin δ′ \u0012 λ∥δ ′ ∥∗ + 1 2α∥δ′ − δ∥2 F \u0013 . It is well known that the proximal gradient method on convex objective converges to a global minimum (Polyak, 1987). Hyperparameters on NLP tasks For NLP tasks, we use full batch to perform GD on training. We only train the query (Wq) and value (Wv) weights of the RoBERTa-base model, which was empirically shown to have good performance (Hu et al., 2021). Furthermore, calculating the proximal operator of a nuclear norm is a computational bottleneck during the training of all Wq and Wv matrices. Therefore, we limit our training to only the last layer of Wq and Wv. To ensure a fair comparison, we apply the same approach to the LoRA updates. Additional information is in Table 1. Hyperparameters on image and speech classification tasks Similar to NLP tasks, we train the last attention layers. Further details are in Table 2. 21LoRA Training in the NTK Regime has No Spurious Local Minima Task SST-2,QNLI MR,CR,QQP,Subj Batch size 32 32 Learning rate (Full, LoRA fine tuning) 0.0005 0.001 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0.01 0.01 Table 1.Hyperparameters on experiment in Section 6 (NLP tasks) Task Image classification Speech classification Batch size 16 16 Learning rate (Full, LoRA fine tuning) 0.005 0.005 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0 0.001 Table 2.Hyperparameters on experiment in Section 6 (Image and speech classification tasks) (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 4.Test curves (accuracy vs. epochs) on different NLP tasks. We used the LoRA rank of 16. Test accuracy. For the setting of Section 6 on NLP tasks, we additionally conduct evaluations on a test set of 1000 samples during training and present the results in Figure 4. We observed that in most tasks the performance using LoRA eventually converges a test accuracy that matches that of full fine-tuning, although the rates of convergence sometimes differ. We list the hyperparameters in Table 3 Task SST-2,QQP,MR,CR Subj QNLI Batch size 32 32 24 Learning rate (Full, LoRA fine tuning) 0.0001 0.001 0.0005 Trained layer Wq, Wv (all layers) Wq, Wv (all layers) Wq, Wv (all layers) Weight decay 0.005 0.005 0.005 Table 3.Hyperparameters on experiment in Figure 4 22LoRA Training in the NTK Regime has No Spurious Local Minima For image and speech classification tasks, we also validate the performance of our linearized update to confirm that the accuracy is on par with actual LoRA updates. Accuracies are averaged over 3 runs (See Table 4). Task Image classification Speech classification Accuracy ( actual / linearized) 86.20 / 87.00 74.67 / 73.67 Table 4.Accuaricies of LoRA updates on vision and speech classification tasks 23",
      "meta_data": {
        "arxiv_id": "2402.11867v3",
        "authors": [
          "Uijeong Jang",
          "Jason D. Lee",
          "Ernest K. Ryu"
        ],
        "published_date": "2024-02-19T06:22:09Z",
        "pdf_url": "https://arxiv.org/pdf/2402.11867v3.pdf",
        "github_url": "https://github.com/UijeongJang/LoRA-NTK"
      },
      "llm_extracted_info": {
        "main_contributions": "This research provides a theoretical analysis of LoRA (Low-Rank Adaptation) fine-tuning within the Neural Tangent Kernel (NTK) regime. The key contributions are: (i) proving that full fine-tuning admits a low-rank solution of rank r \n\n (sqrt(N)); (ii) demonstrating that using LoRA with rank r \n\n (sqrt(N)) eliminates spurious local minima, enabling (stochastic) gradient descent to find these low-rank solutions; and (iii) showing that the low-rank solution found using LoRA generalizes well.",
        "methodology": "The study employs a theoretical analysis centered on the Neural Tangent Kernel (NTK) regime, approximating network changes using first-order Taylor expansion. It investigates the empirical risk with convex, non-negative, and twice-differentiable loss functions (e.g., cross-entropy, MSE). LoRA's low-rank parameterization (\n\n = uv\n\n) is analyzed, and weight decay is interpreted as nuclear norm regularization. The methodology leverages concepts of second-order stationary points (SOSPs) and established theorems on gradient descent convergence. Proofs for low-rank solution existence are inspired by semi-definite programming results, while the absence of spurious local minima for LoRA is demonstrated using a perturbed loss function, Sard’s theorem, and existing matrix factorization characterizations of SOSPs. Generalization guarantees are derived using Rademacher complexity and McDiarmid's inequality.",
        "experimental_setup": "Experiments were conducted on linearized pre-trained models across different modalities to validate the theory. For NLP tasks, prompt-based fine-tuning was used on a linearized RoBERTa-base model, training only the query (Wq) and value (Wv) weights of the last layer, on datasets like SST-2, QNLI, MR, CR, QQP, and Subj, with N=32 data points and K=2 labels, using cross-entropy loss. For image classification, a pre-trained vision transformer was fine-tuned on the bean disease dataset (N=48, K=3 labels). For speech classification, a pre-trained wav2vec2 model was fine-tuned on the SUPERB dataset (N=64, K=4 labels). All experiments used full batch gradient descent. The theoretical LoRA rank requirement was set based on the condition r(r+1)/2 > KN (e.g., r \n\n 11 for NLP tasks). Evaluation included training loss curves and test accuracy.",
        "limitations": "The theoretical analysis relies on the assumption that fine-tuning remains within the NTK (lazy-training) regime, which is empirically supported for prompt-based fine-tuning but may not hold universally. The main theorem on the elimination of spurious local minima applies to a 'perturbed' loss function with a randomly sampled positive semi-definite matrix P; while interpreted to imply generic absence of spurious minima, it doesn't directly apply to standard unperturbed LoRA. The generalization guarantees are primarily shown for cross-entropy loss, although extensions to general convex losses are provided in the appendix. Empirically, while the theory confirms convergence to optimal loss values, the observed rates of convergence differed, with lower LoRA ranks leading to slower convergence, suggesting that lower ranks might create unfavorable loss landscapes (e.g., plateaus or saddle points) that hinder optimization speed.",
        "future_research_directions": "Future work could involve more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, and achieving a better understanding of the minimum rank requirement for LoRA through lower bounds. Another promising direction is to analyze the observed trade-off between training rate and LoRA rank, particularly exploring why lower LoRA ranks lead to slower convergence, and designing remedies for these potentially unfavorable, flatter training landscapes.",
        "experimental_code": "class LogitModelWrapper(nn.Module):\n    def __init__(self, model, binary_classification):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n\n    def forward(self, input_ids, attention_mask, mask_pos):\n        logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n        if self.binary_classification:\n            assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n            logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)   \n        return logits\n        \n# Linearized LoRA update  \nclass LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))  # scaling the initailization by \\frac{1}{sqrt{r}}\n           #torch.nn.init.kaiming_normal_(param)\n      \n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])     \n            \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) \n        self.gradient_dtype = gradient[0].dtype\n\n        # Compute \\langle G(X_i), B*A \\rangle \n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n       \n# Linearized LoRA update \nclass Linearupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.delta = [torch.zeros(self.target_size[i][0], self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.delta_list = nn.ParameterList([nn.Parameter(param) for param in self.delta])\n       \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) #train_gradient = [Batch, label, 768 , 768]\n        self.gradient_dtype = gradient[0].dtype\n\n        #Minimizing nuclear norm is equivalent to minimizing the sum of nuclaer norm of nontrivial diagnoals. \n        \n        output = sum(torch.sum(self.delta_list[i] * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n               \nclass LinearizedLoraTrainer(LinearHeadTrainer):\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        args: TrainingArguments,\n        data_collator: Optional[DataCollator] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Dataset] = None,\n        *posargs,\n        **kwargs\n    ):\n        super().__init__(model, args, data_collator, train_dataset, eval_dataset, *posargs, **kwargs)\n\n        self.grad_dim = None\n        self.train_targets = None\n        self.num_labels = None\n        self.gradient_dtype = None\n \n        self.binary_classification = self.args.binary_classification\n        self.model_wrapper = LogitModelWrapper(self.model, self.args.binary_classification)\n        \n        self.target_layers=[]\n        self.target_size=[]\n        for name, param in self.model_wrapper.named_parameters():\n            \n            if self.args.train_last_layer:\n                # There are 12 layers in RoBERTa-base model\n                if  \"11\" in name and \"attention\" in name and \"query\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n                if  \"11\" in name and \"attention\" in name and \"value\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n            else:\n                if  \"attention\" in name and \"query\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n                if  \"attention\" in name and \"value\" in name and \"weight\" in name :\n                    self.target_layers.append(name)\n                    self.target_size.append(param.size())\n                \n        logger.info(\"Calculating gradient with respect to {}\".format(self.target_layers))\n        \n        #lora\n        if self.model.model_args.apply_lora == True:\n            self.lora_model = LinearLoraupdate(self.model,self.args.binary_classification,self.target_layers,self.target_size).to(self.args.device)\n        else:\n            self.lora_model = Linearupdate(self.model,self.args.binary_classification,self.target_layers,self.target_size).to(self.args.device)\n        \n    ## Method to compute gradient 1\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n            \n        def convert_to_buffer(name):\n            if layer_name in name:\n                #logger.info(\"Including {}\".format(name))\n                return False\n            else:\n                return True\n        \n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n  \n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n                \n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n    \n            \n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n            \n        targets.append(label)\n    \n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))  \n    \n    ## Method to compute gradient 2\n    def compute_gradient_everylayer(self, inputs_outer):\n        \n        grads_outer=[] \n        with torch.no_grad():\n            for layer_name in self.target_layers:   \n                    grads_outer_layer, targets = self.compute_gradient_perlayer(inputs_outer, layer_name)\n                    grads_outer.append(grads_outer_layer)\n                    \n        if self.grad_dim is None:\n            self.grad_dim = sum(np.prod(x.shape[2:]) for x in grads_outer)\n                \n        return (grads_outer, targets) #list of gradients for each layer and targets.\n    \n    def compute_gradient_sharded(self, inputs_outer):\n        with torch.no_grad():\n            grads_outer , targets = self.compute_gradient_everylayer(inputs_outer)\n        return  grads_outer, targets \n\n    #get pre-trained logits and training targets , this corresponds to f(X_i), Y_i\n    def compute_model_logits(self, inputs_outer):\n\n        self.model_wrapper.eval()\n        \n        logits = []\n        targets = []\n        with torch.no_grad():\n            for k, v in inputs_outer.items():\n                if isinstance(v, torch.Tensor):\n                    inputs_outer[k] = v.to(self.args.device)\n            label = inputs_outer.get(\"labels\")\n            if self.args.binary_classification:\n                label = (label * 2 - 1).float()  # convert from {0, 1} to {-1, 1}\n            preds = self.model_wrapper(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))\n            logits.append(preds.detach())\n            targets.append(label)\n\n        logits = torch.cat(logits, dim=0)\n        targets = torch.cat(targets, dim=0);\n\n        return logits, targets \n\n    def train(self, model_path=None, dev_objective=None):\n        if self.args.from_linearhead and model_path is None:\n            super().train(model_path, dev_objective) # Train output layer using LinearHeadTrainer\n        eval_dataset = self.train_dataset\n        return TrainOutput(0, 0.0, {}), None\n    \n    # Fine-tune linearized model\n    def finetune(self, train_dataset, eval_dataset):\n        \n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr) # Weight decay will be implented manually\n\n        if self.args.fp16 and _use_apex:\n            if not transformers.is_apex_available():\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            model, optimizer = amp.initialize(self.lora_model, optimizer, opt_level=self.args.fp16_opt_level)\n\n        # Multi-gpu training (should be after apex fp16 initialization)\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n        \n        epoch_count = 0 \n        \n        #Make sure to freeze other parameters\n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False);\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False);\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False);\n         \n        for name, param in self.lora_model.named_parameters():\n            if param.requires_grad:\n                print(f\"{name} is being trained\")\n                \n                \n        writer = SummaryWriter(f\"./finallasttrain/{self.model.model_args.apply_lora}-{self.args.linear_lr}-{self.model.data_args.task_name}-{self.args.seed}-{self.model.model_args.lora_r}\")\n        \n        file_exists = False\n        eval_file_exists = False\n        saved_gradients_eval = []\n        saved_gradients = []\n        for epoch in range(self.args.linear_num_epoch):\n            \n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n            \n            if self.args.eval_during_training:\n                total_loss_eval = 0\n                eval_preds=[]\n                eval_targets_list=[]\n                \n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False);\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False);\n                    \n                \n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1) \n                    self.gradient_dtype = gradient[0].dtype\n                    \n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)   \n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                ## Weight decay\n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001: #Skip this if there is no weight decay (weight decay = 0)\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)  #Soft-thresholding operator \n                            param = u@s@v;\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param;\n                \n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:  #Skip this if there is no weight decay (weight decay = 0)\n                            reg += (1/2)*(param ** 2).sum();  \n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:   #Skip this if there is no weight decay (weight decay = 0)\n                            reg += torch.norm(param, p = 'nuc');\n                            \n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item();\n                            \n            file_exists = True\n                          \n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd) \n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n            writer.add_scalar(f\"train_loss_{self.model.data_args.task_name}/epoch\", avg_loss, epoch)\n            \n            # Do evaluation during training if needed.\n            if self.args.eval_during_training: \n                with torch.no_grad():\n                    \n                    for i, inputs_outer in enumerate(tqdm(dataloader_outer_eval, desc=\"Evaluating\")):\n                               \n                        if eval_file_exists:\n                            gradient_eval = saved_gradients_eval[i]\n                            for j in range(len(self.target_layers)):\n                                gradient_eval[j].to(self.args.device);\n                                gradient_eval[j].requires_grad_(False);\n                                \n                        else:\n                            gradient_eval, _  = self.compute_gradient_sharded(inputs_outer)\n                            for j in range(len(self.target_layers)):  \n                                gradient_eval[j].requires_grad_(False);\n                        \n                        \n                        output_eval = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient_eval)\n                        eval_logits, eval_targets = self.compute_model_logits(inputs_outer)\n                        eval_logits = eval_logits.to(self.args.device)\n                            \n                        loss_eval = nn.CrossEntropyLoss(reduction = 'sum')(output_eval + eval_logits , eval_targets) \n                        dist.all_reduce(loss_eval, op=dist.ReduceOp.SUM) \n                        total_loss_eval += loss_eval.item();\n                        \n                        eval_preds.append( eval_logits + output_eval );\n                        eval_targets_list.append(eval_targets);\n                        \n                        saved_gradients_eval_cpu = []\n                        for j in range(len(self.target_layers)):\n                            gradient_eval_layer= gradient_eval[j].detach().cpu()\n                            saved_gradients_eval_cpu.append(gradient_eval_layer)\n                        saved_gradients_eval.append(saved_gradients_eval_cpu)\n                        \n                       \n                eval_file_exists = True\n\n                avg_loss_eval = (total_loss_eval / len(dataloader_outer_eval.dataset) )+ (reg * self.args.linear_wd)\n                logger.info(f\"epoch : {epoch+1} eval_loss : {avg_loss_eval}\")\n                writer.add_scalar(f\"eval_loss_{self.model.data_args.task_name}/epoch\", avg_loss_eval, epoch)\n                \n                eval_preds = torch.cat(eval_preds, dim=0)\n                eval_targets = torch.cat(eval_targets_list, dim=0)\n\n                if self.args.local_rank != -1:\n                    logger.info(\"Starting to gather kernel across GPUs\")\n                    eval_preds = varsize_tensor_all_gather(eval_preds.to(self.args.device), torch.distributed.get_world_size())\n                    eval_targets = varsize_tensor_all_gather(eval_targets.to(self.args.device), torch.distributed.get_world_size())\n                    logger.info(\"Finished gathering kernel across GPUs\")\n\n                # Now calculate the accuarcies\n                metrics = None\n                eval_preds = eval_preds.cpu()\n                eval_targets = eval_targets.cpu()\n                \n                if self.args.binary_classification: # Make sure to compute loss before this transformation!\n                    eval_preds = torch.cat([-eval_preds, eval_preds], dim=-1) # convert back to two logits\n                    eval_targets = ((eval_targets + 1) / 2).long() # convert back from {-1, 1} to {0, 1}\n\n                if self.compute_metrics is not None:\n                    metrics = self.compute_metrics(EvalPrediction(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy()))\n\n                # Prefix all keys with eval_\n                for key in list(metrics.keys()):\n                    if not key.startswith(\"eval_\"):\n                        metrics[f\"eval_{key}\"] = metrics.pop(key)\n                metrics[\"eval_loss\"] = avg_loss\n\n                metrics.update(self.metrics())\n                metrics[\"grad_dim\"] = self.grad_dim\n\n                output = PredictionOutput(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy(), metrics=metrics)\n                metrics = output.metrics\n                objective = default_dev_objective(metrics)\n                logger.info(f\"epoch : {epoch+1}  objective : {objective}\")\n                writer.add_scalar(f\"Eval_acc_{self.model.data_args.task_name}/epoch\", objective, epoch)\n                         \n        writer.flush()\n        writer.close()\n        self.save_model(self.args.output_dir)\n        return avg_loss.item() , epoch_count",
        "experimental_info": "Method: Linearized LoRA (or general linearized update)\nTrainer: `LinearizedLoraTrainer` (specified by `training_args.trainer='linearized'`)\n\nModel Configuration (`ModelArguments`):\n- `model_name_or_path`: Path to pretrained model or model identifier.\n- `apply_lora`: Whether to use LoRA for finetuning (default: `True`).\n- `lora_alpha`: Initialization scale for one of the low rank matrices in LoRA (default: `1`).\n- `lora_r`: Inner rank for LoRA matrices (default: `8`). LoRA is only implemented for RoBERTa models.\n\nTraining Arguments (`DynamicTrainingArguments`):\n- `trainer`: Specifies `LinearizedLoraTrainer` (default: `linearized`).\n- `linear_freeze_A`: Freeze layer A in LoRA (default: `False`). This flag prevents `Lora_A` parameters from being updated during finetuning.\n- `linear_lr`: Learning rate for linearized LoRA finetuning (default: `0.01`).\n- `linear_num_epoch`: Number of epochs in linearized LoRA training (default: `30`).\n- `linear_wd`: Weight decay in linearized LoRA (default: `0`). This is used for nuclear norm regularization:\n    - If `apply_lora` is `True`, it applies L2 regularization to LoRA parameters (`param = param - self.args.linear_wd * self.args.linear_lr * param`).\n    - If `apply_lora` is `False`, it applies soft-thresholding on singular values for nuclear norm regularization (`u,s,v = torch.svd(param); s = torch.nn.Threshold(0, 0)(s- self.args.linear_lr * self.args.linear_wd); param = u@s@v`).\n- `eval_during_training`: Whether to perform evaluation during the training process (default: `False`).\n- `train_last_layer`: Whether to train only the last layer (default: `True`). Specifically targets query and value attention weights in the last layer (layer 11 for RoBERTa-base models). If `False`, it targets query and value attention weights in all layers.\n- `optimizer`: `sgd` or `adam` (default: `sgd` for linearized training).\n- `binary_classification`: If `num_classes=2`, convert two softmax logits to single sigmoid logit if needed (default: `False`). This affects how logits are processed in `LogitModelWrapper`.\n\nGradient Computation:\n- Gradients are computed with respect to `target_layers` using `functorch.jacrev` to obtain the Jacobian. \n- `target_layers` are identified as specific attention query and value weight matrices, either only in the last layer or across all layers, depending on `train_last_layer`.\n\nTraining Loop (`LinearizedLoraTrainer.finetune`):\n- An SGD optimizer is used for updating the `lora_model` parameters.\n- The loss function is `nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output, targets)` where `train_logits` are the initial model logits and `output` is the linearized update.\n- Weight decay is applied as described above."
      }
    },
    {
      "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
      "abstract": "Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.",
      "full_text": "LoRA Training in the NTK Regime has No Spurious Local Minima Uijeong Jang 1 Jason D. Lee 2 Ernest K. Ryu 3 Abstract Low-rank adaptation (LoRA) has become the stan- dard approach for parameter-efficient fine-tuning of large language models (LLM), but our the- oretical understanding of LoRA has been lim- ited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with N data points, showing: (i) full fine- tuning (without LoRA) admits a low-rank solu- tion of rank r ≲ √ N; (ii) using LoRA with rank r ≳ √ N eliminates spurious local minima, allow- ing (stochastic) gradient descent to find the low- rank solutions; (iii) the low-rank solution found using LoRA generalizes well. 1. Introduction The modern methodology of using large language models involves (at least) two phases: self-supervised pre-training on a large corpus followed by supervised fine-tuning to the downstream task. As large language models have grown in scale, pre-training has become out of reach for research groups without access to enormous computational resources. However, supervised fine-tuning remains feasible for such groups. One key strategy facilitating this efficient fine- tuning is Parameter-Efficient Fine-Tuning (PEFT), which freezes most of the pre-trained model’s weights while selec- tively fine-tuning a smaller number of parameters within an adapter module. Among various PEFT methodologies, low- rank adaptation (LoRA) (Hu et al., 2021) has emerged as the standard approach. Given a pre-trained matrixW0 ∈ Rm×n, LoRA trains a low-rank update such that the forward pass evaluates W0x + ∆W x= W0x + BAx 1Department of Mathematical Sciences, Seoul National Uni- versity 2Department of Electrical and Computer Engineering, Princeton University 3Department of Mathematics, University of California, Los Angeles. Correspondence to: Ernest Ryu <eryu@math.ucla.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). where r ≪ min(m, n), A ∈ Rr×n is initialized to be a random Gaussian, and B ∈ Rm×r is initialized to be zero. However, despite the widespread adoption of LoRA, our theoretical understanding of its mechanisms remains limited. One notable prior work is (Zeng & Lee, 2024), which ana- lyzes the expressive power of LoRA, showing that for any given function, there exist weight configurations for LoRA that approximate it. However, their work does not address whether LoRA can efficiently learn such configurations. Additionally, Malladi et al. (2023) experimentally demon- strated that under certain conditions, LoRA fine-tuning is nearly equivalent to a kernel regression, where the A matrix provides random features and is essentially not trained. This regime neglects the possibility of the A matrix learning new features and, consequently, leads to a LoRA rank require- ment of r ≥ Θ(1/ε2), where ε is an approximation toler- ance, originating from the use of the Johnson–Lindenstrauss lemma (Johnson & Lindenstrauss, 1984). Crucially, LoRA’s fundamental nature as a quadratic parameterization has not been considered in the prior analysis of trainability and generalizability. Contribution. In this work, we theoretically analyze LoRA fine-tuning and present results on trainability and generalizability. We consider fine-tuning a deep (trans- former) neural network with K-dimensional outputs using N training (fine-tuning) data points. Assuming that train- ing remains under the NTK regime, which we soon define and justify in Section 2, we show the following. First, full fine-tuning (without LoRA) admits a rank-r solution such that r(r+1) 2 ≤ KN . Second, using LoRA with rank r such that r(r+1) 2 > KN eliminates spurious local minima, al- lowing (stochastic) gradient descent to find the low-rank solutions. Finally, the low-rank solution found using LoRA generalizes well. 1.1. Prior works Theory of neural networks. The question of expressive power addresses whether certain neural networks of interest can approximate a given target function. Starting with the classical universal approximation theorems (Cybenko, 1989; Hornik et al., 1990; Barron, 1993), much research has been conducted in this direction. (Delalleau & Bengio, 2011; 1 arXiv:2402.11867v3  [cs.LG]  28 May 2024LoRA Training in the NTK Regime has No Spurious Local Minima Bengio & Delalleau, 2011; Lu et al., 2017; Duan et al., 2023). These can be thought of as existence results. The question of trainability addresses whether one can com- pute configurations of neural networks that approximate tar- get functions. Ghadimi & Lan (2013); Ge et al. (2015); Du et al. (2017); Jin et al. (2017) studied general convergence results of gradient descent and stochastic gradient descent. Soltanolkotabi et al. (2018); Du & Lee (2018); Allen-Zhu et al. (2019a;b); Du et al. (2019); Zou et al. (2020) stud- ied the loss landscape of neural networks and showed that first-order methods converge to global minima under certain conditions. The question of generalization addresses whether neural net- works trained on finite data can perform well on new unseen data. Classical learning theory (Koltchinskii & Panchenko, 2000; Bartlett et al., 2002; Bousquet & Elisseeff, 2002; Hardt et al., 2016; Bartlett et al., 2017) uses concepts such as uniform stability or the Rademacher complexities to ob- tain generalization bounds. Generalization bounds in the context of modern deep learning often utilize different ap- proaches (Wu et al., 2017; Dinh et al., 2017; Zhang et al., 2021), we use the Rademacher complexity for obtaining our generalization results. Neural tangent kernels. The theory of neural tangent kernel (NTK) concerns the training dynamics of certain infinitely wide neural networks. Jacot et al. (2018) shows that the training of an infinitely wide neural network is equivalent to training a kernel machine. Various studies such as (Arora et al., 2019; Chen et al., 2020) expand the NTK theory to more practical settings. Among these works, Wei et al. (2022a) introduced the concept of empirical NTK (eNTK) and showed that kernel regression with pretrained initialization also performs well on real datasets, providing a background to utilize NTK theory in fine-tuning. Theory of transformers and LLMs. As the transformer architecture (Vaswani et al., 2017) became the state-of-the- art architecture for natural language processing and other modalities, theoretical investigations of transformers have been pursued. Results include that transformers are uni- versal approximators (Yun et al., 2019), that transformers can emulate a certain class of algorithmic instructions (Wei et al., 2022b; Giannou et al., 2023), and that weight matrices in transformers increase their rank during training (Boix- Adsera et al., 2023). Also, (Zhang et al., 2020; Liu et al., 2020) presents improved adaptive optimization methods for transformers. PEFT methods and LoRA. Low-rank adaptation (LoRA) (Hu et al., 2021) has become the standard Parameter- Efficient Fine-Tuning (PEFT) method, and many variants of LoRA have been presented (Fu et al., 2023; Dettmers et al., 2023; Lialin et al., 2023). LoRA has proven to be quite versatile and has been used for convolution layers (Yeh et al., 2024) and for diffusion models (Ryu, 2023; Smith et al., 2023; Choi et al., 2023). Theoretically, Aghajanyan et al. (2021) found an intrinsic low-rank structure is critical for fine-tuning language mod- els, although this finding concerns full fine-tuning, not the setting that uses LoRA. Recently, Zeng & Lee (2024) an- alyzed the expressive power of LoRA. However, we still lack a sufficient theoretical understanding of why LoRA is effective in the sense of optimization and generalization. Matrix factorization. In this work, we utilize techniques developed in prior work on matrix factorization problems. Bach et al. (2008); Haeffele et al. (2014) established the sufficiency of low-rank parameterizations in matrix factor- ization problems, and their techniques have also been used in matrix completion (Ge et al., 2016), matrix sensing (Jin et al., 2023), and semidefinite programming (Bhojanapalli et al., 2018). 1.2. Organization Section 2 introduces the problem setting and reviews rel- evant prior notions and results. Section 3 proves the exis- tence of low-rank solutions. Section 4 proves LoRA has no spurious local minima and, therefore, establishes that (stochastic) gradient descent can find the low-rank global minima. Section 5 shows that the low-rank solution gener- alizes well. Finally, Section 6 presents simple experiments fine-tuning pre-trained models for different modalities. The experimental results validate our theory and provide further experimental insights. 2. Problem setting and preliminaries We primarily consider the setup of pre-trained large lan- guage models fine-tuned with LoRA. However, our theory does generally apply to other setups that utilize pre-training and LoRA fine-tuning, such as diffusion models. Matrix notation. For matrices A and B, let ∥A∥∗ denote the nuclear norm, ∥A∥F the Frobenius norm, and ⟨A, B⟩ = tr(A⊺B) the matrix inner product. We letSn and Sn + for the set of n × n symmetric and positive semi-definite matrices, respectively. Let R(·) and N(·) respectively denote the range and the null-space of a linear operator. Neural network. Let fΘ : X →RK be a neural network (e.g., a transformer-based model) parametrized by Θ, where X is the set of data (e.g., natural language text) and RK is the output (e.g., pre-softmax logits of tokens). K is the output dimension of fΘ, where K = k for k-class classification, K = 1 for binary classification, and K is the 2LoRA Training in the NTK Regime has No Spurious Local Minima dimension of the label Y when using mean square error loss. Assume the model has been pre-trained to Θ = Θ0, i.e., the pre-trained model is fΘ0 . Let W = ( W(1), . . . , W(T)) ⊂ Θ be a subset of the weights (e.g., dense layers in QKV-attention) with size W(i) ∈ Rmi×ni for i = 1, . . . , Tthat we choose to fine- tune. Let W0 = (W(1) 0 , . . . , W(T) 0 ) ⊂ Θ0 be their corre- sponding pre-trained weights. With slight abuse of notation, write fW to denote fΘ, where all parameters ofΘ excluding W are fixed to their corresponding values in Θ0. Fine-tuning loss. Assume we wish to fine-tune the pre- trained model with {(Xi, Yi)}N i=1, where N is the number of (fine-tuning) training data. (In many NLP tasks, it is not uncommon to have N < 100.) Denote δ = (δ(1), . . . , δ(T)) ⊂ Θ to be the change of W after the fine-tuning, i.e., fW0+δ is our fine-tuned model. We use the empirical risk ˆL(δ) = 1 N NX i=1 ℓ(fW0+δ(Xi), Yi), with some loss functionℓ. We assumeℓ(x, y) is convex, non- negative, and twice-differentiable with respect to x for any y. (This assumption holds for the cross-entropy loss and the mean squared error loss.) The empirical risk approximates the true risk L(δ) = E (X,Y )∼P \u0002 ℓ(fW0+δ(X), Y) \u0003 with some data distribution P. NTK regime. Under the NTK regime (also referred to as the lazy-training regime), the change of the network can be approximated by its first-order Taylor expansion fW0+δ(X) ≈ fW0 (X) + ⟨∇fW0 (X), δ⟩ (1) sufficiently well throughout (fine-tuning) training. To clarify, fW0+δ(X) ∈ RK, so the NTK regime requires the first- order Taylor expansion to be accurate for all coordinates: f(j) W0+δ(X) ≈ f(j) W0 (X) + ⟨∇f(j) W0 (X), δ⟩, where f(j) W is the j-th coordinate of fW for j = 1, . . . , K. The NTK regime is a reasonable assumption in fine-tuning if δ is small, and this assertion is supported by the empirical evidence of (Malladi et al., 2023). This prior work provides extensive experiments on various NLP tasks to validate that fine-tuning happens within the NTK regime for many, although not all, NLP tasks. Observation 2.1 (Malladi et al. (2023)) . When prompt- based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) is used, fine-tuning a pre-trained language model stays within the NTK regime. Motivated by this empirical observation, we define lin- earized losses ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001 ≈ ˆL(δ) and L(δ) = E (X,Y )∼P h ℓ \u0000 fW0 (Xi) + ⟨∇fW0 (Xi), δ⟩, Yi \u0001i ≈ L(δ). LoRA. We use the low-rank parameterization δ(i) = u(i)(v(i))⊺ ∈ Rmi×ni, where u(i) ∈ Rmi×r, v(i) ∈ Rni×r, for i ∈ {1, ··· , T}. Under the NTK regime, the empirical risk can be approxi- mated as ˆL(uv⊺) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 , where u =   u(1) ... u(T)   ∈ Rm×r, v =   v(1) ... v(T)   ∈ Rn×r with m = PT i=1 mi and n = PT i=1 ni, and G(Xi) = diag \u0000 ∇W(1) fW0 (Xi), . . . ,∇W(T) fW0 (Xi) \u0001 is an collection ofK m×n block diagonal matrices. To clar- ify, G(Xi) ∈ RK×m×n, so ⟨G(Xi), uv⊺⟩ ∈RK should be interpreted as K inner products of m ×n matrices where each matrices correspond to each coordinates of f. More specifically, G(j)(Xi) ∈ Rm×n and \u0000 ⟨G(Xi), uv⊺⟩ \u0001 j = ⟨G(j)(Xi), uv⊺⟩ for j = 1, . . . , K. Note that ˆL(uv⊺) under the NTK regime is non-convex in (u, v) so SGD-training does not converge to the global minimizer, in general. Weight decay on LoRA is nuclear norm regularization. The LoRA training of optimizing ˆL is often conducted with weight decay (Hu et al., 2021; Dettmers et al., 2023), which can be interpreted as solving minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , 3LoRA Training in the NTK Regime has No Spurious Local Minima with regularization parameter λ ≥ 0. This problem is equiv- alent to the rank-constrained nuclear-norm regularized prob- lem minimize δ, rankδ≤r ˆLλ(δ) ≜ ˆL(δ) + λ∥δ∥∗. This is due to the following lemma. Lemma 2.2 (Lemma 5.1 of (Recht et al., 2010)). Let r >0. For δ ∈ Rm×n such that rank(δ) ≤ r, ∥δ∥∗ = 1 2 min uv⊺=δ {∥u∥2 F + ∥v∥2 F |u ∈ Rm×r, v ∈ Rn×r}. (The connection between weight decay on Burer–Monteiro style low-rank factorization and nuclear norm regularization has been previously in different contexts not directly related to LoRA (Cabral et al., 2013; Pilanci & Ergen, 2020).) Second-order stationary points. Let ˆL: Rm×n → R be twice-continuously differentiable. We say U ∈ Rm×n is a (first-order) stationary point if ∇ˆL(U) = 0. We say U ∈ Rm×n is a second-order stationary point (SOSP) if ∇ˆL(U) = 0, ∇2 ˆL(U)[V, V] ≥ 0, for any direction V ∈ Rm×n. We say U is strict saddle if U is a first- but not second-order stationary point. Lastly, we say U ∈ Rm×n is a local minimum if there exists an open ball B that contains U and ˆL(U) ≤ ˆL(U′) for any U′ ∈ B. It follows that a local minimum is an SOSP. The following results, roughly speaking, establish that (stochastic) gradient descent only converges to SOSPs when a loss function is twice-continuously differentiable. Theorem 2.3 (Theorem 4.1 of (Lee et al., 2016)). Gradient descent on twice-differentiable function with random ini- tialization, almost surely, does not converge to strict saddle points. I.e., if gradient descent converges, it converges to an SOSP , almost surely. Theorem 2.4 (Informal, Theorem 1 of (Ge et al., 2015)). Stochastic gradient descent with noise on twice- differentiable strict saddle function (i.e., every stationary point is either a local minimum or a strict saddle) does not converge to strict saddle points with high probability. I.e., if stochastic gradient descent with noise converges, it converges to an SOSP with high probability. Therefore, if we can show that all SOSPs are global minima in our setup of interest, then (stochastic) gradient descent will only converge to global minima. 3. Low-rank solution exists In this section, we show that full fine-tuning in the NTK regime admits a low-rank solution of rank r ≲ √ N. The existence of a low-rank solution provides theoretical legit- imacy to using the low-rank parameterization of LoRA, which, of course, can only find low-rank solutions. Theorem 3.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global minimizer (not necessarily unique). Then there is a rank-r solution such that r(r+1) 2 ≤ KN . The assumption that ˆLλ(δ) has a global minimum is very mild; it is automatically satisfied if λ >0. When λ = 0, the assumption holds if ℓ is the mean squared error loss. The inspiration for Theorem 3.1 comes from the classical results of (Barvinok, 1995; Pataki, 1998; 2000) that establish that semi-definite programs (which have symmetric positive semi-definite matrices as optimization variables) admit low- rank solutions. We clarify that Theorem 3.1 does not require δ to be symmetric nor any notion of “semi-definiteness” (δ is not even square). Proof sketch of Theorem 3.1. We quickly outline the key ideas of the proof while deferring the details to Appendix A. We can show that finding δ⋆ λ ∈ argminδ ˆLλ(δ) with rank(δ⋆ λ) = r is equivalent to finding a rank-r global mini- mum of F : S(m+n) + → R where F(Z) = ˆL( ¯Z) + λ 2 tr(Z) and ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of Z such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . (2) Now suppose Z⋆ ∈ S(m+n) + is a global minimizer of F. Define S(Z⋆) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆)} and a linear operator A: S(m+n) → RKN as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Now let rank(Z⋆) = r and assume {0} = S(Z⋆) ∩ N(A). Then by dimension counting, we have the following inequal- 4LoRA Training in the NTK Regime has No Spurious Local Minima Figure 1.Geometric intuition of Theorem 3.1. The three dimensional space describes the space of 2 by 2 matrices \u0014 1 x y z \u0015 . The surface z = xy represents the rank 1 matrices. The blue region on the surface correspond to the region of smaller objective values, and the set of global minima are depicted with purple. (Left) Plot of (a) with N = 1. The set of global minima is a plane, and the intersection with the surface z = xy (curve) is the set of rank-1 global minima. (Middle) Plot of (b) with N = 2. the set of global minima is a line, and the intersection with the surface (two dots) is the set of rank 1 global minima. (Right) Plot of (c) with N = 3. The set of global minima is a line, and there is no intersection with the surface, i.e., there is no global minimum of rank-1 but admits a rank-2 global minima. ity. 0 = dimS(Z⋆) + dimN(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(m+n)) − dim(S(Z⋆) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆) − KN If there exists nonzero Z ∈ S(m+n) such that Z ∈ S(Z⋆) ∩ N(A), then we can show that there exists nonzero t ∈ R such that Z⋆ + tZ is also a global minimizer of F with strictly lower rank. Replace Z⋆ with Z⋆ + tZ and repeat this process until we find a solution Z⋆ with {0} = S(Z⋆) ∩ N(A). Together with the fact that dimS(Z⋆) = r(r+1) 2 , we have the desired result. Illustration of Theorem 3.1. The following toy example illustrates the geometric intuition of Theorem 3.1. Let ℓ be the mean square error loss, K = 1, δ = \u0014w x y z \u0015 , and λ = 0 (no regularization). Then consider the following objective functions each for N = 1, 2, and 3: ˆL0(δ) = (x + y)2 (a) ˆL0(δ) = 1 2(z + 4)2 + 1 2(x + y)2 (b) ˆL0(δ) = 1 3(w − 1)2 + 1 3(z − 4)2 + 1 3( √ 3x + √ 3y)2 (c) The set of low-rank (rank-1) solutions for the three objec- tives are depicted in Figure 1. 4. GD and LoRA finds low-rank solution In this section, we show that the optimization landscape with LoRA in the NTK regime has no spurious local minima if the LoRA parameterization uses rank r ≳ √ N and if we consider an ε-perturbed loss. This implies that optimizers such as stochastic gradient descent only converge to the low-rank global minimizers. Theorem 4.1. Let λ ≥ 0. Assume ˆLλ(δ) has a global min- imizer (not necessarily unique) and r(r+1) 2 > KN. Con- sider the perturbed loss function ˆLλ,P defined as ˆLλ,P (u, v) ≜ ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 ∈ R(m+n)×r and P ∈ S(m+n) + is pos- itive semi-definite. Then, for almost all nonzero P (with respect to the Lebesgue measure on S(m+n) + ⊂ S(m+n) ∼= R (m+n)(m+n+1) 2 ), all SOSPs of ˆLλ,P are global minimizers of ˆLλ,P . To clarify, the conclusion that ‘all SOSPs are global mini- mizers’ holds with probability 1 even if the distribution of P is supported on {P ∈ S(m+n) + : ∥P∥ ≤ε} for arbitrarily small ε >0. In the practical LoRA fine-tuning setup where no perturbation is used and P = 0 is set deterministically, Theorem 4.1 does not apply. However, we can neverthe- less interpret the result of Theorem 4.1 to show that LoRA fine-tuning generically has no spurious local minima. 5LoRA Training in the NTK Regime has No Spurious Local Minima If we do use a randomly generated small perturbation P so that Theorem 4.1 applies, the solution to the perturbed problem with small P does not differ much from that of the unperturbed problem with P = 0 in the following sense. Corollary 4.2. Consider the setup of Theorem 4.1 and let ε > 0. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). Assume P is randomly sampled with a probability distribution supported in {P ∈ S(m+n) + : ∥P∥F < ε} and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Then for any SOSP (ˆu, ˆv) of ˆLλ,P ˆLλ(ˆuˆv⊺) ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗ = min δ ˆLλ(δ) + 2ε∥δ⋆ λ∥∗. I.e., if (ˆu, ˆv) is an SOSP (and thus a global minimizer by Theorem 4.1) of the perturbed loss ˆLλ,P , then it is an ε- approximate minimizer of the unperturbed loss ˆLλ. So if r(r+1) 2 > KN, then Theorem 2.3, Theorem 2.4, and Corollary 4.2 together establish that (stochastic) gradient descent finds a ˆuˆv⊺ such that its unperturbed empirical risk is ε-close to the the minimum unperturbed empirical risk. 4.1. Proof outlines The proof is done by continuing our analysis of global mini- mum of ˆLλ(δ). Given that low-rank solution exists, which we proved in the previous section, recall that LoRA training with weight decay is equivalent to solving argmin u,v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F . In this section, we relate SOSPs with global minimum, which opens the chance to find a global minimum by using gradient-based optimization methods. We start the analysis from the following lemma, which is a prior characterization of SOSPs in the matrix factorization. Lemma 4.3. (Theorem 2 of (Haeffele et al., 2014)) Let G: S(m+n) + → R be a twice differentiable convex function with compact level sets, H : S(m+n) + → R be a proper con- vex lower semi-continuous function, and r > 0. If the function F : U 7→ G(UU ⊺) +H(UU ⊺) defined over matri- ces U ∈ R(m+n)×r has a second order staionary point at a rank-deficient matrix U, then UU ⊺ is a global minimum of G + H. We build our analysis upon Lemma 4.3. However, Lemma 4.3 is not directly applicable to our setting since it requires that the SOSP must be rank-deficient. However, this can be effectively circumvented by employing a per- turbed empirical risk: minimize u, v ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, where Q = \u0014u v \u0015 , and P is a positive semi-definite matrix. Now we get the following lemma by applying Lemma 4.3 to the perturbed empricial risk. Lemma 4.4. Fix λ ≥ 0. Assume ˆLλ(δ) has a global min- imum (not necessarily unique), P ∈ S(m+n) + is nonzero positive semi-definite, and r >0. If ˆQ = \u0014ˆu ˆv \u0015 ∈ R(m+n)×r is a rank deficient SOSP of ˆLλ,P (u, v) = ˆL(uv⊺) + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F + ⟨P, QQ⊺⟩, then ˆQ is a global minimum of ˆLλ,P (u, v). Proof. Define G, H: S(m+n) + → R to be G(X) = λ 2 tr(X) + ⟨P, X⟩, H (X) = ˆL( ¯X) where ¯X is the off-diagonal submatrix of X defined in (2). Note that G has compact level set for every λ ≥ 0 since tr(X) ≥ 0 and P, Xare positive semi-definite, concluding that ˆQλ,P is a global minimum of F(Q) ≜ G(QQ⊺) + H(QQ⊺) = ˆLλ,P (u, v). We now give a detailed analysis of the proof of Theorem 4.1. The structure of the proof is inspired by the original work of Pataki (1998) and followed by Burer & Monteiro (2003); Boumal et al. (2016); Du & Lee (2018). The proof uses an application of Sard’s theorem of differential geometry. The argument is captured in Lemma 4.5, and its proof is deferred to Appendix B. Lemma 4.5. Let M be m-dimensional smooth manifold embedded in Rd and V be a linear subspace of Rd with dimension n. If m + n < d, then the set M + V = {p + v : p ∈ M, v∈ V } has Lebesgue measure zero in Rd. Proof of Theorem 4.1. We show that second-order station- ary point ˆQλ,P = \u0014ˆu ˆv \u0015 is rank-deficient for almost all posi- tive semi-definite P, then use Lemma 4.4 to complete the proof. Denote f(j) for the j-th coordinate of f. For simplic- ity of notations, define ˆY (j) i ≜ f(j) W0 (Xi) + ⟨G(j)(Xi), uv⊺⟩, 6LoRA Training in the NTK Regime has No Spurious Local Minima and v(j) i ≜ 1 N ∂ ∂ ˆY (j) i ℓ( ˆYi, Yi) for 1 ≤ i ≤ N and 1 ≤ j ≤ K, which depends on u and v. Then for v = {v(j) i } ∈RKN define S(v) ≜ NX i=1 KX j=1 v(j) i G(j)(Xi) ∈ Rm×n. Then by first-order gradient condition, we have  \u0014 0 S(v) S(v)⊺ 0 \u0015 + λI + P | {z } ≜M ! ˆQλ,P = 0 We observe that the range of ˆQλ,P ∈ R(m+n)×r is in the nullspace of M ∈ S(m+n). We now suppose ˆQλ,P has full rank, i.e., rank( ˆQλ,P ) = r. Hence, we have the following inequality: r = rank( ˆQλ,P ) ≤ dim N(M) ≤ m + n Now for r ≤ s ≤ m + n and s ∈ Z, define As = n P : P = M − λI, M∈ S(m+n), dimN(M) = s o . Then from Proposition 2.1 of (Helmke & Shayman, 1995), As is a smooth manifold embedded in R (m+n)(m+n+1) 2 ∼= S(m+n) with dimension dimAs = (m + n + 1)(m + n) 2 − s(s + 1) 2 . Now by definition of P, we know that P ∈ m+n[ s=r (As + R(S)) where “ + ”is the set-sum (Minkowski sum) and R(S) is the range of S(v) in R (m+n)(m+n+1) 2 for any v ∈ RKN . The dimensions can be bounded by dimAs ≤ (m + n)(m + n + 1) 2 − r(r + 1) 2 for r ≤ s ≤ m + n and dimR(S) ≤ KN. Therefore given that r(r+1) 2 > KN, we have dimAs + dimR(S) < (m + n)(m + n + 1) 2 . Then, by Lemma 4.5, which is effectively an application of Sard’s theorem, we can conclude As + R(S) is a measure- zero set, and the finite union of such measure-zero sets is measure-zero. This implies that every P that makes ˆQλ,P to be of full rank must be chosen from measure-zero subset of S(m+m) + ⊂ S(m+n). Therefore we may conclude that rank( ˆQλ,P ) < rfor almost every nonzero positive semi- definite P. Proof of Corollary 4.2. Assume δ⋆ λ ∈ argminδ ˆLλ(δ). We observe the following chain of inequalities. ˆL(ˆδ) + λ∥ˆδ∥∗ ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F ≤ ˆL(ˆuˆv⊺) + λ 2 ∥ˆu∥2 F + λ 2 ∥ˆv∥2 F + ⟨P, ˆQ ˆQ⊺⟩ = ˆLλ,P (ˆu, ˆv), where the first inequality of is from Lemma 2.2, the second is from P and ˆQ ˆQ⊺ being positive semi-definite. On the other hand, we can find u⋆ and v⋆ such that δ⋆ λ = u⋆v⋆⊺ and ∥δ⋆ λ∥∗ = 1 2 (∥u⋆∥2 F + ∥v⋆∥2 F ) by using Lemma 2.2. Now take Q⋆ = \u0014u⋆ v⋆ \u0015 , then we get ˆLλ,P (u⋆, v⋆) = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ⟨P, Q⋆Q⋆⊺⟩ ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆Q⋆⊺∥F ≤ ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥Q⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + ε∥u⋆∥2 F + ε∥v⋆∥2 F = ˆL(δ⋆ λ) + λ∥δ⋆ λ∥∗ + 2ε∥δ⋆ λ∥∗, where the first inequality is Cauchy–Schwartz inequality, and the second inequality is from sub-multiplicativity of ∥ · ∥F . Moreover by Theorem 4.1, ˆLλ,P (ˆu⋆, ˆv⋆) ≤ ˆLλ,P (u⋆, v⋆), and this happens for almost sure, since we sampled P from a probability distribution which is absolutely continuous with respect to the Lebesgue measure on R (m+n)(m+n+1) 2 ∼= S(m+n). 5. Low-rank LoRA solution generalizes well In this section, we establish a generalization guarantee for the low-rank solution obtained by minimizing the perturbed loss ˆLλ,P of Theorem 4.1. For simplicity, we restrict the following main result to the cross-entropy loss. General- ization guarantees for general convex, non-negative, and twice continuously differentiable losses, are provided as Theorem C.6 in Appendix C. Theorem 5.1. Assume ℓ is cross-entropy loss. Assume the population risk L has a minimizer (not necessarily unique) and denote it as δ⋆ true ∈ argminδ L(δ). Assume δ⋆ true ̸= 0. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely 7LoRA Training in the NTK Regime has No Spurious Local Minima (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 2.Training curves (training loss vs. epochs) on different NLP tasks. with respect to the random data X ∼ P. Let ε >0, η ∈ (0, 1), and λ = 2(2 + ε) √ KR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ). Consider the setup of Corollary 4.2 withP randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺)−L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N \u0012 2 + r log 1 η \u0013 . In the context of fine-tuning, where the target task is closely related to the pre-training task, it is natural to assume that δ⋆ true in Theorem 5.1 is “small”. The proof, deferred to Appendix C, utilizes standard arguments with Rademacher complexity. 6. Experiments In this section, we conduct simple experiments on fine- tuning linearized pre-trained models to validate our theory.1 1Code available at https://github.com/UijeongJang/LoRA-NTK. Experimental setup on NLP tasks. We use prompt-based fine-tuning (Schick & Sch¨utze, 2021; Gao et al., 2021) and consider the same architecture and dataset as in (Malladi et al., 2023), which empirically verifies that with prompt- based fine-tuning, the fine-tuning dynamics stay within the NTK regime. We present the results of six NLP tasks that were also considered in (Malladi et al., 2023): sen- timent analysis (SST-2, MR, CR), natural language infer- ence (QNLI), subjectivity (Subj), and paraphrase detection (QQP). We optimize a linearized RoBERTa-base (Liu et al., 2019) model with dataset of size 32 ( N = 32) with two labels (K = 2) using cross entropy loss. With LoRA rank r ≥ 11, our theory guarantees that no spurious local min- ima exist. For a baseline comparison, we also perform full fine-tuning (without LoRA) on the linearized model. The training curves are presented in Figure 2, and additional details are provided in Appendix D. Results showing test accuracy are also presented in Appendix D. Experimental setup on image and speech classification tasks. We use a pre-trained vision transformer (Dosovit- skiy et al., 2021) and fine-tune it on the bean disease dataset (Makerere AI Lab, 2020) to perform an image classification task with 3 labels. We use dataset of size 48 with three labels. Similar to our experiments on NLP tasks, we find that training curves converge to the same loss value, where the rates of convergence differ. For speech classification, we use a pre-trained wav2vec2 (Baevski et al., 2020) model and fine-tune it on a SUPERB dataset (Yang et al., 2021) to perform a speech classification 8LoRA Training in the NTK Regime has No Spurious Local Minima (a) Image classification  (b) Speech classification Figure 3.Training curves (training loss vs. epochs) on image and speech classification tasks. task with 4 labels. We use a dataset of size 64 with four labels. We also find that the training curves converge to the same loss value. The details are the same as with the image classification task. The training curves of both image and speech data are pre- sented in Figure 3, and additional details are provided in Appendix D. Empirical observation. The experiments validate our the- ory as the training curves converge to the same globally optimal loss value. However, we do observe that therates of convergence differ. When the LoRA rank is higher or when full fine-tuning is performed and LoRA is not used, fine-tuning converges faster. Indeed, our theory ensures that spurious local minima do not exist, but it says nothing about how convex or favorable the landscape may or may not be. Our intuitive hypothesis is that using lower LoRA rank creates unfavorable regions of the loss landscape, such as plateaus or saddle points, and they slow down the gradient descent dynamics. If this hypothesis is generally true, we face an interesting tradeoff: lower LoRA rank reduces memory cost and per- iteration computation cost but increases the number of it- erations needed for convergence. Then, using a very low LoRA rank may be suboptimal not due to representation power, presence of spurious local minima, or poor general- ization guarantees, but rather due to unfavorable flat training landscapes slowing down convergence. Exploring this phe- nomenon and designing remedies is an interesting direction for future work. 7. Conclusion In this work, we present theoretical guarantees on the train- ability and generalization capabilities of LoRA fine-tuning of pre-trained models. Together with the work of Zeng & Lee (2024), our results represent a first step in theoretically analyzing the LoRA fine-tuning dynamics of pre-trained models by presenting guarantees (upper bounds). For future work, carrying out further refined analyses under more spe- cific assumptions, relaxing the linearization/NTK regime assumption through a local analysis, better understanding the minimum rank requirement through lower bounds, and, motivated by the observation of Section 6, analyzing the tradeoff between training rate and LoRA rank are exciting directions. Acknowledgments UJ and EKR were supported by the Samsung Science and Technology Foundation (Project Number SSTF-BA2101- 02) and the National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIP) [NRF- 2022R1C1C1010010]. JDL acknowledges support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award 2144994. We thank Jungsoo Kang for the discussion on the proof of Lemma 4.5. We also thank Jisun Park for providing valuable feedback. Impact statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Aghajanyan, A., Zettlemoyer, L., and Gupta, S. Intrin- sic dimensionality explains the effectiveness of language model fine-tuning. Association for Computational Lin- guistics, 2021. Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. International Conference on Machine Learning, 2019a. Allen-Zhu, Z., Li, Y ., and Song, Z. On the convergence rate of training recurrent neural networks. Neural Information Processing Systems, 2019b. Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an infinitely 9LoRA Training in the NTK Regime has No Spurious Local Minima wide neural net. Neural Information Processing Systems, 2019. Bach, F. Learning Theory from First Principles . Draft, 2023. Bach, F., Mairal, J., and Ponce, J. Convex sparse matrix factorizations. arXiv preprint arXiv:0812.1869, 2008. Baevski, A., Zhou, Y ., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Neural Information Processing Systems, 2020. Barron, A. R. Universal approximation bounds for super- positions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930–945, 1993. Bartlett, P. L. and Mendelson, S. Rademacher and gaussian complexities: risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. Bartlett, P. L., Boucheron, S., and Lugosi, G. Model selec- tion and error estimation. Machine Learning, 48:85–113, 2002. Bartlett, P. L., Bousquet, O., and Mendelson, S. Local rademacher complexities. The Annals of Statistics, 33(4): 1497–1537, 2005. Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. Spectrally- normalized margin bounds for neural networks. Neural Information Processing Systems, 2017. Barvinok, A. I. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computational Geometry, 13:189–202, 1995. Bengio, Y . and Delalleau, O. On the expressive power of deep architectures. Algorithmic Learning Theory, 2011. Bhojanapalli, S., Boumal, N., Jain, P., and Netrapalli, P. Smoothed analysis for low-rank solutions to semidefi- nite programs in quadratic penalty form. Conference On Learning Theory, 2018. Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J. Transformers learn through gradual rank increase. Neural Information Processing Systems, 2023. Boumal, N., V oroninski, V ., and Bandeira, A. The non-convex Burer–Monteiro approach works on smooth semidefinite programs. Neural Information Processing Systems, 29, 2016. Bousquet, O. and Elisseeff, A. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. Burer, S. and Monteiro, R. D. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329– 357, 2003. Cabral, R., De la Torre, F., Costeira, J. P., and Bernardino, A. Unifying nuclear norm and bilinear factorization ap- proaches for low-rank matrix decomposition. Interna- tional Conference on Computer Vision, 2013. Chen, Z., Cao, Y ., Gu, Q., and Zhang, T. A generalized neu- ral tangent kernel analysis for two-layer neural networks. Neural Information Processing Systems, 2020. Choi, J. Y ., Park, J., Park, I., Cho, J., No, A., and Ryu, E. K. LoRA can replace time and class embeddings in diffusion probabilistic models. NeurIPS 2023 Workshop on Diffusion Models, 2023. Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of Control, Signals and Systems, 2(4):303–314, 1989. Delalleau, O. and Bengio, Y . Shallow vs. deep sum-product networks. Neural Information Processing Systems, 2011. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoRA: efficient finetuning of quantized llms. Neural Information Processing Systems, 2023. Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y . Sharp minima can generalize for deep nets. International Con- ference on Machine Learning, 2017. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. Du, S. and Lee, J. On the power of over-parametrization in neural networks with quadratic activation. International Conference on Machine Learning, 2018. Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient descent finds global minima of deep neural networks. International Conference on Machine Learning, 2019. Du, S. S., Jin, C., Lee, J. D., Jordan, M. I., Singh, A., and Poczos, B. Gradient descent can take exponential time to escape saddle points. Neural Information Processing Systems, 2017. Duan, Y ., Ji, G., Cai, Y ., et al. Minimum width of leaky- relu neural networks for uniform universal approximation. International Conference on Machine Learning, 2023. 10LoRA Training in the NTK Regime has No Spurious Local Minima Fu, Z., Yang, H., So, A. M.-C., Lam, W., Bing, L., and Collier, N. On the effectiveness of parameter-efficient fine-tuning. AAAI Conference on Artificial Intelligence, 2023. Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. Association for Computational Linguistics, 2021. Ge, R., Huang, F., Jin, C., and Yuan, Y . Escaping from saddle points—online stochastic gradient for tensor de- composition. Conference on Learning Theory, 2015. Ge, R., Lee, J. D., and Ma, T. Matrix completion has no spurious local minimum. Neural Information Processing Systems, 2016. Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers as pro- grammable computers. International Conference on Ma- chine Learning, 2023. Haeffele, B., Young, E., and Vidal, R. Structured low-rank matrix factorization: optimality, algorithm, and applica- tions to image processing. International Conference on Machine Learning, 2014. Hardt, M., Recht, B., and Singer, Y . Train faster, generalize better: stability of stochastic gradient descent. Interna- tional Conference on Machine Learning, 2016. Helmke, U. and Shayman, M. A. Critical points of matrix least squares distance functions. Linear Algebra and its Applications, 215:1–19, 1995. Hornik, K., Stinchcombe, M., and White, H. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551–560, 1990. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., Chen, W., et al. LoRA: low-rank adaptation of large language models. International Conference on Learning Representations, 2021. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker- nel: convergence and generalization in neural networks. Neural Information Processing Systems, 2018. Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. How to escape saddle points efficiently. Interna- tional Conference on Machine Learning, 2017. Jin, J., Li, Z., Lyu, K., Du, S. S., and Lee, J. D. Under- standing incremental learning of gradient descent: A fine-grained analysis of matrix sensing. International Conference on Machine Learning, 2023. Johnson, W. and Lindenstrauss, J. Extensions of lipschitz maps into a hilbert space. Contemporary Mathematics, 26:189–206, 1984. Koltchinskii, V . and Panchenko, D. Rademacher processes and bounding the risk of function learning. In Gin ´e, E., Mason, D. M., and Wellner, J. A. (eds.), High Dimen- sional Probability II, pp. 443–457. Springer, 2000. Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B. Gra- dient descent only converges to minimizers. Conference on Learning Theory, 2016. Lialin, V ., Muckatira, S., Shivagunde, N., and Rumshisky, A. ReLoRA: high-rank training through low-rank up- dates. Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Re- source Optimization, 2023. Liu, L., Liu, X., Gao, J., Chen, W., and Han, J. Under- standing the difficulty of training transformers. Empirical Methods in Natural Language Processing, 2020. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . RoBERTa: a robustly optimized BERT pretraining ap- proach. arXiv preprint arXiv:1907.11692, 2019. Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres- sive power of neural networks: a view from the width. Neural Information Processing Systems, 2017. Makerere AI Lab. Bean disease dataset, 2020. URLhttps: //github.com/AI-Lab-Makerere/ibean/. Malladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S. A kernel-based view of language model fine-tuning. Inter- national Conference on Machine Learning, 2023. Maurer, A. A vector-contraction inequality for rademacher complexities. Algorithmic Learning Theory, 2016. McDiarmid, C. et al. On the method of bounded differences. Surveys in Combinatorics, 141(1):148–188, 1989. Pataki, G. On the rank of extreme matrices in semidefi- nite programs and the multiplicity of optimal eigenval- ues. Mathematics of Operations Research, 23(2):339– 358, 1998. Pataki, G. The geometry of semidefinite programming. In Wolkowicz, H., Saigal, R., and Vandenberghe, L. (eds.), Handbook of Semidefinite Programming: Theory, Algo- rithms, and Applications, pp. 29–65. Springer, 2000. 11LoRA Training in the NTK Regime has No Spurious Local Minima Pilanci, M. and Ergen, T. Neural networks are convex reg- ularizers: exact polynomial-time convex optimization formulations for two-layer networks. International Con- ference on Machine Learning, 2020. Polyak, B. T. Introduction to Optimization . New York, Optimization Software, 1987. Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010. Ryu, S. Low-rank adaptation for fast text-to-image diffu- sion fine-tuning, 2023. URL https://github.com/ cloneofsimo/lora. Schick, T. and Sch¨utze, H. Exploiting cloze questions for few shot text classification and natural language inference. Association for Computational Linguistics, 2021. Smith, J. S., Hsu, Y .-C., Zhang, L., Hua, T., Kira, Z., Shen, Y ., and Jin, H. Continual diffusion: continual customiza- tion of text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027, 2023. Soltanolkotabi, M., Javanmard, A., and Lee, J. D. Theo- retical insights into the optimization landscape of over- parameterized shallow neural networks. IEEE Transac- tions on Information Theory, 65(2):742–769, 2018. Sridharan, K., Shalev-Shwartz, S., and Srebro, N. Fast rates for regularized objectives.Neural Information Processing Systems, 21, 2008. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention is all you need. Neural Information Processing Systems, 2017. Wei, A., Hu, W., and Steinhardt, J. More than a toy: random matrix models predict how real-world neural representa- tions generalize. International Conference on Machine Learning, 2022a. Wei, C., Chen, Y ., and Ma, T. Statistically meaningful approximation: a case study on approximating turing ma- chines with transformers. Neural Information Processing Systems, 2022b. Wu, L., Zhu, Z., et al. Towards understanding generalization of deep learning: perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Yang, S.-w., Chi, P.-H., Chuang, Y .-S., Lai, C.-I. J., Lakho- tia, K., Lin, Y . Y ., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., et al. Superb: Speech processing universal performance benchmark. Interspeech, 2021. Yeh, S.-Y ., Hsieh, Y .-G., Gao, Z., Yang, B. B., Oh, G., and Gong, Y . Navigating text-to-image customization: from LyCORIS fine-tuning to model evaluation. International Conference on Learning Representations, 2024. Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers universal approximators of sequence-to-sequence functions? International Confer- ence on Learning Representations, 2019. Zeng, Y . and Lee, K. The expressive power of low-rank adaptation. International Conference on Learning Repre- sentations, 2024. Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107– 115, 2021. Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. Why are adaptive methods good for attention models? Neural Information Processing Systems, 2020. Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. Machine Learning, 109(3):467–492, 2020. 12LoRA Training in the NTK Regime has No Spurious Local Minima A. Omitted proof of Theorem 3.1 Here, we explain the details in the proof of Theorem 3.1. We first prove the equivalence of minimize δ∈Rm×n ˆL(δ) + λ∥δ∥∗ (P) and minimize Z∈S(m+n) + ˆL( ¯Z) + λ 2 tr(Z) (Q) where ¯Z = Z[1 : m, m+ 1 : m + n] ∈ Rm×n. I.e., ¯Z is a off-diagonal submatrix of X such that Z = \u0014 ∗ ¯Z ¯Z⊺ ∗ \u0015 . Lemma A.1. The following two statements hold. 1. Fix λ ≥ 0 and suppose (P) has a global minimizer (not necessarily unique). Let δ⋆ λ ∈ Rm×n be a global minimizer of (P). Then there exists an Z⋆ λ ∈ S(m+n) + induced from δ⋆ λ such that Z⋆ λ is a global minimizer of (Q), rank(Z⋆ λ) = rank(δ⋆ λ), and has same objective value. 2. Fix λ ≥ 0 and suppose (Q) has a global minimizer (not necessarily unique). Let Z⋆ λ ∈ S(m+n) + be a global minimum of (Q). Then ¯Z⋆ λ ∈ Rm×n is a global minimizer of (P) such that rank( ¯Z⋆ λ) = min(m, n,rank(Z⋆ λ)) and has same objective value. Proof. We prove the two statements at once. Let δ⋆ λ ∈ Rm×n be a global minimizer of (P) and let r = rank(δ⋆ λ). Then by Lemma 2.2, there exists u ∈ Rm×r and v ∈ Rn×r such that ∥δ⋆ λ∥∗ = 1 2 (∥u∥2 F + ∥v∥2 F ) and uv⊺ = δ⋆ λ. Take Z⋆ λ = \u0014u v \u0015\u0002u⊺ v⊺\u0003 = \u0014uu⊺ uv⊺ vu⊺ vv⊺ \u0015 ∈ S(m+n) + . Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = \r\r\r \u0014u v \u0015\r\r\r 2 F = ∥u∥2 F + ∥v∥2 F = 2∥δ⋆ λ∥∗, (Q) with Z⋆ λ has the same objective value with (P) with δ⋆ λ and rank(δ⋆ λ) = rank(Z⋆ λ) = r. Conversely, let Z⋆ λ ∈ S(m+n) + be a global minimizer of (Q) and let rank(Z⋆ λ) = r. Note that r may be larger than m or n. Then there exists Q = \u0014u v \u0015 ∈ R(m+n)×r such that QQ⊺ = Z⋆ λ. Then since tr(Z⋆ λ) = ∥Z⋆ λ∥∗ = ∥Q∥2 F = ∥u∥2 F + ∥v∥2 F ≥ 2∥uv⊺∥∗ = 2∥ ¯Z⋆ λ∥∗, the objective value of (P) with ¯Z⋆ λ ∈ Rm×n has less than or equal to minimum objective value of (Q) and rank( ¯Z⋆ λ) = min(m, n, r). If there exists m × n matrix whose objective value of (P) is strictly less than the minimum objective value of (Q), then we repeat the same step that was applied on δ⋆ λ to induce a solution of (Q) with strictly less objective value, which is a contradiction. Conversely, if there exists positive semi-definite matrix of size m + n whose objective value of (Q) is strictly less than the minimum objective value of (P), then we repeat the same step applied on Z⋆ λ to induce a solution of (P) with strictly less objective value, which is also a contradiction. Therefore if one of (P) and (Q) has a global minimizer, the other must have a global minimizer with same objective value. Next lemma states that if the rank of the global minimizer of (Q) is sufficiently large, then we can find an another solution with strictly less rank. Lemma A.2. Suppose X ∈ Sn + and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists nonzero t∗ ∈ R such that X + t∗Z is positive semi-definite and rank(X + t∗Z) < rank(X). 13LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let r = rank(X). Suppose Q ∈ Rn×r is a matrix where its columns are basis to R(X). Now suppose µ1(Q⊺(X + tZ)Q) > 0 for all t ∈ R where µ1(·) denotes the smallest eigenvalue (note thatµ1(·) is continuous). Then Q⊺(X +tZ)Q ∈ Sr should be positive definite for all t. For contradiction, take v ∈ R(Z) ⊆ R(X) = R(Q) to be an eigenvector of nonzero eigenvalue of Z. Since v⊺Xv >0 and v⊺Zv ̸= 0, there exists some t such that v⊺(X + tZ)v <0. Now take w ∈ Rr such that Qw = v. Then it follows that w⊺(Q⊺(X + tZ)Q)w <0, which is a contradiction. This implies that there exists t⋆ ̸= 0 such that µ1(Q⊺(X + t∗Z)Q) = 0, Hence we have r >rank(Q⊺(X + t∗Z)Q) = rank(X + t∗Z) and Q⊺(X + t∗Z)Q is positive semi-definite. To show thatX + t∗Z is positive semi-definite, take any x ∈ Rn and consider the decomposition x = Qy + z where y ∈ Rr and z ∈ N(Q) = N(X) ⊆ N(Z). Then, we have y⊺(X + t⋆Z)y = (y⊺Q⊺ + z⊺)(X + t∗Z)(Qy + z) = y⊺Q⊺(X + t∗Z)Qy ≥ 0. Finally, the following lemma and its proof are similar to the previous one, but we state it separately for the sake of clarity. It will be used in the proof of Theorem 3.1. Lemma A.3. Suppose X ∈ Sn + which is nonzero and let Z ∈ Sn be a nonzero symmetric matrix such that R(Z) ⊆ R(X). Then there exists t∗ > 0 such that X ± t∗Z is positive semi-definite. Proof. Let rank(X) = r and {y1, . . . , yr} be orthonormal eigenvectors of nonzero eigenvalues of X. Since y⊺ i Xyi > 0 for all yi, i = 1, ··· , r, there exists an interval (−ai, ai) for ai > 0 such that y⊺ i (X ± tZ)yi ≥ 0 for t ∈ (−ai, ai). Take t∗ = min{a1, . . . , ar}. Then t∗ satisfies the statement of the theorem. Now we provide the complete proof of Theorem 3.1. Proof of Theorem 3.1. Suppose Z⋆ λ ∈ S(m+n) + is a global minimizer of F(Z) = ˆL( ¯Z) + λ 2 tr(Z) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), ¯Z⟩, Yi \u0001 + λ 2 tr(Z) which is induced from δ⋆ λ ∈ Rm×n by Lemma A.1. Suppose there exists nonzero symmetric matrix Z such that Z ∈ S(Z⋆ λ) ≜ {Z ∈ S(m+n) : R(Z) ⊆ R(Z⋆ λ)} and ⟨G(Xi), Z⟩ = 0 for 1 ≤ i ≤ N. In other words, Z ∈ S(Z⋆ λ) ∩ N(A) where A: S(m+n) → RKN is a linear operator defined as A(Z)ij = ⟨G(j)(Xi), ¯Z⟩, 1 ≤ i ≤ N, 1 ≤ j ≤ K. Then there exists t >0 such that Z⋆ λ ± tZ is positive semi-definite by Lemma A.3, since Z⋆ must be nonzero. Therefore tr(Z) = 0, otherwise it will contradict the minimality of Z⋆ λ. Also we know that there exists nonzero t∗ ∈ R such that Z⋆ λ + t∗Z is also positive semi-definite with strictly lower rank by Lemma A.2. Since tr(Z) = 0, Z⋆ λ + t∗Z is also a global minimizer of F. Replace Z⋆ λ with Z⋆ λ + tZ and repeat this process until we find a solution Z⋆ λ with {0} = S(Z⋆ λ) ∩ N(A). Now we let rank(Z⋆ λ) = r. Then by dimension counting, we have the following inequality. 0 = dimS(Z⋆ λ) + dimN(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) + dim(S(m+n)) − dimR(A) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆ λ) − KN + dim(S(m+n)) − dim(S(Z⋆ λ) + N(A)) = dimS(Z⋆) − KN + dim(S(Z⋆)⊥ ∩ R(A)) ≥ dimS(Z⋆ λ) − KN 14LoRA Training in the NTK Regime has No Spurious Local Minima Now we prove that dimS(Z⋆ λ) = r(r+1) 2 to complete the proof. Consider the diagonalization Z⋆ λ = UΛU⊺ where U is a orthogonal matrix. Since the dimension of the subspace is invariant under orthogonal transformations, we have dimS(Z⋆ λ) = dimS(Λ) = dim{Z ∈ S(m+n) : R(Z) ⊆ R(Λ)} where Λ is diagonal matrix with nontrivial entries in the leading principle minor of size r × r. This restricts the symmetric matrix Z to have nontrivial entries only in the leading r × r block. Hence, dimS(Z⋆ λ) = r(r+1) 2 . B. Omitted proof of Lemma 4.5 We prove Lemma 4.5 in this section. Proof of Lemma 4.5. Let ΠV ⊥ : Rd → V ⊥ be the orthogonal projection onto the orthogonal complement of V in Rd. Then, ΠV ⊥|M : M →V ⊥ is a smooth mapping between manifolds. Since dimV ⊥ = d − n > m= dimM, p is singular for all p ∈ M. Therefore ΠV ⊥(M) has measure zero in Rd−n by Sard’s theorem. Note that M + V ⊆ ΠV ⊥(M) + V and the measure of ΠV ⊥(M) + V in Rd is zero. This concludes that M + V is measure-zero in Rd. As a remark, the prior works of (Boumal et al., 2016; Du & Lee, 2018) also use dimension-counting arguments that would warrant the use of Lemma 4.5, but they do not provide a precise justification. Our Theorem 4.1 makes a similar argument, but does so fully rigorous through Lemma 4.5. C. Generalization guarantee In this section, let ℓ(·, ·) be our loss function which is convex, non-negative, and twice-differentiable on the first argument. Then, our empirical risk is ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 . We start the analysis from this non-regularized risk and expand it to regularized ones. We assume that our model is class of affine predictors X 7→ fW0 (X) + ⟨G(X), δ⟩ for given data X. Now we apply the theory of Rademacher complexity to derive the upper bound of the generalization bound. To begin with, we start with introducing the classical result in probability theory from (McDiarmid et al., 1989) without proof. Lemma C.1. (McDiarmid inequality) Let X1, . . . , XN ∈ Xbe i.i.d N random samples from dataset X. Let g : XN → R be a function satisfying the following property with c >0: |g(X1, . . . , Xi−1, Xi, Xi+1, . . . , XN ) − g(X1, . . . , Xi−1, X′ i, Xi+1, . . . , XN )| ≤c for all X1, . . . , XN , X′ i ∈ X. Then, for all ε >0, P(|g(X1, . . . , XN ) − E[g(X1, . . . , XN )]| ≥ε) ≤ exp \u0012 − 2ε2 Nc2 \u0013 . Now, we define the Rademacher complexity of the class of functions H from X to R: RN (H) = Eε,D   sup h∈H 1 N NX i=1 εih(Xi) ! , where {εi}1≤i≤N are independent Rademacher random variables, and D = {X1, . . . , XN } is N random samples from X. In our analysis, we will focus on class of affine predictors Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ and composition of affine predictors with loss Xi 7→ ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi). Rademacher complexities are closely related to upper bounds on generalization bound due to the following lemma. 15LoRA Training in the NTK Regime has No Spurious Local Minima Lemma C.2. Let RN (H) be the Rademacher complexity of the class of functions H from X to R and X1, . . . , XN are N samples from X. Then the following inequality holds. E \" sup h∈H   1 N NX i=1 h(Xi) − E[h(X)] !# ≤ 2RN (H), E \" sup h∈H   E[h(X)] − 1 N NX i=1 h(Xi) !# ≤ 2RN (H). Proof. The proof is by using standard symmetrization arguments. We defer its proof to Theorem 8 of (Bartlett & Mendelson, 2002), or Section 4.5 of (Bach, 2023). The next lemma uses a contraction property to reduce the Rademacher complexity of losses to linear predictors. These type of results are widely used in Rademacher analysis and we use the following specific version of contraction, which was originally introduced in Corollary 4 of (Maurer, 2016) and adapted to our setting. Write ∥ · ∥2 for Euclidean vector norm. Lemma C.3. Let A be the class of functions a : X →RK. For 1 ≤ i ≤ N, let ℓi : RK → R be G-Lipschitz continuous on A with respect to the Euclidean norm in the sense that the following holds: |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ∈ X. Then we have the following inequality for independent Rademacher random variables {σi}1≤i≤N and {εij}1≤i≤N,1≤j≤K: Eσ,D \" sup a∈A 1 N NX i=1 σiℓi(a(Xi)) # ≤ √ 2G · Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)  , where aj denotes the j-th coordinate of a and D = {(Xi, Yi)}i∈{1,...,N} are i.i.d N random samples sampled from X. Proof. We defer the proof to the Section 5 of (Maurer, 2016). In Lemma C.3, if we sample D from a probability distribution P, we can relax the Lipschitz continuity condition to hold for P- almost surely. In other words, |ℓ(a(X1)) − ℓ(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. The next lemma states that the Rademacher complexity of class of bounded affine predictors decays at most O( 1√ N ) rate. Lemma C.4. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Assume AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D >0. Suppose ∥G(j)(Xi)∥F ≤ R almost surely with respect to the random data Xi ∼ P. Then, Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   ≤ RD √ K√ N where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. 16LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Eε  sup a∈A 1 N NX i=1 KX j=1 εijaj(Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 f(j) W0 (Xi) + ⟨G(j)(Xi), δ⟩ \u0011   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩ + 1 N NX i=1 KX j=1 εijf(j) W0 (Xi)   = Eε   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   ≤ Eε   sup ∥δ∥F ≤D 1 N NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = Eε  D N sup ∥δ∥F ≤1 NX i=1 KX j=1 εij⟨G(j)(Xi), δ⟩   = D N Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F . The inequality is from the fact that ∥ · ∥F ≤ ∥ · ∥∗, hence {δ : ∥δ∥∗ ≤ D} ⊂ {δ : ∥δ∥F ≤ D}. The last equality is from the fact that ∥ · ∥F is self-dual. Next, we can bound Eε \r\r\rPN i=1 PK j=1 εijG(j)(Xi) \r\r\r F by the following inequalities. Eε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r F ≤ vuuutEε \r\r\r\r\r\r NX i=1 KX j=1 εijG(j)(Xi) \r\r\r\r\r\r 2 F = vuutEε NX i=1 KX j=1 \r\rεijG(j)(Xi) \r\r2 F = vuut NX i=1 KX j=1 \r\rG(j)(Xi) \r\r2 F ≤ R √ NK. a.s. The first inequality is from Jensen’s inequality, the equalities are from i.i.d assumption ofεik. We combine the results and take expectation with respect to D to get Eε,D  sup a∈A 1 N NX i=1 KX j=1 εijak(Xi)   ≤ D N · R √ NK = RD √ K√ N . We then combine the previous results to get the following Lemma. Lemma C.5. Assume D = {(Xi, Yi)}i∈{1,...,N} is i.i.d N random samples sampled from probability distribution P. Let ˆL is non-regularized empirical risk defined as ˆL(δ) = 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 17LoRA Training in the NTK Regime has No Spurious Local Minima and AD = {Xi 7→ fW0 (Xi) + ⟨G(Xi), δ⟩ ∈RK : ∥δ∥∗ ≤ D, δ ∈ Rm×n} is class of affine predictors with bounded nuclear norm D. For 1 ≤ j ≤ K, suppose ∥G(j)(X)∥F ≤ R almost surely with respect to the random data Xi ∼ P. For 1 ≤ i ≤ N, suppose ℓi ≜ ℓ(·, Yi) is G-Lipschitz continuous on A on the first argument (with respect to the Euclidean norm) for almost surely with respect to the random data Xi ⊆ D ∼ P. That is, |ℓi(a(X1)) − ℓi(a′(X2))| ≤G∥a(X1) − a′(X2)∥2 for any a, a′ ∈ A, X 1, X2 ⊆ D ∼ P. Then for any ∥δ∥∗ ≤ D, fixed δ0 such that ∥δ0∥∗ ≤ D, and η ∈ (0, 1), the following inequality holds with probability greater than 1 − η: ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . Proof. Take g of Lemma C.1 to be g = sup∥δ∥∗≤D(ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)), which is a function of X1, . . . , XN . Since ∥δ∥∗ ≤ D implies ∥δ∥F ≤ D and by the Lipschitz continuity of ℓ(·, Yi), we have the following for any (Xi, Yi) ∈ D: |ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ0⟩, Yi \u0001 − ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 | ≤G∥⟨δ0 − δ, G(Xi)⟩∥2 ≤ G vuut KX j=1 ∥δ0 − δ∥2 F ∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 ∥δ0 − δ∥2∗∥G(j)(Xi)∥2 F ≤ G vuut KX j=1 4D2 · R2 = 2GRD √ K. Hence if we change only one data point (Xi, Yi) of g to (X ′ i, Y ′ i ), the deviation of ˆL(δ0) − ˆL(δ) is at most 2GRD √ K N . Then by Lemma C.1, we have sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) < E \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # + t √ 2KGRD√ N with probability greater than 1 − e−t2 . The expectation on the right hand side can be reduced to ED \" sup ∥δ∥∗≤D (ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ)) # = ED \" sup ∥δ∥∗≤D (−ˆL(δ) + L(δ)) + ˆL(δ0) − L(δ0) # = ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # Note that L(δ) − ˆL(δ) = L(δ) − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) = E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i − 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi), 18LoRA Training in the NTK Regime has No Spurious Local Minima where the expectation is taken over Xi ∼ P. Now apply Lemma C.2 to get ED \" sup ∥δ∥∗≤D (L(δ) − ˆL(δ)) # = ED \" sup ∥δ∥∗≤D (E h ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi) i# − ED \" 1 N NX i=1 ℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # ≤ 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # where {σ}1≤i≤N are i.i.d Rademacher variables. Then apply Lemma C.3 to get 2Eσ,D \" sup ∥δ∥∗≤D 1 N NX i=1 σiℓ(fW0 (Xi) + ⟨G(Xi), δ⟩, Yi)) # = 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   where {εij}1≤i≤N,1≤j≤K are i.i.d Rademacher random variables. Finally, use Lemma C.4 to get 2 √ 2GEε,D   sup ∥δ∥∗≤D 1 N NX i=1 KX j=1 εij \u0010 fj W0 (Xi) + ⟨Gj(Xi), δ⟩ \u0011   ≤ 2 √ 2G · RD √ K√ N . Therefore, we conclude that ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N (2 + t) . for ∥δ∥∗ ≤ D with probability greater than 1 − e−t2 . By reparametrization, we get ˆL(δ0) − ˆL(δ) − L(δ0) + L(δ) < √ 2KGRD√ N \u0012 2 + r log 1 η \u0013 . for ∥δ∥∗ ≤ D with probability greater than 1 − η. Now we can extend this generalization guarantee of constrained optimization to regularized optimization, which aligns with our problem of interest. For notational convenience, let Lλ(δ) = L(δ) + λ∥δ∥∗, ˆLλ(δ) = ˆL(δ) + λ∥δ∥∗ We follow the proof structure of (Bach, 2023), which was motivated by (Bartlett et al., 2005) and (Sridharan et al., 2008). Theorem C.6. Fix ε >0 and let 0 ̸= δ⋆ true ∈ argminδ L(δ) be the true optimum of the population risk and consider the setup of Lemma C.5 with D = (2 + ε)∥δ⋆ true∥∗, which is the upper bound on the nuclear norm of the predictors. Let η ∈ (0, 1) and λ = (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Write δ⋆ λ to denote a minimizer (not necessarily unique) of ˆLλ(δ).Consider the setup of Corollary 4.2 with P randomly sampled with a probability distribution supported in n P ∈ S(m+n) + : ∥P∥F < ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ o and is absolutely continuous with respect to the Lebesgue measure on S(m+n) ∼= R (m+n)(m+n+1) 2 . Let (ˆu, ˆv) be an SOSP of ˆLλ,P . Then with probability greater than 1 − η, L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N \u0012 2 + r log 1 η \u0013 . 19LoRA Training in the NTK Regime has No Spurious Local Minima Proof. Let ˜ε = ελ∥δ⋆ true∥∗ 2∥δ⋆ λ∥∗ and consider the convex set C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) ≤ λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Then for ∥δ∥∗ = 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, δ /∈ intC since the following inequalities hold. Lλ(δ) − Lλ(δ⋆ true) = L(δ) − L(δ⋆ true) + λ∥δ∥∗ − λ∥δ⋆ true∥∗ ≥ λ∥δ∥∗ − λ∥δ⋆ true∥∗ = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Therefore the boundary ∂C of C should be ∂C = n δ : ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗, Lλ(δ) − Lλ(δ⋆ true) = λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ o . Now suppose ˆuˆv⊺ /∈ C. Then since δ⋆ true ∈ C, there exists δ in the segment [ˆuˆv⊺, δ⋆ true] such that δ ∈ ∂C. By the convexity of ˆLλ, we have ˆLλ(δ) ≤ max \u0010 ˆLλ(δ⋆ true), ˆLλ(ˆuˆv⊺) \u0011 . Then we get ˆLλ(δ⋆ true) − ˆLλ(δ) ≥ −2˜ε∥δ⋆ λ∥∗ by Corollary 4.2. Therefore, ˆL(δ⋆ true) − ˆL(δ) − L(δ⋆ true) + L(δ) = ˆLλ(δ⋆ true) − ˆLλ(δ) − Lλ(δ⋆ true) + Lλ(δ) ≥ Lλ(δ) − Lλ(δ⋆) − 2˜ε∥δ⋆ λ∥∗ (3) = λ∥δ⋆ true∥∗ Note that ∥δ∥∗ ≤ 2∥δ⋆ true∥∗ + 2˜ε λ ∥δ⋆ λ∥∗ < (2 + ε)∥δ⋆ true∥∗ and λ∥δ⋆ true∥∗ = ∥δ⋆ true∥∗ (2 + ε) √ 2KGR√ N \u0012 2 + r log 1 η \u0013 . Then by Lemma C.5, (3) should happen with probability less than η. Then with probability greater than 1 − η, ˆuˆv⊺ ∈ C. In other words, Lλ(ˆuˆv⊺) − Lλ(δ⋆ true) < λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗. Hence, L(ˆuˆv⊺) + λ∥ˆuˆv⊺∥∗ < Lλ(δ⋆ true) + λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ = L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + 2˜ε∥δ⋆ λ∥∗ ≤ L(δ⋆ true) + 2λ∥δ⋆ true∥∗ + ελ∥δ⋆ true∥∗. Finally, we get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ (2 + ε)2√ 2KGR√ N   2 + r log 1 δ ! . By using the fact that ℓCE is Lipschitz continuous, we can reduce Theorem C.6 to Theorem 5.1. Note that the loss function ℓ may not be Lipschitz continuous in general. However, Lipschitz continuity is a mild assumption when the domain is restricted to a bounded class of predictors AD of Lemma C.5. Proof of Theorem 5.1. If ℓ(·, Y): RK → R is cross entropy loss defined as ℓ(X, Y) = ℓCE (X, Y) = −log   exp X(j) PK i=1 exp X(i) ! = −X(j) + log  KX i=1 exp X(i) ! 20LoRA Training in the NTK Regime has No Spurious Local Minima with true label Y = j, we have ∇ℓCE (X, Y)j = −1 + exp X(j) PK i=1 exp X(i) = − P i̸=j exp X(Y ) PK i=1 exp X(i) and for k ̸= j, ∇ℓCE (X, Y)k = exp X(k) PK i=1 exp X(Y ) Then we can bound the Euclidean norm of the gradient as follows. ∥∇ℓCE (X, Y)∥2 2 = \u0010P i̸=j exp X(i) \u00112 \u0010PK i=1 exp X(i) \u00112 + P i̸=j exp 2X(k) \u0010PK i=1 exp X(i) \u00112 ≤ 1 + 1 = 2. Hence the gradient of the cross entropy loss is bounded by √ 2 and we may replace G in Theorem C.6 with √ 2 to get L(ˆuˆv⊺) − L(δ⋆ true) < ∥δ⋆ true∥∗ 2(2 + ε)2√ KR√ N   2 + r log 1 δ ! . D. Details of experiments Optimizing nuclear norm. Recall that SGD or GD on the loss function with weight decay and with regularization parameter λ is equivalent to minimizing 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), uv⊺⟩, Yi \u0001 + λ 2 ∥u∥2 F + λ 2 ∥v∥2 F , with respect to u and v. In full fine-tuning however, this is equivalent to minimize the following with respect to δ: 1 N NX i=1 ℓ \u0000 fW0 (Xi) + ⟨G(Xi), δ⟩, Yi \u0001 + λ∥δ∥∗. The problem here is that gradient methods no longer apply since the nuclear norm is non-differentiable. Therefore, we use the proximal gradient method: δt+1 = proxαλ∥·∥∗(δt − α∇ˆL(δt)) where proxαλ∥·∥∗(δ) = argmin δ′ \u0012 λ∥δ ′ ∥∗ + 1 2α∥δ′ − δ∥2 F \u0013 . It is well known that the proximal gradient method on convex objective converges to a global minimum (Polyak, 1987). Hyperparameters on NLP tasks For NLP tasks, we use full batch to perform GD on training. We only train the query (Wq) and value (Wv) weights of the RoBERTa-base model, which was empirically shown to have good performance (Hu et al., 2021). Furthermore, calculating the proximal operator of a nuclear norm is a computational bottleneck during the training of all Wq and Wv matrices. Therefore, we limit our training to only the last layer of Wq and Wv. To ensure a fair comparison, we apply the same approach to the LoRA updates. Additional information is in Table 1. Hyperparameters on image and speech classification tasks Similar to NLP tasks, we train the last attention layers. Further details are in Table 2. 21LoRA Training in the NTK Regime has No Spurious Local Minima Task SST-2,QNLI MR,CR,QQP,Subj Batch size 32 32 Learning rate (Full, LoRA fine tuning) 0.0005 0.001 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0.01 0.01 Table 1.Hyperparameters on experiment in Section 6 (NLP tasks) Task Image classification Speech classification Batch size 16 16 Learning rate (Full, LoRA fine tuning) 0.005 0.005 Trained layer Wq, Wv (last layer only) Wq, Wv (last layer only) Weight decay 0 0.001 Table 2.Hyperparameters on experiment in Section 6 (Image and speech classification tasks) (a) SST-2  (b) QNLI  (c) MR (d) CR  (e) QQP  (f) Subj Figure 4.Test curves (accuracy vs. epochs) on different NLP tasks. We used the LoRA rank of 16. Test accuracy. For the setting of Section 6 on NLP tasks, we additionally conduct evaluations on a test set of 1000 samples during training and present the results in Figure 4. We observed that in most tasks the performance using LoRA eventually converges a test accuracy that matches that of full fine-tuning, although the rates of convergence sometimes differ. We list the hyperparameters in Table 3 Task SST-2,QQP,MR,CR Subj QNLI Batch size 32 32 24 Learning rate (Full, LoRA fine tuning) 0.0001 0.001 0.0005 Trained layer Wq, Wv (all layers) Wq, Wv (all layers) Wq, Wv (all layers) Weight decay 0.005 0.005 0.005 Table 3.Hyperparameters on experiment in Figure 4 22LoRA Training in the NTK Regime has No Spurious Local Minima For image and speech classification tasks, we also validate the performance of our linearized update to confirm that the accuracy is on par with actual LoRA updates. Accuracies are averaged over 3 runs (See Table 4). Task Image classification Speech classification Accuracy ( actual / linearized) 86.20 / 87.00 74.67 / 73.67 Table 4.Accuaricies of LoRA updates on vision and speech classification tasks 23",
      "meta_data": {
        "arxiv_id": "2402.11867v3",
        "authors": [
          "Uijeong Jang",
          "Jason D. Lee",
          "Ernest K. Ryu"
        ],
        "published_date": "2024-02-19T06:22:09Z",
        "pdf_url": "https://arxiv.org/pdf/2402.11867v3.pdf",
        "github_url": "https://github.com/UijeongJang/LoRA-NTK"
      },
      "llm_extracted_info": {
        "main_contributions": "This research theoretically analyzes LoRA (Low-Rank Adaptation) fine-tuning within the Neural Tangent Kernel (NTK) regime. It identifies the main research problem as the limited theoretical understanding of LoRA's mechanisms, trainability, and generalizability, particularly its quadratic parameterization. The key findings are: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank r ≲ √N (specifically, r(r+1)/2 ≤ KN); (ii) using LoRA with rank r ≳ √N (specifically, r(r+1)/2 > KN) eliminates spurious local minima, enabling (stochastic) gradient descent to find these low-rank solutions; and (iii) the low-rank solution found using LoRA generalizes well.",
        "methodology": "The methodology is primarily theoretical analysis within the Neural Tangent Kernel (NTK) regime, assuming the network's change during fine-tuning can be approximated by a first-order Taylor expansion. LoRA's low-rank parameterization (δ = uv⊺) is analyzed, with weight decay interpreted as nuclear norm regularization. The study employs optimization landscape analysis, focusing on second-order stationary points (SOSPs) and global minima, drawing from prior work on matrix factorization and related theorems. A key technique involves introducing a small, randomly generated positive semi-definite perturbation P to the loss function to rigorously prove the absence of spurious local minima, using an application of Sard's theorem (via Lemma 4.5). Generalization guarantees are derived using Rademacher complexity and McDiarmid's inequality, particularly for cross-entropy loss.",
        "experimental_setup": "The experimental setup involves simple experiments to validate the theory by fine-tuning linearized pre-trained models. For NLP tasks, a linearized RoBERTa-base model was fine-tuned using prompt-based fine-tuning on datasets of size N=32 with K=2 labels (SST-2, QNLI, MR, CR, QQP, Subj), using cross-entropy loss. Only the query (Wq) and value (Wv) weights of the last layer were trained, with a LoRA rank r ≥ 11, and full fine-tuning as a baseline. For image classification, a pre-trained vision transformer was fine-tuned on the bean disease dataset (N=48, K=3 labels). For speech classification, a pre-trained wav2vec2 model was fine-tuned on a SUPERB dataset (N=64, K=4 labels). Training curves (loss vs. epochs) and test accuracy (for NLP) were observed.",
        "limitations": "The main limitations include the reliance on the Neural Tangent Kernel (NTK) regime assumption, which linearizes network behavior and may not hold for all fine-tuning scenarios. The rigorous theoretical guarantee for the absence of spurious local minima requires a 'perturbed' loss function with a randomly generated term, which does not directly apply to the unperturbed practical LoRA setup. Furthermore, the theory does not provide insights into the 'convexity' or 'favorability' of the loss landscape, as empirical observations suggest that lower LoRA ranks can slow down convergence due to plateaus or saddle points. The primary generalization guarantee is restricted to cross-entropy loss, although a more general version is in the appendix.",
        "future_research_directions": "Future research directions include conducting more refined analyses under specific assumptions, relaxing the linearization/NTK regime assumption through local analysis, better understanding the minimum rank requirement through lower bounds, and analyzing the trade-off between training rate and LoRA rank. This last point is motivated by the observation that lower LoRA ranks might create unfavorable flat training landscapes, slowing down gradient descent dynamics, and suggests designing remedies for this phenomenon.",
        "experimental_code": "class LinearLoraupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.Lora_a = [torch.empty(model.model_args.lora_r, self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.Lora_b = [torch.zeros(self.target_size[i][0], model.model_args.lora_r) for i in range(len(self.target_layers))]\n        for i, param in enumerate(self.Lora_a):\n           torch.nn.init.normal_(param, mean = 0, std= 1/((np.sqrt(model.model_args.lora_r)*self.target_size[i][1])))  # scaling the initailization by \\frac{1}{sqrt{r}}\n           #torch.nn.init.kaiming_normal_(param)\n      \n        self.Lora_A_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_a])\n        self.Lora_B_list = nn.ParameterList([nn.Parameter(param) for param in self.Lora_b])     \n            \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)   \n        \n        self.num_labels = gradient[0].size(1) \n        self.gradient_dtype = gradient[0].dtype\n\n        # Compute \\langle G(X_i), B*A \\rangle \n        output = sum(torch.sum((self.Lora_B_list[i]@self.Lora_A_list[i]) * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n       \nclass Linearupdate(nn.Module):\n    def __init__(self,model,binary_classification,target_layers,target_size):\n        super().__init__()\n        self.model = model\n        self.binary_classification = binary_classification\n        self.model_wrapper = LogitModelWrapper(model, binary_classification)\n        self.target_layers = target_layers\n        self.target_size = target_size\n        \n        self.delta = [torch.zeros(self.target_size[i][0], self.target_size[i][1]) for i in range(len(self.target_layers))]\n        self.delta_list = nn.ParameterList([nn.Parameter(param) for param in self.delta])\n       \n        self.lora_r = model.model_args.lora_r\n        self.scaling = model.model_args.lora_alpha\n        self.num_labels = None\n        self.gradient_dtype = None\n        \n    \n    def forward(self, input_ids, attention_mask, mask_pos, gradient):  #gradient = list of gradients, each element corresponds to each Lora layer\n        \n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask, mask_pos=mask_pos)[0] # don't provide labels\n            if self.binary_classification:\n                assert logits.size(1) == 2, \"--binary_classification should have 2 logits\"\n                logits = (logits[:,1] - logits[:,0]).unsqueeze(-1)\n        \n        self.num_labels = gradient[0].size(1) #train_gradient = [Batch, label, 768 , 768]\n        self.gradient_dtype = gradient[0].dtype\n\n        #Minimizing nuclear norm is equivalent to minimizing the sum of nuclaer norm of nontrivial diagnoals. \n        \n        output = sum(torch.sum(self.delta_list[i] * gradient[i], dim=(-2,-1)) for i in range(len(self.target_layers)) ) \n        \n        return output\n\nclass LinearizedLoraTrainer(LinearHeadTrainer):\n    def compute_gradient_perlayer(self, inputs_outer, layer_name ):\n            \n        def convert_to_buffer(name):\n            if layer_name in name:\n                return False\n            else:\n                return True\n        \n        model_tmp = copy.deepcopy(self.model_wrapper)\n        param_to_buffer(model_tmp, \"\", convert_to_buffer)\n\n        model_tmp.eval()\n  \n        for name , param in model_tmp.named_parameters():\n            param.requires_grad_(True)\n\n        model_fn, params, buffers = make_functional_with_buffers(model_tmp)\n\n        jacobian_fn = jacrev(model_fn)\n\n        def curried_jacobian_fn(input_ids, attention_mask, mask_pos):\n            return jacobian_fn(params, buffers, input_ids, attention_mask, mask_pos)\n\n        targets = []\n        for k, v in inputs_outer.items():\n            if isinstance(v, torch.Tensor):\n                inputs_outer[k] = v.to(self.args.device)\n                \n        grads_outer = curried_jacobian_fn(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"))[0]\n    \n            \n        label = inputs_outer.get(\"labels\")\n        if self.args.binary_classification:\n            label = (label * 2 - 1).float(0)\n            \n        targets.append(label)\n    \n        return (grads_outer, torch.cat(targets, dim=0) if targets else torch.tensor([]))  \n    \n    def compute_gradient_everylayer(self, inputs_outer):\n        \n        grads_outer=[] \n        with torch.no_grad():\n            for layer_name in self.target_layers:   \n                    grads_outer_layer, targets = self.compute_gradient_perlayer(inputs_outer, layer_name)\n                    grads_outer.append(grads_outer_layer)\n                    \n        if self.grad_dim is None:\n            self.grad_dim = sum(np.prod(x.shape[2:]) for x in grads_outer)\n                \n        return (grads_outer, targets)\n    \n    def compute_gradient_sharded(self, inputs_outer):\n        with torch.no_grad():\n            grads_outer , targets = self.compute_gradient_everylayer(inputs_outer)\n        return  grads_outer, targets \n\n    def finetune(self, train_dataset, eval_dataset):\n        \n        dataloader_outer = self.get_unshuffled_dataloader(train_dataset, sharded=True, batch_size=self.args.per_device_train_batch_size)\n        dataloader_outer_eval = self.get_unshuffled_dataloader(eval_dataset, sharded=True, batch_size=self.args.per_device_eval_batch_size)\n        optimizer = optim.SGD(self.lora_model.parameters(), lr=self.args.linear_lr) # Weight decay will be implented manually\n\n        if self.args.fp16 and _use_apex:\n            if not transformers.is_apex_available():\n                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n            model, optimizer = amp.initialize(self.lora_model, optimizer, opt_level=self.args.fp16_opt_level)\n\n        if self.args.n_gpu > 1:\n            model = torch.nn.DataParallel(self.lora_model)\n\n        if self.args.local_rank != -1:\n            model = torch.nn.parallel.DistributedDataParallel(\n                self.lora_model,\n                device_ids=[self.args.local_rank],\n                output_device=self.args.local_rank,\n                find_unused_parameters=True,\n            )\n        \n        epoch_count = 0 \n        \n        if self.model.model_args.apply_lora:\n            for name, param in self.lora_model.named_parameters():\n                if \"Lora\" not in name:\n                    param.requires_grad_(False)\n                if self.args.linear_freeze_A and \"Lora_A\" in name:\n                    param.requires_grad_(False)\n        else:\n            for name, param in self.lora_model.named_parameters():\n                if \"delta\" not in name:\n                    param.requires_grad_(False)\n         \n        for name, param in self.lora_model.named_parameters():\n            if param.requires_grad:\n                print(f\"{name} is being trained\")\n                \n                \n        writer = SummaryWriter(f\"./finallasttrain/{self.model.model_args.apply_lora}-{self.args.linear_lr}-{self.model.data_args.task_name}-{self.args.seed}-{self.model.model_args.lora_r}\")\n        \n        file_exists = False\n        eval_file_exists = False\n        saved_gradients_eval = []\n        saved_gradients = []\n        for epoch in range(self.args.linear_num_epoch):\n            \n            epoch_count += 1\n            reg = 0\n            total_loss = 0\n            \n            if self.args.eval_during_training:\n                total_loss_eval = 0\n                eval_preds=[]\n                eval_targets_list=[]\n                \n            for i, inputs_outer in enumerate(tqdm(dataloader_outer, desc=\"Fine-tuning\")):\n                if file_exists:\n                    gradient = saved_gradients[i]\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                else:\n                    gradient, _  = self.compute_gradient_sharded(inputs_outer)\n                    saved_gradients.append(gradient)\n                    for j in range(len(self.target_layers)):\n                        gradient[j].requires_grad_(False)\n                    \n                \n                if self.num_labels is None:\n                    self.num_labels = gradient[0].size(1) \n                    self.gradient_dtype = gradient[0].dtype\n                    \n                train_logits , targets = self.compute_model_logits(inputs_outer)\n                train_logits = train_logits.to(self.args.device)\n\n                output = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient)\n\n                loss = nn.CrossEntropyLoss(reduction = 'sum')(train_logits + output  , targets)   \n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                if not self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            u,s,v = torch.svd(param)\n                            s = torch.nn.Threshold(0, 0)(s-  self.args.linear_lr * self.args.linear_wd)\n                            param = u@s@v\n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.00000001:\n                            param = param -  self.args.linear_wd * self.args.linear_lr * param\n                \n                if self.model.model_args.apply_lora:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"Lora\" in name and self.args.linear_wd>=0.0000001:\n                            reg += (1/2)*(param ** 2).sum()  \n                else:\n                    for name, param in self.lora_model.named_parameters():\n                        if \"delta\" in name and self.args.linear_wd>=0.0000001:\n                            reg += torch.norm(param, p = 'nuc')\n                            \n                dist.all_reduce(loss, op=dist.ReduceOp.SUM)\n                total_loss += loss.item()\n                            \n            file_exists = True\n                          \n            avg_loss = (total_loss/ len(dataloader_outer.dataset)) + (reg * self.args.linear_wd) \n            logger.info(f\"epoch : {epoch+1} train_loss : {avg_loss}\")\n            writer.add_scalar(f\"train_loss_{self.model.data_args.task_name}/epoch\", avg_loss, epoch)\n            \n            if self.args.eval_during_training: \n                with torch.no_grad():\n                    \n                    for i, inputs_outer in enumerate(tqdm(dataloader_outer_eval, desc=\"Evaluating\")):\n                               \n                        if eval_file_exists:\n                            gradient_eval = saved_gradients_eval[i]\n                            for j in range(len(self.target_layers)):\n                                gradient_eval[j].to(self.args.device)\n                                gradient_eval[j].requires_grad_(False)\n                                \n                        else:\n                            gradient_eval, _  = self.compute_gradient_sharded(inputs_outer)\n                            for j in range(len(self.target_layers)):  \n                                gradient_eval[j].requires_grad_(False)\n                        \n                        \n                        output_eval = model(inputs_outer.get(\"input_ids\"), inputs_outer.get(\"attention_mask\"), inputs_outer.get(\"mask_pos\"), gradient_eval)\n                        eval_logits, eval_targets = self.compute_model_logits(inputs_outer)\n                        eval_logits = eval_logits.to(self.args.device)\n                            \n                        loss_eval = nn.CrossEntropyLoss(reduction = 'sum')(output_eval + eval_logits , eval_targets) \n                        dist.all_reduce(loss_eval, op=dist.ReduceOp.SUM) \n                        total_loss_eval += loss_eval.item()\n                        \n                        eval_preds.append( eval_logits + output_eval )\n                        eval_targets_list.append(eval_targets)\n                        \n                        saved_gradients_eval_cpu = []\n                        for j in range(len(self.target_layers)):\n                            gradient_eval_layer= gradient_eval[j].detach().cpu()\n                            saved_gradients_eval_cpu.append(gradient_eval_layer)\n                        saved_gradients_eval.append(saved_gradients_eval_cpu)\n                        \n                       \n                eval_file_exists = True\n\n                avg_loss_eval = (total_loss_eval / len(dataloader_outer_eval.dataset) )+ (reg * self.args.linear_wd)\n                logger.info(f\"epoch : {epoch+1} eval_loss : {avg_loss_eval}\")\n                writer.add_scalar(f\"eval_loss_{self.model.data_args.task_name}/epoch\", avg_loss_eval, epoch)\n                \n                eval_preds = torch.cat(eval_preds, dim=0)\n                eval_targets = torch.cat(eval_targets_list, dim=0)\n\n                if self.args.local_rank != -1:\n                    logger.info(\"Starting to gather kernel across GPUs\")\n                    eval_preds = varsize_tensor_all_gather(eval_preds.to(self.args.device), torch.distributed.get_world_size())\n                    eval_targets = varsize_tensor_all_gather(eval_targets.to(self.args.device), torch.distributed.get_world_size())\n                    logger.info(\"Finished gathering kernel across GPUs\")\n\n                metrics = None\n                eval_preds = eval_preds.cpu()\n                eval_targets = eval_targets.cpu()\n                \n                if self.args.binary_classification:\n                    eval_preds = torch.cat([-eval_preds, eval_preds], dim=-1)\n                    eval_targets = ((eval_targets + 1) / 2).long()\n\n                if self.compute_metrics is not None:\n                    metrics = self.compute_metrics(EvalPrediction(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy()))\n\n                for key in list(metrics.keys()):\n                    if not key.startswith(\"eval_\"):\n                        metrics[f\"eval_{key}\"] = metrics.pop(key)\n                metrics[\"eval_loss\"] = avg_loss\n\n                metrics.update(self.metrics())\n                metrics[\"grad_dim\"] = self.grad_dim\n\n                output = PredictionOutput(predictions=eval_preds.numpy(), label_ids=eval_targets.numpy(), metrics=metrics)\n                metrics = output.metrics\n                objective = default_dev_objective(metrics)\n                logger.info(f\"epoch : {epoch+1}  objective : {objective}\")\n                writer.add_scalar(f\"Eval_acc_{self.model.data_args.task_name}/epoch\", objective, epoch)\n                         \n        writer.flush()\n        writer.close()\n        self.save_model(self.args.output_dir)\n        return avg_loss.item() , epoch_count",
        "experimental_info": "The method employs LoRA (Low-Rank Adaptation) parameterization and linearized fine-tuning. Key experimental settings include:\n\n**LoRA Parameterization:**\n*   `apply_lora`: A boolean flag (default `True` in `ModelArguments`) indicating whether LoRA low-rank matrices `A` and `B` are used. If `False`, a full `delta` matrix (representing the low-rank update) is optimized directly.\n*   `lora_alpha`: An integer (default `1` in `ModelArguments`) specifying the initialization scale for one of the low-rank matrices.\n*   `lora_r`: An integer (default `8` in `ModelArguments`) defining the inner rank `r` for LoRA matrices, where the low-rank update `δ = uvᵀ` (with `u` as `Lora_B` and `vᵀ` as `Lora_A`).\n*   **Initialization**: `Lora_A` matrices are initialized with `torch.nn.init.normal_` using a mean of `0` and a standard deviation of `1 / (np.sqrt(lora_r) * target_size[i][1])`.\n\n**Linearized Fine-tuning Settings:**\n*   `trainer`: Set to `\"linearized\"` (default in `DynamicTrainingArguments`), which enables the `LinearizedLoraTrainer` for optimization.\n*   `linear_lr`: A float (default `0.01` in `DynamicTrainingArguments`) representing the learning rate for the SGD optimizer used in the linearized LoRA optimization.\n*   `linear_num_epoch`: An integer (default `30` in `DynamicTrainingArguments`) specifying the number of training epochs for the linearized model.\n*   `linear_wd`: A float (default `0` in `DynamicTrainingArguments`) for the weight decay coefficient. Its application depends on `apply_lora`:\n    *   If `apply_lora` is `True`, an L2 regularization term `(1/2)*(param ** 2).sum()` is applied to the individual `Lora_A` and `Lora_B` matrices.\n    *   If `apply_lora` is `False` (optimizing a direct `delta` matrix), a nuclear norm regularization `torch.norm(param, p = 'nuc')` is applied to the `delta` parameter using a soft-thresholding operator.\n*   `linear_freeze_A`: A boolean (default `False` in `DynamicTrainingArguments`). If `True`, the `Lora_A` matrix in the low-rank decomposition is frozen during optimization.\n*   `train_last_layer`: A boolean (default `True` in `DynamicTrainingArguments`). If `True`, gradient computations and parameter updates are restricted to the query and value attention weights exclusively within the last layer of the model. If `False`, these operations are applied to attention query and value weights across all layers."
      }
    },
    {
      "title": "Adapters Strike Back",
      "abstract": "Adapters provide an efficient and lightweight mechanism for adapting trained\ntransformer models to a variety of different tasks. However, they have often\nbeen found to be outperformed by other adaptation mechanisms, including\nlow-rank adaptation. In this paper, we provide an in-depth study of adapters,\ntheir internal structure, as well as various implementation choices. We uncover\npitfalls for using adapters and suggest a concrete, improved adapter\narchitecture, called Adapter+, that not only outperforms previous adapter\nimplementations but surpasses a number of other, more complex adaptation\nmechanisms in several challenging settings. Despite this, our suggested adapter\nis highly robust and, unlike previous work, requires little to no manual\nintervention when addressing a novel scenario. Adapter+ reaches\nstate-of-the-art average accuracy on the VTAB benchmark, even without a\nper-task hyperparameter optimization.",
      "full_text": "Adapters Strike Back Jan-Martin O. Steitz1 Stefan Roth1,2 1Department of Computer Science, TU Darmstadt 2 hessian.AI Abstract Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of dif- ferent tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as vari- ous implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architec- ture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual inter- vention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.† 1. Introduction Transfer learning from an off-the-shelf model, pre-trained on a large dataset like ImageNet [55] to a downstream task by fully fine-tuning the model’s parameters is a common paradigm. A typical CNN architecture, like a ResNet [24], has several tens of millions of parameters. However, since the introduction of transformers [59] into the realm of com- puter vision [4, 5, 13, 51, 52, 63], model sizes have grown exponentially from around a hundred million parameters for a vision transformer (ViT) [ 13] to more than a billion parameters [10, 46]. This leads to huge storage requirements when fine-tuning on multiple downstream tasks because a complete set of the model’s parameters needs to be saved per task. Additionally, large models require correspondingly large datasets [e.g., 56] to be trained to their full potential, yet tend to overfit easily if the target dataset in transfer learn- ing is too small. One solution is linear probing [12], where only the linear classifier is trained, but this usually yields inferior results compared to full fine-tuning. As a consequence, there is a growing interest in parameter- efficient tuning methods. The main idea is to freeze the †Code is available at https://github.com/visinf/adapter_plus. 0 0.1 0.2 0.3 0.4 0.5 0.6 72 74 76 78 Fine-tuning accuracy Linear probing # Parameters (M) Accuracy (%) Adapter+ (ours) SPT -Adapter [21]SSF [39] Adapter+ opt. (ours)SPT -Adapter⟳ SSF⟳ FacT -TK [32] Consolidator [20]VPT [31] FacT -TK⟳ LoRA [29] VPT⟳ Figure 1. Parameter-accuracy characteristics of adaptation methods on the VTAB [65] test sets. We report original results and re-evaluations ( ⟳) after a complete training schedule with suitable data normalization. Our Adapter+ has clearly the best parameter-accuracy trade-off. The vertical, dashed line shows the possible minimal number of tunable parameters when only the classifiers are trained, i.e., using linear probing (61% accuracy). parameters of the pre-trained model and add a compara- tively small amount of parameters to the model, which are then tuned together with the classifier to adapt the model to the downstream task at hand. Representative methods with different underlying concepts include VPT [31], which prepends the sequence of image tokens in the attention with trainable tokens to learn a prompt tuning, LoRA [29], where the attention weights are updated with learnable low-rank decomposition matrices, and Adapters [28], which are small bottleneck modules that are added to every transformer layer of the network. Adapters were first proposed for CNNs by Rebuffi et al. [53] and various formulations [22, 28, 49] exist for the now common ViT architecture. Recent work on parameter-efficient transfer learning [e.g., 20, 21, 31, 32, 39, 67] presents adapters as a baseline method for the adaptation to downstream tasks in computer vision. However, we identified various common issues in their imple- mentations, which we find to have a negative influence on the To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, W A, USA, 2024. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. arXiv:2406.06820v1  [cs.CV]  10 Jun 2024LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 80 81 82 83 84 82.4 82.5 80.3 82.6 82.4 82.2 84.0 Accuracy (%) Natural LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 85 86 84.3 84.6 85.7 84.7 86.3 85.3 86.5 Accuracy (%) Specialized LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 58 60 62 60.1 62.1 58.0 62.3 60.9 60.5 63.3 Accuracy (%) Structured Figure 2. Average accuracy for VTAB subgroups on thetest sets. For methods marked with ⟳, we report results of our re-evaluation after a complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank r∈[1..32]. adaptation performance. For further details, refer to the sup- plemental material. Additionally, while adapters have been well studied in natural language processing (NLP), there is no study that broadly examines the different adapter config- urations for vision transformers. As a result, adapters have seemed to underperform in comparison to recent parameter- efficient adaptation methods, e.g., reported accuracies of adapters on VTAB of 73.9% in [67] and 60.8% in [31]. In this work, we therefore revisit the idea of adapters and investigate how they can perform at their best in con- nection with ViTs. Our contribution hereby is threefold: (1) We show the first in-depth and systematic study on the effects of the adapter position in the transformer and of the adapter’s inner structure with ViTs, as well as evaluate differ- ent variants of parameter initialization. (2) We further pro- pose a learnable, channel-wise scaling as extension to plain adapters, which proves to be beneficial for computer vision tasks. (3) Finally, we present Adapter+, an adapter configu- ration with an excellent parameter-accuracy trade-off com- pared to other work, as shown in Fig. 1. Adapter+ reaches a state-of-the-art average accuracy of 77.6% on VTAB [65] without any hyperparameter optimization per task and 3.7 percentage points (pp) over previous adapter baselines. We also reach a state-of-the-art accuracy of 90.7% on FGVC [31] with the lowest number of parameters compared to other methods. Finally, Adapter+ shows the best robustness in terms of accuracy across the VTAB subgroups, see Fig. 2. 2. Related work One possibility to adapt a pre-trained network to a novel task, apart from full fine-tuning, is to only selectively tune some of the parameters, e.g., only training the classifier [12]. Cai et al. [3] proposed to tune only the biases of an otherwise frozen network to adapt it to a downstream task. BitFit [ 64] then showed the efficacy of this method for NLP transformers. Modular adaptation. The concept of adding small, train- able modules with only a few parameters to an otherwise frozen network was first proposed for adapting CNNs by Rebuffi et al. [53] and called adapters. Other approaches replaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts [19], learned binary masks to prune a pre-trained network per target task [ 41], or created a student network by aug- menting the original network with adapter-like modules and skip connections, which then mimicked a teacher network by disabling parts of its pre-trained and added modules [43]. Following the rise of transformers in NLP [ 11, 50, 59], Houlsby et al. [28] proposed adapter modules in the form of bottlenecks for transformer layers. Pfeiffer et al. [49] conducted an architecture search on NLP tasks to find a more parameter-efficient configuration of adapter modules that only acts on the transformer’s feed-forward network (FFN), thus saving roughly half of the parameters over [28]. Prompt tuning. Inspired by changing the output of a net- work for NLP with hand-crafted textual prompts, which modifies the attention over the original input tokens, Lester et al. [37] proposed prompt tuning: A set of learnable to- kens is added to the input sequence and trained with back- propagation to prompt a frozen language model to perform downstream tasks. Li and Liang [38] extended on prompt tuning by adding learnable tokens at every transformer layer of the model, which they termed prefix tuning. Jia et al. [31] applied prompt tuning to vision transformers, then called visual prompt tuning (VPT), by preprending the sequence of image patch embeddings with such trainable tokens (VPT- Shallow). They also showed a variant resembling prefix tuning with stronger adaptation capabilities that adds tokens at every layer of the network (VPT-Deep). Low-rank approaches. Also focusing on the attention part of the transformer layers, Hu et al. [29] proposed low-rank adaptation (LoRA) where the attention weights are updated with low-rank decomposition matrices. The matrices can be merged with the attention weights for inference. The structure of LoRA is very similar to an adapter, which can be seen as a superset of LoRA acting on the transformer’s FFN. He et al. [22] proposed a formalism to unify LoRA, adapters, and prefix tuning [ 38]. It allowed them to combine the beneficial aspects of all three methods into a scaled parallel adapter (Scaled PA) for NLP tasks. AdaptFormer [6] then applied the concept of Scaled PA to vision transformers. 2Other related work. Newer approaches for vision trans- formers proposed different techniques to further enhance the parameter-accuracy trade-off in adaptation. NOAH [67] per- forms an architecture search for a combination of adapters, LoRA, and VPT for each task. SSF [39] scales and shifts the features in the network after every operation, i.e., attention, FFN, layer normalization, with task-specific, trainable mod- ules. Jie and Deng [32] aggregate the weights of a ViT into a single 3D tensor. Task-specific weight updates of this tensor are learned as a matrix decomposed into parameter-efficient factors, hence they termed their method factor-tuning (FacT). SPT [21] measures the importance of the weights of a pre- trained network for a downstream task. Based on a desired parameter budget, the most important parameters are chosen for tuning and adapters or LoRA are used for weight matrices that contain enough parameters of importance. Consolidator [20] adapts weights in multiple orderings of channel-wise groups. The updates for all groups are merged for efficient storage and inference. Despite these new developments, we show that the sim- ple concept of adapters exhibits an even better parameter- accuracy trade-off in combination with vision transformers – if done right and with the addition of a channel-wise scaling. 3. Adapters for vision transformers 3.1. Vision transformer basics In this work, we concentrate on the parameter-efficient adaptation of vision transformers (ViT) [ 13]. The ViT is closely modeled after the transformer model for natural lan- guage processing (NLP) proposed by Vaswani et al. [59]. A learned linear projection embeds non-overlapping and flat- tened patches of the input image into a sequence of n tokens x ∈ Rn×d, where d is called the hidden dimension of the transformer. A positional encoding is added to the embed- dings and the sequence is prepended with a trainable [CLS] token. The sequence length and the dimension of the tokens stay fixed throughout the architecture. The sequence is sent through consecutive transformer layers that each consist of a multi-head self-attention and a feed-forward network (FFN). For the self-attention, the tokens are projected to queries, keys, and values (Q, K, and V ) and the output of each of the M attention heads is calculated as Attention(x) =Softmax \u0012Q(x)K(x)T √ d′ \u0013 V (x), (1) with d′ = d/M being the inner dimension of the head. The FFN consists of a multilayer perceptron with two linear layers (with weights Wi and biases bi) and a GELU [ 26] non-linearity as activation in between: FFN(x) =GELU(xW1 + b1)W2 + b2. (2) Both attention and FFN are employed with a preceding layer normalization (LN) [1] and a skip connection and, therefore, transform an input sequence x sequentially as x 7→ Attention(LN(x)) +x (3a) x 7→ FFN(LN(x)) +x. (3b) To keep the notation concise, we will omit the LNs of atten- tion and FFN in the following; each attention and FFN is assumed to be always preceded by an LN. 3.2. Adapters and their inner structure Adapters [28] are small modules that are added to the trans- former layers. They allow to tailor a network to a new task or domain, where instead of tuning the parameters of the whole network, only the adapter parameters and the classi- fier are trained. Adapters take the form of bottlenecks with an inner dimension of r ≪ d. We call r the rank of the adapter. In detail, a down-projection to dimension r with weights Wdown ∈ Rd×r and biases bdown ∈ Rr is followed by a non-linear activation function σ(·), typically a GELU [26] as used throughout the ViT, and an up-projection with weights Wup ∈ Rr×d and biases bup ∈ Rd back to the hid- den dimension d of the transformer layer. This yields a base adapter module Adapterbase(x) =σ(xWdown + bdown)Wup + bup . (4) The base adapter module can be further enhanced with a normalization layer, e.g., a layer normalization (LN) [ 1]. Additionally, the output of the bottleneck can be scaled by s as Adapter(x) =s · Adapterbase \u0000 LN(x) \u0001 . (5) For layer-wise scaling, the factor s is taken to be a scalar, i.e. s ∈ R, and can be either fixed as a hyperparameter or learned during training. Layer-wise scaling was proposed by He et al. [22] and Hu et al. [29] but deemed not effective compared to a fixed scaling for tasks in NLP. Here, we additionally propose to use a channel-wise, learned scaling where s ∈ Rd. We investigate its capabilities in Sec. 4.3. In most cases, the adapter is used with a skip connection, hence the complete feature transformation becomes x 7→ Adapter(x) +x. (6) The complete inner structure of an adapter including its skip connection is visualized in Fig. 3a. 3.3. Adapter positions Although the architecture of bottleneck adapters for trans- formers is rather simple, there are various ways to plug them into the transformer layer. Previous work has not yet in- vestigated what the optimum position is for the use with a ViT [13]. Here, we evaluate four possible adapter positions, shown in Figs. 3b to 3e. We postulate that it is easier for an adapter to learn to modify features previously transformed 3FF down FF up Act LN Scaling (a) Inner structure FFN Adapter (b) Pre FFN Adapter (c) Post FFN Adapter (d) Parallel FFN Adapter (e) Intermediate Figure 3. Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layer normalization (LN) and scaling, (b)–(d) different possible adapter positions to connect the adapter to the FFN section of the transformer layer. Modules with trainable parameters are shown in red and frozen modules in blue. by a frozen module in the network rather than to anticipate what changes to the features are needed in adapting for a frozen module that follows the adapter. Putting it differently, we argue that the adapter should follow a frozen module. Pre-Adapter. The first adapter position we analyze ap- plies the adapter to the output x of the attention section of the transformer layer before it is passed into the FFN, but with the skip connection of the attention already added (Fig. 3b). The feature transformation of the FFN section with the adapter attached, therefore, becomes x 7→ FFN \u0000 Adapter(x) +x \u0001 + \u0000 Adapter(x) +x \u0001 . (7) Note that the two occurrences of Adapter(x) in Eq. (7) refer to the same instantiation. In this configuration, the adapter has the full information from the feature transformation hap- pening in the attention but needs to estimate the transforma- tion that will be happening in the FFN that follows. As a result, especially the last FFN before the linear classifier will be hard to adapt. To the best of our knowledge, this adapter position has not been considered in the literature. Post-Adapter. In this case, the adapter is positioned at the very end of the transformer layer on the output of the FFN with its skip connection added as x 7→ Adapter \u0000 FFN(x) +x \u0001 + \u0000 FFN(x) +x \u0001 , (8) where the FFNs refer to the same intantiation (Fig. 3c). That way, the adapter has access to the feature transformation happening in the FFN and the unmodified features via the skip connection. This position has been proposed by Pfeiffer et al. [49] as the result of an architecture search, but only for adapting transformers for NLP tasks and not for a ViT. Parallel-Adapter. Next, we consider a parallel setting as proposed by [22], where the adapter is located parallel to the FFN and both share a skip connection (Fig. 3d): x 7→ FFN(x) +Adapter(x) +x. (9) Therefore, both adapter and FFN work on the output of the attention section of the transformer layer and the adapter needs to learn the necessary residual transformation to the one produced by the frozen FFN. Intermediate-Adapter. Finally, we consider the original adapter position as proposed by Houlsby et al. [28]. The adapter is plugged behind the FFN but before the skip con- nection of the FFN is added (Fig. 3e). The adapter addition- ally possesses its own skip connection: x 7→ Adapter \u0000 FFN(x) \u0001 + FFN(x) +x. (10) Note that the two occurrences of FFN(x) in Eq. (10) refer to the same instantiation. The adapter sees the transformed features coming from the FFN but cannot access the features added later on by the skip connection of the FFN. 3.4. Initialization of adapter parameters Since training a deep learning model is a non-convex opti- mization problem, the initialization of parameters is impor- tant. In this work, we evaluate three different variants of parameter initializations for adapters proposed in the litera- ture. All of them have the goal to initialize the adapters in a way that minimizes the initial influence of the adapters at the start of their training. This is a sensible goal since adapters extend an already pre-trained frozen network. Houlsby initialization. Houlsby et al. [28] propose to draw the weights of the projection matrices from a zero-centered Gaussian distribution with a standard deviation of σ = 0.01, truncated at 2σ, and use zero for their biases. BERT initialization. For the BERT model [11], the initial- ization works similar to [28] but the Gaussian distribution has a standard deviation of σ = 0.02 and is not truncated. This form of initialization is used by Pfeiffer et al. [49]. LoRA initialization. LoRA [29] initializes the weights and biases of the down-projection with a uniform Kaiming He ini- tialization [23]; the weights and biases of the up-projection 4are initialized to zero. Therefore, the output of the adapter at the beginning of training equals zero and the adapter initially does not contribute. 3.5. Data normalization in pre-processing Data normalization is common practice during image pre- processing. It is typically done by shifting and scaling of each input pixel xij for each channel c as ˆxijc = (xijc − µc)/σc . (11) Most widely used are the mean µ = (0.485, 0.456, 0.406)T and standard deviation σ = (0.229, 0.224, 0.225)T of the ImageNet dataset [55], commonly referred to as ImageNet normalization. Another option is using 0.5 for every element of µ and σ, which is commonly referred to as Inception normalization because it is used for the Inception family of CNN architectures, starting with Inception-v3 [58]. The Im- ageNet normalization aims to center the input data around 0 with a standard deviation of 1. The Inception normalization, on the other hand, transforms the input values such they are strictly in range [−1, 1]. Because we try to adapt to a target domain on a very low parameter budget, it is important to use the data normaliza- tion the network saw during its pre-training. Otherwise, the parameter-efficient transfer method of choice needs to first compensate for the shift in input data statistics and loses parts of its capacity to adapt to the target domain. 4. Experiments 4.1. Datasets In order to carry out a detailed study of the utility of adapters in the context of ViT models, we experiment with two stan- dard benchmarks for task adaptation. VTAB. The Visual Task Adaptation Benchmark (VTAB) [65] consists of 19 tasks, which are further grouped into three categories: Natural, Specialized, and Structured. The Natural group contains natural images captured using stan- dard photographic equipment. The Specialized group is built from datasets of images captured with specialized equip- ment, from remote sensing and medical domains. Lastly, the Structured group is for evaluating the understanding of the scene structure. Here, the majority of the datasets are compiled from synthetic images with scenes that are easy to assess for humans but have a large domain gap to natural image datasets. Each task of VTAB consists of 800 train- ing and 200 validation images. The test sets have the same number of images as the test sets in the original datasets. FGVC. Following Jia et al. [31], we compile five datasets for fine-grained visual classification (FGVC): CUB-200- 2011 [61], NABirds [ 27], Oxford Flowers [ 45], Stanford Dogs [34], and Stanford Cars [17]. Because VTAB bench- marks task adaptation in a low-data regime in terms of the Table 1. Adapter position. We report the average accuracy in % (± std. dev.) on the VTABval sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 82.4 ± 0.4 86.2 ± 0.8 57.5 ± 0.5 75.3 ± 0.3 Intermediate 83.0 ± 0.4 85.0 ± 0.8 57.2 ± 0.5 75.1 ± 0.3 Parallel 83.0 ± 0.3 86.2 ± 0.6 57.7 ± 0.6 75.6 ± 0.3 Post 83.0 ± 0.3 85.7 ± 0.4 59.1 ± 0.3 76.0 ± 0.2 number of available training images, we use FGVC to eval- uate adaptation methods in settings where training data is abundant. Where validation sets are not available in FGVC, we follow Jia et al. [31] to create the validation splits. For further details regarding the dataset properties of VTAB and FGVC, see supplemental material. 4.2. Experimental settings For all our experiments, we use a ViT-B/16 network [13] that was pre-trained on ImageNet-21k [55]. We follow its pre- training settings, in particular, regarding input data normal- ization. We train all models with an AdamW [40] optimizer with a learning rate of 10−3, a weight decay of 10−4, and a batch size of 64, following [ 67]. For full fine-tuning, we use a learning rate of 10−4, which we found leads to better results. We use a cosine learning rate schedule with a linear warm-up over the first 10 epochs and train for 100 epochs in total. We use stochastic depth with linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen network and with a drop rate of 0.1 for the adapters during training. Apart from data normalization (cf . Sec. 3.4), we resize input images to 224×224 px for VTAB and use a randomly resize crop to 224×224 px and horizontal flipping for FGVC. For the ablations and to determine hyperparam- eters, we evaluate on the validation splits. We include the validation sets in the training data for producing final results. 4.3. Exploring adapter configurations Adapter position. We first evaluate the four possible posi- tions to connect an adapter to the FFN section of the trans- former layer, as described in Sec. 3.3. In our ablation, we use Adapterbase (cf . Eq. (4)) with rank r=8 and use the Houlsby initialization. In this experiment, the adapters neither have a layer normalization nor use scaling. The results on the VTAB validation set for all four adapter positions are presented in Tab. 1. The Post-Adapter yields the best result with 76.0% average accuracy over all VTAB subgroups. It confirms our hypothesis that the adapter should follow the frozen FFN module because it can then post-hoc modify the features flowing through the network. The par- allel configuration comes in second with 75.6% average accuracy, receiving the same input as the FFN but having to 5Table 2. Inner adapter structure. We evaluate the different com- ponents of the adapter structure, e.g., normalization layer (Norm), layer-wise and channel-wise learnable scaling on the VTAB val sets. The difference to Adapterbase (first row) is shown in ∆base. Bias Norm Scaling Initialization Accuracy (%) ∆base ✓ Houlsby 76.0 ± 0.2 0.0 Houlsby 75.6 ± 0.4 −0.4 ✓ LoRA 75.5 ± 0.3 −0.5 ✓ BERT 75.8 ± 0.3 −0.2 ✓ ✓ Houlsby 75.9 ± 0.3 −0.1 ✓ ✓ layer Houlsby 75.9 ± 0.3 −0.1 ✓ layer Houlsby 76.2 ± 0.3 +0.2 ✓ ✓ channel Houlsby 75.8 ± 0.3 −0.2 ✓ channel Houlsby 76.5 ± 0.2 +0.5 learn a residual modification to the FFN instead of a subse- quent one. Pre-Adapter and Intermediate-Adapter are subpar compared to the other positions. They either do not have access to the feature transformation happening afterwards in the FFN or to the features of the skip connection containing the output of the attention. Inner structure. Next, we investigate the impact of the in- ner structure of adapters including their initialization. Tab. 2 shows our findings with average accuracies calculated over the three VTAB subgroups. Removing the biases from the linear layers leads to a decrease in accuracy of 0.4 percent- age points (pp). We find that the Houlsby initialization of the adapter parameters is best while BERT and LoRA initializa- tions reduce the accuracy by 0.2 pp and 0.5 pp. Adding layer normalization (LN) to the adapter is slightly detrimental for all settings, both with scaling and without, while addition- ally adding 2d parameters per layer. We find that a learned scaling is in general beneficial for image-classification tasks. Adding layer-wise scaling leads to a gain of 0.2 pp. The inclusion of a learned, channel-wise scaling, as proposed here, gives the strongest improvement of 0.5 pp, reaching an accuracy of 76.5% on the VTAB validation set while only adding half of the parameters compared to LN. What makes a great adapter? From our systematic explo- ration of possible adapter configurations, we conclude that adapter modules in the Post-Adapter position with a learn- able, channel-wise scaling and Houlsby initialization work best for computer vision tasks. We call our proposed adapter configuration Adapter+. The addition of layer normaliza- tion, as suggested by Pfeiffer et al. [49], is not necessary and even leads to detrimental effects in our setting. Configurations from previous work. Different configu- rations of adapters have been established in previous work. We compare their configurations to our systematic approach with rank r=8 on the VTAB validation sets. Using our own implementations already leads to better results than reported in literature but enables us to compare on equal footing. Houlsby et al. [28] use an Intermediate-Adapter with their Table 3. Comparison of Adapter+ with adapter configurations from previous work. We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param (M) Natural Specialized Structured Average Houlsby [28], r=8 0.39 82.9 ± 0.2 85.5 ± 0.3 58.9 ± 0.8 75.8 ± 0.3 Houlsby [28], r=4 0.24 82.9 ± 0.4 84.9 ± 0.3 58.3 ± 0.6 75.4 ± 0.3 Pfeiffer [49] 0.21 82.9 ± 0.3 86.1 ± 0.9 58.4 ± 0.7 75.8 ± 0.4 AdaptFormer [6] 0.19 83.0 ± 0.4 85.0 ± 0.2 57.4 ± 0.5 75.2 ± 0.2 Adapter+ 0.20 83.0 ± 0.2 86.8 ± 0.6 59.7 ± 0.4 76.5 ± 0.2 proposed initialization both at the FFN section as well at the attention part of the transformer layer. Additionally, they adapt the LN parameters of the backbone. We, therefore, compare their setting additionally with r = 4to compare on roughly the same parameter budget. Pfeiffer et al. [49] suggest a Post-Adapter like us but with a BERT initialization and they employ a layer normalization inside the adapter. AdaptFormer [6] has the same configuration as a scaled parallel adapter (Scaled PA) [22], which was proposed for NLP tasks, the only difference being the layer-wise scalings. Scaled PA uses a fixed scaling of s = 4 for the adapters whereas AdaptFormer suggests to use s = 0.1 for vision tasks. Optimizing s for VTAB may lead to better results. Our results are presented in Tab. 3. We see a clear advantage of our Adapter+ configuration, gaining at least 0.7 pp over all previous adapter realizations considered despite having the second lowest number of trainable parameters. 4.4. Main results VTAB. We evaluate Adapter+ on the VTAB test sets and compare to other methods in Tab. 4. We provide results for full fine-tuning and tuning only the linear classifier while freezing the rest of the backbone [12] as a baseline of classi- cal fine-tuning methods. As competing parameter-efficient tuning methods, we include LoRA [29], VPT [31], NOAH [67], SSF [39], FacT [32], Consolidator [20], and SPT [21]. Wherever possible, we re-evaluate the other methods with a suitable data normalization for the pre-trained backbone and after the full training schedule to enable a fair compar- ison. For LoRA, we use our own implementation because the original work does not cover VTAB. For VPT, we adopt the number of tokens per task from their hyperparameter optimization but find that we do not need to tune learning rate and weight decay per task. Additionally, deviating from the original implementation, we optimize with AdamW [40] instead of SGD [54] and change to an appropriate data nor- malization. We present the original results from [ 31] on VTAB together with our re-evaluation. Our improved imple- mentation of VPT increases the average accuracy by 4.4 pp from 72.0% to 76.4%. SSF, FacT, and SPT released code to evaluate on VTAB. For FacT and SPT, we change the data normalization to match the backbone; SSF already uses the correct one. We re-run the provided code and present the 6Table 4. Detailed results on the VTAB test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. †: Early-stopping based on the test set, •: unsuitable data normalization, E: per-task hyperparameter optimization. 1Average across the average accuracies of the VTAB groups, following previous work. 2No complete code release for Consolidator, hence training and evaluation details are unknown. Natural Specialized Structured # Param (M) Cifar100 [35] Caltech101 [15] DTD [8] Flower102 [45] Pets [47] SVHN [44] Sun397 [62] Average Camelyon [60] EuroSAT [25] Resisc45 [7] Retinopathy [14] Average Clevr-Count [33] Clevr-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori [42] sNORB-Azi. [36] sNORB-Ele. [36] Average Global Average1 Full 85.8 73.2 92.6 70.4 97.9 86.2 90.6 39.6 78.6 87.1 96.6 87.5 74.0 86.3 66.6 61.0 49.8 79.7 82.6 51.9 33.5 37.0 57.8 74.2 Linear 0.04 78.1 88.1 69.0 99.1 90.0 36.0 56.9 73.9 79.8 90.7 73.7 73.7 79.5 32.4 30.5 35.9 61.9 11.2 26.2 14.3 24.5 29.6 61.0 LoRA [29] 0.29 83.0 91.7 71.6 99.2 90.9 83.8 56.7 82.4 86.2 95.7 83.5 71.9 84.3 77.7 62.3 49.0 80.2 82.2 51.7 31.0 47.0 60.1 75.6 VPT-Deep E• [31] 0.60 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.5 81.8 96.1 83.4 68.4 82.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 55.0 72.0 VPT-Deep E⟳ 0.60 83.0 93.0 71.2 99.0 91.3 84.1 56.0 82.5 84.9 96.6 82.5 74.5 84.6 77.5 58.7 49.7 79.6 86.2 56.1 37.9 50.7 62.1 76.4 NOAH E†•◦ [67] 0.43 69.6 92.7 70.2 99.1 90.4 86.1 53.7 80.2 84.4 95.4 83.9 75.8 84.9 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 61.3 75.5 SSF E† [39] 0.24 69.0 92.6 75.1 99.4 91.8 90.2 52.9 81.6 87.4 95.9 87.4 75.5 86.6 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 59.0 75.7 SSF E⟳ 0.24 61.9 92.3 73.4 99.4 92.0 90.8 52.0 80.3 86.5 95.8 87.5 72.8 85.7 77.4 57.6 53.4 77.0 78.2 54.3 30.3 36.1 58.0 74.6 FacT-TK8 E†• [32] 0.05 70.3 88.7 69.8 99.0 90.4 84.2 53.5 79.4 82.8 95.6 82.8 75.7 84.2 81.1 68.0 48.0 80.5 74.6 44.0 29.2 41.1 58.3 74.0 FacT-TK8 E⟳ 0.05 74.9 92.7 73.7 99.1 91.3 85.5 57.7 82.1 86.8 94.9 84.1 70.9 84.2 81.9 64.1 49.2 77.2 83.8 53.1 28.2 44.7 60.3 75.5 FacT-TK≤32 E†• [32] 0.10 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.6 84.8 96.2 84.5 75.7 85.3 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.7 75.6 FacT-TK≤32 E⟳ 0.10 74.6 93.7 73.6 99.3 90.6 88.7 57.5 82.6 87.6 95.4 85.5 70.4 84.7 84.3 62.6 51.9 79.2 85.5 52.0 36.4 46.6 62.3 76.5 Consolidator 2 [20] 0.30 74.2 90.9 73.9 99.4 91.6 91.5 55.5 82.4 86.9 95.7 86.6 75.9 86.3 81.2 68.2 51.6 83.5 79.8 52.3 31.9 38.5 60.9 76.5 SPT-Adapter †• [21] 0.23 72.9 93.2 72.5 99.3 91.4 84.6 55.2 81.3 85.3 96.0 84.3 75.5 85.3 82.2 68.0 49.3 80.0 82.4 51.9 31.7 41.2 60.8 75.8 SPT-Adapter ⟳ 0.22 74.7 94.1 73.0 99.1 91.2 84.5 57.5 82.0 85.7 94.9 85.7 70.2 84.1 81.3 63.2 49.1 80.7 83.5 52.0 26.4 41.5 59.7 75.3 SPT-Adapter †• [21] 0.43 72.9 93.2 72.5 99.3 91.4 88.8 55.8 82.0 86.2 96.1 85.5 75.5 85.8 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 61.4 76.4 SPT-Adapter ⟳ 0.43 74.9 93.2 71.6 99.2 91.1 87.9 57.2 82.2 87.0 95.4 86.5 72.4 85.3 81.1 63.2 50.3 80.2 84.4 51.4 31.5 42.2 60.5 76.0 Adapter+, r=1 0.07 85.4 92.4 73.1 99.1 91.3 83.1 58.1 83.2 87.2 96.6 85.3 72.6 85.5 80.7 60.6 50.9 79.9 83.3 55.6 27.1 43.0 60.1 76.3 Adapter+, r=2 0.09 85.4 93.0 72.7 99.2 90.6 85.3 58.0 83.5 87.9 96.8 85.5 71.4 85.4 83.2 61.0 51.6 80.1 86.1 56.3 30.7 46.5 61.9 76.9 Adapter+, r=4 0.13 84.8 93.8 72.7 99.2 90.6 86.5 57.4 83.6 87.5 96.9 85.9 71.5 85.4 83.4 61.6 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r=8 0.20 84.6 94.2 72.3 99.3 90.7 87.6 56.7 83.6 87.7 97.0 86.7 72.3 85.9 83.2 60.9 53.8 80.3 88.1 55.6 35.7 47.7 63.1 77.6 Adapter+, r=16 0.35 83.7 94.2 71.5 99.3 90.6 88.2 55.8 83.3 87.5 97.0 87.4 72.9 86.2 82.9 60.9 53.7 80.8 88.4 55.2 37.3 46.9 63.3 77.6 Adapter+, r∈[1..4] E 0.11 85.4 93.8 72.7 99.1 90.6 86.5 58.1 83.7 87.5 96.8 85.9 71.4 85.4 83.4 61.0 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r∈[1..8] E 0.16 85.4 93.8 72.7 99.1 90.7 87.6 58.1 83.9 87.7 96.8 86.7 72.3 85.9 83.4 60.9 53.8 80.3 88.1 55.3 35.7 47.7 63.1 77.7 Adapter+, r∈[1..32] E 0.27 85.4 93.8 72.7 99.1 90.7 88.2 58.1 84.0 87.5 96.8 87.8 73.9 86.5 83.4 60.9 53.8 80.3 87.2 55.3 37.9 47.7 63.3 77.9 results after a full training schedule. For completeness, we also report the results from the original publications. How- ever, we found that the code releases of [21, 32, 39] use early stopping based on the best result on the test set. We argue that tuning hyperparameters such as the number of training epochs on the test set goes against established practices in machine learning; rather the validation set should be used for early stopping. Yet, due to the limited size of the training and validation sets in VTAB, it is not feasible to report test results without also training on the validation data. Hence, we chose to complete a full training schedule of 100 epochs instead of using early stopping. Training SSF for the full schedule leads to a decrease in average accuracy of 1.1 pp over the original publication and re-evaluating SPT leads to a decrease of up to 0.5 pp, even with a corrected data normalization. FacT on the other hand benefits from our re-revaluation, since the accuracy decrease from training a complete schedule is offset by improvements from applying the appropriate data normalization. There was no complete code release with configurations to train Consolidator on VTAB at the time of writing, hence we report results as-is. Adapter+ shows the best parameter-accuracy trade-off among all methods evaluated. This can also be clearly seen in Fig. 1. Additionally, Adapter+ sets a new state of the art with an average accuracy of up to 77.6% over all VTAB subgroups even without any per-task hyperparameter optimization. If we determine the optimal rankr per task on the validation set, we can further improve the accuracy to 77.9%. Optimizing the rank leads to a better parameter-accuracy trade-off than using a fixed rank across all tasks. In Fig. 2, we compare the average accuracy on the sub- groups of VTAB. Wherever possible, we present the results of re-evaluating methods after the last training epoch and matching the data normalization to the backbone. The aver- age accuracies of Adapter+ with r ∈ [1..32] are consistently higher than those of the competing methods. Note that the accuracies of other methods except SPT differ drastically across the different VTAB subgroups. Adapter+, on the other hand, shows a high degree of robustness to the domain shifts between groups. FGVC. Next, we present our results on the FGVC bench- mark in Tab. 5. From the contenders, only SSF [ 39] has released code and hyperparameter configurations for train- ing on FGVC at the time of writing. As we know from the code releases for VTAB, the reported numbers show the accuracy for early stopping based on the test set. There- fore, we expect a similar evaluation for FGVC. While we do not endorse early stopping based on the test set, we ad- 7Table 5. Detailed results on the FGVC test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. # Param (M) CUB200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] Average Full 86.0 88.0 81.5 99.2 85.6 90.6 89.0 Linear 0.18 88.9 81.8 99.5 92.6 52.8 83.1 VPT-Deep [31] 0.85 88.5 84.2 99.0 90.2 83.6 89.1 VPT-Deep ⟳ 0.85 90.1 83.3 99.6 90.3 85.0 89.7 SSF [39] 0.39 89.5 85.7 99.6 89.6 89.2 90.7 SSF ⟳ 0.39 88.9 85.0 99.6 88.9 88.9 90.3 SPT-Adapter [21] 0.40 89.1 83.3 99.2 91.1 86.2 89.8 SPT-LoRA [21] 0.52 88.6 83.4 99.5 91.4 87.3 90.1 Adapter+, r∈[1..32] 0.34 90.0 83.2 99.6 91.6 89.1 90.7 Adapter+ (best epoch) 0.34 90.4 85.0 99.7 92.6 89.1 91.4 ditionally provide numbers for that setting in Tab. 5 for the sake of comparability. Even when training for a complete schedule, Adapter+ shows the best average accuracy with 90.7% over all five datasets in FGVC,0.4 pp over the second best method under similar evaluation. When early stopping with the test set, Adapter+ reaches 91.4% average accuracy, 0.7 pp over the second best method and 2.4 pp better than full fine-tuning. This demonstrates that Adapter+ also yields state-of-the-art results for task adaptation when training data is abundant while having the best parameter efficiency. 4.5. Ablations Data normalization. We showcase the effect of using an unsuitable data normalization for the chosen ViT in Tab. 6. The gap between ImageNet and Inception normalization (see Sec. 3.5) is largest for VPT [31], with a 3.4 pp difference in average accuracy, which explains around two-thirds of the gain for our re-evaluation as shown in Fig. 1. We suspect that VPT has less of an ability to scale and shift the data because the learnable tokens only act on the attention mechanism. LoRA [29], FacT [32], and adapters all employ linear layers that can directly scale and shift the features of the frozen backbone and thus compensate better for improper data nor- malization. It is worth mentioning that our Adapter+ is the most robust to improper normalization out of the methods evaluated, with a gap of only 2.6 pp average accuracy. Training regularization. We investigate the importance of training regularization methods like stochastic depth [30] and dropout [16] for training adapters on a frozen ViT backbone and evaluate on the VTAB validation sets. We use linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen layers of the ViT model, and a drop rate Table 6. Effects of ImageNet vs. Inception data normalization. All methods are evaluated on the VTABval sets. In column ∆Average we report the increase in accuracy in pp across all VTAB subgroups. ImageNet norm Inception normNatural Specialized Structured Average Natural Specialized Structured Average ∆Average VPT 79.2 83.0 53.8 72.0 82.2 86.2 57.9 75.4 3.4 LoRA 78.4 84.1 53.2 71.9 82.0 85.8 56.4 74.7 2.8 FacT-TK 78.0 83.3 56.1 72.4 81.6 85.6 58.1 75.1 2.7 Adapter+ 80.5 85.0 56.0 73.9 83.0 86.8 59.7 76.5 2.6 Table 7. Influence of training regularization. We evaluate accu- racy in % with Adapterbase with rank r=8 on the VTAB val sets. Adapter Stochastic Depth Dropout None ViT Stochastic Depth 76.0 75.4 75.3 None 74.5 74.3 73.7 of 0.1 when using dropout or stochastic depth for the adapter modules. The results in Tab. 7 show a clear benefit for using stochastic regularization for the frozen layers as well as the adapters during training. Using dropout in the adapters is only slightly better than no regularization for adapters, with a gain of only 0.1 pp. With an increase in accuracy of 0.7 pp, stochastic depth is the preferred regularization method for adapters. However, our results show that the more important part is the stochastic depth regularization for the frozen modules of the ViT backbone. Disabling it in training leads to a loss of 1.5 pp accuracy compared to a training where stochastic depth is used throughout the model. 5. Conclusion Applied at the right position and with an optimal inner struc- ture, the simple concept of adapters produces state-of-the-art results for task adaptation. To understand how adapters can “strike back”, we conducted the first systematic and in-depth study on how to best construct adapters and integrate them with vision transformers. This allowed us to determine the optimal connection point for the adapter in the transformer layer. Further, we proposed to use a learnable, channel-wise scaling and showed its benefit for computer vision tasks. Our insights led us to the creation of Adapter+ that yields the highest accuracy and the best parameter-accuracy trade-off on VTAB (77.6%, 0.2M) without any per-task hyperparame- ter optimization and on FGVC (90.7%, 0.34M), showing its superiority over more complicated methods. Acknowledgements. This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. 8References [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016. [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Si- mon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801 [cs.AI], 2016. [3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for efficient on-device learn- ing. In NeurIPS*2020. [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End- to-end object detection with transformers. In ECCV, pages 213–229, 2020. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. InICCV, pages 9630–9640, 2021. [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapt- ing vision transformers for scalable visual recognition. In NeurIPS*2022. [7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):1865–1883, 2017. [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606–3613, 2014. [9] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. RandAugment: Practical automated data augmentation with a reduced search space. In NeurIPS*2020. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahen- dran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bast- ings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birod- kar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480–7512, 2023. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, pages 4171–4186, 2019. [12] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647–655, 2014. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Emma Dugas, Jorge Jared, and Will Cukierski. Diabetic retinopathy detection. Kaggle, 2015. [15] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learn- ing of object categories. IEEE T. Pattern Anal. Mach. Intell., 28(4):594–611, 2006. [16] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learn- ing. In ICML, pages 1050–1059, 2016. [17] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In AAAI, pages 4502–4508, 2017. [18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):1231–1237, 2013. [19] Yunhui Guo, Yandong Li, Liqiang Wang, and Tajana Rosing. Depthwise convolution is all you need for learning multiple visual domains. In AAAI, pages 8368–8375, 2019. [20] Tianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang Ding. Consolidator: Mergable adapter with group connections for visual adaptation. In ICLR, 2023. [21] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023. [22] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In ICCV, pages 1026–1034, 2015. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InCVPR, pages 770–778, 2016. [25] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217–2226, 2019. [26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:1606.08415 [cs.LG], 2023. [27] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Be- longie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595–604, 2015. [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 2790–2799, 2019. [29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 9LoRA: Low-rank adaptation of large language models. In ICLR, 2022. [30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, pages 646–661, 2016. [31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709–727, 2022. [32] Shibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, pages 1060–1068, 2023. [33] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988–1997, 2017. [34] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image catego- rization. In CVPR Workshop on Fine-grained Visual Classifi- cation, 2011. [35] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian Institute for Ad- vanced Research, 2009. [36] Yann LeCun, Fu Jie Huang, and Léon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, pages 97–104, 2004. [37] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582–4597, 2021. [39] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In NeurIPS*2022. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [41] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy- back: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 72–88, 2018. [42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset, 2017. [43] Pedro Morgado and Nuno Vasconcelos. NetTailor: Tuning the architecture, not just the weights. In CVPR, pages 3044–3054, 2019. [44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, pages 722–729, 2008. [46] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po- Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193 [cs.CV], 2023. [47] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InCVPR, pages 3498–3505, 2012. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS*2019, pages 8024–8035. [49] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In EACL, pages 487–503, 2021. [50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gener- ative pre-training. Technical report, OpenAI, 2018. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, ICML. [52] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. InICCV, pages 12159– 12168, 2021. [53] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS*2017, pages 506–516. [54] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Rev., 65(6):386–408, 1958. [55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vision, 115(13):211–252, 2015. [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION- 5B: An open large-scale dataset for training next generation image-text models. In NeurIPS*2022. [57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your ViT? Data, augmentation, and regularization in vision transformers. Trans. Mach. Learn. Res., 2022. [58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the In- 10ception architecture for computer vision. In CVPR, pages 2818–2826, 2016. [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS*2017, pages 5998–6008. [60] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. In MICCAI, pages 210–218, 2018. [61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical report, California Institute of Technology, 2011. [62] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485–3492, 2010. [63] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, José M. Álvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS*2021, pages 12077–12090. [64] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, 2022. [65] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv:1910.04867 [cs.CV], 2020. [66] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In ICLR, 2018. [67] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv:2206.04673 [cs.CV], 2022. 11Adapters Strike Back Supplementary Material In this appendix, we provide further details and results, which could not be included in the main paper due to space limitations. A. Why did adapters underperform for ViTs? First, we want to shed more light on why adapters do not rank well in the literature for parameter-efficient transfer learning for vision tasks. By comparison of numbers reported for adapters on VTAB in the publications referenced in Tab. 4 of the main paper, we found that they essentially stem from only two sources. The first source is VPT [31], where results for an adapter with a reduction factor of 256, amongst other configurations, are reported. For a ViT-B/16 with a hidden dimension of d=768, this is equal to an adapter with rank r=3. Despite citing Pfeiffer et al. [49], who suggest a Post-Adapter po- sition, the actual implementation in the code base * equals an Intermediate-Adapter that performs worse on VTAB (see Sec. 3.3 of the main paper). The initialization used for the adapter parameters most resembles a LoRA initialization but sets the adapter parameters to zero everywhere. Therefore, there is no randomization in the initialization of the adapter parameters, and different seeds only affect the initialization of the classifier. Additionally, the intermediate features in the adapter bottlenecks then become all zero, leading to iden- tical gradients in the up-projections at the start of training, which hinders optimization. As a result, the adapter baseline used by VPT only reaches 60.0% average accuracy on the VTAB test sets. This is a gap of 17.6 percentage points (pp) compared to our Adapter+ with rank r=8 (77.6% average accuracy). Even when considering the loss of around 2 pp to 3 pp caused by an unsuitable data normalization in the VPT implementation, this is still a very significant gap. The numbers for an adapter with rank r =3 from VPT are also reported in [39] as a baseline. The second source for adapter baseline results is the NOAH pre-print [67]. There, an adapter with rank r = 8 is used. Its implementation† performs the following feature transformation: x 7→ Adapter \u0000 FFN(x) \u0001 + x. (12) This is closest to the Intermediate-Adapter (cf . Eq. (10) of the main paper) but misses the skip connection bypassing the adapter and containing FFN(x). Thus, the adapter does not learn a residual function to an identity mapping but instead must learn a more complex mapping to transform its input. *https://github.com/KMnP/vpt †https://github.com/ZhangYuanhan-AI/NOAH Therefore, the adapter becomes harder to train [24], leading to an average accuracy of 73.9% on the VTAB test sets or 3.7 pp behind our Adapter+. For the NOAH adapter results, we see a proliferation to the publications of FacT [32] and SPT [21]. The adapter implementation from NOAH is also used in the code released for Consolidator ‡ [20] but their results are produced with rank r=16, giving a slightly better average accuracy of 74.3%, or 3.3 pp less than Adapter+. In summary, the examined baseline implementations dif- fer from the configurations proposed by Houlsby et al. [28] and Pfeiffer et al. [49] and introduce issues that lead to their underperformance. In our paper, we show that adapters are capable of reaching 77.6% average accuracy for rank r=8 and 77.9% for our optimized version of Adapter+, uplifting adapters from an easy-to-beat baseline to a state-of-the-art transfer method. B. Dataset properties In Tabs. 8 and 9, we show the statistics of each task in VTAB [65] and FGVC [31] with regard to the number of classes and the number of images in the train, validation, and test splits. The tables are largely “borrowed” from [31]. Table 8. Dataset details for VTAB. Group Task # Classes Splits Train Val Test Natural CIFAR-100 [35] 100 800 200 10 000 Caltech-101 [15] 102 6 084 DTD [8] 47 1 880 Oxford Flowers [45] 102 6 149 Pets [47] 37 3 669 SVHN [44] 10 26 032 Sun397 [62] 397 21 750 Specialized Patch Camelyon [60] 2 800 200 32 768 EuroSAT [25] 10 5 400 RESISC45 [7] 45 6 300 Diabetic Retinopathy [14] 5 42 670 Structured CLEVR-Count [33] 8 800 200 15 000 CLEVR-Distance [33] 6 15 000 DMLab [2] 6 22 735 KITTI-Distance [18] 4 711 dSprites-Location [42] 16 73 728 dSprites-Orientation [42] 16 73 728 smallNORB-Azimuth [36] 18 12 150 smallNORB-Elevation [36] 9 12 150 ‡https://github.com/THU-MIG/Consolidator iTable 9. Dataset details for FGVC.For datasets markedwith *, we follow [31] to randomly sample train and validation splits because validation sets are not available from the original datasets. Dataset # Classes Splits Train Val Test CUB-200-2011* [61] 200 5 394 600 5 794 NABirds* [27] 555 21 536 2 393 6 084 Oxford Flowers [45] 102 1 020 1 020 6 149 Stanford Dogs* [34] 120 10 800 1 200 8 580 Stanford Cars* [17] 196 7 329 815 8 041 C. More experimental settings For all experiments conducted with our implementation, we average the results over three seeds. This includes the (re-)evaluations of LoRA and VPT. We built our implemen- tation on PyTorch [48], PyTorch Lightning,§ and timm.¶ We run experiments with bfloat16 mixed precision on a NVIDIA RTX A6000 GPU. For our experiments in the main paper, we report results for a fixed adapter rank r as well as ranks optimized per task. For the per-task optimization of Adapter+, we use a hyper- parameter sweep over the set of ranks r∈{1, 2, 4, 8, 16, 32}. We evaluate on the validation sets of VTAB and FGVC and choose the per-task ranks from the specified range(s) to steer the number of average parameters. The ranks we used to produce the results on the VTAB and FGVC test sets (see Tabs. 4 and 5 in the main paper) are shown in detail in Tab. 10 and Tab. 11, respectively. D. Calculation of no. of trainable parameters Suppose we have a ViT with a hidden dimension d, N trans- former layers, and adapters with rank r. The total num- ber of learnable parameters for Adapter base modules ( cf . Eq. (4) of the main paper) attached to the FFN of every transformer layer then amounts to N(2dr + r + d). Includ- ing layer normalization in the adapter modules amounts to N2d additional parameters. The addition of learned, layer- wise scaling amounts to N extra parameters and choosing learned, channel-wise scaling instead adds Nd extra parame- ters. Adapter+ (see Sec. 4.3 of the main paper) thus amounts to N(2dr + 2d + r) total parameters. Additionally, for a task with c classes, we add a classifier with dc + c learnable parameters. E. Vision transformer pre-training As we add only very few parameters to an otherwise frozen backbone, the generalization capability of the feature repre- sentations produced by the backbone is important. For ViTs, there are a number of off-the-shelf models available with §https://lightning.ai/pytorch-lightning ¶https://github.com/huggingface/pytorch-image-models Table 10. Adapter rank r for each VTAB task for optimized versions of Adapter+ with different ranges of permitted ranks. Natural Specialized Structured # Param (M) CIFAR-100 [35] Caltech-101 [15] DTD [8] Flowers [45] Pets [47] SVHN [44] Sun397 [62] Camelyon [60] EuroSAT [25] RESISC45 [7] Retinopathy [14] CLEVR-Count [33] CLEVR-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori. [42] sNORB-Azi. [36] sNORB-Ele. [36] r∈[1..4] 0.11 1 4 2 1 4 4 1 4 2 4 2 4 2 4 4 4 4 4 4 r∈[1..8] 0.16 1 4 2 1 8 8 1 8 2 8 8 4 8 8 8 8 4 8 8 r∈[1..32] 0.27 1 4 2 1 8 16 1 16 2 32 32 4 8 8 8 32 4 32 8 Table 11. Adapter rank r for each FGVC dataset for optimized versions of Adapter+ with different ranges of permitted ranks. # Param (M) CUB-200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] r∈[1..32] 0.34 2 2 1 1 32 differences in their training procedures. Here, we examine three different pre-trainings as examples: (1) Original: The ViT-B/16 weights used in the main paper, pre-trained with su- pervision on ImageNet-21k [55] following the training proce- dure of the original ViT publication [13],|| (2) ImageNet-1k: the same ViT weights further fine-tuned on ImageNet-1k [55],** and (3) AugReg: weights from a pre-training with stronger data augmentation in the form of Mixup [66] and RandAugment [9] following [57].†† In Tab. 12, we summarize our results for Adapter+ with rank r=8 evaluated on the VTAB validation sets. We notice that additional fine-tuning on ImageNet-1k gives a slight edge (83.4% average accuracy over 83.0% for second best) in adaption for tasks that contain natural images. However, the fine-tuning is detrimental for the Specialized and Structured group. Not fine-tuning on ImageNet-1k is beneficial for the Structured group with a large increase of 3.7 pp. The Aug- Reg training setting improves the transfer to the Specialized group but is worse than the other settings for natural images. Overall, the original supervised training on ImageNet-21k generalizes best across all tasks in VTAB with an average accuracy of 76.5%, 0.3 pp better than AugReg training and 1.2 pp better than ImageNet-1k fine-tuning. ||https://storage.googleapis.com/vit_models/imagenet21k/ ViT-B_16.npz **https://storage.googleapis.com/vit_models/imagenet21k+ imagenet2012/ViT-B_16-224.npz ††https://storage.googleapis.com/vit_models/augreg/B_16- i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz iiTable 12. Influence of ViT pre-training. We use Adapter+ with rank r=8 for the evaluation and report the average accuracy in % for each subgroup and across all groups on the VTAB val sets. Pre-training Natural Specialized Structured Average ImageNet-1k 83.4 86.5 56.0 75.3 AugReg 81.6 87.2 59.7 76.2 Original 83.0 86.8 59.7 76.5 Table 13. Adapter position with DINO backbone. We report average accuracy in % ( ± std. dev.) on the VTAB val sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 76.8 ± 0.4 86.2 ± 0.6 53.6 ± 0.7 72.2 ± 0.3 Intermediate 76.8 ± 0.4 85.8 ± 0.8 52.6 ± 0.9 71.8 ± 0.4 Parallel 76.7 ± 0.3 86.8 ± 0.4 54.1 ± 0.7 72.5 ± 0.3 Post 76.9 ± 0.2 86.3 ± 0.5 55.3 ± 0.7 72.8 ± 0.3 Table 14. Comparison of Adapter+ with adapter configurations from previous work with DINO backbone.We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param Natural Specialized Structured Average Houlsby [28], r=8 0.39 77.4 ± 0.4 86.5 ± 0.7 52.9 ± 0.8 72.3 ± 0.4 Houlsby [28], r=4 0.24 77.2 ± 0.5 86.2 ± 0.5 53.2 ± 0.8 72.2 ± 0.3 Pfeiffer [49] 0.21 76.8 ± 0.4 86.2 ± 0.3 54.4 ± 1.0 72.5 ± 0.4 AdaptFormer [6] 0.19 76.5 ± 0.4 85.8 ± 0.4 53.0 ± 0.5 71.8 ± 0.3 Adapter+ 0.20 76.7 ± 0.3 86.4 ± 0.5 55.4 ± 0.8 72.8 ± 0.3 F. Generality of the conclusions Using DINO [5] as an example of a ViT trained with self- supervision, we show in Tab. 13 that the orders of best-to- worst adapter position is consistent with that of a supervised backbone in terms of average accuracy, albeit with a higher standard deviation. The ranking also stays the same for the comparison of Adapter+ with adapter configurations from previous work as presented in Tab. 14. This shows that our conclusions generalize beyond backbones with super- vised pre-training to backbones based on self-supervised pre-training. iii",
      "meta_data": {
        "arxiv_id": "2406.06820v1",
        "authors": [
          "Jan-Martin O. Steitz",
          "Stefan Roth"
        ],
        "published_date": "2024-06-10T22:07:57Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06820v1.pdf",
        "github_url": "https://github.com/visinf/adapter_plus"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides an in-depth and systematic study of adapter configurations for vision transformers (ViTs), uncovering common pitfalls in previous implementations. It proposes an improved adapter architecture, called Adapter+, which features a learnable, channel-wise scaling and an optimal positioning within the transformer layer. Adapter+ achieves state-of-the-art average accuracy on the VTAB benchmark (77.6% without per-task hyperparameter optimization, and 77.9% with optimization) and on the FGVC benchmark (90.7%) with high parameter efficiency. It also demonstrates superior robustness across different VTAB subgroups compared to other parameter-efficient tuning methods.",
        "methodology": "The methodology involved a systematic evaluation of various adapter configurations for ViTs. This included investigating: (1) Adapter position (Pre-Adapter, Post-Adapter, Parallel-Adapter, Intermediate-Adapter relative to the FFN section), (2) Adapter inner structure (impact of biases, layer normalization, layer-wise scaling, and the proposed channel-wise learned scaling), and (3) Parameter initialization strategies (Houlsby, BERT, LoRA). The core adapter module is a bottleneck structure with down- and up-projections and a non-linear activation. Adapter+ specifically utilizes the Post-Adapter position with a learnable, channel-wise scaling and Houlsby initialization, foregoing layer normalization within the adapter. The study also emphasized the critical importance of matching data normalization to the pre-trained backbone and evaluated the influence of training regularization methods like stochastic depth and dropout.",
        "experimental_setup": "Experiments were conducted using a ViT-B/16 network pre-trained on ImageNet-21k. Additional experiments verified the generality of conclusions with ViTs fine-tuned on ImageNet-1k and trained with AugReg, as well as a DINO-trained ViT. The evaluation benchmarks were the Visual Task Adaptation Benchmark (VTAB), comprising 19 tasks grouped into Natural, Specialized, and Structured categories, and a Fine-Grained Visual Classification (FGVC) benchmark consisting of five datasets. Models were trained with an AdamW optimizer (learning rate 10^-3, weight decay 10^-4, batch size 64) for 100 epochs, using a cosine learning rate schedule with a linear warm-up. Input images were resized to 224x224px, with random resize crop and horizontal flipping for FGVC. Data normalization (ImageNet normalization) was consistently applied to match the pre-training. Comparison included full fine-tuning, linear probing, and re-evaluations of contemporary parameter-efficient methods such as LoRA, VPT, NOAH, SSF, FacT, Consolidator, and SPT, ensuring fair comparison by aligning data normalization and training schedules.",
        "limitations": "Not mentioned",
        "future_research_directions": "Not mentioned",
        "experimental_code": "import math\nfrom functools import partial\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom timm.models.layers import DropPath, PatchEmbed, Mlp\nfrom timm.models.helpers import checkpoint_seq\nfrom timm.models.vision_transformer import (\n    Block,\n    ResPostBlock,\n    VisionTransformer,\n    checkpoint_filter_fn,\n    build_model_with_cfg,\n)\n\n# for Adapter+ set norm_layer to None and scaling to \"channel\"\nclass Adapter(nn.Module):\n    def __init__(\n        self,\n        embed_dim,\n        bottleneck_dim=8,\n        drop_path=0.0,\n        dropout=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        scaling=1.0,\n        init=\"houlsby\",\n        bias=True,\n        pre_dropout=False,\n    ):\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Dropout(dropout) if dropout > 0 and pre_dropout else nn.Identity(),\n            nn.Linear(embed_dim, bottleneck_dim, bias=bias),\n            act_layer() if act_layer else nn.Identity(),\n            nn.Dropout(dropout) if dropout > 0 and not pre_dropout else nn.Identity(),\n            nn.Linear(bottleneck_dim, embed_dim, bias=bias),\n        )\n        self.norm_a = norm_layer(embed_dim) if norm_layer else nn.Identity()\n        self.drop_path_a = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.bottleneck_dim = bottleneck_dim\n        if scaling == \"learned\":\n            self.scaling = nn.Parameter(torch.ones(1))\n        elif scaling == \"channel\":\n            self.scaling = nn.Parameter(torch.ones(embed_dim))\n        else:\n            self.scaling = scaling\n\n        # init following (Houslby 2019)\n        if init == \"houlsby\":\n            std = 0.01  # paper value, houlsby code implementation: std = 0.001\n            nn.init.trunc_normal_(\n                self.bottleneck[1].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[1].bias is not None:\n                nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.trunc_normal_(\n                self.bottleneck[4].weight, std=std, a=-2 * std, b=2 * std\n            )\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n\n        # LoRa init\n        elif init == \"lora\":\n            # leave in projection with default init\n            nn.init.kaiming_uniform_(self.bottleneck[1].weight, a=math.sqrt(5))\n            if self.bottleneck[1].bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(\n                    self.bottleneck[1].weight\n                )\n                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n                nn.init.uniform_(self.bottleneck[1].bias, -bound, bound)\n            # set out projection to zeros\n            nn.init.zeros_(self.bottleneck[4].weight)\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n\n        elif init == \"bert\":\n            nn.init.normal_(self.bottleneck[1].weight, mean=0.0, std=0.02)\n            if self.bottleneck[1].bias is not None:\n                nn.init.zeros_(self.bottleneck[1].bias)\n            nn.init.normal_(self.bottleneck[4].weight, mean=0.0, std=0.02)\n            if self.bottleneck[4].bias is not None:\n                nn.init.zeros_(self.bottleneck[4].bias)\n\n        else:\n            raise ValueError(f\"Initialization {init} not implemented!\")\n\n    def forward(\n        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        x = self.norm_a(x)\n        x = self.drop_path_a(self.bottleneck(x))\n        x = x * self.scaling\n\n        y = x\n        if skip is not None:\n            y = y + skip\n\n        return y\n\n\nclass LoRAAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        attn_drop=0.0,\n        proj_drop=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        lora_config=None,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.lora_config = lora_config\n        self.adapters = nn.ModuleDict(\n            {\n                l: Adapter(\n                    dim,\n                    bottleneck_dim=lora_config.dim,\n                    act_layer=act_layer if lora_config.act_layer else None,\n                    norm_layer=norm_layer if lora_config.norm_layer else None,\n                    scaling=lora_config.scaling,\n                    bias=((qkv_bias or l == \"o\") and lora_config.bias),\n                    drop_path=lora_config.drop_path,\n                    dropout=lora_config.dropout,\n                    pre_dropout=lora_config.pre_dropout,\n                )\n                for l in lora_config.location\n            }\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C // num_heads\n        )\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        if \"q\" in self.adapters:\n            q = q + (\n                self.adapters[\"q\"](x)\n                .reshape(B, N, self.num_heads, C // self.num_heads)\n                .permute(0, 2, 1, 3)  # B, num_heads, N, C // num_heads\n            )\n        if \"k\" in self.adapters:\n            k = k + (\n                self.adapters[\"k\"](x)\n                .reshape(B, N, self.num_heads, C // self.num_heads)\n                .permute(0, 2, 1, 3)  # B, num_heads, N, C // num_heads\n            )\n        if \"v\" in self.adapters:\n            v = v + (\n                self.adapters[\"v\"](x)\n                .reshape(B, N, self.num_heads, C // self.num_heads)\n                .permute(0, 2, 1, 3)  # B, num_heads, N, C // num_heads\n            )\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        if \"o\" in self.adapters:\n            x = x + self.adapters[\"o\"](x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass AdapterBlock(Block):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        proj_drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values: Optional[float] = None,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer: nn.Module = nn.LayerNorm,\n        mlp_layer: nn.Module = Mlp,\n        adapter_config=None,\n        lora_config=None,\n        prompt_config=None,\n        patch_size=16,\n    ):\n        super().__init__(\n            dim,\n            num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            proj_drop=proj_drop,\n            attn_drop=attn_drop,\n            init_values=init_values,\n            drop_path=drop_path,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            mlp_layer=mlp_layer,\n        )\n\n        self.adapter_config = adapter_config\n        self.lora_config = lora_config\n        self.prompt_config = prompt_config\n\n        if adapter_config is not None:\n            self.adapter = Adapter(\n                dim,\n                bottleneck_dim=adapter_config.dim,\n                dropout=adapter_config.dropout,\n                drop_path=adapter_config.drop_path,\n                act_layer=act_layer if adapter_config.act_layer else None,\n                norm_layer=norm_layer if adapter_config.norm_layer else None,\n                bias=adapter_config.bias,\n                scaling=adapter_config.scaling,\n                init=adapter_config.init,\n            )\n            if adapter_config.attn_adapter:\n                self.adapter_attn = Adapter(\n                    dim,\n                    bottleneck_dim=adapter_config.dim,\n                    dropout=adapter_config.dropout,\n                    drop_path=adapter_config.drop_path,\n                    act_layer=act_layer if adapter_config.act_layer else None,\n                    norm_layer=norm_layer if adapter_config.norm_layer else None,\n                    bias=adapter_config.bias,\n                    scaling=adapter_config.scaling,\n                    init=adapter_config.init,\n                )\n\n        if lora_config is not None and lora_config.config == \"attention\":\n            self.attn = LoRAAttention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                lora_config=lora_config,\n            )\n\n        if prompt_config is not None:\n            self.prompt = nn.Parameter(torch.zeros(prompt_config.num_tokens, dim))\n            self.prompt_dropout = nn.Dropout(prompt_config.dropout)\n\n            val = math.sqrt(6.0 / float(3 * patch_size**2 + prompt_config.num_tokens))\n            nn.init.uniform_(self.prompt, -val, val)\n\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward_pre(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = self.adapter(x, skip=x)\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n    def forward_pfeiffer(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        skip = self.drop_path2(self.ls2(self.mlp(self.norm2(y))))\n        x = y + skip\n        x = self.adapter(x, skip=skip)\n        x = x + y\n        return x\n\n    def forward_intermediate(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        y = self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x + self.adapter(y, skip=y)\n\n    def forward_houlsby(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.adapter_attn(y, skip=y)\n        y = self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x + self.adapter(y, skip=y)\n\n    def forward_parallel(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        z = y + self.drop_path2(self.ls2(self.mlp(self.norm2(y))))\n        return self.adapter(y, skip=z)\n\n    def forward_no_adapter(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n\n    def include_prompt(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = torch.cat(\n            (\n                x[:, :1, :],\n                self.prompt_dropout(self.prompt.expand(B, -1, -1)),\n                x[:, (1 + self.prompt_config.num_tokens) :, :],\n            ),\n            dim=1,\n        )\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\n\nclass AdapterResPostBlock(ResPostBlock):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        proj_drop: float = 0.0,\n        attn_drop: float = 0.0,\n        init_values: Optional[float] = None,\n        drop_path: float = 0.0,\n        act_layer: nn.Module = nn.GELU,\n        norm_layer: nn.Module = nn.LayerNorm,\n        mlp_layer: nn.Module = Mlp,\n        adapter_config=None,\n        lora_config=None,\n        prompt_config=None,\n        patch_size=16,\n    ) -> None:\n        super().__init__(\n            dim=dim,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            proj_drop=proj_drop,\n            attn_drop=attn_drop,\n            init_values=init_values,\n            drop_path=drop_path,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            mlp_layer=mlp_layer,\n        )\n\n        self.adapter_config = adapter_config\n        self.lora_config = lora_config\n        self.prompt_config = prompt_config\n\n        if adapter_config is not None:\n            self.adapter = Adapter(\n                dim,\n                bottleneck_dim=adapter_config.dim,\n                dropout=adapter_config.dropout,\n                drop_path=adapter_config.drop_path,\n                act_layer=act_layer if adapter_config.act_layer else None,\n                norm_layer=norm_layer if adapter_config.norm_layer else None,\n                bias=adapter_config.bias,\n                scaling=adapter_config.scaling,\n                init=adapter_config.init,\n            )\n            if adapter_config.attn_adapter:\n                self.adapter_attn = Adapter(\n                    dim,\n                    bottleneck_dim=adapter_config.dim,\n                    dropout=adapter_config.dropout,\n                    drop_path=adapter_config.drop_path,\n                    act_layer=act_layer if adapter_config.act_layer else None,\n                    norm_layer=norm_layer if adapter_config.norm_layer else None,\n                    bias=adapter_config.bias,\n                    scaling=adapter_config.scaling,\n                    init=adapter_config.init,\n                )\n\n        if lora_config is not None and lora_config.config == \"attention\":\n            self.attn = LoRAAttention(\n                dim,\n                num_heads=num_heads,\n                qkv_bias=qkv_bias,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                lora_config=lora_config,\n            )\n\n        if prompt_config is not None:\n            self.prompt = nn.Parameter(torch.zeros(prompt_config.num_tokens, dim))\n            self.prompt_dropout = nn.Dropout(prompt_config.dropout)\n\n            val = math.sqrt(6.0 / float(3 * patch_size**2 + prompt_config.num_tokens))\n            nn.init.uniform_(self.prompt, -val, val)\n\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        x = self.adapter(x, skip=x)\n        return x\n\n    def forward_pre(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = self.adapter(x, skip=x)\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n    def forward_pfeiffer(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.norm1(self.attn(x)))\n        skip = self.drop_path2(self.norm2(self.mlp(y)))\n        x = y + skip\n        x = self.adapter(x, skip=skip)\n        x = x + y\n        return x\n\n    def forward_intermediate(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        y = self.drop_path2(self.norm2(self.mlp(x)))\n        return x + self.adapter(y, skip=y)\n\n    def forward_houlsby(self, x: torch.Tensor) -> torch.Tensor:\n        y = self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.adapter_attn(y, skip=y)\n        y = self.drop_path2(self.norm2(self.mlp(x)))\n        return x + self.adapter(y, skip=y)\n\n    def forward_parallel(self, x: torch.Tensor) -> torch.Tensor:\n        y = x + self.drop_path1(self.norm1(self.attn(x)))\n        z = y + self.drop_path2(self.norm2(self.mlp(y)))\n        return self.adapter(y, skip=z)\n\n    def forward_no_adapter(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.norm1(self.attn(x)))\n        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n        return x\n\n    def include_prompt(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = torch.cat(\n            (\n                x[:, :1, :],\n                self.prompt_dropout(self.prompt.expand(B, -1, -1)),\n                x[:, (1 + self.prompt_config.num_tokens) :, :],\n            ),\n            dim=1,\n        )\n        return x\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.prompt_config:\n            x = self.include_prompt(x)\n        if self.adapter_config is None:\n            return self.forward_no_adapter(x)\n        elif self.adapter_config.config == \"pfeiffer\":\n            return self.forward_pfeiffer(x)\n        elif self.adapter_config.config == \"post\":\n            return self.forward_post(x)\n        elif self.adapter_config.config == \"pre\":\n            return self.forward_pre(x)\n        elif self.adapter_config.config == \"houlsby\":\n            return self.forward_houlsby(x)\n        elif self.adapter_config.config == \"intermediate\":\n            return self.forward_intermediate(x)\n        elif self.adapter_config.config == \"parallel\":\n            return self.forward_parallel(x)\n        else:\n            raise ValueError(f\"Unknown adapter config: {self.adapter_config.config}\")\n\n\n# modified from timm.models.vision_transformer.VisionTransformer\nclass VisionTransformerAdapter(VisionTransformer):\n    \"\"\"Vision Transformer with Adapter support\n\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        global_pool=\"token\",\n        embed_dim=768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_norm: bool = False,\n        init_values=None,\n        class_token=True,\n        pos_embed=\"learn\",\n        no_embed_class=False,\n        reg_tokens=0,\n        pre_norm=False,\n        fc_norm=None,\n        dynamic_img_size=False,\n        dynamic_img_pad=False,\n        drop_rate=0.0,\n        pos_drop_rate=0.0,\n        patch_drop_rate=0.0,\n        proj_drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        weight_init=\"\",\n        fix_init=False,\n        embed_layer=PatchEmbed,\n        norm_layer=None,\n        act_layer=None,\n        mlp_layer=Mlp,\n        block_fn=AdapterBlock,\n        adapter_config=None,\n        lora_config=None,\n        prompt_config=None,\n    ):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            global_pool (str): type of global pooling for final sequence (default: 'token')\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            init_values: (float): layer-scale init values\n            class_token (bool): use class token\n            fc_norm (Optional[bool]): pre-fc norm after pool, set if global_pool == 'avg' if None (default: None)\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            weight_init (str): weight init scheme\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            act_layer: (nn.Module): MLP activation layer\n        \"\"\"\n        super().__init__(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            num_classes=num_classes,\n            global_pool=global_pool,\n            embed_dim=embed_dim,\n            depth=depth,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            init_values=init_values,\n            class_token=class_token,\n            pos_embed=pos_embed,\n            no_embed_class=no_embed_class,\n            reg_tokens=reg_tokens,\n            pre_norm=pre_norm,\n            fc_norm=fc_norm,\n            dynamic_img_size=dynamic_img_size,\n            dynamic_img_pad=dynamic_img_pad,\n            drop_rate=drop_rate,\n            pos_drop_rate=pos_drop_rate,\n            patch_drop_rate=patch_drop_rate,\n            proj_drop_rate=proj_drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate,\n            weight_init=weight_init,\n            fix_init=fix_init,\n            embed_layer=embed_layer,\n            norm_layer=norm_layer,\n            act_layer=act_layer,\n            block_fn=block_fn,\n            mlp_layer=mlp_layer,\n        )\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n        self.prompt_config = prompt_config\n        self.num_cls_token = 0\n        if self.num_cls_token is not None:\n            self.num_cls_token += 1\n        dpr = [\n            x.item() for x in torch.linspace(0, drop_path_rate, depth)\n        ]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(\n            *[\n                block_fn(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_norm=qk_norm,\n                    init_values=init_values,\n                    proj_drop=proj_drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    act_layer=act_layer,\n                    mlp_layer=mlp_layer,\n                    adapter_config=adapter_config,\n                    lora_config=lora_config,\n                    prompt_config=prompt_config,\n                    patch_size=patch_size,\n                )\n                for i in range(depth)\n            ]\n        )\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n\n        if self.prompt_config:\n            B, _, C = x.shape\n            x = torch.cat(\n                (\n                    x[:, : self.num_cls_token, :],\n                    torch.zeros(B, self.prompt_config.num_tokens, C).to(x.device),\n                    x[:, self.num_cls_token :, :],\n                ),\n                dim=1,\n            )\n\n        if self.grad_checkpointing and not torch.jit.is_scripting():\n            x = checkpoint_seq(self.blocks, x)\n        else:\n            x = self.blocks(x)\n        x = self.norm(x)\n        return x\n\n\n# modified from timm.models.vision_transformer._create_vision_transformer\ndef _create_vision_transformer_adapter(\n    variant: str, pretrained: bool = False, adapter=False, **kwargs\n) -> VisionTransformer:\n    out_indices = kwargs.pop(\"out_indices\", 3)\n    if \"flexi\" in variant:\n        # FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed\n        # interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.\n        _filter_fn = partial(\n            checkpoint_filter_fn, interpolation=\"bilinear\", antialias=False\n        )\n    else:\n        _filter_fn = checkpoint_filter_fn\n\n    # FIXME attn pool (currently only in siglip) params removed if pool disabled, is there a better soln?\n    strict = True\n    if \"siglip\" in variant and kwargs.get(\"global_pool\", None) != \"map\":\n        strict = False\n\n    if adapter:\n        # rewrite block_fn to use adapters\n        block_fn = kwargs.pop(\"block_fn\", Block)\n        if block_fn == Block:\n            block_fn = AdapterBlock\n        elif block_fn == ResPostBlock:\n            block_fn = AdapterResPostBlock\n        else:\n            raise ValueError(f\"Adapters not implemented for {block_fn}!\")\n        return build_model_with_cfg(\n            VisionTransformerAdapter,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            block_fn=block_fn,\n            **kwargs,\n        )\n    else:\n        return build_model_with_cfg(\n            VisionTransformer,\n            variant,\n            pretrained,\n            pretrained_filter_fn=_filter_fn,\n            pretrained_strict=strict,\n            feature_cfg=dict(out_indices=out_indices, feature_cls=\"getter\"),\n            **kwargs,\n        )",
        "experimental_info": "The methodology involved a systematic evaluation of various adapter configurations for ViTs. These configurations are primarily implemented in the `Adapter` class and its integration within `AdapterBlock` and `AdapterResPostBlock`.\n\n**1. Adapter Position:**\n   The repository investigates different adapter positions relative to the FFN section within the Vision Transformer blocks. These include:\n   - **Post-Adapter:** The adapter is applied after the MLP (Feed-Forward Network) block (`forward_post` method in `AdapterBlock`).\n   - **Pre-Adapter:** The adapter is applied before the MLP block, but after the attention block (`forward_pre` method).\n   - **Intermediate-Adapter:** The adapter is applied within the residual branch of the MLP block (`forward_intermediate` method).\n   - **Parallel-Adapter:** The adapter runs in parallel to the main MLP residual connection (`forward_parallel` method).\n   - **Houlsby Configuration:** Adapters are applied both after the attention block and after the MLP block (`forward_houlsby` method, utilizing `self.adapter_attn` and `self.adapter`).\n   - A Pfeiffer-style adapter position is also available (`forward_pfeiffer`).\n\n**2. Adapter Inner Structure:**\n   The internal design of the `Adapter` module (a bottleneck structure) was varied by considering:\n   - **Biases:** The inclusion of biases in the linear projections within the bottleneck (`bias` parameter in `nn.Linear` layers of the `Adapter` module).\n   - **Layer Normalization:** The presence of `nn.LayerNorm` within the adapter (`norm_layer` parameter in `Adapter`).\n   - **Layer-wise Scaling:** A single learnable scaling parameter for the entire adapter output (`scaling=\"learned\"`).\n   - **Channel-wise Learned Scaling:** A learnable scaling parameter for each channel of the adapter output (`scaling=\"channel\"`).\n\n**3. Parameter Initialization Strategies:**\n   Different weight initialization schemes for the adapter's linear layers were evaluated:\n   - **Houlsby Initialization:** Using `nn.init.trunc_normal_` with a standard deviation of 0.01 for weights and zero initialization for biases (`init=\"houlsby\"`).\n   - **BERT Initialization:** Using `nn.init.normal_` with mean 0.0 and std 0.02 for weights and zero initialization for biases (`init=\"bert\"`).\n   - **LoRA Initialization:** `nn.init.kaiming_uniform_` for the input projection and zero initialization for the output projection (`init=\"lora\"`).\n\n**4. Adapter+ Specifics (as described in the method):**\n   - **Position:** Post-Adapter (`adapter_config.config = \"post\"`).\n   - **Scaling:** Channel-wise learned scaling (`scaling=\"channel\"` in `Adapter`'s `__init__`).\n   - **Initialization:** Houlsby initialization (`init=\"houlsby\"` in `Adapter`'s `__init__`).\n   - **Layer Normalization:** Foregoing layer normalization within the adapter (`norm_layer=None` in `Adapter`'s `__init__`).\n\n**5. Data Normalization:**\n   The study emphasized matching data normalization to the pre-trained backbone. This is implemented by retrieving the `mean` and `std` values from the `vit.default_cfg` of the loaded pre-trained model and applying them during data loading (e.g., in `data/fgvc.py` and `data/vtab.py`).\n\n**6. Training Regularization:**\n   The influence of common regularization methods was evaluated:\n   - **Stochastic Depth:** Applied via `drop_path_rate` in `VisionTransformerAdapter` (stochastic depth decay rule `dpr`) and `drop_path` in the `Adapter` module.\n   - **Dropout:** Applied within the `Adapter` module (`dropout` parameter).\n   - **Label Smoothing:** Utilized during the training loss calculation (`label_smoothing` in `AdapterModel`)."
      }
    },
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces the \"Spectral Adapter,\" a novel Parameter-Efficient Fine-Tuning (PEFT) method that incorporates the spectral information of pretrained weight matrices. The main contributions include: (1) Proposing two spectral adaptation mechanisms: additive tuning (Spectral AdapterA) and orthogonal rotation (Spectral AdapterR) of the top singular vectors, performed after Singular Value Decomposition (SVD) of pretrained weights. (2) Providing theoretical analysis showing Spectral AdapterA doubles the rank capacity of low-rank adapters compared to LoRA for a fixed trainable parameter budget and demonstrating that dominant spectral directions align with ideal neuron directions. (3) Empirically demonstrating superior parameter efficiency, tuning performance, and benefits for multi-adapter fusion compared to various state-of-the-art PEFT methods (LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA). (4) Showcasing its practicality with negligible runtime and storage overhead for large models. This work is the first to propose fine-tuning spectral space in a parameter-efficient and storage-economic way, enhancing existing PEFT methods.",
        "methodology": "The core methodology involves applying Singular Value Decomposition (SVD) to pretrained model weights (W = USV^T) and then fine-tuning only the top columns of the singular vector matrices (U and V). Two specific spectral adaptation mechanisms are investigated: (1) Spectral AdapterA (Additive Tuning): Modifies the top-r columns of U and V additively, defining the tuned weight as [U1 + AU U2]S[V1 + AV V2]^T, where AU and AV are trainable matrices. It initializes AU and AV to zero. (2) Spectral AdapterR (Orthogonal Rotation): Rotates the top-r columns of U and V using trainable orthogonal matrices, defining the tuned weight as [U1RU U2]S[V1RV V2]^T, where RU and RV are trainable r x r orthogonal matrices. The orthogonality constraint for RU and RV is efficiently maintained using Cayley parameterization, allowing for exact rotation and preserving the SVD structure for subsequent fine-tuning without recomputing SVD. For multi-adapter fusion, Spectral AdapterA distributes different concept tunings along distinct columns of the singular vector matrices and merges them following a FedAvg-like approach: (U0 + ∑λiUi)S0(V0 + ∑λiVi)^T.",
        "experimental_setup": "The proposed Spectral Adapter was evaluated on large language models and diffusion models. For language models, experiments included: (1) Fine-tuning Llama3 8B model on the Orca Math dataset and evaluating on the GSM8K benchmark. (2) Fine-tuning DeBERTaV3-base (185M parameters) on GLUE benchmarks (MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, STS-B). (3) Fine-tuning Mistral 7B model for the GSM8K task. For diffusion models, experiments were conducted using the Chilloutmix diffusion model for: (1) Multi-adapter fusion with custom animal and toy concepts, and multi-character generation with photos of computer scientists, utilizing T2I-Adapter with sketch/keypose conditions. (2) Parameter efficiency comparisons with custom vase, chair, and table concepts. Baselines for PEFT comparisons included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, and full fine-tuning. For multi-adapter fusion, baselines included Gradient Fusion, Orthogonal Adaptation, and FedAvg. Evaluation metrics included training loss and validation scores (e.g., GSM8K exact matching, GLUE accuracy), visual quality of generated images, and CLIP-based alignment scores (cosine similarity between generated/reference images and prompt texts). All experiments were performed using NVIDIA RTX A6000 GPUs, with hyperparameters generally following official implementations of baselines or tuned specifically for Spectral AdapterA.",
        "limitations": "The primary limitation of the current work lies in the empirical choice of tuning only the top spectral space. Although its validity is theoretically verified under simple settings, a more comprehensive investigation into tuning different columns or subspaces of singular vector matrices is crucial for a deeper understanding of spectral information's role in fine-tuning. Additionally, fine-tuning the spectral representation of different specific components (e.g., only the attention layer) of large models warrants further dedicated study. Another practical constraint is the increasing time consumption of the Singular Value Decomposition (SVD) procedure as models scale larger, necessitating the exploration and integration of faster SVD methods.",
        "future_research_directions": "Future research directions include: (1) Investigating the fine-tuning of spectral representation of specific components within large models, such as focusing solely on the attention layer. (2) Exploring dynamic combinations of spectral adaptation with other advanced PEFT methods, such as AdaLoRA, to potentially enhance tuning efficiency and performance further. (3) Conducting more in-depth studies on the effects of tuning different columns or subspaces of singular vector matrices beyond just the top spectral space, which is critical for a complete understanding of spectral information's role in fine-tuning. (4) Developing or integrating faster Singular Value Decomposition (SVD) methods to alleviate the increasing time consumption associated with larger model sizes and improve practicality.",
        "experimental_code": "import math\n\nimport torch\nimport torch.nn as nn\nfrom diffusers.models.attention_processor import AttnProcessor\nfrom diffusers.utils.import_utils import is_xformers_available\nimport torch.nn.functional as F\nimport numpy as np\n\nif is_xformers_available():\n    import xformers\n    \n\ndef remove_edlora_unet_attention_forward(unet):\n    def change_forward(unet):  # omit proceesor in new diffusers\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and name == 'attn2':\n                layer.set_processor(AttnProcessor())\n            else:\n                change_forward(layer)\n    change_forward(unet)\n\n\nclass EDLoRA_Control_AttnProcessor:\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n    def __init__(self, cross_attention_idx, place_in_unet, controller, attention_op=None):\n        self.cross_attention_idx = cross_attention_idx\n        self.place_in_unet = place_in_unet\n        self.controller = controller\n        self.attention_op = attention_op\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            is_cross = False\n            encoder_hidden_states = hidden_states\n        else:\n            is_cross = True\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available() and not is_cross:\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            attention_probs = self.controller(attention_probs, is_cross, self.place_in_unet)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass EDLoRA_AttnProcessor:\n    def __init__(self, cross_attention_idx, attention_op=None):\n        self.attention_op = attention_op\n        self.cross_attention_idx = cross_attention_idx\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available():\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\ndef revise_edlora_unet_attention_forward(unet):\n    def change_forward(unet, count):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:\n                layer.set_processor(EDLoRA_AttnProcessor(count))\n                count += 1\n            else:\n                count = change_forward(layer, count)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0)\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx)\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx)\n    print(f'Number of attention layer registered {cross_attention_idx}')\n\n\ndef revise_edlora_unet_attention_controller_forward(unet, controller):\n    class DummyController:\n        def __call__(self, *args):\n            return args[0]\n\n        def __init__(self):\n            self.num_att_layers = 0\n\n    if controller is None:\n        controller = DummyController()\n\n    def change_forward(unet, count, place_in_unet):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:  # only register controller for cross-attention\n                layer.set_processor(EDLoRA_Control_AttnProcessor(count, place_in_unet, controller))\n                count += 1\n            else:\n                count = change_forward(layer, count, place_in_unet)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0, 'down')\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx, 'mid')\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx, 'up')\n    print(f'Number of attention layer registered {cross_attention_idx}')\n    controller.num_att_layers = cross_attention_idx\n\nclass SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        rank = 8\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r\n\n    def cayley(self, data: torch.Tensor) -> torch.Tensor:\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)\n        else:\n            raise Exception('')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv :\n            raise Exception('')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)",
        "experimental_info": "The core method, \"Spectral AdapterR (Orthogonal Rotation)\", is implemented in the `SpectralLinearLayer_OFT` class found in `adapter_efficiency/mix_spectral/Mix-of-Show/mixofshow/models/edlora.py`. This class performs Singular Value Decomposition (SVD) on the `original_module.weight` to obtain U, S, and V matrices. It then applies orthogonal rotations to selected 'top-r' columns of the U and V matrices and scales corresponding singular values in S.\n\nKey experimental settings and mechanisms:\n-   **SVD Application**: For any given `original_module` (e.g., linear layers in a neural network), its weight matrix `W` undergoes SVD: `U, S, V = torch.svd(W)`. These U, S, V are stored as non-trainable parameters.\n-   **Orthogonal Rotation (Spectral AdapterR)**: Trainable matrices `self.spectral_A` and `self.spectral_B` are used to create orthogonal rotation matrices via the `cayley` function. These rotations are applied to specific blocks of columns (defined by `self.idx*self.rank:(self.idx+1)*self.rank`) within the U and V singular vector matrices. This directly corresponds to the `[U1RU U2]S[V1RV V2]^T` formulation.\n-   **Singular Value Scaling**: A trainable parameter `self.spectral_C` is used to scale the singular values in the `S` matrix for the same selected block of indices.\n-   **Initialization**: `self.spectral_A` and `self.spectral_B` are initialized to zero matrices (meaning `cayley(0)` results in an identity matrix, so no initial rotation is applied). `self.spectral_C` is initialized to ones, meaning no initial scaling is applied to singular values.\n-   **Recomposition**: The modified `pad_U`, `pad_S`, and `pad_V` are recomposed to form the updated weight matrix `pad_W = pad_U@pad_S.diag()@pad_V.T`.\n-   **Fine-tuning Focus**: The `idx` and `rank` parameters in `SpectralLinearLayer_OFT` indicate that fine-tuning is targeted at specific 'top-r' (where `r` is `self.rank`) columns of the singular vector matrices, allowing for localized adaptation.\n\n*Note*: The `Spectral AdapterA (Additive Tuning)` mechanism, which involves additive modifications `[U1 + AU U2]S[V1 + AV V2]^T` with `AU` and `AV` initialized to zero, is described in the method but not explicitly implemented by the `SpectralLinearLayer_OFT` directly. The provided `fedavg_gradient/fedavg_fusion.py` uses a FedAvg-like approach for merging concepts by averaging learned LoRA weights or using quasi-Newton optimization on derived weight updates, which differs from the described `(U0 + ∑λiUi)S0(V0 + ∑λiVi)^T` fusion of singular vectors."
      }
    },
    {
      "title": "Multi-Head Adapter Routing for Cross-Task Generalization",
      "abstract": "Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists\nin pre-training adapters on a multi-task training set before few-shot\nadaptation to test tasks. Polytropon [Ponti et al., 2023] ($\\texttt{Poly}$)\njointly learns an inventory of adapters and a routing function that selects a\n(variable-size) subset of adapters for each task during both pre-training and\nfew-shot adaptation. In this paper, we investigate the role that adapter\nrouting plays in its success and design new variants based on our findings.\nFirst, we build on the intuition that finer-grained routing provides more\nexpressivity. Hence, we propose $\\texttt{MHR}$ (Multi-Head Routing) which\ncombines subsets of adapter parameters and outperforms $\\texttt{Poly}$ under a\ncomparable parameter budget; by only fine-tuning the routing function and not\nthe adapters ($\\texttt{MHR}$-$z$) we achieve competitive performance with\nextreme parameter efficiency. Second, we find that\n$\\texttt{Poly}$/$\\texttt{MHR}$ performance is a result of better multi-task\noptimization, rather than modular inductive biases that facilitate adapter\nrecombination and local adaptation, as previously hypothesized. In fact, we\nfind that $\\texttt{MHR}$ exhibits high gradient alignment between training\ntasks. We find that routing is most beneficial during multi-task pre-training\nrather than during few-shot adaptation and propose $\\texttt{MHR}$-$\\mu$, which\ndiscards routing and fine-tunes the average of the pre-trained adapters on each\ndownstream tasks. This establishes $\\texttt{MHR}$-$\\mu$ as an effective method\nfor single-adapter fine-tuning. We also show that $\\texttt{MHR}$-$\\mu$ can be\nused as an effective zero-shot transfer method by training the average of the\npre-trained adapters for a few additional steps on the multi-task training set:\nthis yields gains up to 3% on absolute accuracy w.r.t. the baselines.",
      "full_text": "Multi-Head Adapter Routing for Cross-Task Generalization Lucas Caccia Edoardo Ponti Zhan Su Matheus Pereira Nicolas Le Roux Alessandro Sordoni Microsoft Research, McGill University, MILA, University of Edinburgh, Université de Montréal, University of Copenhagen lucas.page-caccia@mail.mcgill.ca,alsordon@microsoft.com Abstract Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ( Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose MHR (Multi-Head Routing), which combines blocks of parameters from different adapters and outperforms Poly under a comparable parameter budget; by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency. Second, we find that Poly/MHR performance is a result of better multi-task optimization, rather than modular inductive biases that facilitate adapter recombination and local adaptation, as previously hypothesized. In fact, we find that MHR exhibits high gradient alignment between training tasks. We find that routing is most beneficial during multi-task pre-training rather than during few-shot adaptation and propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks. This establishes MHR-µ as an effective method for single-adapter fine-tuning. We also show thatMHR-µ can be used as an effective zero-shot transfer method by training the average of the pre-trained adapters for a few additional steps on the multi-task training set: this yields gains up to 3% on absolute accuracy w.r.t. the baselines. Code is available at https://github.com/microsoft/mttl. 1 Introduction The ability to train effective models with a relatively small number of training data is of paramount importance due to the paucity of annotated examples for most tasks. One effective few-shot learning approach is to leverage large models pre-trained on a vast amount of unlabelled data and fine-tune them on the few examples available for each downstream task. To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022], SFT [Ansell et al., 2022], or (IA)3 [Liu et al., 2022]. These only fine-tune adapters while leaving the pre-trained model ‘frozen’. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2211.03831v3  [cs.AI]  13 Nov 2023Add & Norm Feed-Forward  Multi-Head  Attention Add & Norm Wpretrained     LoRA + A BT Head # 2 Head # 1 Tasks Active Task Modules Z = Multi-Head Routing  (MHR) Head # 2 Head # 1 GLUE-rte … ZMHR- Q W0 K V O W1 Figure 1: Left: A LoRA adapter with weight AB⊤ is trained on top of a frozen, pre-trained linear layer W. Our method MHR partitions the A, Bparameter indexes into h blocks (or heads). For each block, a separate routing function selects the active modules for the current task amongm copies with different parameter values, and combines them via averaging to form a task-specific head. The heads are then concatenated to form the LoRA adapter. Using multiple heads allows for more fine-grained mixing of task parameters with a negligible increase in overall parameter count. Right: During few-shot adaptation, one can fine-tune only the multi-head routing parameters (MHR-z), keeping the modules frozen, resulting in highly parameter-efficient adaptation. Nevertheless, it remains unclear how to best exploit a set of training tasks to better generalize to a set of unseen test tasks in a sample-efficient fashion, based on just a few examples. One straightforward solution is to perform multi-task pre-training, i.e. first train the large model on the union of the examples from the training tasks, then fine-tune the obtained model to the test task [Liu et al., 2022, Ye et al., 2021]. However, this solution does not take into account that test tasks may require solving different combinations of sub-problems compared to training tasks [Vu et al., 2020], thus failing to achieve compositional generalization [Rosenbaum et al., 2019, Ponti, 2021]. Moreover, specializing the model towards different tasks during training may result in negative transfer, due to their corresponding gradients being misaligned [Wang et al., 2021]. Several PEFT approaches have been proposed to enable better cross-task generalization by training adapters (or soft prompts) on each task independently [Pfeiffer et al., 2021, Vu et al., 2021, Asai et al., 2022, Chronopoulou et al., 2023]. Given a new test task, parameters from similar training tasks are aggregated, which enables transfer. While solely having task-specific parameters is an effective strategy to mitigate interference across training tasks, it also inhibits any positive transfer within the same task pool. Polytropon (Poly) was recently proposed by Ponti et al. [2023] to address these issues: the model assumes that task-specific adapters are learned combinations of a reusable inventory of basis adapters ormodules. In practice, each module is implemented as a LoRA [Hu et al., 2022] adapter, which modifies a large pre-trained model, such as T5 [Raffel et al., 2020]. During both multi-task pre-training and few-shot adaptation, Poly learns both the inventory of adapters and a (continuously relaxed) binary task–module routing matrix, which determines which module is active for each task. While Poly shows promising results, several questions remain unanswered: 1) Does the expressivity of the routing function matter? 2) Why do routing-based PEFT methods yield superior performance? 3) Is routing useful during both multi-task pre-training and few-shot adaptation? To answer the first question, we propose a new routing function, MHR, that mixes adapters at a more granular level. Differently from Poly, where routing decisions are made for each adapter as a whole, in MHR we linearly combine blocks of the adapter dimensions (i.e. heads), each with different combination coefficients. We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]. Based on our results, we report thatMHR outperforms Poly and single adapter baselines. Additionally, we show that, thanks to the increased expressivity of the routing function, it becomes possible to fine-tune only the parameters of the routing function (and not the adapters) during few-shot adaptation: the resulting method, MHR-z, yields competitive performance while requiring orders of magnitude fewer parameters. 2Regarding the second and third questions, we uncover that optimization during multitask pretraining plays a key role in explaining the downstream performance of routing-based PEFT approaches. Specifically, we find that MHR exhibits a higher cosine similarity between gradients from different tasks than Poly and single-adapter multi-task training. Hence, routing enables more knowledge transfer and less interference across tasks during multi-task pre-training. This finding led us to investigate whether routing is useful also during few-shot adaptation. It has been hypothesized [Ponti et al., 2023] that one of the reasons behind Poly’s performance resides in the inductive bias of the modular architecture, which allows test tasks to recombine and locally adapt the most relevant modules. To test this hypothesis, we propose MHR-µ, where the routing function is discarded and all available adapter parameters are averaged before few-shot adaptation. We find that MHR-µ can recover the performance of MHR, hinting that Poly/MHR gains are only a result of better multi-task optimization. Finally, we show that MHR-µ can also be used as an effective zero-shot transfer method by training the average of the pre-trained adapters for a few additional steps on the multi-task training set. This yields gains up to 3% on absolute accuracy w.r.t. to strong baselines such as T0-11B. 2 Background In cross-task generalization, we are given a set of tasks T = {T1, ..,T|T |}, with each task Ti dataset containing a set of samples Di = {(x1, y1), ...,(xn, yn)}. The set of all tasks is partitioned into training and test tasks, T = Ttrain ∪ Teval, and the objective is to leverage data inTtrain and transfer knowledge to facilitate learning of the test tasks Teval. For all the methods discussed, learning takes place in two phases, excluding the original unsupervised pre-training of the language model backbone on a separate corpus. The first phase consists of multi-task pre-training, in which either an adapter, such as LoRA or (IA)3, or the full backbone is trained on the set of training tasks Ttrain. The second phase consists in few-shot adaptation, where the learned adapters are fine-tuned independently on each test task in Teval. We follow the procedure from [Raffel et al., 2020] and formulate each task as a text-to-text problem, enabling standard maximum-likelihood training with teacher forcing [Bengio et al., 2015] and a cross-entropy loss. 2.1 Adapters: LoRA & (IA)3 LoRA [Hu et al., 2022] and (IA)3 [Liu et al., 2022] are two recently proposed adapter architectures that achieve competitive trade-offs between performance and parameter efficiency [Karimi Mahabadi et al., 2021, Liu et al., 2022]. For each linear transformation corresponding to the query (q), key (k), value (v) and output (o) of the self-attention layers, LoRA modifies the base model parameters as follows: hq,k,v,o = Wq,k,v,o 0 x + s · Aq,k,v,o(Bq,k,v,o)⊤x, (LoRA) where W0 are the (frozen) weights of the pre-trained model (e.g. T5 [Raffel et al., 2020]). A, B ∈ Rd×r are low-rank learnable parameters and s ≥ 1 is a tunable scalar hyperparameter. (IA)3, on the other hand, modifies key and value representations in self-attention element-wise, and also modifies the feed-forward MLP (f): hk,v = lk,v ⊙ (Wk,v 0 x); hf = (lf ⊙ γ(Wf 1 x))Wf 2 , ((IA)3) where lk,v,f ∈ Rd are learnable parameters , Wf 1,2 the frozen parameters of the feed-forward layer in the backbone, and γ a non-linearity. For clarity, we will drop the superscripts q, k, v, oin the rest of the paper. 2.2 Polytropon: Adapter Routing Typical adapter methods either fully share adapters across tasks or train individual adapters for each task. Poly addresses the multi-task problem by softly sharing adapter parameters across tasks. Each Poly layer contains 1) an inventory of adapter modules M = {ϕ1, . . . , ϕm} with |M| ≪ |T |; 2) a routing function r(·) that chooses which subset of the modules to combine for each task. Each module corresponds to a LoRA adapter, where ϕi are its associated parameters A(i), B(i) ∈ Rd×r. r(·) is implemented as a task–module routing matrix Z ∈ R|T |×|M|. zτ = Zτ,: ∈ R|M| is a routing vector of task Tτ , with cell Zτ,j being the probability logits of using module ϕj for task Tτ in the current layer. Differently from mixture-of-experts [Fedus et al., 2022], which perform 3Method Pre-Training Fine-Tuning Inference Full FT d × d d × d d × d LoRA d × 2r d × 2r d × 2r Poly d × 2r × |M|+ |T | × |M| d × 2r × |M|+ |M| d × 2r Poly-z d × 2r × |M|+ |T | × |M| |M| |M| MHR-µ d × 2r × |M|+ |T | × |M| d × 2r d × 2r MHR-z d × 2r × |M|+ |T | × |M| ×h |M| ×h |M| ×h MHR d × 2r × |M|+ |T | × |M| ×h d × 2r × |M|+ |M| ×h d × 2r Table 1: Number of parameters (per layer) used for each method. The calculation uses LoRA as the base adapter, modifying a linear transform in Rd×d. Note that the total number of parameters changed by Full FT is larger, given that the method also changes parameters for layers not modified by LoRA. token-level top-k routing, Z converges to a binary matrix, defining a soft partition over modules. This is achieved by using a Gumbel-sigmoid distribution [Jang et al., 2017] during training, with ˆZτ,j ∼ Gumbel(Zτ,j ). At each forward pass, Poly can be defined as : Aτ = X i αiA(i); Bτ = X i αiB(i) (Poly) where αi = ˆZτ,iP j ˆZτ,j , and A(i), B(i), Aτ , Bτ ∈ Rd×r. We normalize the mixing coefficients ˆZτ,i for each task to ensure that the number of active modules does not affect the norm ofAτ , Bτ . Overall, this approach enables different subsets of modules to be activated for the current layer and combined in a task-specific way. Following LoRA, the output of the Poly layer is added to the output of the original layer of the frozen backbone: h = W0x + sAτ (Bτ )⊤x. During multi-task pre-training, for each query, key, value, and output projection in self-attention layers, the parameters learned by Poly are the adapter parameters, {Ai, Bi}|M| i=1 , and the routing matrices Z. During fine-tuning, for each test task τ, Poly randomly initialize the routing vector zτ ∈ R1×|M| and fine-tunes both zτ and all the modules parameters M. 3 Multi-Head Adapter Routing ( MHR) In Poly, module combination remains coarse: only linear combinations of modules are possible, and thus the resulting aggregated adapter remains a linear function of the modules. We propose to augment the expressivity of the module combination while keeping the parameter count similar. MHR (Fig. 1) takes inspiration from multi-head attention [Vaswani et al., 2017]: it partitions the input dimensions into h different disjoint blocks, performs a separate Poly-style combination for each of them, and finally concatenates them. This corresponds to learning a different routing matrix Z for each block of input features, therefore enabling the model to select different adapters for different blocks of the input dimensions. This aggregation approach is piecewise linear (i.e., linear within disjoint intervals), which allows for more expressive combinations of modules. In each MHR layer, the routing function is a third-order tensor Z ∈ R|T |×|M|×h, where Z:,:,h ∈ R|T |×|M|is a 2D slice of the tensor Z. A slice represents the routing matrix for each of the h heads. Let us denote with W[k] ∈ R d h ×r the k-th partition along the rows of the matrix W ∈ Rd×r. The adapter parameters Aτ ∈ Rd×r for task τ, and for each adapter layer, are computed as (similarly for Bτ ): Aτ k = X j Aj[k] · ˆZτ,j,k P j ˆZτ,j,k with Aτ k ∈ R d h ×r, (MHR) Aτ = concat(Aτ 1, . . . ,Aτ h) where concat concatenates along the first dimension. Multi-task pre-training and fine-tuning are similar to Poly. Note that MHR results in only a negligible increase in the total amount of parameters, since most of the parameters are contained in the LoRA weights A, B (Tab. 1). 4Routing-Only Fine-Tuning (MHR-z) Prior work [Shao et al., 2023, inter alia] has shown that compositional generalization can be achieved by learning to (re-)combine in novel ways pre-existing modules. We investigate whether fine-tuning the module parameters is really needed for few-shot adaptation in the context of both Poly and MHR. Therefore, we name Poly-z and MHR-z the variants that, during few-shot adaptation, keep the parameters of the modules learned during multi-task pre-training fixed and just update the routing parameters Z. Crucially, this enables highly parameter- efficient adaptation: for LoRA adapters, A and B matrices constitute the overwhelming majority of parameters. Therefore, by freezing the A, B matrices and only updating Z, we can significantly reduce the parameter cost when transferring knowledge to a new task. Adapter Average Fine-Tuning (MHR-µ) To assess the importance of the routing parameters during few-shot adaptation, we propose an additional variant of MHR, MHR-µ, which shares the same multi- task pre-training procedure as MHR, but for each test task τ, fixes zτ = (1/|M|, . . . ,1/|M|) during few-shot adaptation. This is equivalent to discarding the routing parameters and averaging the module parameters into a single one before fine-tuning. Specifically, the adapter used during fine-tuning is initialized with (similarly for Bτ ): Aτ = 1 |M| X i A∗ i ; Aτ ∈ Rd×r (MHR-µ) where A∗ i are the parameters of the adapters after MHR multi-task pre-training. Note that, differently from MHR, MHR-µ fine-tunes the same amount of parameters as the single adapter baseline. Thus, any difference in performance between the single adapter baseline and MHR-µ comes from differences in the adapter initialization and must be due to the optimization process taking place in the multi-task pre-training, before few-shot adaptation. Routing Granularity In the original Poly, Ponti et al. [2023] showed that learning a routing matrix Z for each model layer gave better performance than sharing a single Z matrix across all layers. We therefore investigate whether this holds true also for its multi-head counterpart, MHR. In addition, we explore intermediate approaches between one Z per layer and a single one shared for the entire model. In particular, we consider sharing Z 1) for the adapter layers belonging to the same Transformer block; or 2) for every block of l layers, which enables us to easily trade off expressivity for parameter efficiency. As we will demonstrate in section 5.1, this is an efficient mechanism to navigate this Pareto front in regimes of very small budgets of parameters per task. 4 Experiments Our experimental evaluation aims to answer three research questions: 1) Does the expressivity of the routing function matter? 2) Why do routing-based PEFT methods yield superior performance? 3) Is routing useful during both multi-task pre-training and few-shot adaptation? We first present the baselines and datasets used in our evaluation and then discuss each question in turn.1 4.1 Baselines In addition to Poly, we compare MHR to the following baselines for task-level generalization. LoRA/(IA)3 trains a single adapter common to all pre-training tasks, which is then fine-tuned on each test task separately. This is arguably the most widespread approach for parameter-efficient cross-task generalization [Liu et al., 2022, Pfeiffer et al., 2023]. AdapterSoup Chronopoulou et al. [2023] trains a different adapter for each task. The method only averages the adapter weights of the training tasks most similar to a given test task, before proceeding with few-shot adaptation. To compute task relatedness, we measure the cosine similarity of sentence embeddings for each task averaged over their training dataset. Notably, unlike the methods proposed in this paper, there is no knowledge sharing (nor interference) during multi-task pre-training as task adapters are trained independently. 1We note that all experiments were run on a single NVIDIA A100 GPU. 5T0 Dataset Avg. Test Backbone T5-XL-LM (IA)3 62.40.4 AdapterSoup 62.11.0 LoRA 66.01.6 LoRA-big 65.40.9 Poly-z 66.40.3 Poly 68.01.0 MHR-z 68.30.8 MHR 69.11.0 Backbone T0-3B T-Few Liu et al. [2022] 66.2 0.5 AdapterSoup 66.10.6 LoRA 67.40.8 Poly-z 65.31.0 Poly 69.00.8 MHR-z 68.41.2 MHR 69.31.2 IA3 AdapterSoup LoRA LoRA-big Poly-z Poly MHR 5K 500K 5M 50M 62 63 64 65 66 67 68 69 Adaptation Parameters Accuracy SOTA results MHR-z Figure 2: Left: Results of few-shot adaptation on T0 dataset Sanh et al. [2022]. We report the mean of the best validation accuracy for each test task. Subscripts correspond to standard deviation. Right: Accuracy of PEFT methods on the T0 dataset when applied on top of T5-XL. The x-axis shows the parameter count during the fine-tuning process. 4.2 Datasets We test our methods on the T0 Sanh et al. [2022] evaluation suite, following the same setup as Liu et al. [2022], and SuperNI Wang et al. [2022a], a large-scale dataset with more than 1,600 training tasks. T0 Tasks We follow the pre-training and fine-tuning procedure discussed in Liu et al. [2022], using hyper-parameters and losses suggested in the public codebase for T-Few.2 All methods were tested with T5-XL Raffel et al. [2020] and T0-3B Sanh et al. [2022] as the backbone model. Crucially, T5 is simply pre-trained on (masked) language modelling, whereas T0 is further instruction tuned: in particular, the full model is fine-tuned on examples from multiple training tasks that have been augmented with task instructions. To ensure fairness for all methods, we report the median and standard deviation of the best validation accuracy for each test task across 3 seeds, when evaluated every 50 training epochs. We treat each data subset–template pair as a unique task, yielding a total of 313 tasks. SuperNI To limit computational costs, we report the result on 20 out of 119 test tasks. Tasks were chosen at random, with the requirement that at least 300 examples were available, and were equally split into 100 training, 100 validation and 100 test examples. For every method, we perform early stopping on the validation set. We report results with Rouge-L averaged across 3 seeds. All methods use T5-XL [Raffel et al., 2020] as the backbone and not T0, as T0 training tasks and SuperNI test tasks may overlap. 5 Results and Discussion 5.1 Does the expressivity of the routing function matter? MHR outperforms PEFT approaches We start our analysis by evaluating the effectiveness of our proposed technique when applied over a backbone that has not undergone prior training on 2https://github.com/r-three/t-few 6instruction-following data (T5-XL). As indicated in the T0 benchmark results in the top table of Fig. 2, it is clear that multi-head routing techniques have a distinct advantage, outperforming both single-head routing Poly by 1.1%, and surpassing standard LoRA approaches by an impressive 3.1%. We also study the impact of performing instruction tuning of the full backbone before adapter training. To this end, we also experiment with T0-3B as a backbone. In the bottom table of Fig. 2, we can observe that while the relative gap between MHR and baselines is smaller, multi-head routing still manages to yield favourable results. Hence, the gains of MHR compound with other multi-task methods such as instruction tuning. Finally, we turn our attention towards the SuperNI dataset (Tab. 2). Here, MHR continues to surpass analogous baselines. MHR-z facilitates extreme parameter efficiency Fig. 2 (right) reveals intriguing findings regarding MHR-z. When we restrict training to only the routing parameters Z in the original Poly, the results are unfortunately not up to par with its version where both routing and adapters are updated. However, when we apply the same constraint to MHR, the performance is significantly closer to the optimum achieved in this setting. In fact, MHR-z surpasses prior baselines while simultaneously necessitating fewer parameters for effective adaptation to new tasks. Moreover, by controlling the number of layers which share the same Z allocation (see sec. 3), MHR-z is able to trade-off performance for parameter efficiency, even surpassingPoly-z in settings with only 3K trainable parameters per test task (see also § A.2.1 for a more in-depth analysis). This trend is similarly observed in the SuperNI benchmark (Tab. 2), where updates restricted to the routing parameters yield performance on par with standard fine-tuning. We therefore conclude that the MHR-z represents a robust approach for achieving extreme parameter efficiency in adaptation. SuperNI Dataset Rouge-L LoRA 67.60.8 LoRA-big 67.20.7 Poly-z 64.60.3 Poly 67.80.8 MHR-z 68.00.2 MHR 68.50.3 Table 2: Results on SuperNI dataset. Sub- scripts are standard deviation. Additional routing heads is more beneficial than extra modules In the original Poly ap- proach, a tradeoff between capacity and param- eter efficiency can be achieved by adding ex- tra modules for each adapter layer. However, this results in a linear increase in the number of multi-task parameters, which can become im- practical. To explore a more effective tradeoff, we investigate the option of adding additional routing heads instead of extra modules. Fig 3 (right) presents the comparison between the two approaches. It demonstrates that increasing the number of routing heads leads to better perfor- mance compared to adding more modules. Importantly, the benefit of multi-head routing is twofold: it provides increased expressivity for the model, while also maintaining parameter efficiency. This finding highlights the advantage of multi-head routing as a more effective approach for balancing expressivity and parameter count in few-shot adaptation scenarios. T0 Dataset Avg. Test Backbone T0-11B T-Few Liu et al. [2022] 72.5 0.9 LoRA 72.31.0 Poly-z 70.00.6 Poly 74.90.6 MHR-z 72.90.8 MHR 74.70.6 Table 3: Few-shot results over 11B pa- rameter backbones. Routing-based methods also excel at the 11B scale We proceed to evaluate if Poly and MHR surpass established PEFT approaches when trained over a larger model back- bone. To accomplish this, we employ the 11B version of T0. As depicted in Tab. 3, routing-based methods once again outshine standard adapter training, surpassing our reproduction of the previous state-of-the-art in Liu et al. [2022] by over 2%. We observe that Poly and MHR show similar performance in standard fine-tuning, but MHR z-tuning remains more performant in routing-only fine- tuning. Indeed, MHR-z (221K params) outperforms Poly-z (3.5K params) by 2.9%, while still remaining more param- eter efficient than Liu et al. [2022] (1.1M params). 5.2 Why do routing-based PEFT methods yield superior performance? While our proposed methods have demonstrated promising results across a broad spectrum of datasets and varying adaptation parameter budgets, the question of why routing-based PEFT exhibits superior 7102 103 104 105 Training Steps 0 2 4 6 8 10 12 14Cumulative Cosine Similarity Cumulative Gradient Cosine Similarity  MHR Poly LoRA 20 40 60 80 100 120 140 Multi-T ask pre-training trainable params (M) 67.8 68.0 68.2 68.4 68.6 68.8 69.0T0 T est Accuracy 8 modules 32 modules 64 modules 8 heads 32 heads 64 heads 128 heads T est Performance w.r.t Trainable Parameter Budget MHR Poly Figure 3: Left: Gradient alignment between tasks during multi-task pretraining. Right: Increasing the number of heads offer better scaling properties than increasing the number of modules. performance remains unanswered. In this section, we aim to uncover the key components that drive MHR’s superior performance. Learning the Routing Function is essential Given that Poly and MHR have access to more parameters than standard adapters during multi-task pretraining, we investigate whether this, and not the routing mechanism, is responsible for their superior performance. To do so, we compare them to a baseline approach. Instead of learning the routing function, we randomly assign a binary module allocation to each data point in a minibatch, disregarding task information. This random routing approach, akin to Wang et al. [2022b], allows us to directly assess the influence of additional parameters during multi-task training. At test time, the learned modules are averaged into a single one before fine-tuning; we therefore refer to this baseline as Random-µ. On the T0 benchmark with the T5-XL backbone, Random-µ performs similarly to a standard LoRA adapter (66.0%), while Poly and MHR outperform it by 2% and 3.1% respectively. Therefore, we conclude that learning a routing function is crucial, and merely increasing capacity during training does not directly lead to improvements. MHR fosters transfer and mitigates interference across pretraining tasks Recognizing the pivotal role of the multi-task pretraining step in bolstering Poly’s performance, we explore the extent of transfer and interference across training tasks. By monitoring the average gradient alignment for each task pair (in terms of cosine similarity) throughout the training process, we are able to gauge the level of positive transfer. As Fig. 3 (left) shows, MHR displays a greater degree of gradient cosine similarity across tasks compared to other PEFT alternatives, including Poly. This finding suggests that the enhanced flexibility offered by multi-head routing may serve to mitigate interference across tasks to a larger extent than standard routing while simultaneously promoting positive transfer. 5.3 Is routing important for task generalization? T0 Dataset Test Acc. LoRA 66.01.6 AdapterSoup 62.11.0 Poly 68.00.8 Poly-µ 67.80.6 MHR 69.11.1 MHR-µ 69.10.9 SuperNI Rouge-L LoRA 67.60.8 Poly 67.80.8 Poly-µ 68.30.5 MHR 68.50.6 MHR-µ 68.50.8 Table 4: Evaluating the impact of modular adaptation at test time. We assessed the importance of routing during pre- training. We now proceed to verify whether it is important to learn routing during few-shot adapta- tion, too. Poly-µ and MHR-µ consistently outper- form LoRA, and match the performance ofPoly / MHR (Tab. 4). This demonstrates that, for few-shot adap- tation, the average of the pre-trained modules pro- vides a better initialization than learning an adapter shared across all the tasks during pre-training. The consistently superior performance of Poly-µ with respect to Random-µ and AdapterSoup stresses the importance of routing during multi-task pre-training (but not during adaptation), which provides an effec- tive adapter initialization for few-shot learning. This 8Method Zero-Shot Test with k-shot Extra Training k = 0 k = 1000 k = 5000 k = 10000 Backbone T5-XL-LM 43.2 LoRA 56.5 56.0 56.1 55.7 Poly-µ 46.0 53.0 56.8 56.3 MHR-µ 48.0 58.0 57.1 56.3 Backbone T0-11B [Sanh et al., 2022] 61.0 LoRA 61.2 61.6 61.5 61.5 Poly-µ 62.1 63.6 63.9 64.4 MHR-µ 63.5 64.5 64.5 64.4 Table 5: Zero-shot performance for MHR and the baselines, reported as the average over the 11 evaluation datasets from Sanh et al. [2022]. To obtain these zero-shot results, we average the learnt Poly/MHR adapters before performing k additional fine-tuning steps on the multi-task pretraining data. This effectively enables zero-shot transfer to downstream tasks using the same amount of parameters/flops as the baseline LoRA. MHR outperform baseline LoRA by up to 3% absolute accuracy points on T0-11B. finding could potentially inspire future work for improving meta-learning and weight-averaging approaches [Izmailov et al., 2018]. MHR-µ excels at zero-shot learning For many downstream tasks of interest, additional labelled data may not be available. In such settings, it is unclear how to leverage MHR-µ and Poly-µ methods. To address this, we fine-tune the average of the multi-task trained adapters on the multi-task pre- training data (instead of using the downstream few-shot data), for an additional k steps. The results are presented in Table 5. We find that without any additional fine-tuning ( k = 0), averaging the adapters does not yield good results. This is due to a potential mismatch between adapters learned via task-specific routing, and the uniform routing strategy. We can observe that when fine-tuning the average of the adapters on the multi-task pre-training data for an additional k steps, MHR-µ show strong performance when evaluated in a zero-shot manner. For a fair comparison, we also additionally fine-tune LoRA for the same number of additional steps. Our best model achieves a zero-shot performance of 64.5 on top of T0-11B, achieving an absolute gain of 3.5% accuracy points. 6 Related Work Multi-task learning is effective for low-resource tasks [Wei et al., 2022, Aribandi et al., 2022, Sanh et al., 2022], as knowledge can be borrowed from similar tasks by sharing the model parameters. Multi- task learning has also been applied across languages and modalities [Ponti et al., 2019, Bugliarello et al., 2022]. In the context of NLP, several families of methods enable learning new tasks from a limited set of labelled examples. Few-shot in-context learning [ICL; Brown et al., 2020], where examples of a new task are concatenated into an input prompt, enables models to generalize tounseen tasks without any gradient-based training. Such approaches are however sensitive to the prompt format and example ordering [Zhao et al., 2021]. More importantly, ICL methods incur a significant compute overhead, as for every prediction, the full set of examples must be processed by the model [Liu et al., 2022]. To remedy this, many parameter-efficient fine-tuning (PEFT) methods have been proposed as an alternative to ICL, where a small number of new parameters are added over the frozen pretrained network. To name a few, LoRA [Hu et al., 2022] injects learnable low-rank matrices into each Transformer layer. Alternatively, the learnable matrix can be sparse, selecting nonzero shifts via the Lottery-Ticket hypothesis [Ansell et al., 2021] or via their approximate Fisher information [Sung et al., 2021]. Finally, prefix-tuning methods [Li and Liang, 2021] prepend learnable embeddings to the input or intermediate representations to specialize the model towards a downstream task. Modular networks partition their parameters into several expert modules, each of them specialized to handle specific sub-tasks [Jacobs et al., 1991, Kirsch et al., 2018]. Modular networks are an appealing solution to the problem of adapting to unseen tasks [Corona et al., 2021], as the model 9can leverage its existing modules and recombine them in a novel way, thus achieving systematic generalization [Bahdanau et al., 2019]. They have also been tested in learning scenarios with data presented sequentially [Ostapenko et al., 2021], and with changing environments Goyal et al. [2021]. In NLP, mixture-of-experts (MoE) models [Shazeer et al., 2017, Fedus et al., 2022], where a learned gating mechanism routes token representations to appropriate experts (Feed-Forward layers), have shown success in scaling the number of parameters while retaining time efficiency. This results in higher performance when compared to their dense counterparts using a similar compute budget. 7 Conclusions In this paper, we tackle the challenge of generalizing to new tasks based on a few examples after multi- task pre-training. Specifically, we focus on Polytropon [Ponti et al., 2023], a model where each task is associated with a subset of adapters by a routing function. We investigate how varying the level of control afforded by the routing function impacts performance on two comprehensive benchmarks for multi-task learning, T0 and Super-Natural Instructions. First, a newly proposed variant of the routing function, where multiple heads are responsible for different blocks of input dimensions, improves consistently over all other baselines, including LoRA and (IA)3 adapters. Second, we identify the cause of the success of routing in its ability to prevent interference between tasks, as it yields a better alignment between their gradients. Third, we find that simple averaging of all multi-task pre-trained adapters before few-shot adaptation to new tasks provides comparable performance, thus offering state-of-the-art performance for single-adapter few-shot learning. Multi-head routing demonstrates the importance of fine-grained adapter selection for sample-efficient generalization and holds promise to improve other modular methods, such as Mixtures of Experts [MoEs; Fedus et al., 2022] in future research. References A. Ansell, E. M. Ponti, J. Pfeiffer, S. Ruder, G. Glavaš, I. Vuli ´c, and A. Korhonen. MAD-G: Multilingual adapter generation for efficient cross-lingual transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4762–4781, Nov. 2021. doi: 10.18653/v1/ 2021.findings-emnlp.410. URL https://aclanthology.org/2021.findings-emnlp.410. A. Ansell, E. M. Ponti, A. Korhonen, and I. Vuli ´c. Composable sparse fine-tuning for cross- lingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022. URL https://arxiv.org/pdf/2110.07560.pdf. V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta, H. Zhuang, V . Q. Tran, D. Bahri, J. Ni, J. Gupta, K. Hui, S. Ruder, and D. Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=Vzh1BFUCiIX. A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6655–6672, 2022. D. Bahdanau, S. Murty, M. Noukhovitch, T. H. Nguyen, H. de Vries, and A. Courville. Systematic generalization: What is required and can it be learned? In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HkezXnA9YX. S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. E. Bugliarello, F. Liu, J. Pfeiffer, S. Reddy, D. Elliott, E. M. Ponti, and I. Vuli´c. IGLUE: A benchmark for transfer learning across modalities, tasks, and languages. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2370–2392. 10PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/bugliarello22a. html. A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models. arXiv preprint arXiv:2302.07027, 2023. R. Corona, D. Fried, C. Devin, D. Klein, and T. Darrell. Modular networks for compositional instruction following. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1033–1040, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 81. URL https://aclanthology.org/2021.naacl-main.81. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022. URL http://jmlr.org/papers/v23/21-0998.html. A. Goyal, A. Lamb, J. Hoffmann, S. Sodhani, S. Levine, Y . Bengio, and B. Schölkopf. Recurrent independent mechanisms. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=mLcmdlEUxy-. E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. CoRR, abs/1803.05407, 2018. URL http: //arxiv.org/abs/1803.05407. R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79–87, 1991. URL https://watermark.silverchair.com/neco. 1991.3.1.79.pdf. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-softmax. In International Conference on Learning Representations , 2017. URL https://openreview.net/pdf?id= rkE3y85ee. R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for Transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 565–576, Aug. 2021. URL https://aclanthology.org/ 2021.acl-long.47. L. Kirsch, J. Kunze, and D. Barber. Modular networks: Learning to decompose neural computation. Advances in Neural Information Processing Systems, 31, 2018. URL https://proceedings. neurips.cc/paper/2018/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf. X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Pro- ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pages 4582–4597, Online, Aug. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. Raffel. Few-shot parameter- efficient fine-tuning is better and cheaper than in-context learning, 2022. URL https://arxiv. org/abs/2205.05638. O. Ostapenko, P. Rodriguez, M. Caccia, and L. Charlin. Continual learning via local module composition. Advances in Neural Information Processing Sys- tems, 34, 2021. URL https://proceedings.neurips.cc/paper/2021/file/ fe5e7cb609bdbe6d62449d61849c38b0-Paper.pdf. 11J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics , pages 487–503, Apr. 2021. URL https://aclanthology.org/2021.eacl-main.39. J. Pfeiffer, S. Ruder, I. Vuli ´c, and E. M. Ponti. Modular deep learning. arXiv preprint arXiv:2302.11529, 2023. URL https://arxiv.org/pdf/2302.11529.pdf. E. Ponti. Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning . PhD thesis, University of Cambridge, 2021. URL https://aspace.repository.cam.ac.uk/ bitstream/handle/1810/319303/thesis_electronic.pdf. E. M. Ponti, H. O’Horan, Y . Berzak, I. Vuli ´c, R. Reichart, T. Poibeau, E. Shutova, and A. Ko- rhonen. Modeling language variation and universals: A survey on typological linguistics for natural language processing. Computational Linguistics , 45(3):559–601, 2019. URL https://watermark.silverchair.com/coli_a_00357.pdf. E. M. Ponti, A. Sordoni, Y . Bengio, and S. Reddy. Combining parameter-efficient modules for task- level generalisation. In Proceedings of the 17th Conference of the European Chapter of the Associ- ation for Computational Linguistics, pages 687–702, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.49. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, P. J. Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. C. Rosenbaum, I. Cases, M. Riemer, and T. Klinger. Routing networks and the challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774, 2019. URL https: //arxiv.org/pdf/1904.12774.pdf. V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V . Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Févry, J. A. Fries, R. Teehan, S. Biderman, L. Gao, T. Bers, T. Wolf, and A. M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, 2022. URL https://arxiv.org/pdf/2110.08207.pdf. N. Shao, Z. Cai, H. xu, C. Liao, Y . Zheng, and Z. Yang. Compositional task representations for large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=6axIMJA7ME3. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/pdf?id=B1ckMDqlg. Y .-L. Sung, V . Nair, and C. Raffel. Training neural networks with fixed sparse masks. In A. Beygelz- imer, Y . Dauphin, P. Liang, and J. W. Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=Uwh-v1HSw-x. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706. 03762. T. Vu, T. Wang, T. Munkhdalai, A. Sordoni, A. Trischler, A. Mattarella-Micke, S. Maji, and M. Iyyer. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7882–7926, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.635. URL https://aclanthology.org/2020.emnlp-main.635. T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. 12Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, E. Pathak, G. Karamanolakis, H. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, M. Patel, K. K. Pal, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. K. Sampat, S. Doshi, S. Mishra, S. Reddy, S. Patro, T. Dixit, X. Shen, C. Baral, Y . Choi, N. A. Smith, H. Hajishirzi, and D. Khashabi. Benchmarking generalization via in-context instructions on 1,600+ language tasks, 2022a. URL https://arxiv.org/abs/2204.07705. Y . Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, and J. Gao. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprint arXiv:2205.12410, 2022b. Z. Wang, Y . Tsvetkov, O. Firat, and Y . Cao. Gradient vaccine: Investigating and improving multi- task optimization in massively multilingual models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=F1vEjWK-lH_. J. Wei, M. Bosma, V . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. Q. Ye, B. Y . Lin, and X. Ren. CrossFit: A few-shot learning challenge for cross-task generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163–7189, Nov. 2021. doi: 10.18653/v1/2021.emnlp-main.572. URL https: //aclanthology.org/2021.emnlp-main.572. Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of language models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697–12706. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr. press/v139/zhao21c.html. 13A Appendix A.1 Additional Results More detailed numbers on the T0 Sanh et al. [2022] and SuperNI Wang et al. [2022a] datasets using different backbones, and different adapter layouts over the base model are found in Table 6. Multi-Task params is the number of additional parameters that must be conserved after Model Multi-Task Params Adaptation Params Avg. Test T0 Dataset Backbone T5-XL-LM Multi-Task Full Finetuning + LoRA 2.8B 2.2M 68.9 x.x (IA)3 540K 540K 62.4 0.4 AdapterSoup 84M 2.2M 62.1 1.0 LoRA 2.2M 2.2M 66.0 1.6 LoRA-big 35M 35M 65.4 0.9 Poly-z 17M 3.5K 66.4 0.3 Poly 17M 2.2M 68.0 1.0 MHR-z (64 h) 17M 220K 68.30.8 MHR (64 h) 17M 2.2M 69.11.0 Backbone T0-3B T-Few Liu et al. [2022] 540K 540K 66.2 0.5 AdapterSoup 84M 2.2M 66.1 0.6 LoRA 2.2M 2.2M 67.4 0.8 LoRA-big 35M 35M 68.0 0.8 Poly-z 17M 3.5K 65.3 1.0 Poly 17M 2.2M 69.0 0.8 MHRz (64 h) 17M 220K 68.41.2 MHR (8 h) 17M 2.2M 69.31.2 Backbone T0-3B light version : (k, v, ff layers only) l-LoRA (rank 1) 934K 934K 66.2 0.9 l-LoRA (rank 16) 15M 15M 67.6 1.1 AdapterSoup (l-LoRA) 35M 934K 64.9 1.0 l-Poly-z 7.5M 2.1K 62.9 1.2 l-Poly 7.5M 934K 68.0 0.5 l-MHRz (32 h) 7.5M 74K 66.81.1 l-MHR (8 h) 7.5M 934K 68.50.7 SuperNI Dataset Rouge-L Backbone T5-XL-LM light version : (k, v, ff layers only) l-LoRA 934K 934K 67.6 0.8 l-LoRA-big 18M 18M 67.2 0.7 l-Poly-z 7.5M 2.1K 64.6 0.3 l-Poly 7.5M 934K 67.8 0.8 l-MHRz (64 h) 7.5M 147K 68.00.2 l-MHR (8 h) 7.5M 934K 68.50.3 Table 6: (top) Results on T0 dataset Sanh et al. [2022], we report the mean of the best validation accuracy for each test task, when evaluated every 50 train epochs. T-Few is our reproduction of the results in Liu et al. [2022]. LoRA-big means a LoRA adapter with a larger rank. (bottom) Results on SuperNatural Instructions dataset. multi-task pretraining to enable transfer to a downstream task. Adaptation Params refer to the number of parameters required to learn a new downstream task. For e.g.Poly and MHR, the multi-task parameters includes the learned modules, but not the routing over the training tasks, as these are not required for transfer on a new task. Moreover, variants which average the learned modules prior to fine-tuning (MHR-µ and Poly-µ) will have both multi-task and adaptation parameters equal to that of a single shared adapter, since after multi-task pretraining one can average the modules. 14A.2 Navigating the parameter efficiency / performance trade-off of tuning only the routing Here we provide additional results on how different routing based methods can be more expressive when only learning a new routing function (over frozen modules) to adapt to a new task. Head # 2 Head # 1 Tasks Active Task Modules Tasks Active Task Modules Feed-Forward  Add & Norm Multi-Head  Attention Add & Norm W0 W1 V O Q K Tasks Modules Polytropon Multi-Head Routing (MHR) Sharing Z allocation in MHR  Figure 4: Different ways to control the expressivity of routing based methods. Left : In Polytropon, one can only add additional modules, resulting in a linear parameter increase. Right : In MHR, additional heads only introduce routing matrices Z, resulting in a negligible parameter increase. In Fig. 4 (left), we see that in order to build more expressive routing functions Z, in Poly one can only do so by increasing the number of skills at each layer. However, this has a significant impact on the number of multi-task parameters which much be kept in order to perform few-shot transfer. MHR on the other hand, can increase routing capacity in a much more parameter efficient way. A.2.1 On the granularity of routing tensor in MHR Here we provide additional results when modifying the granularity of Z for MHR. We see that one can easily trade-off more parameters for better performance. 103 104 105 T est-task param count 65.5 66.0 66.5 67.0 67.5 68.0 68.5T0 T est Accuracy Performance when tuning only Z w.r.t T est Parameter Budget MHR Poly Figure 5: Routing-Only Fine-Tuning (MHR-z) 15B Broader Impact In our work, we focus on advancing parameter-efficient fine-tuning methods for cross-task general- ization. While our research primarily addresses technical challenges and performance improvements, when applying such methods, it is crucial to consider the potential negative societal impacts. Specif- ically, we believe that prior to applying our proposed adaptation method, critically examining the potential biases and ethical implications of the underlying large language model, and the data itself must be properly addressed. This includes issues related to fairness, privacy, and the spread of misinformation. 16",
      "meta_data": {
        "arxiv_id": "2211.03831v3",
        "authors": [
          "Lucas Caccia",
          "Edoardo Ponti",
          "Zhan Su",
          "Matheus Pereira",
          "Nicolas Le Roux",
          "Alessandro Sordoni"
        ],
        "published_date": "2022-11-07T19:35:55Z",
        "pdf_url": "https://arxiv.org/pdf/2211.03831v3.pdf",
        "github_url": "https://github.com/microsoft/mttl"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Multi-Head Routing (MHR) for parameter-efficient fine-tuning (PEFT) to achieve cross-task generalization, building upon the Polytropon (Poly) framework. MHR enhances the expressivity of adapter routing by partitioning adapter dimensions into multiple 'heads' and performing separate module combinations for each, outperforming Poly. It also proposes MHR-z, a highly parameter-efficient variant that fine-tunes only the routing function during adaptation. The research reveals that the success of routing-based PEFT methods like Poly and MHR primarily stems from better multi-task optimization during pre-training, leading to higher gradient alignment between tasks, rather than modular inductive biases facilitating recombination during adaptation. Furthermore, it demonstrates MHR-µ, which fine-tunes the average of pre-trained adapters, achieves competitive single-adapter performance and can be effectively used for zero-shot transfer with additional pre-training data fine-tuning.",
        "methodology": "The core methodology extends Polytropon (Poly), which learns an inventory of LoRA adapters (modules) and a routing function (a task–module routing matrix Z) to select and combine modules for each task. MHR refines this by partitioning the adapter parameters (A, B matrices) into 'h' disjoint blocks (heads). For each head, a separate routing function selects and combines active modules, resulting in a third-order routing tensor Z. This allows for more granular and piecewise linear module combinations. MHR-z is a variant where, during few-shot adaptation, only the routing parameters Z are updated, keeping the adapter modules fixed. MHR-µ simplifies adaptation by discarding routing and averaging all pre-trained adapter modules into a single one before fine-tuning. For zero-shot transfer, MHR-µ is further fine-tuned on the multi-task pre-training data. Tasks are formulated as text-to-text problems, optimized with maximum-likelihood training and cross-entropy loss.",
        "experimental_setup": "The methods were evaluated for few-shot task adaptation on two comprehensive benchmarks: the T0 task suite and the Super-Natural Instructions (SuperNI) dataset. Backbone models used include T5-XL, T0-3B, and T0-11B. T0 consisted of 313 unique tasks (data subset–template pairs), where the mean of the best validation accuracy across 3 seeds was reported. SuperNI involved 20 randomly selected test tasks (out of 119) with 100 training, 100 validation, and 100 test examples, using Rouge-L averaged across 3 seeds. Baselines included LoRA, (IA)3, Poly, Poly-z, AdapterSoup, T-Few (reproduction of Liu et al. [2022]), and a random routing baseline (Random-µ). All experiments were conducted on a single NVIDIA A100 GPU.",
        "limitations": "The paper does not explicitly list limitations of MHR. However, it implicitly points out that for zero-shot learning, MHR-µ does not perform well without additional fine-tuning on the multi-task pre-training data (k=0 case), indicating a potential mismatch between adapters learned via task-specific routing and a uniform routing strategy when no adaptation occurs. The computational cost for the SuperNI dataset was limited by evaluating only 20 out of 119 test tasks, which is a practical constraint on the experiments. The Broader Impact section also highlights general ethical concerns regarding potential biases in underlying large language models and training data, which are not specific to the proposed method but are relevant for its application.",
        "future_research_directions": "The authors suggest that multi-head routing holds promise for improving other modular methods, specifically mentioning Mixtures of Experts (MoEs). Additionally, the finding that averaging multi-task pre-trained adapters can serve as an effective initialization for single-adapter few-shot learning (MHR-µ's success) could inspire future work in meta-learning and weight-averaging approaches.",
        "experimental_code": "mttl/models/modifiers/lora.py: @dataclassclass SkilledLoRAConfig(LoRAConfig):n_skills: int = 1n_splits: int = 1mttl/models/modifiers/lora.py: @Modifier.register(\"skilled_lora\", config_cls=SkilledLoRAConfig)class SkilledLoRA(LoRA):def __init__(self, config: SkilledLoRAConfig, layer: nn.Module, **kwargs):self.n_splits = config.n_splitsself.n_skills = config.n_skillssuper().__init__(config, layer)# ...@classmethoddef parallel_linear_weighted_forward(cls,input: torch.Tensor,skilled_loras: List[\"SkilledLoRAView\"],weights: torch.Tensor,dim_names: List[str],merge_after: bool = False,):# ...if merge_after:partial_out = torch.einsum(\"bld,beqdr->bleqr\", input_lora, skilled_loras_a)adapter_out = torch.einsum(\"bleqr,berqd,blqe->blqd\", partial_out, skilled_loras_b, weights)adapter_out = adapter_out.flatten(2, 3)else:A = torch.einsum(\"blqe,beqdr->blqdr\", (weights, skilled_loras_a))B = torch.einsum(\"blqe,berqd->blrqd\", (weights, skilled_loras_b))batch_size, sequence_length, rank, n_splits, d_split = B.shapeA, B = A.flatten(2, 3), B.flatten(3, 4)partial_out = torch.einsum(\"bld,bldr->blr\", (input_lora, A))adapter_out = torch.einsum(\"blr,blrd->bld\", (partial_out, B))# ...mttl/models/containers/selectors/poly_selector.py: @dataclassclass PolySelectorConfig(SelectorConfig):n_splits: int = 1task_names: List[str] = Noneslow_unknown_tasks: bool = Falsemttl/models/containers/selectors/poly_selector.py: def _get_weights(self, task_names: List[str] = None) -> torch.Tensor:# ...module_logits = torch.sigmoid(self.module_logits[task_ids])module_logits = module_logits.view(module_logits.size(0), self.config.n_splits, self.n_experts)module_weights = module_logits / (module_logits.sum(dim=-1, keepdim=True) + EPS)return module_weightsmttl/models/lightning/expert_module.py: class MoEModule(LightningEfficientCheckpoint, LightningTrainingMixin):# ...def training_step(self, batch, _):output, context = self.forward(**batch, return_context=True)loss = output.loss# ...routing_gates = context[\"routing_gates\"]if routing_gates:# ... Mutual Information Loss calculation ...mi_loss = -entropy_of_avg + entropy_of_route# ...if self.hparams.moe_ent_reg > 0.0:total_loss += self.hparams.moe_ent_reg * mi_losselif self.hparams.moe_ent_free_bits > 0.0:normalized_entropy = entropy_of_route / math.log(self.moe_num_experts)total_loss += ((1.0 - normalized_entropy) >= self.hparams.moe_ent_free_bits) * -entropy_of_routemttl/arguments.py: @dataclassclass FinetuneConfig(MultiExpertConfig):finetune_type: str = None # [\"F\", \"A\", \"Z\", \"MuZ\", \"Poly\", \"PolyRand\"]# ...finetune_regime: str = None # polylib_full, lib_mu, polylib_selector# ...projects/modular_llm/finetune_experts.py: @register_finetune_func(\"polylib_selector\")def finetune_polylib_sel(args: FinetuneConfig, dm):args.trainable_param_names = \"|.*module_logits.*|.*selector.*\"assert args.router_selector is not Nonemodule = MoEModule(**vars(args), device_map=\"auto\")for n, p in module.named_parameters():if \"selector\" in n:assert p.requires_gradmodule.to(\"cuda\")return train_module(args, module, dm)mttl/models/library/library_transforms.py: @dataclassclass WeightedLinearMergeConfig(LibraryTransformConfig):weights: dict = Nonemttl/models/library/library_transforms.py: @LibraryTransform.register(\"weighted_linear_merge\", WeightedLinearMergeConfig)class WeightedLinearMerge(LibraryTransform):@torch.no_grad()def transform(self, library) -> Expert:# ... averages expert weights ...for k, v in base_expert.expert_weights.items():base_expert.expert_weights[k] /= len(experts)# ...return base_expertprojects/modular_llm/finetune_experts.py: @register_finetune_func(\"lib_mu\")def finetune_lib_mu(args: FinetuneConfig, dm):args.router_selector = Nonemean_expert: Expert = create_mean_expert(args)if args.finetune_task_name:mean_expert.name = args.finetune_task_namemodule = MultiExpertModel(**vars(args)).to(\"cuda\")module.add_expert_instance(mean_expert, is_default=True)return (train_module(args, module, dm),)projects/modular_llm/finetune_experts.py: def create_mean_expert(args: FinetuneConfig, library: ExpertLibrary = None) -> Expert:if library is None:library = args.library_idreturn WeightedLinearMerge(WeightedLinearMergeConfig()).transform(library)",
        "experimental_info": "MHR (Multi-Head Routing) Core Method:- **Adapter Parameter Partitioning**: Adapter parameters (A, B matrices) are partitioned into 'h' disjoint blocks (heads) through `SkilledLoRAConfig` (defining `n_skills` and `n_splits`) and implemented by the `SkilledLoRA` class, which customizes layer creation and skill management.- **Third-Order Routing Tensor (Z)**: A third-order routing tensor is implicitly formed. The `PolySelector` with its `n_splits` configuration (via `PolySelectorConfig`) directly controls this, shaping the `module_logits` (routing weights) for each task, split, and expert. The `SkilledLoRA.parallel_linear_weighted_forward` method uses these weights and dimension names (`dim_names`) to combine modules.- **Granular Module Combinations**: The `SkilledLoRA.parallel_linear_weighted_forward` method, using `torch.einsum`, allows for weighted combinations of adapter parameters across different splits (heads) and experts, enabling more granular and piecewise linear module combinations based on the routing `weights`.- **Optimization with Cross-Entropy Loss and Regularization**: Tasks are formulated as text-to-text problems. The optimization uses maximum-likelihood training with cross-entropy loss (standard in `LightningTrainingMixin`). The `MoEModule.training_step` specifically incorporates mutual information loss (`mi_loss`), calculated from `routing_gates` (derived from the routing tensor Z), and allows for entropy regularization (`moe_ent_reg` or `moe_ent_free_bits`) to control the routing distribution.MHR-z Variant (Few-Shot Adaptation - Routing Only):- **Targeted Parameter Update**: During few-shot adaptation, only the routing parameters `Z` are updated. This is configured by setting `args.trainable_param_names` to target only `module_logits` or `selector` parameters. Adapter modules are kept fixed (not trained).- **Fine-tuning Regime**: Implemented by the `finetune_polylib_selector` function in `projects/modular_llm/finetune_experts.py`. This function sets the `trainable_param_names` accordingly and utilizes `MoEModule` for training.MHR-µ Variant (Simplified Adaptation - Averaging Modules):- **Module Averaging**: Adaptation is simplified by discarding explicit routing and averaging all pre-trained adapter modules into a single one before fine-tuning. This averaging is performed using `LibraryTransform` implementations like `WeightedLinearMerge`, `TiesMerge`, or `WudiMerge`.- **Module Averaging Configuration**: `WeightedLinearMergeConfig` (and similar for `TiesMerge` and `WudiMerge`) defines how modules are averaged, including optional `weights` for non-uniform averaging. The `WeightedLinearMerge.transform` method exemplifies this averaging process.- **Fine-tuning Regime**: Implemented by the `finetune_lib_mu` function in `projects/modular_llm/finetune_experts.py`. This function first calls `create_mean_expert` to generate the averaged adapter and then fine-tunes a `MultiExpertModel` with this single, averaged expert."
      }
    },
    {
      "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
      "abstract": "Multi-Task Learning (MTL) networks have emerged as a promising method for\ntransferring learned knowledge across different tasks. However, MTL must deal\nwith challenges such as: overfitting to low resource tasks, catastrophic\nforgetting, and negative task transfer, or learning interference. Often, in\nNatural Language Processing (NLP), a separate model per task is needed to\nobtain the best performance. However, many fine-tuning approaches are both\nparameter inefficient, i.e., potentially involving one new model per task, and\nhighly susceptible to losing knowledge acquired during pretraining. We propose\na novel Transformer architecture consisting of a new conditional attention\nmechanism as well as a set of task-conditioned modules that facilitate weight\nsharing. Through this construction (a hypernetwork adapter), we achieve more\nefficient parameter sharing and mitigate forgetting by keeping half of the\nweights of a pretrained model fixed. We also use a new multi-task data sampling\nstrategy to mitigate the negative effects of data imbalance across tasks. Using\nthis approach, we are able to surpass single task fine-tuning methods while\nbeing parameter and data efficient (using around 66% of the data for weight\nupdates). Compared to other BERT Large methods on GLUE, our 8-task model\nsurpasses other Adapter methods by 2.8% and our 24-task model outperforms by\n0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger\nvariant of our single multi-task model approach performs competitively across\n26 NLP tasks and yields state-of-the-art results on a number of test and\ndevelopment sets. Our code is publicly available at\nhttps://github.com/CAMTL/CA-MTL.",
      "full_text": "Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI -TASK LEARNING : IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA Jonathan Pilault1∗, Amine El hattami1∗, Christopher Pal1,2,3 1Polytechnique Montreal & Mila, 2Element AI, 3Canada CIFAR AI Chair {jonathan.pilault,amine.elhattami,christopher.pal}@polymtl.ca ABSTRACT Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overﬁtting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best perfor- mance. However, many ﬁne-tuning approaches are both parameter inefﬁcient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efﬁcient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model ﬁxed. We also use a new multi-task data sam- pling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task ﬁne-tuning methods while being parameter and data efﬁcient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task ﬁne-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL. 1 I NTRODUCTION The introduction of deep, contextualized Masked Language Models (MLM) 1 trained on massive amounts of unlabeled data has led to signiﬁcant advances across many different Natural Language Processing (NLP) tasks (Peters et al., 2018; Liu et al., 2019a). Much of these recent advances can be attributed to the now well-known BERT approach (Devlin et al., 2018). Substantial improvements over previous state-of-the-art results on the GLUE benchmark (Wang et al., 2018) have been obtained by multiple groups using BERT models with task speciﬁc ﬁne-tuning. The “BERT-variant + ﬁne-tuning” formula has continued to improve over time with newer work constantly pushing the state-of-the-art forward on the GLUE benchmark. The use of a single neural architecture for multiple NLP tasks has shown promise long before the current wave of BERT inspired methods (Collobert & Weston, 2008) and recent work has argued that autoregressive language models (ARLMs) trained on large-scale datasets – such as the GPT family of models (Radford et al., 2018), are in practice multi-task learners (Brown et al., 2020). However, even with MLMs and ARLMs trained for multi-tasking, single task ﬁne-tuning is usually also employed to achieve state-of-the-art performance on speciﬁc tasks of interest. Typically this ﬁne-tuning process may entail: creating a task-speciﬁc ﬁne-tuned model (Devlin et al., 2018), training specialized model components for task-speciﬁc predictions (Houlsby et al., 2019) or ﬁne-tuning a single multi-task architecture (Liu et al., 2019b). ∗Joint ﬁrst-authors 1For reader convenience, all acronyms in this paper are summarized in section A.1 of the Appendix. 1 arXiv:2009.09139v3  [cs.LG]  21 Apr 2022Published as a conference paper at ICLR 2021 Figure 1: CA-MTL base architecture with our uncertainty-based sampling algorithm. Each task has its own decoder. The input embedding layer and the lower Transformer layers are frozen. The up- per Transformer layer and Conditional Alignment module are modulated with the task embedding. Single-task ﬁne-tuning overall pretrained model parameters may have other issues. Recent analy- ses of such MLM have shed light on the linguistic knowledge that is captured in the hidden states and attention maps (Clark et al., 2019b; Tenney et al., 2019a; Merchant et al., 2020). Particularly, BERT has middle Transformer (Vaswani et al., 2017) lay- ers that are typically the most transferable to a downstream task (Liu et al., 2019a). The model proxies the steps of the traditional NLP pipeline in a localizable way (Tenney et al., 2019a) — with basic syntactic information appearing earlier in the network, while high-level semantic information ap- pearing in higher-level layers. Since pretraining is usually done on large-scale datasets, it may be use- ful, for a variety of downstream tasks, to conserve that knowledge. However, single task ﬁne-tuning causes catastrophic forgetting of the knowledge learned during MLM (Howard & Ruder, 2018). To preserve knowledge, freezing part of a pretrained network and using Adapters for new tasks have shown promising results (Houlsby et al., 2019). Inspired by the human ability to transfer learned knowledge from one task to another new task, Multi-Task Learning (MTL) in a general sense (Caruana, 1997; Rajpurkar et al., 2016b; Ruder, 2017) has been applied in many ﬁelds outside of NLP. Caruana (1993) showed that a model trained in a multi-task manner can take advantage of the inductive transfer between tasks, achieving a better generalization performance. MTL has the advantage of computational/storage efﬁciency (Zhang & Yang, 2017), but training models in a multi-task setting is a balancing act; particularly with datasets that have different: (a) dataset sizes, (b) task difﬁculty levels, and (c) different types of loss functions. In practice, learning multiple tasks at once is challenging since negative transfer (Wang et al., 2019a), task interference (Wu et al., 2020; Yu et al., 2020) and catastrophic forgetting (Serrà et al., 2018) can lead to worse data efﬁciency, training stability and generalization compared to single task ﬁne-tuning. Using Conditionally Adaptive Learning, we seek to improve pretraining knowledge retention and multi-task inductive knowledge transfer. Our contributions are the following: • A new task conditioned Transformer that adapts and modulates pretrained weights (Section 2.1). • A novel way to prioritize tasks with an uncertainty based multi-task data sampling method that helps balance the sampling of tasks to avoid catastrophic forgetting (Section 2.2). Our Conditionally Adaptive Multi-Task Learning (CA-MTL) approach is illustrated in Figure 1. To the best of our knowledge, our work is the ﬁrst to explore the use of a latent representation of tasks to modularize and adapt pretrained architectures. Further, we believe our work is also the ﬁrst to examine uncertainty sampling for large-scale multi-task learning in NLP. We show the efﬁcacy of CA-MTL by: (a) testing on 26 different tasks and (b) presenting state-of-the-art results on a number of test sets as well as superior performance against both single-task and MTL baselines. Moreover, we further demonstrate that our method has advantages over (c) other adapter networks, and (d) other MTL sampling methods. Finally, we provide ablations and separate analysis of the MT-Uncertainty Sampling technique in section 4.1 and of each component of the adapter in 4.2. 2 M ETHODOLOGY This section is organized according to the two main MTL problems that we will tackle: (1) How to modularize a pretrained network with latent task representations? (2) How to balance different tasks in MTL? We deﬁne each task as: Ti ≜ {pi(yi|xi,zi),Li,˜pi(xi)}, where zi is task i’s learnable shallow embedding, Li is the task loss, and ˜pi(xi) is the empirical distribution of the training data pair {xi,yi}, for i∈{1,...,T }and T the number of supervised tasks. The MTL objective is: min φ(z),θ1,...,θT T∑ i=1 Li(fφ(zi),θi(xi),yi) (1) 2Published as a conference paper at ICLR 2021 where f is the predictor function (includes encoder model and decoder heads), φ(z) are learnable generated weights conditioned on z, and θi are task-speciﬁc parameters for the output decoder heads. z is constructed using an embedding lookup table. 2.1 T ASK CONDITIONED TRANSFORMER Our task conditioned Transformer architecture is based on one simple concept. We either add conditional layers or modulate existing pretrained weights using a task representation by extending Feature Wise Linear Modulation (Perez et al., 2018) functions in several ways depending on the Transformer layer. We deﬁne our framework below. Deﬁnition 1 (Conditional Weight Transformations). Given a neural network weight matrixW, we compute transformations of the formφ(W|zi) = γi(zi)W + βi(zi), whereγi and βi are learned functions that transform the weights based on a learned vector embeddingzi, for taski. Deﬁnition 2 (Conditionally Adaptive Learning). In our setting, Conditionally Adaptive Learning is the process of learning a set ofφs for the conditionally adaptive modules presented below along with a set of task embedding vectorszi for T tasks, using a multi-task loss (see equation 1). In the subsections that follow: We introduce a new Transformer Attention Module using block- diagonal Conditional Attention that allows the original query-key based attention to account for task-speciﬁc biases (section 2.1.1). We propose a new Conditional Alignment method that aligns the data of diverse tasks and that performs better than its unconditioned and higher capacity predecessor (section 2.1.2). We adapt layer normalization statistics to speciﬁc tasks using a new Conditional Layer Normalization module (section 2.1.3). We add a Conditional Bottleneck that facilitates weight sharing and task-speciﬁc information ﬂow from lower layers (section 2.1.4). In our experiments we provide an ablation study of these components (Table 1) examining performance in terms of GLUE scores. 2.1.1 C ONDITIONAL ATTENTION Figure 2: Conditional Attention Module Given d, the input dimensions, the query Q, the key K, and the value V as deﬁned in Vaswani et al. (2017), we redeﬁne the attention operation: Attention(Q,K,V,zi)) = softmax [ M(zi) + QKT √ d ] V M(zi) = N⨁ n=1 A′ n(zi), A ′ n(zi) = Anγi(zi) + βi(zi) where ⨁is the direct sum operator (see section A.6), N is the number of block matrices An ∈R(L/N)×(L/N) along the diagonal of the attention matrix, Lis the input sequence, M(zi) = diag(A′ 1,...,A ′ N) is a block diagonal conditional matrix. Note that An is constructed using L/N trainable and randomly initialized L/N dimensional vectors. While the original attention matrix depends on the hidden states h, M(zi) is a learnable weight matrix that only depends on the task embedding zi ∈Rd. γi,βi : Rd ↦→RL2/N2 are Feature Wise Linear Modulation (Perez et al., 2018) functions. We also experimented with full-block Conditional Attention ∈RL×L. Not only did it have N2 more parameters compared to the block-diagonal variant, but it also performed signiﬁcantly worse on the GLUE development set (see FBA variant in Table 10). It is possible that GLUE tasks derive a certain beneﬁt from localized attention that is a consequence of M(zi). With M(zi), each element in a sequence can only attend to other elements in its subsequence of length L/N. In our experiments we used N = d/L. The full Conditional Attention mechanism used in our experiments is illustrated in Figure 2. 2.1.2 C ONDITIONAL ALIGNMENT Wu et al. (2020) showed that in MTL havingT separate alignment modules R1,...,R T increases BERTLARGE avg. scores on ﬁve GLUE tasks (CoLA, MRPC, QNLI, RTE, SST-2) by 2.35%. Inspired by this work, we found that adding a task conditioned alignment layer between the input embedding 3Published as a conference paper at ICLR 2021 layer and the ﬁrst BERT Transformer layer improved multi-task model performance. However, instead of having T separate alignment matrices Ri for each T task, one alignment matrix ˆR is generated as a function of the task embedding zi. As in Wu et al. (2020), we tested this module on the same ﬁve GLUE tasks and with BERTLARGE. Enabling task conditioned weight sharing across covariance alignment modules allows us to outperformsBERTLARGE by 3.61%. This is 1.26 % higher than having T separate alignment matrices. Inserting ˆRinto BERT, yields the following encoder function ˆf: ˆf = T∑ t=1 gθi(E(xi) ˆR(zi)B), ˆR(zi) = Rγi(zi) + βi(zi) (2) where xi ∈Rd is the layer input, gθi is the decoder head function for task iwith weights θi, Ethe frozen BERT embedding layer, Bthe BERT Transformer layers and Rthe linear weight matrix of a single task conditioned alignment matrix. γi,βi : Rd ↦→Rd are Feature Wise Linear Modulation functions. 2.1.3 C ONDITIONAL LAYER NORMALIZATION (CLN) We extend the Conditional Batch Normalization idea from de Vries et al. (2017) to Layer Normaliza- tion (Ba et al., 2016). For task Ti, i∈{1,...,T }: hi = 1 σ ⊙(ai −µ) ∗ˆγi(zi) + βi(zi), ˆγi(zi) = γ′γi(zi) + β′ (3) where hi is the CLN output vector, ai are the preceding layer activations associated with task i, µ and σare the mean and the variance of the summed inputs within each layer as deﬁned in Ba et al. (2016). Conditional Layer Normalization is initialized with BERT’s Layer Normalization afﬁne transformation weights and bias γ′and β′from the original formulation: h = 1 σ ⊙(a −µ) ∗γ′+ β′. During training, the weight and bias functions of γi(∗) and βi(∗) are always trained, while the original Layer Normalization weight may be kept ﬁxed. This module was added to account for task speciﬁc rescaling of individual training cases. Layer Normalization normalizes the inputs across features. The conditioning introduced in equation 2.1.3 allows us to modulate the normalization’s output based on a task’s latent representation. 2.1.4 C ONDITIONAL BOTTLENECK Figure 3: a) Conditional Bottleneck for CA-MTLBASE. b) Conditional Bottleneck for CA-MTLLARGE. We created a task conditioned two layer feed-forward bot- tleneck layer (CFF up/down in Figure 3). The conditional bottleneck layer follows the same transformation as in equation 2. The module in Figure 3a is added to the top most Transformer layers ofCA-MTLBASE and uses a CLN. For CA-MTLLARGE this module is the main building block of the skip connection added alongside all Transformer layers seen in Figure 3b. The connection at layer jtakes in the matrix sum of the Transformer layer output at j and the previous connection’s output atj−1. The Con- ditional bottleneck allows lower layer information to ﬂow upwards depending on the task. Our intuition for intro- ducing this component is related to recent studies (Tenney et al., 2019a) that showed that the “most important layers for a given task appear at speciﬁc positions”. As with the other modules described so far, each task adaptation is created from the weights of a single shared adapter that is modulated by the task embedding. 2.2 M ULTI -TASK UNCERTAINTY SAMPLING MT-Uncertainty Sampling is a task selection strategy that is inspired by Active Learning techniques. Our algorithm 1 is outlined in the Appendix, Section A.2. Similar to Active Learning, our algorithm ﬁrst evaluates the model uncertainty. MT-Uncertainty Sampling uses Shannon Entropy, an uncertainty measure, to choose training examples by ﬁrst doing forward pass through the model with b×T input samples. For an output classiﬁcation prediction with Ci possible classes and probabilities 4Published as a conference paper at ICLR 2021 (pi,1,...,p i,Ci), the Shannon Entropy Hi, for task Ti and i∈{1,...,T }, our uncertainty measure U(x) are given by: Hi = Hi(fφ(zi),θi(x)) = − Ci∑ c=1 pc log pc, U(xi) = Hi(fφ(zi),θi(x)) ˆH×H′ i (4) ˆH = max i∈{1,...,T} ¯Hi = max [ 1 b ∑ x∈xi Hi ] , H ′ i = − Ci∑ c=1 1 Ci log [ 1 Ci ] (5) where ¯Hi is the average Shannon Entropy across bsamples of task t, H′ i, the Shannon entropy of choosing classes with uniform distribution and ˆH, the maximum of each task’s average entropy over bsamples. H′ i is normalizing factor that accounts for differing number of prediction classes (without the normalizing factor H′ i, tasks with a binary classiﬁcation Ci = 1 were rarely chosen). Further, to limit high entropy outliers and to favor tasks with highest uncertainty, we normalize with ˆH. The measure in eq. 4 allows Algorithm 1 to choose bsamples from b×T candidates to train the model. 3 R ELATED WORK Multi-Tasking in NLP. To take advantage of the potential positive transfer of knowledge from one task to another, several works have proposed carefully choosing which tasks to train as an intermediate step in NLP before single task ﬁne-tuning (Bingel & Søgaard, 2017; Kerinec et al., 2018; Wang et al., 2019a; Standley et al., 2019; Pruksachatkun et al., 2020; Phang et al., 2018). The intermediate tasks are not required to perform well and are not typically evaluated jointly. In this work, all tasks are trained jointly and all tasks usedare evaluated from a single model. In Natural Language Understanding (NLU), it is still the case that to get the best task performance one often needs a separate model per task (Clark et al., 2019c; McCann et al., 2018). At scale, Multilingual NMT systems (Aharoni et al., 2019) have also found that MTL model performance degrades as the number of tasks increases. We notice a similar trend in NLU with our baseline MTL model. Recently, approaches in MTL have tackled the problem by designing task speciﬁc decoders on top of a shared model (Liu et al., 2019b) or distilling multiple single-task models into one (Clark et al., 2019c). Nonetheless, such MTL approaches still involves single task ﬁne-tuning. In this paper, we show that it is possible to achieve high performance in NLU without single task ﬁne-tuning. Adapters. Adapters are trainable modules that are attached in speciﬁc locations of a pretrained network. They provide another promising avenue to limit the number of parameters needed when confronted with a large number of tasks. This approach is useful with pretrained MLM models that have rich linguistic information (Tenney et al., 2019b; Clark et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a). Recently, Houlsby et al. (2019) added an adapter to a pretrained BERT model by ﬁne-tuning the layer norms and adding feed forward bottlenecks in every Transformer layer. However, such methods adapt each task individually during the ﬁne-tuning process. Unlike prior work, our method harnesses the vectorized representations of tasks to modularize a single pretrained model across all tasks. Stickland et al. (2019) and Tay et al. (2020) also mix both MTL and adapters with BERT and T5 encoder-decoder (Raffel et al., 2019) respectively by creating local task modules that are controlled by a global task agnostic module. The main drawback is that a new set of non-shared parameters must be added when a new task is introduced. CA-MTL shares all parameters and is able to re-modulate existing weights with a new task embedding vector. Active Learning, Task Selection and Sampling. Our sampling technique is similar to the ones found in several active learning algorithms (Chen et al., 2006) that are based on Shannon entropy estimations. Reichart et al. (2008) and Ikhwantri et al. (2018) examined Multi-Task Active Learning (MTAL), a technique that chooses one informative sample forT different learners (or models) for each T tasks. Instead we choose T tasks samples for one model. Moreover, the algorithm weights each sample by the corresponding task score, and the Shannon entropy is normalized to account for various losses (see equation 5). Also, our algorithm is used in a large scale MTL setup (≫2 tasks). Recently, Glover & Hokamp (2019) explored task selection in MTL using learning policies based on counterfactual estimations (Charles et al., 2013). However, such method considers only ﬁxed stochastic parameterized policies while our method adapts its selection criterion based on model uncertainty throughout the training process. 5Published as a conference paper at ICLR 2021 Hypernetworks. CA-MTL is a hypernetwork adapter. The method to generate task-conditioned adapter weights is inspired by von Oswald et al. (2020). Hypernetwork layers have also been ﬁnetuned along with pretrained models. For example, Ponti et al. (2021) uses stochastic variational inference Hoffman et al. (2013) to produce language and task latent codes that conditionally generates the weights of a BERT prediction head, a single hypernetwork linear layer shared across multiple languages and tasks. Unlike previous methods however, CA-MTL conditionally modulates pretrained weights and biases, attention matrices, hidden representations and normalization statistics with task embeddings. Further, CA-MTL can preserve the pretraining knowledge by freezing the underlying Transformer model. Finally, we show a synergy between our hypernetwork adapter and our active task sampling technique (see section 2.2) that allows CA-MTL to continue surpassing fully tuned models as we scale the number of tasks (see ﬁgure 7). 4 E XPERIMENTS AND RESULTS We show that our adapter of section 2 achieve parameter efﬁcient transfer for 26 NLP tasks. Our implementation of CA-MTL is based on HuggingFace (Wolf et al., 2019). Hyperparameters and our experimental set-up are outlined in A.5. To preserve the weights of the pretrained model, CA-MTL’s bottom half Transformer layers are frozen in all experiments (except in section 4.4). We also tested different layer freezing conﬁgurations and found that freezing half the layers worked best on average (see Section A.8). 4.1 M ULTI -TASK UNCERTAINTY SAMPLING 0 250005000075000100000125000150000175000200000Training iteration 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82Average score MT-UncertaintyCouterfactualTask sizeRandom Figure 4: MT-Uncertainty vs. other task sam- pling strategies: median dev set scores on 8 GLUE tasks and using BERTBASE. Data for the Counter- factual and Task Size policyπ|task|(eq. 6) is from Glover & Hokamp (2019). Our MT-Uncertainty sampling strategy, from section 2.2, is compared to 3 other task selection schemes: a) Counterfactual b) Task size c) Random. We used a BERTBASE (no adapters) on 200k iterations and with the same hyperparameters as in Glover & Hokamp (2019). For more information on Counterfactual task selection, we invite the reader to consult the full expla- nation in Glover & Hokamp (2019). For T tasks and the dataset Di for tasks i ∈{1,...,T }, we rewrite the deﬁnitions of Randomπrand and Task sizeπ|task| sampling: πrand = 1/T, π|task|= |Di| [ T∑ i=1 |Di| ]−1 (6) 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (a) Random 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (b) MT-Uncertainty MNLI-mm dev scoreCoLA dev score MNLI-mm train entropyCoLA train entropy Figure 5: CoLA/MNLI Dev set scores and Entropy for πrand (left) and MT-Uncertainty (right). In Figure 4, we see from the results that MT- Uncertainty converges faster by reaching the 80% average GLUE score line before other task sampling methods. Further, MT-Uncertainty maximum score on 200k iterations is at 82.2, which is 1.7% higher than Counterfactual sam- pling. The datasets in the GLUE benchmark offers a wide range of dataset sizes. This is useful to test how MT-Uncertainty manages a jointly trained low resource task (CoLA) and high resource task (MNLI). Figure 5 explains how catastrophic forgetting is curtailed by sam- pling tasks before performance drops. With πrand, all of CoLA’s tasks are sampled by it- eration 500, at which point the larger MNLI dataset overtakes the learning process and CoLA’s dev set performance starts to diminish. On the other hand, with MT-Uncertainty sampling, CoLA is sampled whenever Shannon entropy is higher than MNLI’s. The model ﬁrst assesses uncertain samples using Shannon Entropy then decides what data is necessary to train on. This process allows lower resource tasks to keep performance steady. 6Published as a conference paper at ICLR 2021 We provide evidence in Figure 8 of A.2 that MT-Uncertainty is able to manage task difﬁculty — by choosing the most difﬁcult tasks ﬁrst. 4.2 A BLATION AND MODULE ANALYSIS Table 1: Model ablation studya on the GLUE dev set. All models have the bottom half layers frozen. Model changes Avg Task σ % data GLUE GLUE used BERTBASE MTL (πrand) 80.61 14.41 100 + Conditional Attention 82.41 10.67 100 + Conditional Adapter 82.90 11.27 100 + CA and CLN 83.12 10.91 100 + MT-Uncertainty 84.03 10.02 66.3(CA-MTLBERT-BASE) aCA=Conditional Alignment, CLN=Conditional Layer Normal- ization, Task σ=scores standard deviation across tasks. In Table 1, we present the results of an ablation study to determine which elements of CA-MTLBERT-BASE had the largest positive gain on average GLUE scores. Starting from a MTL BERTBASE baseline trained us- ing random task sampling ( πrand). Apart for the Conditional Adapter, each module as well as MT- Uncertainty lift overall performance and reduce vari- ance across tasks. Please note that we also included accuracy/F1 scores for QQP, MRPC and Pearson/ Spearman correlation for STS-B to calculate score standard deviation Task σ. Intuitively, when negative task transfer occurs between two tasks, either (1) task interference is bidirectional and scores are both impacted, or (2) interference is unidirectional and only one score is impacted. We calculate Task σ to characterize changes in the dynamic range of performance across multiple tasks. We do this to asses the degree to which performance im- provements are distributed across all tasks or only subsets of tasks. As we can see from Table 1, Conditional Attention, Conditional Alignment, Conditional Layer Normalization, MT-Uncertainty play roles in reducing Task σand increasing performance across tasks. This provides partial evidence of CA-MTL’s ability to mitigating negative task transfer. Figure 6: Task performance vs. avg. covariance similarity scores (eq. 7) for MTL and CA-MTL. We show that Conditional Alignment can learn to capture covariate distribution differences with task embeddings co-learned from other adapter compo- nents of CA-MTL. In Figure 6, we arrive at similar conclusions as Wu et al. (2020), who proved that neg- ative task transfer is reduced when task covariances are aligned. The authors provided a “covariance simi- larity score” to gauge covariance alignment. For task iand j with mi and mj data samples respectively, and given d dimensional inputs to the ﬁrst Trans- former layer Xi ∈Rmi×d and Xj ∈Rmj×d, we rewrite the steps to calculate the covariance similarity score between task iand j: (a) Take the covariance matrix X⊤ i Xi, (b) Find its best rank- ri approxima- tion Ui,riDi,riU⊤ i,ri, where ri is chosen to contain 99% of the singular values. (c) Apply steps (a), (b) to Xj, and compute the covariance similarity score CovSimi,j: CovSimi,j := ∥(Ui,riD1/2 i,ri )⊤Uj,rj D1/2 j,rj ∥F ∥Ui,riD1/2 i,ri ∥F ·∥Uj,rj D1/2 j,rj ∥F . CovSimi = 1 T −1 ∑ j̸=i CovSimi,j (7) Since we are training models with T tasks, we take the average covariance similarity score CovSimi between task iand all other tasks. We measure CovSimi using equation 7 between 9 single-task models trained on individual GLUE tasks. For each task in Figure 6, we measure the similarity score on the MTL trained BERTBASE baseline, e.g., CoLA (MTL), or CA-MTLBERT-BASE model, e.g., MNLI (CA-MTL). Our score improvement measure is the % difference between a single task model and MTL or CA-MTL on the particular task. We ﬁnd that covariance similarity increases for 9 tasks and that performance increases for 7 out 9 tasks. These measurements conﬁrm that the Conditional Alignment is able to align task covariance, thereby helping alleviate task interference. 4.3 J OINTLY TRAINING ON 8 TASKS : GLUE In Table 2, we evaluate the performance of CA-MTL against single task ﬁne-tuned models, MTL as well as the other BERT-based adapters on GLUE. As in Houlsby et al. (2019), MNLIm and MNLImm are treated as separate tasks. Our results indicate that CA-MTL outperforms both the BASE adapter, 7Published as a conference paper at ICLR 2021 Table 2: Adapters with layer freezing vs. ST/MT on GLUE test set. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. * Individual scores not available. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Results from: 1Devlin et al. (2018) 2Stickland et al. (2019). 3Houlsby et al. (2019) . Method Type Total Trained # tasks GLUE params params/task g.e. ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Test Server Results BERTBASE 1 ST 9.0× 100% — 52.1 84.6/83.4 88.9 90.5 71.2 66.4 93.5 85.8 79.6 BERTBASE 2 MTL 1.0× 11.1% 2 51.2 84.0/83.4 86.7 89.3 70.8 76.6 93.4 83.6 79.9 PALs+Anneal Samp.2 MTL 1.13× 12.5% 4 51.2 84.3/83.5 88.7 90.0 71.5 76.0 92.6 85.8 80.4 CA-MTLBERT-BASE (ours) MTL 1.12× 5.6 % 5 53.1 85.9/85.8 88.6 90.5 69.2 76.4 93.2 85.3 80.9 Large Models — Test Server Results BERTLARGE 1 ST 9.0× 100% — 60.5 86.7/85.9 89.3 92.7 72.1 70.1 94.9 86.5 82.1 Adapters-2563 ST 1.3× 3.6% 3 59.5 84.9/85.1 89.5 90.7 71.8 71.5 94.0 86.9 80.0 CA-MTLBERT-LARGE (ours) MTL 1.12× 5.6% 3 59.5 85.9/85.4 89.3 92.6 71.4 79.0 94.7 87.7 82.8 PALS+Anneal Sampling (Stickland et al., 2019), and the LARGE adapter, Adapters-256 (Houlsby et al., 2019). Against single task (ST) models, CA-MTL is 1.3% higher than BERTBASE, with 5 out 9 tasks equal or greater performance, and 0.7% higher than BERTLARGE, with 3 out 9 tasks equal or greater performance. ST models, however, need 9 models or close to 9×more parameters for all 9 tasks. We noted that CA-MTLBERT-LARGE’s average score is driven by strong RTE scores. While RTE beneﬁts from MTL, this behavior may also be a side effect of layer freezing. In Table 10, we see that CA-MTL has gains over ST on more and more tasks as we gradually unfreeze layers. 4.4 T RANSFER TO NEW TASKS Table 3: Domain adaptation results on dev. sets for BASE models. 1Liu et al. (2019b), 2Jiang et al. (2020) % data used SciTail SNLI 0.1% 1% 10% 100% 0.1% 1% 10% 100% BERTBASE 1 51.2 82.2 90.5 94.3 52.5 78.1 86.7 91.0 MT-DNN1 81.9 88.3 91.1 95.7 81.9 88.3 91.1 95.7 MT-DNNSMART 2 82.3 88.6 91.3 96.1 82.7 86.0 88.7 91.6 CA-MTLBERT 83.2 88.7 91.4 95.6 82.8 86.2 88.0 91.5 In Table 3 we examine the ability of our method to quickly adapt to new tasks. We performed domain adaptation on SciTail (Khot et al., 2018) and SNLI (Bowman et al., 2015) datasets, using a CA-MTLBASE model trained on GLUE and a new linear decoder head. We tested several pretrained and randomly initialized task embeddings in a zero-shot setting. The com- plete set of experiments with all task embeddings can be found in the Appendix, Section A.4. We then selected the best task embedding for our results in Table 3. STS-B and MRPC MTL-trained task embeddings performed best on SciTail and SNLI respectively. CA-MTLBERT-BASE has faster adapta- tion than MT-DNNSMART (Jiang et al., 2020) as evidenced by higher performances in low-resource regimes (0.1% and 1% of the data). When trained on the complete dataset, CA-MTLBERT-BASE is on par with MT-DNNSMART. Unlike MT-DNNSMART however, we do not add context from a semantic similarity model – MT-DNNSMART is built off HNN (He et al., 2019). Nonetheless, with a larger model, CA-MTL surpasses MT-DNNSMART on the full SNLI and SciTail datasets in Table 6. 4.5 J OINTLY TRAINING ON 24 TASKS : GLUE/S UPER -GLUE, MRQA AND WNUT2017 Figure 7: Effects of adding more datasets on avg GLUE scores. Experiments conducted on 3 epochs. When 23 tasks are trained jointly, performance of CA-MTLBERT-BASE continues to improve. Effects of Scaling Task Count. In Figure 7 we continue to test if CA-MTL mitigates task in- terference by measuring GLUE average scores when progressively adding 9 GLUE tasks, 8 Super-GLUE tasks (Wang et al., 2019b), 6 MRQA tasks (Fisch et al., 2019). Tasks are described in Appendix section A.9. The results show that adding 23 tasks drops the performance of our baseline MTL BERTBASE (πrand). MTL BERT increases by 4.3% when adding MRQA but, with 23 tasks, the model performance drops by 1.8%. The opposite is true when CA-MTL modules are integrated into the model. CA-MTL continues to show gains with a large number of tasks and surpasses the baseline MTL model by close to 4% when trained on 23 tasks. 8Published as a conference paper at ICLR 2021 Table 4: 24-task CA-MTL vs. ST and vs. 24-task MTL with frozen layers on GLUE, SuperGLUE, MRQA and NER development sets. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Details in section A.5. Model Task Grouping Avg # tasks Total GLUE SuperGLUE MRQA NER e.g. ST Params BERT-LARGE models STReImp 84.5 68.9 79.7 54.1 76.8 — 24× MTLReImp 83.2 72.1 77.8 42.2 76.4 9/24 1× CA-MTL 86.6 74.1 79.5 49.0 79.1 17/24 1.12× RoBERTa-LARGE models STReImp 88.2 76.5 83.6 57.8 81.9 — 24× MTLReImp 86.0 78.6 80.7 49.3 80.7 7/24 1× CA-MTL 89.4 80.0 82.4 55.2 83.1 15/24 1.12× 24-task CA-MTL. We jointly trained large MTL baselines and CA-MTL models on GLUE/Super-GLUE/MRQA and Named En- tity Recognition (NER) WNUT2017 (Derczyn- ski et al., 2017). Since some dev. set scores are not provided and since RoBERTa results were reported with a median score over 5 random seeds, we ran our own single seed ST/MTL baselines (marked “ReImp”) for a fair compar- ison. The dev. set numbers reported in Liu et al. (2019c) are displayed with our baselines in Table 9. Results are presented in Table 4. We notice in Table 4 that even for large models, CA-MTL provides large gains in performance on average over both ST and MTL models. For the BERT based models, CA-MTL provides 2.3% gain over ST and higher scores on 17 out 24 tasks. For RoBERTa based models, CA-MTL provides 1.2% gain over ST and higher scores on 15 out 24 tasks. We remind the reader that this is achieved with a single model. Even when trained with 16 other tasks, it is interesting to note that the MTL baseline perform better than the ST baseline on Super GLUE where most tasks have a small number of samples. Also, we used NER to test if we could still outperform the ST baseline on a token-level task, signiﬁcantly different from other tasks. Unfortunately, while CA-MTL performs signiﬁcantly better than the MTL baseline model, CA-MTL had not yet overﬁt on this particular task and could have closed the gap with the ST baselines with more training cycles. Table 5: Our 24-task CA-MTL vs. other large models on GLUE. F1 is reported for QQP/MRPC, Spearman’s corr. for STS-B, Matthew’s corr. for CoLA and accuracy for other tasks. *Split not available. **Uses intermediate task ﬁne-tuning + ST. Model GLUE tasks AvgCoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B BERT-LARGE based models on Dev set. MT-DNN 63.5 87.1 /86.7 91.0 92.9 89.2 83.4 94.3 90.6 85.6 STILTS ** 62.1 86.1* 92.3 90.5 88.5 83.4 93.2 90.8 85.9 BAM! 61.8 87.0* – 92.5 – 82.8 93.6 89.7 – 24-task CA-MTL 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa-LARGE based models on Test set. RoBERTA** with 67.8 91.0/90.8 91.6 95.4 74.0 87.9 97.5 92.5 87.3Ensemble 24-task CA-MTL 62.2 89.0/88.4 92.0 94.7 72.3 86.2 96.3 89.8 85.7 Comparisons with other methods. In Table 5, CA-MTLBERT is com- pared to other Large BERT based methods that either use MTL + ST, such as MT-DNN (Liu et al., 2019b), intermediate tasks + ST, such as STILTS (Phang et al., 2018) or MTL model distillation + ST, such as BAM! (Clark et al., 2019c). Our method scores higher than MT-DNN on 5 of 9 tasks and by 1.0 % on avg. Against STILTS, CA-MTL realizes a 0.7 % avg. score gain, surpassing scores on 6 of 9 tasks. We also show that CA-MTLRoBERTa is within only 1.6 % of a RoBERTa ensemble of 5 to 7 models per task and that uses intermediate tasks. Using our 24-task CA-MTL large RoBERTa-based model, we report NER F1 scores on the WNUT2017 test set in Table 6a. We compare our result with RoBERTaLARGE and XLM-RLARGE (Nguyen et al., 2020) the current state-of-the-art (SOTA). Our model outperforms XLM-RLARGE by 1.6%, reaching a new state-of-the-art. Using domain adaptation as described in Section 4.4, we report results on the SciTail test set in Table 6b and SNLI test set in Table 6b. For SciTail, our model matches the current SOTA2 ALUM (Liu et al., 2020), a RoBERTa large based model that additionally uses the SMART (Jiang et al., 2020) ﬁne-tuning method. For SNLI, our model outperforms SemBert, the current SOTA3. Table 6: CA-MTL test performance vs. SOTA. (a) WNUT2017 F1 RoBERTaLARGE 56.9 XLM-RLARGE 57.1 CA-MTLRoBERTa (ours) 58.0 (b) SciTail % Acc MT-DNN 94.1 ALUMRoBERTa 96.3 ALUMRoBERTa-SMART 96.8 CA-MTLRoBERTa (ours) 96.8 (c) SNLI % Acc MT-DNN 91.6 MT-DNNSMART 91.7 SemBERT 91.9 CA-MTLRoBERTa (ours) 92.1 2https://leaderboard.allenai.org/scitail/submissions/public on 09/27/2020 3https://nlp.stanford.edu/projects/snli/ on 09/27/2020 9Published as a conference paper at ICLR 2021 5 C ONCLUSION We believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. In a large-scale 24-task NLP experiment, CA-MTL outperforms fully tuned single task models by 2.3% for BERT Large and by 1.2% for RoBERTa Large using 1.12 times the number of parameters, while single task ﬁne-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. When a BERT vanilla MTL model sees its performance drop as the number of tasks increases, CA-MTL scores continue to climb. Performance gains are not driven by a single task as it is often the case in MTL. Each CA-MTL module that adapts a Transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. This evidence shows that CA-MTL is able to mitigate task interference and promote more efﬁcient parameter sharing. We showed that MT-Uncertainty is able to avoid degrading performances of low resource tasks. Tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. Overall, CA-MTL offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. Extending such ideas will be an objective for future work. 10Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This research was supported by the Canada CIFAR AI Chairs Program, NSERC and PROMPT. Exper- iments in this article were conducted with Compute Canada and MILA computational infrastructure and we thank them for their support. We would like to thank Colin Raffel, Sandeep Subramanian, and Nicolas Gontier for their useful feedback and the anonymous reviewers for helpful comments, discussions and suggestions. REFERENCES Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3874–3884, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388. URL https://www.aclweb.org/anthology/N19-1388. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41–48, 2009. Joachim Bingel and Anders Søgaard. Identifying beneﬁcial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 164–169, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/E17-2026. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, pp. arXiv–2005, 2020. Rich Caruana. Multitask learning. Mach. Learn., 28(1):41–75, July 1997. ISSN 0885-6125. doi: 10.1023/A:1007379606734. URL https://doi.org/10.1023/A:1007379606734. Richard Caruana. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning, pp. 41–48. Morgan Kaufmann, 1993. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001. Denis Charles, Max Chickering, and Patrice Simard. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14:3207–3260, November 2013. Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer. An empirical study of the behavior of active learning for word sense disambiguation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 120–127, New York City, USA, June 2006. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/N06-1016. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. CoRR, abs/1711.02257, 2017. URL http://arxiv.org/abs/1711.02257. 11Published as a conference paper at ICLR 2021 Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. doi: 10.18653/ v1/N19-1300. URL https://www.aclweb.org/anthology/N19-1300. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August 2019b. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https: //www.aclweb.org/anthology/W19-4828. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V . Le. Bam! born-again multi-task networks for natural language understanding. CoRR, abs/1907.04829, 2019c. URL http://arxiv.org/abs/1907.04829. Edward Collins, Nikolai Rozanov, and Bingbing Zhang. Evolutionary data measures: Understanding the difﬁculty of text classiﬁcation tasks. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pp. 380–391, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/K18-1037. URL https://www.aclweb.org/ anthology/K18-1037. Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neural networks with multitask learning. In ICML, pp. 160–167, 2008. URL https://doi. org/10.1145/1390156.1390177. Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investi- gating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107– 124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.601. URL https://ojs.ub.uni-konstanz. de/sub/index.php/sub/article/view/601. Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems 30 , pp. 6594– 6604. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7237-modulating-early-visual-processing-by-language.pdf . Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 140–147, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4418. URL https://www.aclweb.org/ anthology/W17-4418. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/anthology/I05-5002. Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Güney, V olkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine.CoRR, abs/1704.05179, 2017. URL http://arxiv.org/abs/1704.05179. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 1–13, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5801. URL https: //www.aclweb.org/anthology/D19-5801. 12Published as a conference paper at ICLR 2021 John Glover and Chris Hokamp. Task selection policies for multitask learning. CoRR, 2019. URL http://arxiv.org/abs/1907.06214. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausi- ble alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main confer- ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se- mantic Evaluation (SemEval 2012), pp. 394–398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/S12-1052. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioriti- zation for multitask learning. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network model for commonsense reasoning. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pp. 13–21, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-6002. URL https://www.aclweb.org/ anthology/D19-6002. Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(4):1303–1347, 2013. URL http://jmlr.org/ papers/v14/hoffman13a.html. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. CoRR, abs/1902.00751, 2019. URL http://arxiv.org/abs/1902.00751. Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://www.aclweb.org/anthology/ P18-1031. Fariz Ikhwantri, Samuel Louvan, Kemal Kurniawan, Bagas Abisena, Valdi Rachman, Alfan Farizki Wicaksono, and Rahmad Mahendra. Multi-task active learning for neural semantic role labeling on low resource conversational corpus. InProceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pp. 43–50, 2018. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled regular- ized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2177–2190, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.197. URL https://www.aclweb.org/anthology/2020. acl-main.197. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601– 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/ v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. CoRR, abs/1907.10529, 2019. URL http://arxiv.org/abs/1907.10529. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CoRR, abs/1705.07115, 2017. URL http://arxiv.org/ abs/1705.07115. Emma Kerinec, Chloé Braud, and Anders Søgaard. When does deep multi-task learning work for loosely related document classiﬁcation tasks? In Proceedings of the 2018 EMNLP Workshop 13Published as a conference paper at ICLR 2021 BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 1–8, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5401. URL https://www.aclweb.org/anthology/W18-5401. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. InProceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL https://www.aclweb.org/anthology/N18-1023. Tushar Khot, A. Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question answering. In AAAI, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Hector J. Levesque. The winograd schema challenge. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning. AAAI, 2011. URL http://dblp.uni-trier.de/ db/conf/aaaiss/aaaiss2011-6.html#Levesque11. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855, 2019a. URL http://arxiv.org/abs/1903.08855. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019b. URL http://arxiv.org/ abs/1901.11504. Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019c. URL http://arxiv.org/abs/1907.11692. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to bert embeddings during ﬁne-tuning? arXiv preprint arXiv:2004.14448, 2020. Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. Bertweet: A pre-trained language model for english tweets. arXiv preprint arXiv:2005.10200, 2020. Hao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h - 1 heads is better than h heads. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6566–6577, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.587. URL https://www.aclweb.org/anthology/2020. acl-main.587. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. 14Published as a conference paper at ICLR 2021 Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. CoRR, abs/1808.08949, 2018. URL http: //arxiv.org/abs/1808.08949. Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. CoRR, abs/1811.01088, 2018. URL http://arxiv. org/abs/1811.01088. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 67–81, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1007. URL https://www.aclweb.org/ anthology/D18-1007. Edoardo M. Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela Parovic, Roi Reichart, and Anna Korhonen. Parameter space factorization for zero-shot learning across tasks and languages. Transactions of the Association for Computational Linguistics, 9:410–428, 2021. doi: 10.1162/tacl_a_00374. URL https://aclanthology.org/2021.tacl-1.25. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R Bowman. Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? arXiv preprint arXiv:2005.00628, 2020. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016a. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/ anthology/D16-1264. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016b. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. Multi-task active learning for linguistic annotations. In Proceedings of ACL-08: HLT, pp. 861–869, 2008. Sebastian Ruder. An overview of multi-task learning in deep neural networks.ArXiv, abs/1706.05098, 2017. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. CoRR, abs/1810.04650, 2018. URL http://arxiv.org/abs/1810.04650. Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, pp. 4555–4564, 2018. URL http:// proceedings.mlr.press/v80/serra18a.html. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. 15Published as a conference paper at ICLR 2021 Trevor Standley, Amir Roshan Zamir, Dawn Chen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? CoRR, abs/1905.07553, 2019. URL http://arxiv.org/abs/1905.07553. Asa Cooper Stickland, Iain Murray, someone, and someone. BERT and PALs: Projected attention layers for efﬁcient adaptation in multi-task learning. volume 97 of Proceedings of Machine Learning Research, pp. 5986–5995, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/stickland19a.html. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid: Efﬁcient multi-task transformers with grid-wise decomposable hyper projections. arXiv preprint arXiv:2007.05891, 2020. Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. CoRR, abs/1905.05950, 2019a. URL http://arxiv.org/abs/1905.05950. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316, 2019b. URL http://arxiv.org/abs/1905.06316. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 191–200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL https: //www.aclweb.org/anthology/W17-2623. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and João Sacramento. Continual learning with hypernetworks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgwNerKvB. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019a. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537, 2019b. URL http://arxiv.org/abs/ 1905.00537. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. CoRR, abs/1805.12471, 2018. URL http://arxiv.org/abs/1805.12471. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //www.aclweb.org/anthology/N18-1101. 16Published as a conference paper at ICLR 2021 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771. Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer in multi-task learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylzhkBtDB. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Com- putational Linguistics. doi: 10.18653/v1/D18-1259. URL https://www.aclweb.org/ anthology/D18-1259. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885, 2018. URL http://arxiv.org/abs/1810.12885. Yu Zhang and Qiang Yang. A survey on multi-task learning. CoRR, abs/1707.08114, 2017. URL http://arxiv.org/abs/1707.08114. 17Published as a conference paper at ICLR 2021 A A PPENDIX A.1 S UMMARY OF ACRONYMS Acronyms of datasets and descriptions can be found below in section A.9. Table 7: List of acronyms used in this paper. Acronym Description ARLM Autoregressive Language Models CA-MTL Conditional Adaptive Multi-Task Learning: our architecture CFF Conditional Feed-Forward: a feed-forward layer modulated by a conditioning vector CLN Conditional Layer Normalization in section 2.1.3 EDM Evolutionary Data Measures (Collins et al., 2018): a task difﬁculty estimate GLUE General Language Understanding Evaluation Wang et al. (2018): a benchmark with multiple datasets QA Question Answering MT Multi-Task MTAL Multi-Task Active Learning: ﬁnding the most informative instance for multiple learners (or models) MLM Masked Language Model: BERT Devlin et al. (2018) is an example of an MLM MTL Multi-Task Learning: \"learning tasks in parallel while using a shared representation\" (Caruana, 1997) MRQA Machine Reading for Question Answering Fisch et al. (2019): a benchmark with multiple datasets NER Named Entity Recognition NLP Natural Language Processing SOTA State of the art ST Single Task ﬁne-tuning: all weights are typically updated ST-A ST with Adapter modules: one adapter per task is trained and pretrained weights are optionally updated A.2 U NCERTAINTY SAMPLING : A LGORITHM AND ADDITIONAL RESULTS Algorithm 1: Multi-task Uncertainty Sampling Input: Training data Dt for task t∈[1,...,T ]; batch size b; Ct possible output classes for task t; f := fφ(zi),θi our model with weights φ,θi; Output: B′- multi-task batch of size b 1 B←∅ 2 for t←1 to T do 3 Generate xt := {xt,1,...,x t,b} i.i.d. ∼Dt 4 for i←1 to bdo 5 Ht,i ←−∑Ci c=1 pc(f(xt,i)) log pc(f(xt,i)) ⊿Entropy of each sample 6 7 end 8 Compute ¯Ht ←1 b ∑ x∈xi Ht,i ⊿Average entropy for task t 9 10 Compute H′ t ←−∑Ct c=1 1 Ct log [ 1 Ct ] ⊿Max entropy (uniform distribution) 11 12 B←B∪ xt and Dt ←Dt \\xt 13 if Dt = ∅then 14 Reload Dt 15 end 16 for i←1 to bdo 17 Compute: Ut,i ←Ht,i/H′ t ⊿Uncertainty normalized with max entropy 18 end 19 end 20 Compute ˆH← maxi∈{1,...,T}[ ¯Ht] ⊿Entropy of task with highest average entropy 21 Update Ut,i ←Ut,i/ˆH ⊿Normalize each sample’s uncertainty measure 22 B′←top_b({Ut,i|t∈[1,...,T ],i ∈[1,...,b ]}) ⊿b samples w/ highest uncertainty Return: With B′, solve eq. 1 with gradient descent; updated model f 18Published as a conference paper at ICLR 2021 An advantage of our MT-Uncertainty Sampling approach is its ability to manage task difﬁculty. This is highlighted in Figure 8. In this experiment, we estimated task difﬁculty using the Evolutionary Data Measures (EDM)4 proposed by Collins et al. (2018). The task difﬁculty estimate relies on multiple dataset statistics such as the data size, class diversity, class balance and class interference. Interestingly, estimated task difﬁculty correlates with the ﬁrst instance that the selection of a speciﬁc task occurs. Supposing that QNLI is an outlier, we notice that peaks in the data occur whenever tasks are ﬁrst selected by MT Uncertainty sampling. This process follows the following order: 1. MNLI 2. CoLA 3. RTE 4. QQP 5. MRPC 6.SST-2, which is the order from highest task difﬁculty to lowest task difﬁculty using EDM. As opposed to Curriculum Learning (Bengio et al., 2009), MT-Uncertainty dynamically prioritizes the most difﬁcult tasks. As also discovered in MTL vision work (Guo et al., 2018), this type of prioritization on more difﬁcult tasks may explain MT-Uncertainty’s improved performance over other task selection methods. In MTL, heuristics to balance tasks during training is typically done by weighting each task’s loss differently. We see here how MT-Uncertainty is able to prioritize task difﬁculty. 0 500 1000 1500 2000 2500 3000 Train iteration 0 4 8 12 16 20 24 28 32Number of samples Task Difﬁculty MNLI 4.2 QNLI 3.8 CoLA 3.7 RTE 3.6 MRPC 3.5 QQP. 3.5 SST-2 3.2 Figure 8: Task composition of MT-Uncertainty sampling and estimated task difﬁculty using EDM: number of training samples per task at each iteration for batch size of 32. The occurrence of ﬁrst peaks and estimated difﬁculty follow the same order: From highest to lowest: MNLI >CoLA >RTE >QQP = MRPC >SST-2. While the EDM difﬁculty measure is shown to correlate well with model performance, it lacks precision. As reported in Collins et al. (2018), the average score achieved on the Yahoo Answers dataset is 69.9% and its difﬁculty is 4.51. The average score achieved on Yelp Full is 56.8%, 13.1% less than Yahoo Answers and its difﬁculty is 4.42. The authors mention that “This indicates that the difﬁculty measure in its current incarnation may be more effective at assigning a class of difﬁculty to datasets, rather than a regression-like value”. A.3 O THER RELATED WORK Multi-Tasking in NLP and other ﬁelds.MTL weight sharing algorithms such as Mixture-of-Experts (MoE) have found success in NLP (Lepikhin et al., 2020). CA-MTL can complement MoE since the Transformers multi-headed attention can be seen as a form of MoE (Peng et al., 2020). In Vision, MTL can also improve with optimization (Sener & Koltun, 2018) or gradient-based approaches (Chen et al., 2017; Yu et al., 2020). Active Learning, Task Selection and Sampling. Ikhwantri et al. (2018) examined multi-task active learning for neural semantic role labeling in a low resource setting, using entity recognition as the sole auxiliary task. They used uncertainty sampling for active learning and found that 12% less data could be used compared to passive learning. Reichart et al. (2008) has examined different active learning techniques for the two task annotation scenario, focusing on named entity recognition and syntactic parse tree annotations. In contrast, here we examine the larger scale data regime, the modularization of a multi-task neural architecture, and the many task (≫2) setting among other differences. Other than MTAL (Reichart et al., 2008; Ikhwantri et al., 2018), Kendall et al. (2017) leveraged model uncertainty to balance MTL losses but not to select tasks as is proposed here. 4https://github.com/Wluper/edm 19Published as a conference paper at ICLR 2021 A.4 Z ERO -SHOT RESULTS ON SCITAIL AND SNLI Before testing models on domain adaptation in section 4.4, we ran zero-shot evaluations on the development set of SciTail and SNLI. Table 8 outlines8-task CA-MTLBERT-BASE’s zero-shot transfer abilities when pretrained on GLUE with our MTL approach. We expand the task embedding layer to accommodate an extra task and explore various embedding initialization. We found that reusing STS-B and MRPC task embeddings worked best for SciTail and SNLI respectively. Table 8: CA-MTL is ﬂexible and extensible to new tasks. However, CA-MTL is sensitive to the new task’s embedding. We tested multiple task embeddings that worked best on either SciTail or SNLI by checking performance in a zero shot setting or using 0% of the data. Initialization of new SciTail SNLI task embedding layer 0% of data 0% of data CoLA’s embeddings 43.0 34.0 MNLI’s embeddings 24.2 33.0 MRPC’s embeddings 34.5 45.5 STS-B’s embeddings 46.9 33.2 SST-2’s embeddings 25.8 34.2 QQP’s embeddings 31.7 37.3 QNLI’s embeddings 32.0 38.0 RTE’s embeddings 32.3 40.6 WNLI’s embeddings 29.0 30.4 Average 28.7 37.7 Random initialization 46.8 34.0 Xavier initialization 29.8 37.6 A.5 M ORE EXPERIMENTAL DETAILS We used a batch size of 32 and a seed of 12 in all experiments. We used Adam (Kingma & Ba, 2015) as the optimizer with a learning rate of 2e-5. We applied a learning rate decay with warm up over the ﬁrst 10% of the training steps. Unless otherwise speciﬁed, we used 5 epochs, a seed of 12 and a sequence length of 128. Additional details are outlined in section . Our data prepossessing and linear decoder heads are the same as in Devlin et al. (2018). We used the same dropout rate of 0.1 in all layers. To run our experiments, we used either four NVIDIA P100 GPU for base models or four NVIDIA V100 GPU for larger ones. We did not perform parameter search. We do not use ensemble of models or task-speciﬁc tricks (Devlin et al., 2018; Liu et al., 2019b; Clark et al., 2019c). All models are either 12 Transformer layers for BASE and 24 Transformer layers for LARGE. Apart from CA-MTL, models trained in multi-task learning (BERT or RoBERTa without adapters) used random task sampling. For Table 1 and Figure 7, all BERT-based model have half their layers frozen (untrained) for a fair comparison of ablation results. For the 24-task MTL and CA-MTL models in Tables 4 and 5, we increased the input sequence length to 256 and used 8 epochs. A.6 T HE DIRECT SUM OPERATOR In section 2.1.1, we used the direct sum operator ⊕. This operation allows us to create a block diagonal matrix. The direct sum of a matrix A∈Rn×m and B ∈Rp×q results in a matrix of size (m+ p) ×(n+ q), deﬁned as: A ⊕B = [ A 0 0 B ] =   a11 ··· a1n 0 ··· 0 ... ... ... ... ... ... am1 ··· amn 0 ··· 0 0 ··· 0 b11 ··· b1q ... ... ... ... ... ... 0 ··· 0 bp1 ··· bpq   20Published as a conference paper at ICLR 2021 A.7 B ASELINES AND OTHER EXPERIMENTAL RESULTS In this section, we present our baseline results for BERT, RoBERTa, CA-MTL as well as other models. Our single task results (ST) that we ran ourselves surpass other paper’s reported scores in Table 9. Liu et al. (2019c) reports random seed median scores for RoBERTa. However, our RoBERTa ST baseline matches or surpasses the original paper’s scores4 out 7 times on the development set when scores are comparable (QQP F1 and STS-B spearman are not reported). Table 9: F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. ST=Single Task, MTL=Multitask. *QNLI v1 (we report v2) **F1 score or Spearman’s correlation is not reported. ***Unknown random seeds. Results from: 1Stickland et al. (2019) 2Liu et al. (2019b) 3Phang et al. (2018) 4Liu et al. (2019c). Method Total Trained GLUE params params/task CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Dev set Results PALs+Anneal Samp.1 1.13× 12.5% – – – – – – – – 81.70 8-task CA-MTLBERT-BASE (ours) 1.12× 5.6% 60.9 82.7/83.1 88.9 90.7 90.3 79.1 91.9 88.8 84.03 BERT LARGE Models — Dev set Results ST BERT-LARGE2 9× 100% 60.5 86.7/85.9 89.3 92.7* 89.3 70.1 94.9 86.5 84.0 ST BERT-LARGE3 9× 100% 62.1 86.2/86.2 92.3 89.4 88.5 70.0 92.5 90.1 84.1 ST BERT-LARGE (ours) 9× 100% 63.6 86.5/86.0 91.4 91.0 88.5 70.2 94.7 88.2 84.5 24-task CA-MTLBERT-LARGE (ours) 1.12× 5.6% 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa LARGE Models — Dev set Results RoBERTa-LARGE4 9× 100% 68.0 90.2 90.9 94.7 ** 86.6 96.4 ** –(Median 5 runs)*** ST RoBERTa-LARGE (ours) 9× 100% 68.3 89.2/88.9 92.6 94.8 84.6 87.0 96.4 91.7 88.2 24-task CA-MTLRoBERTa-LARGE (ours) 1.12× 5.6% 69.7 89.4/89.3 93.9 94.9 88.8 91.0 96.2 91.0 89.4 A.8 S OME RESULTS ON LAYER FREEZING AND WITH FULL BLOCK ATTENTION . All experiments in this section were run for only 5 epochs, exclusively on the GLUE dataset for the large BERT-based 8-task CA-MTL model. Results in Table 10 reveal that as we freeze more layers, performance tends to decrease. However, since we wanted to preserve as much pretrained knowledge as possible, we chose to keep at least 50% of layers frozen. While this has slightly lowered our performance on 9 GLUE tasks, we believe that keeping as much of the original pretrained weights is beneﬁcial when increasing the total number of tasks in MTL to 24 or more tasks. However, we did not explore this hypothesis more. Table 10: 8-task CA-MTLBERT-LARGE (see section 4.3) for various layer freezing conﬁgurations. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. FBA = Full Block Attention Method % frozen # tasks GLUE layers g.e ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg LARGE Models — Dev set Results ST BERT-LARGE (ours) 0% — 63.6 86.5/86.0 91.4 91.0 88.5 70.2 93.1 88.2 84.3 CA-MTL 0% 7 60.2 86.2/86.0 92.0 91.5 88.7 76.3 93.3 89.5 84.9 CA-MTL 25% 6 63.7 86.1/85.8 89.1 91.2 88.6 79.7 92.9 88.5 85.1 CA-MTL 50% 3 63.2 85.5/85.5 91.8 90.9 88.3 81.4 93.0 90.1 85.5 CA-MTL FBA 50% 0 60.2 81.7/81.1 88.0 85.8 85.7 78.7 88.6 87.1 81.8 A.9 D ATASET DESCRIPTION The datasets that were used for the domain adaptation experiments were SciTail 5 and SNLI6. We jointly trained a CA-MTLRoBERTa-LARGE model on 9 GLUE tasks, 8 Super-GLUE7 tasks, 6 MRQA8 tasks, and on WNUT20179 (Derczynski et al., 2017). All GLUE tasks are binary classiﬁcation, except STS-B (regression) and MNLI (three classes). We used the same GLUE data preprocessing as in Devlin et al. (2018). 5https://allenai.org/data/scitail; Leaderboard can be found at: https://leaderboard.allenai.org/scitail/submissions/public 6https://nlp.stanford.edu/projects/snli/ 7https://super.gluebenchmark.com/tasks 8https://github.com/mrqa/MRQA-Shared-Task-2019 9https://github.com/leondz/emerging_entities_17 21Published as a conference paper at ICLR 2021 Table 11: GLUE (Wang et al., 2018) dataset description. References: 1Warstadt et al. (2018), 2Socher et al. (2013), 3Dolan & Brockett (2005), 4Cer et al. (2017), 5Williams et al. (2018), 6Wang et al. (2018), 7Levesque (2011) Acronym Corpus |Train| Task Domain CoLA1 Corpus of Linguistic Acceptability 8.5K acceptability miscellaneous SST-22 Stanford Sentiment Treebank 67K sentiment detection movie reviews MRPC3 Microsoft Research Paraphrase Corpus 3.7K paraphrase detection news STS-B4 Semantic Textual Similarity Benchmark 7K textual similarity miscellaneous QQP Quora Question Pairs 364K paraphrase detection online QA MNLI5 Multi-Genre NLI 393K inference miscellaneous RTE6 Recognition Textual Entailment 2.5K inference/entailment news, Wikipedia WNLI7 Winograd NLI 634 coreference ﬁction books Table 12: Super-GLUE (Wang et al., 2019b) dataset description. References:1Clark et al. (2019a),2de Marneffe et al. (2019), 3Gordon et al. (2012), 4Khashabi et al. (2018), 5Zhang et al. (2018), 6Wang et al. (2019b), 7Poliak et al. (2018), 8Levesque (2011) Acronym Corpus |Train| Task Domain BoolQ1 Boolean Questions 9.4K acceptability Google queries, Wikipedia CB2 CommitmentBank 250 sentiment detection miscellaneous COPA3 Choice of Plausible Alternatives 400 paraphrase detection blogs, encyclopedia MultiRC4 Multi-Sentence Reading Comprehension5.1K textual similarity miscellaneous ReCoRD5 Reading Comprehension 101K paraphrase detection news and Commonsense Reasoning RTE6 Recognition Textual Entailment 2.5K inference news, Wikipedia WiC7 Word-in-Context 6K word sense disambiguationWordNet, VerbNet WSC8 Winograd Schema Challenge 554 coreference resolution ﬁction books Table 13: MRQA (Fisch et al., 2019) dataset description. References: 1Rajpurkar et al. (2016a), 2Trischler et al. (2017), 3Joshi et al. (2017), 4Dunn et al. (2017), 5Yang et al. (2018), 6Kwiatkowski et al. (2019) Acronym Corpus |Train| Task Domain SQuAD1 Stanford QA Dataset 86.6K crowdsourced questions Wikipedia NewsQA2 NewsQA 74.2K crowdsourced questions news TriviaQA3 TriviaQA 61.7K trivia QA web snippets SearchQA4 SearchQA 117.4K Jeopardy QA web snippets HotpotQA5 HotpotQA 72.9K crowdsourced questions Wikipedia Natural Questions6 Natural Questions 104.7K search logs Wikipedia SuperGLUE has a more diverse task format than GLUE, which is mostly limited to sentence and sentence-pair classiﬁcation. We follow the same preprocessing procedure as in Wang et al. (2019b). All tasks are binary classiﬁcation tasks, except CB (three classes). Also, WiC and WSC are span based classiﬁcation tasks. We used the same modiﬁed MRQA dataset and preprocessing steps that were used in Joshi et al. (2019). All MRQA tasks are span prediction tasks which seeks to identify start and end tokens of an answer span in the input text. Table 14: SNLI (Bowman et al., 2015) and SciTail (Khot et al., 2018) datasets description. Acronym Corpus |Train| Task Domain SNLI1 Stanford Natural Language Inference 550.2k inference human-written English sentence pairs SciTail2 Science and Entailment 23.5K entailment Science question answering SNLI is a natural inference task where we predict three classes. Examples of three target labels are: Entailment, Contradiction, and Neutral (irrelevant). SciTail is a textual entailment dataset. The hypotheses in SciTail are created from multiple-choice science exams and the answer candidates (premise) are extracted from the web using information retrieval tools. SciTail is a binary true/false classiﬁcation tasks that seeks to predict whether the premise entails the hypothesis. The two datasets are used only for domain adaptation in this study (see section A.4 for the details of our approach). 22",
      "meta_data": {
        "arxiv_id": "2009.09139v3",
        "authors": [
          "Jonathan Pilault",
          "Amine Elhattami",
          "Christopher Pal"
        ],
        "published_date": "2020-09-19T02:04:34Z",
        "pdf_url": "https://arxiv.org/pdf/2009.09139v3.pdf",
        "github_url": "https://github.com/CAMTL/CA-MTL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses challenges in Multi-Task Learning (MTL) for NLP, such as overfitting to low-resource tasks, catastrophic forgetting, negative task transfer, parameter inefficiency, and knowledge loss during fine-tuning. It proposes Conditionally Adaptive Multi-Task Learning (CA-MTL), a novel Transformer-based Adapter with a new conditional attention mechanism and task-conditioned modules that facilitate weight sharing. The approach fixes half of the pretrained model weights to mitigate forgetting and introduces an uncertainty-based multi-task data sampling strategy to balance data imbalance across tasks. CA-MTL outperforms single-task fine-tuning and other adapter methods on GLUE, achieving competitive performance across 26 NLP tasks and state-of-the-art results on several test and development sets. The work is the first to explore latent task representations to modularize and adapt pretrained architectures and to examine uncertainty sampling for large-scale multi-task learning in NLP.",
        "methodology": "The methodology centers on a Task Conditioned Transformer architecture that adapts and modulates pretrained weights using task embeddings (zi). This is achieved through: 1. Conditional Weight Transformations: Learned functions (γi, βi) transform weights based on task embedding zi. 2. Conditional Attention: A block-diagonal conditional attention mechanism M(zi) is introduced to account for task-specific biases. 3. Conditional Alignment: A task-conditioned alignment layer ˆR(zi) is added between the input embedding and the first Transformer layer, generating a single alignment matrix as a function of zi. 4. Conditional Layer Normalization (CLN): Extends Conditional Batch Normalization to Layer Normalization, modulating output based on task's latent representation. 5. Conditional Bottleneck: A task-conditioned two-layer feed-forward bottleneck layer facilitates weight sharing and task-specific information flow. The CA-MTL approach freezes the bottom half of Transformer layers to preserve pretraining knowledge. For task balancing, Multi-Task Uncertainty Sampling (MT-Uncertainty Sampling) is used, an active learning-inspired strategy that employs Shannon Entropy to select training examples, prioritizing tasks with higher uncertainty and dynamically managing task difficulty to avoid catastrophic forgetting.",
        "experimental_setup": "The method was evaluated on 26 NLP tasks, including the GLUE benchmark (9 tasks), Super-GLUE (8 tasks), MRQA (6 tasks), and WNUT2017 (Named Entity Recognition). Domain adaptation experiments were conducted on SciTail and SNLI. Baselines included single-task fine-tuning (ST), various MTL models (e.g., BERTBASE MTL with random sampling), and other BERT-based adapter networks (Adapters-256, PALs+Anneal Sampling). Validation involved ablation studies of CA-MTL components, comparisons of MT-Uncertainty Sampling with other task sampling strategies (Counterfactual, Task size, Random), analysis of task covariance similarity, and performance evaluation under varying numbers of tasks (scaling task count). Metrics included average GLUE scores, F1, Spearman's correlation, Matthew's correlation, and accuracy. The implementation used HuggingFace, Adam optimizer (2e-5 LR with warm-up), a batch size of 32, a seed of 12, 5 or 8 epochs, and a sequence length of 128 or 256. Experiments were run on NVIDIA P100/V100 GPUs, without parameter search or model ensembles, and with half of the Transformer layers typically frozen.",
        "limitations": "Keeping 50% of the Transformer layers frozen, while intended for knowledge preservation, was observed to slightly lower performance on 9 GLUE tasks, although it's hypothesized to be beneficial for larger task counts (24+). The Evolutionary Data Measures (EDM) used to estimate task difficulty were noted to lack precision for regression-like values, being more effective for assigning a class of difficulty. For the Named Entity Recognition (NER) task, CA-MTL, while significantly outperforming the MTL baseline, had not yet overfit and could have potentially closed the performance gap with ST baselines with more training cycles. During zero-shot transfer, the method was found to be sensitive to the initialization of the new task's embedding, requiring selection of the 'best' pretrained embedding. Unlike some baselines (e.g., MT-DNNSMART), CA-MTL does not incorporate context from semantic similarity models.",
        "future_research_directions": "Future work will involve extending the ideas of task-conditioned adaptive learning. Further investigation into the impact of freezing more original pretrained weights when scaling to a larger number of tasks (e.g., 24 or more) is also an objective. Additionally, exploring the precision of task difficulty measures and longer training cycles for specific tasks where CA-MTL has not yet reached its full potential compared to single-task baselines could be beneficial.",
        "experimental_code": "import torchimport numbersimport torch.nn as nnimport torch.nn.functional as Fclass FiLM(nn.Module):    \"\"\" Feature-wise Linear Modulation (FiLM) layer\"\"\"    def __init__(self, input_size, output_size, num_film_layers=1, layer_norm=False):        \"\"\"        :param input_size: feature size of x_cond        :param output_size: feature size of x_to_film        :param layer_norm: true or false        \"\"\"        super(FiLM, self).__init__()        self.input_size = input_size        self.output_size = output_size        self.num_film_layers = num_film_layers        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None        film_output_size = self.output_size * num_film_layers * 2        self.gb_weights = nn.Linear(self.input_size, film_output_size)        self.gb_weights.bias.data.fill_(0)    def forward(self, x_cond, x_to_film):        gb = self.gb_weights(x_cond).unsqueeze(1)        gamma, beta = torch.chunk(gb, 2, dim=-1)        out = (1 + gamma) * x_to_film + beta        if self.layer_norm is not None:            out = self.layer_norm(out)        return outclass CBDA(nn.Module):    \"\"\" Conditional Block Diagonal Attention (CBDA) layer\"\"\"    def __init__(self, input_size, output_size, blocks=1, num_film_layers=1, layer_norm=False):        \"\"\"        :param input_size: feature size of x_cond        :param output_size: feature size of x_to_film        :param layer_norm: true or false        \"\"\"        super(CBDA, self).__init__()        self.input_size = input_size        self.output_size = output_size        self.num_film_layers = num_film_layers        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None        self.blocks = blocks        film_output_size = self.output_size * num_film_layers * 2        self.gb_weights = nn.Linear(self.input_size, film_output_size)        self.gb_weights.bias.data.fill_(0)    def forward(self, x_cond, x_to_film):        gb = self.gb_weights(x_cond).unsqueeze(1)        gamma, beta = torch.chunk(gb, 2, dim=-1)        out = (1 + gamma) * x_to_film + beta        if self.layer_norm is not None:            out = self.layer_norm(out)        out = [torch.block_diag(*list(out_b.chunk(self.blocks, 0))) for out_b in out]        out = torch.stack(out)        return out[:, :, :out.size(1)]class ConditionalLayerNorm(nn.Module):    r\"\"\"Applies Conditional Layer Normalization over a mini-batch of inputs.    .. math::        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma(z) + \\beta(z)    The mean and standard-deviation are calculated separately over the last    certain number dimensions which have to be of the shape specified by    :attr:`normalized_shape`.    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.    .. note::        Unlike Batch Normalization and Instance Normalization, which applies        scalar scale and bias for each entire channel/plane with the        :attr:`affine`, Layer Normalization applies per-element scale and        bias with :attr:`elementwise_affine`.    This layer uses statistics computed from input data in both training and    evaluation modes. The affine transformation is molulated by a conditional tensor.    In our case, we use task embeddings z.    Args:        normalized_shape (int or list or torch.Size): input shape from an expected input            of size            .. math::                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]            If a single integer is used, it is treated as a singleton list, and this module will            normalize over the last dimension which is expected to be of that specific size.        eps: a value added to the denominator for numerical stability. Default: 1e-5        elementwise_affine: a boolean value that when set to ``True``, this module            has learnable per-element affine parameters initialized to ones (for weights)            and zeros (for biases). Default: ``True``.    Shape:        - Input: :math:`(N, *)`        - Output: :math:`(N, *)` (same shape as input)    Examples::        >>> input_ = torch.randn(20, 5, 10, 10)        >>> condition = torch.randn(20, 10)        >>> # With Learnable Parameters        >>> m = ConditionalLayerNorm([10, 10])        >>> # Normalize over last dimension of size 10        >>> m = nn.LayerNorm(10)        >>> # Activating the module        >>> output = m(input_, condition)    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450    .. _`Conditional Layer Normalization`: https://arxiv.org/    \"\"\"    __constants__ = ['normalized_shape', 'condition_size', 'weight', 'bias', 'eps']    def __init__(self, normalized_shape, condition_size, eps=1e-5):        super(ConditionalLayerNorm, self).__init__()        if isinstance(normalized_shape, numbers.Integral):            normalized_shape = (normalized_shape,)        self.normalized_shape = tuple(normalized_shape)        self.condition_size = condition_size        self.eps = eps        self.weight = nn.Parameter(torch.Tensor(*normalized_shape))        self.ln_weight_modulation = FiLM(condition_size, sum(normalized_shape))        self.bias = nn.Parameter(torch.Tensor(*normalized_shape))        self.reset_parameters()    def reset_parameters(self):        nn.init.ones_(self.weight)        nn.init.zeros_(self.bias)    def forward(self, input_, condition, task_id):        unique_task_ids = torch.unique(task_id)        cln_output = torch.zeros_like(input_)        for unique_task_id in unique_task_ids:            task_id_filter = task_id == unique_task_id            task_emb = condition[task_id_filter][0].unsqueeze(0)            weight = self.ln_weight_modulation(task_emb, self.weight).view(-1)            cln_output[task_id_filter] = F.layer_norm(input_[task_id_filter], self.normalized_shape, weight, self.bias, self.eps)        return cln_output    def extra_repr(self):        return '{normalized_shape}, {condition_size}, eps={eps}'.format(**self.__dict__)class ConditionalBottleNeck(nn.Module):    \"\"\"Down projection and up projection with FiLM layers within Transformer layer.\"\"\"    def __init__(self, config):        super(ConditionalBottleNeck, self).__init__()        self.emb_transf = nn.Linear(config.hidden_size, config.hidden_size)        self.hidden_modulation = FiLM(config.hidden_size, config.hidden_size)        self.down_proj_layer = nn.Linear(config.hidden_size, config.hidden_size//3)        self.up_proj_layer = nn.Linear(config.hidden_size//3, config.hidden_size)    def forward(self, x_cond, hidden_states):        x_cond = self.emb_transf(x_cond)        hidden_states = self.hidden_modulation(x_cond=x_cond, x_to_film=hidden_states)        hidden_states = self.down_proj_layer(hidden_states)        hidden_states = self.up_proj_layer(hidden_states)        return hidden_states",
        "experimental_info": "The methodology centers on a Task Conditioned Transformer architecture that adapts and modulates pretrained weights using task embeddings (zi). This is achieved through: 1. Conditional Weight Transformations: Learned functions (γi, βi) transform weights based on task embedding zi, implemented via FiLM layers. 2. Conditional Attention: A block-diagonal conditional attention mechanism M(zi) is introduced to account for task-specific biases, implemented via the CBDA layer that modulates a `random_weight_matrix` and adds it to the original attention scores. 3. Conditional Alignment: A task-conditioned alignment layer `R(zi)` is added between the input embedding and the first Transformer layer, implemented by a FiLM layer applied to the `embedding_output`. 4. Conditional Layer Normalization (CLN): Extends Conditional Batch Normalization to Layer Normalization, modulating output based on the task's latent representation. 5. Conditional Bottleneck: A task-conditioned two-layer feed-forward bottleneck layer facilitates weight sharing and task-specific information flow.For the `CA-MTL-base` model, layers 0-5 are standard BERT layers with Conditional Block Diagonal Attention (CBDA) in their attention mechanism, but no Conditional Layer Normalization (CLN) in their self-output or feed-forward output. Layers 6-10 utilize CBDA in attention and apply CLN in both their self-attention output (`MyBertSelfOutput9`) and feed-forward output (`MyBertOutput9`). The final layer (layer 11) is a `MyBertAdapterLayer9` which integrates an attention mechanism with CBDA and CLN, followed by a feed-forward network with CLN, and finally incorporates a `BertAdapter9` (Conditional Bottleneck) with a skip connection and an additional CLN.For the `CA-MTL-large` model, layers 0-11 are standard BERT layers with CBDA in their attention but no CLN in their self-output or feed-forward output. Layers 12-23 utilize CBDA in attention and apply CLN in both their self-attention output (`MyBertSelfOutput10`) and feed-forward output (`MyBertOutput10`). Additionally, all layers (0-23) in the `CA-MTL-large` model's encoder are equipped with a `ConditionalBottleNeck` which is applied with a skip connection (`hidden_states + hidden_film`) in a progressive manner throughout the encoder.The CA-MTL approach includes freezing mechanism: users can specify a range of encoder layers (`<start_layer>-<end_layer>`) to freeze. However, specific modules within these layers (e.g., `random_weight_matrix` for CBDA, `film.gb_weights` for FiLM, `ln_weight_modulation.gb_weights` for CLN, and `adapter` for Conditional Bottleneck) are explicitly unfrozen to allow for task-specific adaptation.For task balancing, Multi-Task Uncertainty Sampling (MT-Uncertainty Sampling) is used. This active learning-inspired strategy employs Shannon Entropy, calculated from the decoder's softmax probabilities, to select training examples. The batch entropy is normalized by the mean entropy across tasks (`test_batch_entropy_mean / max_mean_batch_entropy`) to prioritize tasks with higher uncertainty and dynamically manage task difficulty. Training arguments `use_mt_uncertainty` (boolean) enables this sampling method, `uniform_mt_sampling` (boolean) allows sampling each task an equal amount, and `percent_of_max_data_size` (float) controls the proportion of data sampled per task based on the largest dataset. The learning rate schedule uses `warmup_proportion` (float) for linear warmup."
      }
    },
    {
      "title": "Sparse High Rank Adapters",
      "abstract": "Low Rank Adaptation (LoRA) has gained massive attention in the recent\ngenerative AI research. One of the main advantages of LoRA is its ability to be\nfused with pretrained models, adding no overhead during inference. However,\nfrom a mobile deployment standpoint, we can either avoid inference overhead in\nthe fused mode but lose the ability to switch adapters rapidly, or suffer\nsignificant (up to 30% higher) inference latency while enabling rapid switching\nin the unfused mode. LoRA also exhibits concept-loss when multiple adapters are\nused concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA),\na new paradigm which incurs no inference overhead, enables rapid switching, and\nsignificantly reduces concept-loss. Specifically, SHiRA can be trained by\ndirectly tuning only 1-2% of the base model weights while leaving others\nunchanged. This results in a highly sparse adapter which can be switched\ndirectly in the fused mode. We further provide theoretical and empirical\ninsights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing\nconcept loss. Our extensive experiments on LVMs and LLMs demonstrate that\nfinetuning only a small fraction of the parameters in the base model\nsignificantly outperforms LoRA while enabling both rapid switching and\nmulti-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA\nimplementation based on Parameter-Efficient Finetuning (PEFT) Library which\ntrains at nearly the same speed as LoRA while consuming up to 16% lower peak\nGPU memory, thus making SHiRA easy to adopt for practical use cases. To\ndemonstrate rapid switching benefits during inference, we show that loading\nSHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU.",
      "full_text": "Sparse High Rank Adapters Kartikeya Bhardwaj∗§ Nilesh Prasad Pandey∗† Sweta Priyadarshi† Viswanath Ganapathy† Shreya Kadambi Rafael Esteves Shubhankar Borse Paul Whatmough § Risheek Garrepalli Mart Van Baalen Harris Teague § Markus Nagel§ Qualcomm AI Research‡ §{kbhardwa,pwhatmou,hteague,markusn}@qti.qualcomm.com Abstract Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5×-16× faster than LoRA fusion on a CPU.¶ 1 Introduction Low Rank Adaptation (LoRA) [13] is an established technique to tune the behavior of large generative models such as Large Language Models (LLMs) [30, 29] and Stable Diffusion [24, 22]. As the name suggests, LoRA requires very few parameters since it trains low rank projection weights that consume very low memory during the finetuning process while producing excellent results. Moreover, these low rank weights can be fused analytically into the base model, thereby incurring no additional overhead during inference. Despite its success, there are still several limitations of low rank adaptation methods. First, if LoRA parameters are fused into the corresponding pretrained base model weights, they modify the entire weight tensor. Therefore, deploying LoRA on large models such as LLaMA-1/2 (7B+ parameters) or Stable Diffusion (1.5B+ parameters) on mobile devices would require changing a large number of weights during inference. Consequently, for mobile scenarios, if an application requiresrapid adapter switching, existing low rank methods would incur a significant memory and latency cost. This is a major deployment challenge because, unlike large GPUs, local memory of small AI accelerators is limited and cannot store all weights at the same time. These challenges can be partially addressed by ∗Equal contribution. †Work done while employed at Qualcomm AI Research. ‡Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. ¶Code: https://github.com/Qualcomm-AI-research/SHiRA. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.13175v2  [cs.LG]  27 Jan 2025LoRA Car  dog in space  Elephant  House, mountain  Thunder Bird  knight                                      BLUEFIRE SHiRA-SNIP                                      PAINTINGS                                       MULTI-ADAPTER Figure 1: Sparse High Rank Adapters (SHiRA): Changing about 1-2% weights of the pretrained generative model is often sufficient to achieve high performance. Due to its extreme sparsity, SHiRA enables rapid switching and also reduced concept loss during multi-adapter fusion. In contrast, LoRA modifies majority of parameters when fused, thus prohibiting rapid switching on mobile devices, and also experiences concept loss during multi-adapter fusion. For LoRA, elephant for single “paintings” adapter case has artifacts (extra/broken tusks); bird and knight for multi-adapter case lose “paintings” concept and keep only the “blue fire” effects. SHiRA does not experience these issues. running LoRA in unfused mode; however, unfused inference can incur as high as30% additional latency compared to the base model [1] (see section 2.1 for details). This increased inference time in unfused mode and time for adapter switching significantly hampers user experience; hence, this is an important problem which has been a focus of recent research by various industries [9]. Second, LoRA has a well-known limitation called concept loss when using multiple concurrent adapters, e.g., combining multiple style transfer adapters, etc. Specifically, it has been well documented [34, 26, 8] that a simple additive merging of multiple LoRA adapters leads to concept loss of one or more adapters. Finally, recent literature also contributes important theoretical and empirical knowledge towards the value of high rank adapters. For instance, Kalajdzievski [16] shows that the high rank adapters can greatly outperform low rank adapters when used with correct scaling factors. This calls for further investigation into whether other high rank adapters would significantly outperform LoRA. In view of the above, we address the following key problems in this paper: (i) How can we perform rapid switching for fused adapters? ( ii) Is there a simpler solution for multi-adapter fusion to reduce concept loss? (iii) Can we build high rank adapters that have high expressive power without significantly increasing the training or inference costs? To this end, we propose Sparse High Rank Adapters (SHiRA), a single solution to all three problems above. SHiRA is a highly sparse but a high rank adapter which relies on training only a very small subset of parameters from the original pretrained network. One of the crucial insights we demonstrate is that even finetuning merely 1-2% parameters of the pretrained generative model is sufficient to achieve high performance on many adapter tasks (see Fig. 1). However, unlike LoRA layers that modify all parameters in the weight tensors in the fused mode, SHiRA still keeps a very low percentage of parameters that need to be switched, thus enabling rapid switching at inference time. Moreover, since the pretrained weights are huge, SHiRA being a very sparse adapter greatly aids multi-adapter fusion by significantly reducing concept loss. Finally, we theoretically and emprically analyze the high rank vs. sparsity properties of SHiRA and why that helps with adapter performance. Overall, we make the following key contributions: • We propose SHiRA, a new high rank adapter paradigm to demonstrate that changing as few as 1-2% parameters of the original network is sufficient for adaptation. Our crucial insight is that even the most basic masking criteria (to identify the top 1-2% parameters) enable SHiRA to significantly outperform LoRA on diverse vision and language tasks. • SHiRA enables on-device rapid adapter switching and provides a natural multi-adapter fusion technique due to high sparsity, thus, significantly reducing concept loss. We also theoretically analyze SHiRA through the lens of high rank adaptation vs. sparsity. • We conduct extensive experiments on LLMs (LLaMA-7B, LLaMAv2-7B) and LVMs (Stable Diffusion, SDXL) where we demonstrate that SHiRA significantly outperforms LoRA on both single- and multi-adapter tasks. On LLMs, we show that SHiRA achieves up to 2.7% better accuracy than LoRA on commonsense reasoning. SHiRA also complements advanced variants of LoRA such as DoRA [20] and can be easily applied on top of them. 2• Finally, on the training side, we provide a PEFT-based latency- and memory-efficient implementation for SHiRA which trains nearly as fast as standard LoRA while consuming 16% lower peak GPU memory. Beyond PEFT, we provide a simple way to turn any trainer into SHiRA finetuning. For inference, we demonstrate that SHiRA weights can be loaded on a CPU up to 5×-16× faster than equivalent LoRA fusing, thereby enabling rapid switching. The rest of this paper is organized as follows: section 2 presents the background and related work. We propose SHiRA in section 3 while describing its theoretical properties in section 4. We then conduct extensive experiments for SHiRA in section 5. Finally, we discuss the key findings in section 6 and conclude the paper in section 7. 2 Background and Related Work 2.1 Background: Edge Deployment Challenges for LoRA There are three existing deployment options for LoRA: (i) fuse the adapter offline and then deploy on-device: this changes a large fraction of the weight tensors compared to base model which prohibits rapid switching since it will increase DRAM traffic considerably; (ii) keep the adapter unfused and run the inference in unfused mode: this can help with rapid switching but would incur significant addi- tional (up to 30% higher) latency as shown in [1] since we would have LoRA branches in the forward pass during inference; (iii) use the Huggingface/Diffusers pipeline [1] (built for server-grade GPUs) for mobile inference. This pipeline consists of load→fuse→inference→unfuse→unload to switch adapters. Here, unfused LoRA-A and LoRA-B weights (see Fig. 2(a)) are first loaded into the memory and then fused into the base model by computing Wnew = W + AB; this new weight is used for inference. To switch the adapter, we can unfuse the adapter as W = Wnew − AB and then unload existing LoRA weights to load the new ones. We provide further evidence in Appendix A to demonstrate that such a pipeline is not feasible for edge devices. This is primarily because edge devices are memory-limited and not all weights of large generative models can be stored in the local memory at the same time. Hence, loading and fusing needs to happen layerwise on a mobile device that obviously results in massive inference latency costs. 2.2 Related Work LoRA, its variants, and sparse adapters. Many LoRA variants exist in literature: DoRA [ 20], LoRA+ [11], VeRA [17], LoRA-FA [35], RS-LoRA [16], among many others. The crucial difference between this literature and our work is that we develop a high rank adapter without increasing training and inference costs. Also, for such methods, the final fused adapter still updates all elements in the pretrained weight tensor, thus prohibiting rapid switching. Moreover, for completeness, we will also show that SHiRA is orthogonal to and can be applied on top of some of the latest, more advanced LoRA variants such as DoRA [20] while preserving the benefits of rapid switching. A few other LoRA variants have also explored a combination of sparsity and low rank adaptation. Ex- amples include RoSA [21], SoRA [6], Sparse-Adapters [12], etc. Among these, Sparse-Adapters [12] explores the use of popular pruning techniques (e.g., SNIP [19]) to prune out adapters to improve their efficiency. SoRA [6] proposes an adaptive rank version of LoRA by gating elements of down and up projection layers and pruning out the zero entries at inference. Finally, RoSA [21] combines a sparse adapter with a low rank one to achieve some high rank benefits. However, since they combine their method with LoRA, the fused adapter weight still overwrites the entire pretrained weight tensor. Partial Finetuning. Our work is most closely related to partial finetuning techniques that were mostly proposed in the pre-LoRA era [36, 28, 3, 33, 10]. These methods use a mix of fixed sparse masks [28] or learned masks [36, 10] to finetune a pretrained network. Note that, these techniques have been mostly explored for relatively small language models, and not for recent LLMs and diffusion models. Since the LoRA models exploded in popularity, it has been unclear if other sparse finetuning techniques would achieve comparable results to LoRA on generic adapter tasks, particularly in the vision domain. One significant limitation of partial finetuning, as opposed to LoRA-based methods, is its high GPU memory consumption, making it impractical to be used for large generative models. Consequently, the reduced memory consumption for finetuning was a key factor to LoRA’s success and its widespread adoption. To this end, we provide a memory- and latency-efficient PEFT-based implementation for SHiRA which trains as efficiently as LoRA, thus requiring significantly lower memory consumption compared to prior partial finetuning techniques. Further, we explore the effectiveness of sparse finetuning on both large language and vision models and provide a detailed analysis on rapid switching and multi-adapter fusion of the high rank adapters. 3Backward Pass Forward Pass Trainable weightsFrozen weights Weights 1 1 111 11 1 Masked GradientsMaskOriginal Gradients Linear Layer Input Features Output Features Non-Zero GradientsZero Gradients Pretrained Weights, W b.Sparse High Rank Adaptation(SHiRA)a.Low Rank Adaptation(LoRA) LoRA-A LoRA-B + Rank r h xFused weight at inference would modify all elements of pretrained weight W⊙= Figure 2: (a) LoRA when fused into the pretrained model modifies all weights and prevents rapid adapter switching. (b) SHiRA does not require additional weights during training but finetunes very few pretrained weights. Our approach relies on a sparse mask for gradient-masking during training. We show that finetuning as low as1-2% parameters is sufficient to achieve high accuracy. A notable concurrent work is SpIEL [4] which scales partial finetuning to modern LLMs and also has a PEFT implementation that results in comparable speed and memory as LoRA. The main differences between SpIEL and SHiRA are as follows: (i) SpIEL works with dynamic masks while SHiRA uses a static mask. (ii) Dynamic mask in SpIEL requires users to install custom sparse linear layer kernels for the GPUs. In contrast, SHiRA does not require installing any custom kernels and directly works with native Pytorch. Hence, SHiRA’s biggest advantage is its ease of training/inference deployment. (iii) We also analyze multi-adapter fusion properties, e.g., impact of sparsity on orthogonality between adapters, which were not discussed in SpIEL. (iv) Finally, SHiRA demonstrates its effectiveness on both vision and language tasks, whereas SpIEL only discusses the language tasks. Multi-Adapter Fusion. Existing Multi-adapter fusion methods focus on preventing concept loss [8, 34, 26]. However, these methods usually either just use the base LoRA as it is (and then perform some non-trivial postprocessing on them) [34, 26], or some create some minor variants [8]. In contrast, we introduce a new adapter for the concept loss problem where multiple concepts naturally do not interfere with each other. In that respect, our work is orthogonal to the prior multi-adapter fusion work since our adapter can be further postprocessed using such techniques. 3 Proposed Approach 3.1 S parse High Rank Adapters (SHiRA) SHiRA exploits highly sparse trainable parameters in the pretrained model. In its simplest form, our adapter can be trained by masking gradients such that only a fraction of original weights get updated. Specifically, we do not add any new weights to the forward pass like LoRA (see Fig. 2(a)) but rather make a small percentage of existing weights trainable (see Fig. 2(b) top). To this end, we first create an extremely sparse (∼98-99% zeros) mask M ∈Rn×m = {0, 1}n×m, where n, mare dimensions of the pretrained weight matrix. M is then used to mask the gradients during backpropagation using a Hadamard product (see Fig. 2(b) bottom). Thus, very few parameters get updated during training and our adapter consists of just those sparse weights. Concrete gradient masking-based and another latency-/memory-efficient PEFT implementations for SHiRA are discussed in section 3.3. We consider the following masks M (only 1-2% trainable parameters, see also Appendix B): 1. SHiRA-Struct: In this structured mask, certain rows or columns of the weight as well as its diagonal are set to be trainable. All other rows/columns are not trainable. The diagonal makes the mask high rank whereas the structured trainable rows/columns – set to 1 to enable gradient flow to corresponding parameters – lead to a rank 1 adapter. Thus, SHiRA-Struct is a combination of a high rank but very sparse adapter and a rank 1 adapter. 2. SHiRA-Rand: This mask is obtained by randomly setting 1-2% parameters as trainable. 3. SHiRA-WM: Here we pick top-K parameters to train based on their weight magnitudes (WM), the absolute value of the weight for each layer. 4SHiRAAdapter 1SHiRAAdapter 2Fused Multi-Adapter+ α2 =+ α1 Sparse WeightsIndices +Storing [Sparse Weights + Indices] consumes much less memory than pretrained weightsWeights trained for SHiRA b.Multi-adapter fusiona.Rapid adapter switching Base Model Weights that changed during adaptationFrozen weights Non-Zero Weights for SHiRAAdapter 2Zero WeightsNon-Zero Weights for SHiRAAdapter 1Base Model Weights Figure 3: (a) Rapid adapter switching: The sparse finetuned weights can be stored as weights and their indices. At inference time, these weights can be loaded on the base model weights. Since only 1-2% weights need to be overwritten, the adapter can be efficiently switched with different weights at inference, eliminating the need for a separate fusion stage. (b) Multi-adapter fusion: Concept-loss can be reduced if multiple adapters do not significantly interfere with each other. 4. SHiRA-Grad: This is a gradient-based mask. We first collect gradients on a small calibra- tion set and then pick top 1-2% weights that receive the highest gradient magnitudes. 5. SHiRA-SNIP: The SNIP metric from the pruning literature [19] combines weight magnitude and gradient strategies, i.e., SNIP equals magnitude of the gradient times the weight. 3.2 Rapid Adapter Switching, Multi-Adapter Fusion, and High Rank Since very few base weights change during the SHiRA training, we can simply extract them out and store them as sparse weights and their indices (see Fig. 3(a)). Hence, SHiRA is comparable to LoRA in model size but overwrites only a fraction of the pretrained weights at inference time. In contrast, LoRA fuses into base weights as Wnew = W + AB and changes the entire weight. Note that, we do not actually need to fuse SHiRA but rather just need to overwrite the modified value at the correct index in the pretrained weight tensor. This enables rapid switching on resource-constrained devices. To verify that SHiRA indeed provides rapid switching benefits compared to LoRA, we provide an optimized implementation based on scatter_op to overwrite base model weights instead of fusing them like LoRA. We demonstrate that on a CPU, weight loading for SHiRA adapters can be up to 5×-16× faster than equivalent LoRA fusing for inference (see Appendix C and Fig 7). Next, we discuss multi-adapter fusion in SHiRA. Given two adapters A1 and A2 with sparse masks M1 and M2, we ask the following questions: (i) What is the impact of sparsity on relative interference between adapters in the multi-adapter setting? (ii) Is it possible to create masks that result in nearly orthogonal SHiRA weights so they do not significantly interfere with each other at inference time? Getting adapters that do not interfere with each other is essential to avoid concept-loss. To this end, we define specific metrics in section 4.2 to analyze orthogonality properties between adapter weights for various SHiRA strategies. We theoretically show that at least one of the SHiRA methods, i.e., SHiRA-Struct can in fact create near-orthogonal adapters. We further experimentally demonstrate in section 5.2.2 that SHiRA-Struct indeed outperforms other methods for multi-adapter fusion. Finally, since we do not have any low rank weights in the forward pass, our proposed adapters can be high rank albeit highly sparse. We theoretically analyze the rank vs. sparsity properties in section 4. 3.3 Memory- and Latency-Efficient SHiRA Training We have created two implementations for SHiRA: (i) a backward hook-based gradient masking to turn any trainer into SHiRA finetuning (see Appendix D), and (ii) a PEFT-based implementation. As discussed in Appendix E, the PEFT-based SHiRA implementation consumes16.63% lower peak GPU memory and trains almost at a similar speed as LoRA. On the contrary, DoRA exhibits a 40.99% and 28.9% increase in memory and training time respectively compared to LoRA. 4 Theoretical Insights for SHiRA 4.1 Rank vs. Sparsity Below we discuss parameter and learning complexity, parallels between LoRA and SHiRA, as well as its optimization properties from the lens of rank and sparsity. Lemma 4.1. The parameter complexity and learning complexity of SHiRA is equal to the number of non-zero elements in the adapter. Appendix F.1 provides the proof. This lemma suggests that despite high rank property of SHiRA, it would not require significantly larger datasets to converge. 5Lemma 4.2. If we specify a sparsity factor, the LoRA is r rank approximation of SHiRA with approximation error bounded by σ2 r+1, the (r + 1)th singular value of the SHiRA adapter. The above lemma is proved in section F.2. As a consequence of this lemma, anyr rank LoRA adapter of size (m, n) can be seen as an approximation of a SHiRA adapter with mr + rn non-zero elements. Lemma 4.3. Scaling factor for SHiRA is independent of the rank of the adapter and can be set to 1. Please see the proof in Appendix F.3. Lemma 4.3 states that we do not need scaling factors to stabilize the training and, therefore, we do not need additional hyperparameters like α or independent learning rates for separate A and B matrices like in LoRA[13] or LoRA+ [11]. Of note, the scaling factor α can still be used at inference time to vary the intensity of the adapter. 4.2 Adapter Weight Orthogonality in Multi-Adapter Fusion In this section, we provide theoretical and empirical insights by studying properties of SHiRA and LoRA adapter designs for multi-adapter fusion. Lemma 4.4. Consider two adapters, ∆W1 and ∆W2. If one of the adapters, ∆W1 or ∆W2 lies in the null space of the other, then the adapters will not interfere multiplicatively. Proof is given in Appendix F.4. The above lemma implies that two adapters can be efficiently fused without interference if they are orthogonal. In order to analyze the orthogonality between any two adapter weights, we define the following metrics: Definition 1. Adapter Weight Orthogonality Magnitude (AWOM)is defined as the l2 norm of the product AT 1 A2 for two sparse adapter weights A1, A2 ∈ Rn×m. AWOM enables us to understand how far the product AT 1 A2 is from a zero matrix O ∈ Rm×m (Oi,j = {0}∀i, j). Definition 2. Adapter Weight Orthogonality Ratio (AWOR)is defined as the sparsity ratio of the product AT 1 A2. Specifically, AWOR = h 1 − \u0010 ||AT 1 A2||0 m2 \u0011i , where m2 is #elements in AT 1 A2. Together, AWOM and AWOR can provide us an idea of relative orthogonality between adapter weights A1 and A2. Next, we analyze how at least one of the SHiRA strategies (i.e., SHiRA-Struct) can result in near-orthogonal adapters. Recall that, SHiRA-Struct adapters train certain rows/columns and the diagonal elements while keeping all other parameters frozen. Hence, the final trained adapter (after subtracting the pretrained weight) contains a structured pattern of rows/columns and diagonal elements, everything else being zero. Now, without loss of generality, consider two SHiRA-Struct adapters for a layer with square m × m weights: A1 = I + S1 and A2 = I + S2, where S1 and S2 are row-wise patterns of trained weights for two different tasks, and I is an identity matrix. Also, S1 and S2 are non-overlapping, e.g., both have same number of non-zero rows but are offset from each other such that they do not have any common trained rows. Then, the following result holds: Lemma 4.5. Non-overlapping SHiRA-Struct adapters are nearly orthogonal: AWOR for non- overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. Since all SHiRA masks are highly sparse,AT 1 A2 has a lot of zeros, thus making the adapters nearly orthogonal. Proof is provided in Appendix F.5. We demonstrate the orthogonality properties of various adapters and report the simulation results in Fig. 4. For our experiment, we compute AWOM and AWOR for a variety of adapter designs - Figure 4: Comparison of average AWOM (left) and AWOR (right) for 50 randomly initialized adapters. We compare different adapters, namely - Dense, Sparse LoRA, SHiRA-WM and SHiRA-Struct. dense, sparse-LoRA [12] (sparse LoRA A and B weights), SHiRA-WM and SHiRA- Struct based adapters. As shown in Fig. 4, both dense and sparse LoRA have low AWOR for adapters with larger dimen- sions, e.g., 4096 × 4096 which is typical in LLMs. This signifies that these adapter weights are non-orthogonal. On the con- trary, SHiRA-WM achieves much higher AWOR than the LoRA variants. More inter- estingly, SHiRA-Struct is nearly orthogo- nal. Note that, due to high sparsity, AWOM also tends to be much lower for SHiRA adapters than the dense counterparts. Com- bined with the fact that AWOR of SHiRA 6adapters is 63-96% higher sparsity than LoRA, this may suggest that AT 1 A2 would be closer to zero for SHiRA adapters, thus potentially bringing them closer to orthogonality and less interference. Finally, although we have shown interesting properties for SHiRA-Struct, it is still a rank 1 + diagonal adapter. Hence, we need to tradeoff single adapter performance (which strongly depends on adapter’s expressive power) against the multi-adapter fusion capabilities. For instance, next we will see that while SHiRA-Struct is good for vision, SHiRA-SNIP performs well across both LVMs and LLMs. Remark 1. The orthogonality property shown here can lead to disentangled representation for adapter outputs before they merge into the base model. However, this property does not hold for other SHiRA masks that do not have a regular sparsity pattern like SHiRA-Struct even if other SHiRA strategies are still more orthogonal than LoRA weights (e.g., see SHiRA-WM AWOR in Fig. 4(right)). Interestingly, for unstructured sparse masks like SHiRA-WM, SHiRA-Grad, SHiRA-SNIP, etc., both overlapping and non-overlapping adapters have similar orthogonality properties. We discuss this in more detail in section 5.3.2. Finally, this analysis only focuses on orthogonality of adapter weights and not on orthogonality of subspaces. We leave the subspace analysis of SHiRA for future work. 5 Experiments 5.1 Training Setup and Datasets For the vision tasks, we use the RealisticVision-v3 model checkpoint for Stable Diffusion-v1.5, and finetune it using different adapters on two style transfer datasets collected using public domain images. The first dataset is called Bluefire which provides a “blue fire” effect to images. The second dataset is a painting dataset which gives a “paintings” effect (see Appendix section G for more details). For both these datasets, we conduct single- and multi-adapter experiments. To quantify the image quality, we use the Human Preference Score-V2 (HPSv2) [32]. On the language domain, we experiment with LLaMA 7B [29], LLaMA2-7B [30] and evaluate it on various commonsense reasoning benchmarks such as HellaSwag, PIQA, SIQA, BoolQ, Arc-easy, Arc-challenge, OpenBookQA and Winogrande. Similar to our vision investigations, we conduct single- and multi-adapter experiments on LLMs as well. Specifically, for language finetuning, we follow the setup adopted by [14, 20] for training and evaluating LoRA [13], DoRA [20], and SHiRA based finetuned models on downstream tasks. Finally, we also explore generalizability of SHiRA to other popular LoRA models and applications such as SDXL [22] and DreamBooth [25]. Detailed training setups are provided in the Appendix H. 5.2 Vision Results 5.2.1 Impact of Various SHiRA Masks We first evaluate the image quality for SHiRA and LoRA on Paintings and Blue- fire datasets for both single and multi-adapter usecases. Fig. 1 demonstrates com- parison between SHiRA-SNIP and LoRA. As evident, by merely changing 2% pre- trained weights, SHiRA generates high quality images for both finetuning tasks. Style Method %Params HPSv2 score( ↑) α = 1 α = 0.5 Paintings LoRA 3.84 24.7 ± 1.8 31 .3 ± 1.5 SHiRA-Struct 1.99 31.2 ±1.7 33 .0 ±1.8 SHiRA-Grad 2.05 30.3 ± 1.8 32 .3 ± 1.8 SHiRA-SNIP 2.05 29.8 ± 1.8 31 .6 ± 1.8 Bluefire LoRA 3.84 32.6 ± 1.9 33 .6 ± 1.6 SHiRA-Struct 1.99 34.2 ±1.6 34 .1 ±1.5 SHiRA-Grad 2.05 34.2 ±1.5 33.7 ± 1.7 SHiRA-SNIP 2.05 33.7 ± 1.7 33 .7 ± 1.6 Table 1: HPSv2 score of various adapters on Paintings and Bluefire. SHiRA-Struct outperforms all other methods. Next, we compare various types of SHiRA masks in Fig. 5. Clearly, all SHiRA schemes produce impressive images for different prompts and sig- nificantly outperform LoRA. We fur- ther quantify the image quality using HPSv2 for each of the masks. The results are presented in Table 1. As evident, all variants of SHiRA con- sistently achieve superior or similar HPSv2 scores than LoRA, especially for larger α (see details on scaling factor α in Appendix I). More results are provided in Appendices J and K: see Table 10 and Fig. 10, 11, 12. 5.2.2 SHiRA Adapters aid Multi-Adapter Fusion As explained in section 4.2, high sparsity of SHiRA reduces their AWOM and increases the AWOR metrics by increasing the number of zeros in AT 1 A2 product even for unstructured schemes such as SHiRA-WM, SHiRA-Grad, and SHiRA-SNIP. We hypothesized that this may lead to improved multi-adapter fusion performance. This was also pointed out by [26, 8, 31]: naively merging multiple LoRA adapters leads to poor performance and concept loss. 7LoRA thunder bird  Cat  Ship, sunset, sea  House, Prairie  fox  night flower SHiRA-Struct SHiRA-Grad                                     BLUEFIRE SHiRA-SNIP                                     PAINTINGS                                      MULTI-ADAPTER Figure 5: Comparison between different SHiRA masking methods for single- and multi-adapter image generation. For multi-adapter fusion, SHiRA-Struct outperforms all other adapters by generating exceptional images with high frequency details and good concept fusion (e.g., see fox and flower). We now validate the effectiveness of various SHiRA schemes on multi-adapter fusion. The right two columns in Fig. 1 and Fig. 5 show our results. SHiRA is clearly better at capturing both concepts than LoRA. For example, both bird and knight images in Fig. 1 generated with LoRA lose most of the paintings concept. Similarly, for the fox image in Fig. 5, LoRA does not show significant bluefire concept. In contrast, SHiRA-Struct and SHiRA-SNIP consistently perform well on many different prompts and produce exceptional images for multi-adapter fusion. Please refer to Appendix K.1 (Fig. 10, 11, 12, and 13) for additional results. For certain classes that were not included in the training set for both adapters (e.g., see Koala in Fig. 10, 12, and 13 in Appendix), we observe that LoRA produces significant artifacts whereas SHiRA generates high quality images. 5.3 Language Results 5.3.1 Single Adapter SHiRA Finetuning Similar to vision results, we demonstrate the effectiveness of SHiRA on language tasks. For our experiments, each adapter (i.e., weight-magnitude, gradient-magnitude, and SNIP based SHiRA) is trained on the combined 170K sample commonsense reasoning dataset released by [14, 20]. Similar to [20], we train our SHiRA adapters for 3 epochs and compare it against the LoRA baselines. As shown in Table 2, various SHiRA adapters outperform LoRA by 1.9-2.7% on an average on LLaMA-7B. Importantly, SHiRA only modifies 1% base parameter weights as compared to 66.72% (4.5B weights) changed by LoRA in the fused mode, thus enabling rapid switching on edge devices. Interestingly, we found that SHiRA-Struct does not perform well on language tasks likely because it is a rank 1 + diagonal adapter and may not have sufficient expressive power. Moreover, when compared to newer techniques like DoRA [20], our proposed work takes an orthogo- nal approach by finetuning very few parameters of the pretrained weights. This strategy allows for an efficient integration of our adapter with methods like DoRA to improve the expressiveness of the adapters. As we show in Table 2, our proposed adapter benefits from DoRA based finetuning and achieves almost comparable performance (within 0.3%) to DoRA on an average, with an added benefit of changing only 1% parameters at inference time. In contrast, DoRA would lead to 66.72% (4.5B weights ≈ 9GB memory in FP16 format) parameter change in the fused mode. Therefore, SHiRA is orthogonal to other existing low rank methods and can be efficiently integrated with them. 8Model %Params %C BoolQ( ↑) PIQA( ↑) Arc-e( ↑) Arc-c( ↑) WG( ↑) OBQA( ↑) HS( ↑) SIQA( ↑) Avg.( ↑) LoRA 0.83 66.72 68.9 80.7 77.8 61.3 78.8 74.8 78.1 77.4 74.7 (+0%) SHiRA-Grad 1.0 1.0 68.4 80.9 80.2 64.7 80.4 78.2 80.3 79.4 76.6 (+1.9%) SHiRA-WM 1.0 1.0 69.6 81.6 81.5 66.5 79.8 79.4 79.6 77.8 77.0 (+2.3%) SHiRA-SNIP 1.0 1.0 68.3 80.6 81.5 67.9 80.0 79.6 82.1 79.1 77.4(+2.7%) DoRA 0.84 66.72 68.5 82.9 81.4 65.8 80.8 81.0 84.8 79.6 78.1 (+0%) SHiRA-WM-DoRA 6.25 ∗ 1.0 70.9 81.9 81.7 64.9 80.8 79.2 84.5 78.6 77.8(-0.3%) Table 2: Evaluation of LLaMA-7B on Commonsense Reasoning. WG and HS denote WinoGrande and HellaSwag, respectively. %C represents parameters changed in the fused mode. (↑): the higher the better. Green denotes improvement. ∗Trained by masking a high-rank DoRA with a WM mask of top 1% weights, thus changing only 1% of the model during both training and inference. Model %Params %C BoolQ( ↑) PIQA( ↑) Arc-e( ↑) Arc-c( ↑) WG( ↑) OBQA( ↑) HS( ↑) SIQA( ↑) Avg.( ↑) LoRA 0.83 66.72 69.90 79.9 79.8 64.7 82.6 81.0 83.6 79.5 77.61 (+0%) DoRA 0.84 66.72 71.8 83.7 83.7 68.2 82.6 82.4 89.1 76.0 79.68(+2.07%) SHiRA-SNIP 1.0 1.0 70.42 81.71 83.25 68.6 80.51 81.0 89.78 79.01 79.28(+1.67%) Table 3: Results for LLaMA2-7B on Commonsense Reasoning. Finally, we experiment with LLaMA2-7B [30] and demonstrate that SHiRA-SNIP – which achieved the best results on LLaMA-7B – yields significant accuracy gains compared to LoRA and nearly the same accuracy as DoRA (within 0.4%, see Table 3). 5.3.2 Multi-Adapter Fusion on LLMs We now extend our LLM experiments to the multi-adapter fusion setting. To this end, we create anew setup where we independently train multiple adapters on training sets of individual commonsense reasoning benchmarks, i.e., one adapter each for BoolQ, PIQA, and Arc-Easy. In contrast, each adapter in section 5.3.1 was trained on a combined dataset containing 170K samples from all eight commonsense benchmarks as proposed in [ 14, 20]. In the present section, the goal is to evaluate how much accuracy drop various adapters experience when we perform multi-adapter fusion. Due to its simplicity towards constructing a mask, we will use SHiRA-WM in the rest of this paper. Further, we explore two settings - overlapping and non-overlapping SHiRA-WM adapters. The overlapping mask consists of top 1% parameters being trained for all tasks. On the other hand, the non-overlapping setting trains the top 1% weights for the first task, next top 1% for the second task, and so on. We compare the performance of both LoRA and SHiRA across the multi-adapter fusion of these three tasks. As shown in Table 4, both overlapping and non-overlapping multi-SHiRA outperform multi-LoRA on all three commonsense benchmarks. This is inline with our theoretical analysis in section 4.2 where we suggest that even unstructured sparse SHiRA adapters such as SHiRA-WM would have more orthogonal behavior than LoRA due to high sparsity (see higher AWOR of SHiRA-WM in Fig. 4(right)). In comparison, independently trained LoRA adapters would have no such property and suffer greatly during multi-adapter fusion. As a result, we see that both SHiRA models outperform LoRA by more than 6.5% accuracy on average. Further analysis of the properties of these trained adapters is discussed in Appendix K.3 (see Table 13 and Fig. 9). Of note, this experiment also demonstrates the value of creating a good mask for single adapter performance: Non-overlapping masks achieve lower single adapter accuracy than the corresponding overlapping masks since they train less important parameters. Hence, creating an optimal mask for SHiRA should be of significant interest to future research. 5.4 Content/Style Personalization: Generalizing SHiRA to SDXL and DreamBooth Finally, we extend SHiRA to focus on DreamBooth [25] using a much bigger vision model called SDXL [22]. We follow a similar setup as adopted by [2]. Specifically, one content (vase) and two style (wooden sculpture and canvas) datasets with five images each were collected from the DreamBooth dataset [25] and public domains, respectively. These datasets were used to train various content and style adapters. For our experiments, we use SDXL [ 23] as our base model and train both LoRA and SHiRA adapters with comparable trainable parameters on individual single-concept datasets. During training, prompts containing special identifier tokens like \"<CONTENT>\" or \"<STYLE>\" (e.g., <SBU> as content token for vase and <SZN> as style token for wooden sculpture and canvas) are used 9Single Adapter Multi-Adapter Model BoolQ( ↑) PIQA( ↑) Arc_e( ↑) Avg( ↑) BoolQ( ↑) PIQA( ↑) Arc_e( ↑) Avg( ↑) %Drop ( ↓) LoRA 80.52 79.05 75.67 78.41 77.22 71.27 57.45 67.33 (+0%) 11.08 SHiRA-WM-Overlap 78.07 79.71 77.57 78.45 77.43 76.88 67.76 74.02(+6.69%) 4.43 SHiRA-WM-Non-Overlap 76.94 79.71 75.97 77.54 74.22 78.4 69.15 73.92 (+6.59%) 3.62 Table 4: Multi-adapter fusion evaluation of independently trained SHiRA and LoRA adapters on BoolQ, PIQA, and Arc-Easy. %Drop is calculated as drop in average accuracy for multi-adapter fusion compared to the single adapter average accuracy for each adapter. LoRA  SHiRA  LoRA  SHiRA  LoRA  SHiRA Figure 6: LoRA- vs. SHiRA-based DreamBooth on SDXL. Prompts for content/style personalization - left pair: \"A picture of a dog in<STYLE:WOODEN-SCULPTURE> style in a bucket\",center pair: \"A pic- ture of a <CONTENT:VASE> with flowers\", and right pair: \"A picture of a sunset in <STYLE:CANVAS> style\". Here, \"<CONTENT>\" and \"<STYLE>\" are special identifier tokens for content/style. to finetune the SDXL network for content or style personalization, respectively. During inference, similar prompts are used to generate images from LoRA- or SHiRA-based DreamBooth. Fig 6 shows DreamBooth generated images for LoRA and SHiRA. Clearly, our proposed adapter produces high quality personalized images of target concept in different scenarios. This highlights the broad applicability of our adapter while still preserving the benefits of rapid adapter switching. 6 Discussion To summarize our main contributions, we highlight that SHiRA – when used with even the most basic pruning metrics (such as weight- or gradient-magnitude, SNIP, structured masks, etc.) – significantly outperforms LoRA on a variety of large-scale tasks in both large vision and large language domains. For LVM style transfer applications, we found that SHiRA-Struct is the most effective masking technique due to its special orthogonality properties that aid multi-adapter fusion. However, SHiRA- SNIP and SHiRA-Grad are not too far behind and achieve competitive performance as SHiRA-Struct. On the LLM commonsense reasoning side, SHiRA-SNIP is the best strategy out of the masking techniques we have considered in this work. Specifically, SHiRA-Struct did not achieve good results on the more complex commonsense reasoning tasks since it is a combination of a rank-1 + a highly sparse diagonal adapter. SHiRA-Grad on LLMs is about 0.8% worse accuracy than SHiRA-SNIP (76.6% vs. 77.4% average accuracy on commonsense reasoning for LLaMA-1). Therefore, in conclusion, for the applications/fields and the masking techniques considered in this paper, SHiRA- SNIP works well across both language and vision domains. Hence, we recommend that SHiRA-SNIP is one of the strongest candidates that we have considered for sparse finetuning. 7 Conclusion In this paper, we have proposed SHiRA, a new high rank adapter paradigm to demonstrate that even finetuning merely 1-2% parameters of the pretrained generative models is sufficient to achieve high performance on many adapter tasks. We have demonstrated SHiRA’s ability to rapidly switch adapters and to avoid concept loss with support from both theory and experiments. Furthermore, we have shown how specially designed sparse masks can lead to near-orthogonal adapter weights which allows for natural multi-adapter fusion. We have conducted extensive single- and multi-adapter experiments on several vision and language tasks to demonstrate the superiority of SHiRA over LoRA. Our latency- and memory-efficient PEFT-based implementation for training SHiRA runs at nearly the same speed as LoRA while consuming about 16% lower peak GPU memory. Finally, for inference, we have provided a scatter_op based method that can load our SHiRA 5×-16× faster than equivalent LoRA fusion on a CPU, thus demonstrating our rapid switching benefits. 10Acknowledgments We thank anonymous reviewers for insightful comments and constructive feedback which significantly improved the quality of our work. References [1] Goodbye cold boot - how we made LoRA Inference 300% faster. https://huggingface. co/blog/lora-adapters-dynamic-loading . Accessed: 2024-05-15. [2] Sdxl lora for dreambooth. https://github.com/huggingface/diffusers/blob/main/ examples/dreambooth/README_sdxl.md. Accessed: 2024-05-15. [3] Alan Ansell, Edoardo Maria Ponti, Anna Korhonen, and Ivan Vuli ´c. Composable sparse fine-tuning for cross-lingual transfer. arXiv preprint arXiv:2110.07560, 2021. [4] Alan Ansell, Ivan Vuli´c, Hannah Sterz, Anna Korhonen, and Edoardo M Ponti. Scaling sparse fine-tuning to large language models. arXiv preprint arXiv:2401.16405, 2024. [5] Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong.Mathematics for machine learning. Cambridge University Press, 2020. [6] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint arXiv:2311.11696, 2023. [7] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218, 1936. [8] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [9] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. [10] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. [11] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354, 2024. [12] Shwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao. Sparseadapter: An easy approach for improving the parameter-efficiency of adapters. arXiv preprint arXiv:2210.04284, 2022. [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [14] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. [15] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019. [16] Damjan Kalajdzievski. A rank stabilization scaling factor for fine-tuning with lora. arXiv preprint arXiv:2312.03732, 2023. [17] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023. [18] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611, 2020. [19] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. 11[20] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. [21] Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh. Rosa: Accurate parameter-efficient fine- tuning via robust adaptation. arXiv preprint arXiv:2401.04679, 2024. [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [23] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500–22510, 2023. [26] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023. [27] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. [28] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:24193–24205, 2021. [29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [30] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [31] Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. Orthogonal subspace learning for language model continual learning. arXiv preprint arXiv:2310.14152, 2023. [32] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [33] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise a child in large language model: Towards effective and generalizable fine-tuning. arXiv preprint arXiv:2109.05687, 2021. [34] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. arXiv preprint arXiv:2311.03099, 2023. [35] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303, 2023. [36] Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. Masking as an efficient alternative to finetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. 12A Edge Deployment Challenges for LoRA (Contd.) To understand the overhead of each of the stages to the standard huggingface LoRA in- ference pipeline (i.e., load, fuse, unfuse, unload), we experiment with the pipeline Stage Server-GPU (s) Desktop-CPU (s) load 0.883 ± 0.085 0 .786 ± 0.056 fuse 0.306 ± 0.044 3 .003 ± 0.023 unfuse 0.206 ± 0.041 2 .916 ± 0.014 unload 0.007 ± 0.001 0 .007 ± 0.001 Table 5: Latency (in s) to load, fuse, unfuse, unload [1] adapters on SDXL on Server-GPU and Desktop- CPU. On a mobile device, fusing/un- fusing would happen for each layer iteratively since we cannot store all weights at the same time on local on- chip memory (unlike a large GPU), re- sulting in much higher overhead. provided in [ 1] and iteratively add adapters to SDXL model [22]. As evident from Table 5, on a server-grade GPU, load time dominates whereas fuse/unfuse/unload times are relatively negligible. However, if we try to run the exact same pipeline on an everyday device like a desktop-grade CPU, we see that the fuse and unfuse times start dominat- ing and can hinder rapid adapter switching. Note that, on an even more constrained device like a mobile phone, AI accelerators do not have sufficient memory to store weights from all layers at the same time in the local memory. Hence, on such devices, we would need to load base model weights for each layer into the local memory, and then fuse corre- sponding LoRA weights before we can run inference for that layer. This obviously leads to a massive inference latency overhead. As a result, existing deployment options are not feasible for rapid switching on mobile devices. B More Details on SHiRA Masks Selecting important salient weights pertinent to a task can be done in many ways, and one popular approach is to use masks. In this section we discuss various strategies to construct sparse mask based on different heuristics to select weights for efficient finetuning of large generative models. B.1 Structured Sparse Mask (SHiRA-Struct) This is a simple structured mask. We begin with making every f rows or columns in a weight matrix trainable, where we call f as the frequency parameter and we choose it based on how much sparsity we need in the adapter. That is, the mask M consists of every f rows or columns containing ones and everything else as zeros. This actually makes it a rank 1 mask because all rows and columns would be linearly dependent. Therefore, to make it high rank, we also add a diagonal parameter which makes the resulting mask M high rank. B.2 Unstructured Sparse Random Mask (SHiRA-Random) Unstructured sparse random masks involve masking individual weights without any specific pattern or structure. The masked weights are randomly scattered throughout the weight tensor, resulting in a sparse weight tensor. However, as the weights are selected without considering their salience to the task, randomly selected unstructured masks may often be sub-optimal for finetuning. One common way of constructing random sparse marks is using Bernoulli sampling: f(k; p) = \u001ap if k = 1, 1 − p if k = 0. (1) where, p is the probability of sampling 1 from the distribution. B.3 Weight Magnitude-Based Sparse Mask (SHiRA-WM) Many earlier works [ 18, 27] have shown the importance of weight magnitude based masks for identifying important weights in the network. Motivated by this literature, we design a weight magnitude based proxy to adapt the behavior of the pretrained network. Specifically, we create a mask by choosing the top-K weight magnitudes at specific layers where SHiRA is employed. We finetune only these top-K weights and keep the rest of them frozen to their pretrained values. Typically,K is a very small percentage of parameters so that the overall number of parameters to be tuned stays comparable to LoRA and its variants. 13B.4 Gradient Based Sparse Mask (SHiRA-Grad) Despite the efficacy of employing weight magnitude based scheme, this approach lacks an inherent awareness of the specific task for which the model is being finetuned. To address this challenge, we design a similar gradient magnitude based proxy to identity important top-K weights for the task and only adapt them during the finetuning process. B.5 SNIP Based Sparse Mask (SHiRA-SNIP) SNIP [19] combines both weight and gradient based schemes and is computed as the magnitude of the product of the weight and its corresponding gradient. This formulation effectively captures the interplay between the weight magnitude, which reflects its overall contribution to the model’s output, and its gradient information, which encodes the weight’s task-specific relevance during finetuning. SNIP for a weight parameter is defined as: SNIP ≜ |⟨Θi, ∇θiL⟩| (2) where ⟨.⟩ represents inner product, Θi is the weight parameter, ∇θiL is the gradient of weight parameter with respect to the task loss L for the ith parameter in the network. C Fuse and Scatter Op implementation In this section, we compare fusing times of LoRA with our efficient scatter_op (torch.Tensor.scatter_) based implementation for SHiRA. For our experiments, we perform benchmarking on a Desktop-grade CPU and compute the average times for various tensor dimensions (e.g., tensor dimension = 4096 implies a weight of size 4096 × 4096, which is typical in modern LLMs). As shown in Fig. 7, our scatter_op-based SHiRA inference pipeline is up to 13×-16× faster than fusing LoRA weights, specially for larger dimensions. 13x13x 13x16x Figure 7: Comparison between average times for LoRA-fuse and SHiRA-scatter_op implementa- tion for 50 randomly initialized weights of various dimensions on a CPU (e.g., dimension = 4096 means that the weight has shape 4096 × 4096). For fusing, we compute time taken to merge LoRA adapters into the base weights (W + AB). Similarly, for the scatter_op, we report time taken to overwrite base weights with SHiRA weights using the scatter op (torch.Tensor.scatter_) based implementation in Pytorch. Next, we present end-to-end switching times for prevalent LVMs and LLMs: SDXL and LLaMA2-7B. Notably, even for a smaller model like SDXL (2.6B params compared to 7B params in LLaMA2-7B), SHiRA achieves a 4.68x faster switching time (0.77s vs. 3.6s), while for LLaMA2-7B, with larger tensor dimensions, SHiRA attains a 5.71x speedup (4.93s vs. 28.15s) on a consumer grade CPU (see Table 6). Note that, fusing LoRA adapters for LLaMA2-7B on a CPU is 28.15s (nearly half a minute). Indeed, waiting half a minute for the adapter to switch/fuse is quite substantial and hampers user experience significantly. In contrast, SHiRA can get the adapter ready for inference within 4.93s, 14Model LoRA SHiRA Speed-up SDXL 3.64 ±0.10 0.77±0.09 4 .68× LLaMA2-7B 28.15 ±1.62 4.93±0.23 5 .71× Table 6: End-to-End switching time on CPU for SDXL and LLaMA2-7B: We achieve a very high (4.7×-5.7×) speed up in switching time compared to LoRA. thus significantly improving the user experience. Note that, once the adapters are fused, inference time on the hardware is equal for both LoRA and SHiRA. Moreover, as discussed in [1], for unfused LoRA case (which can enable rapid switching), the inference latency can be up to 30% higher which is not the case with SHiRA. D Turn any Trainer into SHiRA: Gradient Hook based Implementation In this section, we provide a method to convert any floating point training into SHiRA based finetuning. Specifically, SHiRA can be implemented directly using a functionality called post_accumulate_gradient_hooks available in Pytorch 2.1.0. This gradient_hook can be used to mask gradients after the gradient accumulation step is completed. Moreover, this enables us to apply SHiRA on any publicly available trainer (e.g., Transformers.Trainer, SFT_Trainer, etc.). Therefore, implementing SHiRA on any task is trivial and can be done even without PEFT library, thus making SHiRA very easy to implement. With this gradient hook based implementation, we were able to train all our adapters (including for models such as LLaMA-7B, LLaMA2-7B and SD-1.5) on a single NVIDIA A100 GPU at nearly the same speed as PEFT based LoRA implementation. SHiRA runs at 2.17 it/sec as compared to LoRA which is at 2.42 it/sec for LLaMA-7B finetuning. E Latency- and Memory-Efficient PEFT based Implementation for SHiRA As discussed in Appendix C, scatter_op can be utilized to manage sparse weight updates during inference. Given that SHiRA only finetunes a small subset of the pretrained model weights, we adopt a similar scatter_op-based approach for training. This allows us to retain only the sparse training parameters in the optimizer, thereby significantly reducing the peak GPU memory utilization during training. As shown in Table 7, SHiRA not only trains at almost similar speed as LoRA, but also consumes ∼ 16% lower peak GPU memory. Compared to other variants like DoRA, SHiRA training consumes significantly (∼ 40%) lower peak GPU memory and also trains much faster (SHiRA is about 36% faster than DoRA). All memory requirement data was collected using psutil utility used within the Transformers.Trainer training loop for LLaMA2-7B. Finally, note that, partial finetuning techniques proposed in the pre-LoRA era [36, 28, 3, 33, 10] do not have such memory-efficient implementations, which makes them impractical for large generative models. Therefore, SHiRA significantly outperforms prior partial finetuning techniques in training memory costs and is highly practical for modern LVM and LLM adaptations tasks. Adapter Peak GPU memory (GB) #Training steps/s LoRA-PEFT 35.10 0 .69 DoRA-PEFT 49.49 (+40.99%) 0.49 (-28.98%) SHiRA-PEFT 29.26 (-16.63%) 0.67 (-2.89%) Table 7: Peak GPU memory consumption (in GBs) and #Training steps per second during training for PEFT-based implementation of various adapters for LLaMA2-7B. Relative changes compared to LoRA are highlighted: Green indicates improved performance (lower memory consumption, faster training speed), while Red indicates degraded performance (higher memory consumption, slower training speed). SHiRA trains at nearly the same speed as LoRA but consumes up to 16% lower peak GPU memory. 15F Proofs of Lemma F.1 Lemma 4.1 Lemma 4.1. The parameter complexity and learning complexity of SHiRA is equal to the number of non-zero elements in the adapter. Proof. The parameter complexity and learning complexity depends on the parameters to be learned. The number parameters of the adapter is equal to || ∆W ||0. F.2 Lemma 4.2 Lemma 4.2. If we specify a sparsity factor, the LoRA is r rank approximation of SHiRA with approximation error bounded by σ2 r+1, the (r + 1)th singular value of the SHiRA adapter. Proof. Let ∆W be the given SHiRA adapter of size (m, n) and sparsity factor ρ. Consider the SVD decomposition of ∆W. Next, we construct an r rank matrix approximation using the r largest singular values of the adapter. This reconstructedr rank matrix can be seen as a LoRA adapter. Based on Eckart-Young theorem ([7]) and theorem 4.95 in [5], the approximation error is equal to (r + 1)-th singular value of the SHiRA adapter (σ2 r+1). If the ∆W is an r rank matrix then the approximation error is zero. F.3 Lemma 4.3 Lemma 4.3. Scaling factor for SHiRA is independent of the rank of the adapter and can be set to 1. Proof. The LoRA update equation for any given adapter is as follows: Yout = (W + αrBA)Xin + b. (3) Note αr = α r is the scaling factor, where α is a hyperparameter and r is the rank. Three possible initialization for A and B are as follows: • if A and B are initialized to zero, no learning occurs since this corresponds to saddle point [11]. • A and B are initialized to N(0, σ2 a) and 0 respectively. Here, σ2 a = Θ(n−1), to ensure that AT xi remains bounded with width n of the adapter. • A and B are initialized to 0 and N(0, 1) respectively. Here, it is important to note that the variance of B does not depend of the width of the adapter. However, to avoid gradient collapse for higher ranks, [ 16] recommends to set αr as α√r . Further, optimal convergence the update of A and B matrix updates have different learning rates [11]. For the SHiRA adapter, the update equation is given below: Yout = (W + S)Xin + b. (4) where, S is the sparse matrix with a designed sparsity ratio. All non-zero locations in S are implicitly initialized to the base matrix weights. This initialization ensures that the updates remain bounded during the finetuning stage using stochastic gradient descent. It is also important to note that the scaling is independent of the rank for SHiRA. F.4 Lemma 4.4 Lemma 4.4. Consider two adapters, ∆W1 and ∆W2. If one of the adapters, ∆W1 or ∆W2 lies in the null space of the other, then the adapters will not interfere multiplicatively. 16Proof. The proof leverages two facts: (i) ∆W1 T ∆W2 = O given that one adapter lies in the null space of other. Here, O is a zero matrix ( Oi,j = {0}∀i, j). ( ii) Power series expansion of the non-linear activation function: The power series expansion has terms involving the matrix product of adapters. Since each adapter is in the null space of the other, all terms involving product of adapters are equal to zero. Therefore the adapters do not interfere multiplicatively. This lemma can be extended to a scenario with more than two parallel additive adapters. If all possible pairs of adapters lie in the null space of each others all cross-terms between adapters are zero. F.5 Lemma 4.5 Lemma 4.5. Non-overlapping SHiRA-Struct adapters are nearly orthogonal. That is, AWOR for non-overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. Since all SHiRA masks are highly sparse, this means that the product AT 1 A2 has a lot of zeros, thus making the adapters nearly orthogonal. Proof. Continuing from the adapter definitions used in the main text for this lemma, let us compute AT 1 A2 and then analyze its AWOR: AT 1 A2 = (I + S1)T (I + S2) = I + IS2 + ST 1 I + ST 1 S2 = I + S2 + ST 1 (5) Here, ST 1 S2 is zero by design because S1 and S2 do not have common non-zero rows. Moreover, since both S1 and S2 are highly sparse, AT 1 A2 has a sparsity equal to the sum of sparsity of I, S1 and S2. Note that, I + S2 = A2. Thus, AWOR for non-overlapping SHiRA-Struct adapters is at most the sum of sparsity of individual adapters. G Dataset and Evaluation Metric Descriptions G.1 Datasets G.1.1 Language Datasets Dataset #Train #Val Test PiQA 16K 2K 3K BoolQ 9.4K 2.4K 2.4K SIQA 33.4K 1.9K 1.9K OBQA 4.9K 0.5K 0.5K Winogrande 9.2K 1.3K 1.8K HellaSwag 39.9K 10K 10K Arc_easy 2.25K 570 2.36K Arc_challenge 1.12K 299 1.12K Table 8: Commonsense Benchmarks For language finetuning tasks, we use the commonsense rea- soning datasets, which comprise 8 sub-tasks, each with a pre- defined training and testing set as shown in Table 8. We follow the setting of [ 14] for SHiRA Single Adapter training. The common sense reasoning training dataset is a combination of the training datasets provided by [15], while we evaluate each evaluation dataset separately as in Table 2. For multi-adapter LLM experiments, we train each adapter from one particu- lar task, and then perform multi-adapter evaluation on all the tasks. G.1.2 Vision Datasets For style transfer adaptation tasks as described in sections 5.2.1 and 5.2.2, we use two datasets, Bluefire and Paintings. Images present in both of these datasets are collected from public-domain (CC-0 license). The Bluefire dataset consists of a total of 54 images consisting of 6 different concepts - Cars, Dragons, Birds, Foxes, Men and Castles. For all these concepts, images with \"blue-fire\" effect are collected and used for style transfer finetuning. The validation of the Bluefire dataset consists of 30 images. 9 of the 30 images contain one of the 6 concepts in the training set, and the rest 21 are new. A few examples of unseen concepts in the validation set: football, monster, sword, chess rook, lion, koala etc. Similarly, the painting datasets contain a total of 90 images of \"painting\" style images of 9 different concepts - fire, birds, elephants, ships, horses, flowers, women, men and tigers. The validation set of the Paintings dataset consists of 21 images, out of which 9 contain concepts from the training set. The remaining 12 are new concepts not included in the training set. A few examples of unseen concepts in the validation set: lion, tiger, dog, cat, koala, panda, and other landscapes. 17Alpha=0.0  Alpha=0.25  Alpha=0.50                                                 blazing fiery car, lightning Alpha=0.75  Alpha=1.0  Alpha=1.25 Figure 8: Effect of α scaling on image quality. α = 0.0 is the base model output without any adapter effects. We can see that as theα increases, the SHiRA adapter effect increases similar to how it works for LoRA inference. G.2 Evaluation Metrics HPSv2 metric evaluation For all style transfer finetuning experiments with Bluefire and Paintings dataset, we report HPS metric to quantify the quality of the generated images. For Bluefire validation, 30 images per validation prompt are generated for different seeds, hence generating 900 images for HPS analysis. We follow a similar paradigm for Paintings and generate 630 images with 21 prompts. H Training Details In this section, we list hyperparameters used for our experiments for Language and Vision finetuning tasks in Table 9. Method Adapter Target Modules Optimizer LR LR-Scheduler Rank LoRA LVM q-proj,k-proj,v-proj,up-proj,down-proj AdamW 1e − 4 Cosine 64 SHiRA LVM 1e − 4 Cosine NA LoRA LLM 2e − 4 Linear 32 DoRA LLM 2e − 4 Linear 32 SHiRA LLM 5e − 4 Linear NA Table 9: Training hyperparameters used for finetuning experiments. All finetuning and evaluation experiments for language and vision tasks are done using a single NVIDIA A100 GPU. I Effect of Scaling Factor α during Inference As described in section 3.1, in order to adapt the pretrained model to a new task, we only finetune very few weight parameters relevant to the task. For our adapter, we can easily extract out these modified weights as S = Wnew − W, where Wnew is the weight obtained after SHiRA training, and W is the prertained weight. Since only 1-2% weights change during SHiRA training, S is highly sparse and thus constitutes our sparse adapter. Hence, the new finetuned weights of the base model can be viewed as Wnew = W + S. Similar to LoRA, the strength of SHiRA adapter at inference time can be modified using a scaling factor α. For any defined α scaling, the new weights of the model can be expressed as Wnew = W + αS. Fig. 8 shows the effect of varying α on the output image. As evident, choosing an α <1 reduces the \"blue fire\" in the generated image and whereas α >1 amplifies the style transfer effect. For α = 0.0, the adapter is disabled and the model’s output is the same as that for the base model. J More Detailed Comparison among Various Masks We provide HPSv2 scores for all SHiRA masking schemes in Table 10. 18Adapter Style Adapter Method %Params HPSv2 score( ↑) α = 1 α = 0.75 α = 0.5 Paintings LoRA 3.84 24.7 ± 1.8 28 .4 ± 1.4 31 .3 ± 1.5 SHiRA-Struct 1.99 31.2 ± 1.7 32 .1 ± 1.8 33 .0 ± 1.8 SHiRA-Rand 2.05 30.7 ± 1.9 31 .7 ± 1.8 32 .7 ± 1.9 SHiRA-WM 2.05 29.7 ± 1.9 30 .6 ± 1.7 32 .1 ± 1.8 SHiRA-Grad 2.05 30.3 ± 1.8 31 .3 ± 1.7 32 .3 ± 1.8 SHiRA-SNIP 2.05 29.8 ± 1.8 30 .8 ± 1.8 31 .6 ± 1.8 Bluefire LoRA 3.84 32.6 ± 1.9 34 .1 ± 1.5 33 .6 ± 1.6 SHiRA-Struct 1.99 34.2 ± 1.6 34 .7 ± 1.5 34 .1 ± 1.5 SHiRA-Rand 2.05 33.4 ± 1.9 34 .1 ± 1.5 33 .7 ± 1.7 SHiRA-WM 2.05 31.9 ± 2.1 33 .3 ± 1.6 33 .1 ± 1.7 SHiRA-Grad 2.05 34.2 ± 1.5 34.4 ± 1.5 33 .7 ± 1.7 SHiRA-SNIP 2.05 33.7 ± 1.7 34 .3 ± 1.4 33 .7 ± 1.6 Table 10: Comparison between LoRA and various SHiRA schemes with respect to HPSv2 metric. For vision problems, SHiRA-Struct outperforms all other methods. Adapter cifar10 cifar100 food101 dtd LoRA 97.94 87 .97 84 .27 69 .41 SHiRA 98.05 88.15 84.43 69.73 Table 11: LoRA vs SHiRA for Image Classification using ViT-Base model. SHiRA consistently outperforms LoRA on these transfer learning tasks. K More Results K.1 Additional Sample Images for Vision Style Transfer Applications We show many more sample images for various adaptation usecases in Fig. 10, 11, 12, and 13. K.2 Image Classification and GLUE We further conduct more experiments on image classification and GLUE tasks using SHiRA-WM. For image classification, we finetune Vision Transformer (ViT) using LoRA and SHiRA for four common transfer learning datasets, namely, CIFAR-10, CIFAR-100, Food101, and Describable Textures Dataset (DTD) (see Table 11). Both methods have comparable parameters around 300K. As shown in Table 11, we outperform LoRA on all image classification tasks. For GLUE, we use the code released by SoRA [6] which relies on dynamically adjusting the ranks of the adapters. In Table 12, we report accuracy on four common GLUE tasks: QNLI, COLA, SST2, and MRPC. Accuracy numbers for LoRA and SoRA are directly taken from the SoRA paper since we are using the official code to run SHiRA experiments. As evident, with nearly 2x smaller adapter, SHiRA outperforms LoRA by 1.1% accuracy on average. Further, SHiRA achieves a similar accuracy as SoRA while being 30% smaller in adapter size. Indeed, SoRA cannot enable rapid switching like SHiRA. Therefore, we again demonstrate that a simple approach like SHiRA-WM outperforms LoRA and its advanced variants with a similar or significantly better accuracy while providing additional deployment benefits. K.3 Analysis of Trained Adapters Are adapter tasks sufficiently different? Table 13 shows the L2 analysis for the adapters trained in Table 4. We compute the L2 distance between each adapter and the original pretrained weights (all adapters train top 1% weights in the overlap setting) as well as the L2 distance between each adapter. Clearly, each adapter is closer to the pretrained weights compared to the other adapters. This demonstrates that the tasks are sufficiently different. Why does SHiRA-WM-Overlap perform well? Next, as shown in Fig. 9, for unstructured SHiRA masks, both overlapping and non-overlapping adapters have identical AWOR and AWOM values. This suggests that their orthogonality characteristics are quite similar due to the high sparsity. We hypothesize that this is the main reason for the good performance of SHiRA-WM-overlap and explains the results in Table 4. 19Adapter#Params COLA QNLI MPRC SST2 Average LoRA 1.33M 69.73 93 .76 89 .71 95 .57 87 .19(+0%) SoRA 910K 71.48 94.28 91.98 95 .64 88.34(+1.15%) SHiRA 636K 70.62 93 .90 92.15 96.50 88.29(+1.10%) Table 12: GLUE benchmarking for the DeBERTa-V3-base. As evident, with nearly 2x smaller adapter, SHiRA outperforms LoRA by 1.1% accuracy on average. Further, SHiRA achieves a similar accuracy as SoRA while being 30% smaller in adapter size. Hence, SHiRA generalizes to other language tasks as well. Base Arc_e BoolQ PIQA Base 0 37.0 67 .0 75 .0 Arc_e 0 75 .0 81 .5 BoolQ 0 98 .5 PIQA 0 Table 13: L2 distances between pretrained base weights and SHiRA adapters vs. distances between adapters: Adapters are closer to the base model weights than to each other. 128 256 512 1024 2048 4096 8192 Dimension of the Adapter Weight 102 103 104 105 106 Adapter Weight Orthogonality Magnitude SHiRA-WM-overlap SHiRA-WM-nonoverlap Dense 128 256 512 1024 2048 4096 8192 Dimension of the Adapter Weight 0.0 0.2 0.4 0.6 0.8 1.0Adapter Weight Orthogonality Ratio Figure 9: Adapter Weight Orthogonality Magnitude (AWOM: L2 magnitude) and Adapter Weight Orthogonality Ratio (AWOR: Sparsity Ratio) of the productAT 1 A2 between two adapters for unstruc- tured SHiRA-WM overlap and non-overlapping cases (99% sparse). We vary the adapter dimensions (e.g., 4096 refers to a pretrained weight of dimensions 4096 × 4096) and measure AWOM and AWOR for each weight size (averaged over 50 seeds). For unstructured SHiRA masks, overlapping and non-overlapping adapters achieve coinciding AWOR and AWOM, thus suggesting that their orthogonality properties are very similar due to high sparsity. This explains our multi-adapter LLM results in Table 4. L Societal Impact Our work enables on-device deployment of adapters which can have a clear positive impact on society as it allows for privacy-preserving generative AI use. With our work, users would be able to rapidly generate images in specific styles directly on-device. On the other hand, while efficient finetuning techniques have many advantages, they bring the potential risk of digital forgery. This is mainly due to finetuning the generative models on a much smaller subset of data, leading to potential overfitting. As our proposed method is also a parameter-efficient finetuning technique, it suffers from similar potential risk as the other PEFT algorithms. M Limitations and Future Work In this work, we show that our proposed sparse high rank adapter, SHiRA, with merely finetuning 1- 2% parameters of the pretrained generative models is sufficient to achieve high performance on many adapter tasks. However, in order to adopt our method for mobile deployment, hardware-software 20co-design techniques, such as lookup-table (LUT) based approaches, may be necessary to optimize the implementation for edge devices. Moreover, as discussed in the main text, building optimal sparse masks (i.e., which parameters to train for a given task) warrants further investigation. LoRA Man  Lion, Forest  Ship, sunset, sea  House, Prairie  Koala Bear  Horse, Knight SHiRA-Struct SHiRA-Grad                                     BLUEFIRE SHiRA-SNIP                                     PAINTINGS                                      MULTI-ADAPTER Figure 10: More image samples for single and multi-adapter fusion. We observe that LoRA exhibits artifacts for koala and concept loss for knight in Multi-Adapter fusion while SHiRA produces significantly better images. 21LoRA Man  Astronaut in Galaxy  House on Mountain  Bird  Thunder bird  Fox SHiRA-Struct SHiRA-Rand SHiRA-WM SHiRA-Grad                                         BLUEFIRE SHiRA-SNIP                                         PAINTINGS                                          MULTI-ADAPTER Figure 11: More image samples for single and multi-adapter fusion. We observe that LoRA images exhibit concept loss for bird in Multi-Adapter fusion. 22LoRA car  lion  bird  Ship,sunset  koala bear  Tiger SHiRA-Struct SHiRA-Rand SHiRA-WM SHiRA-Grad                                         BLUEFIRE SHiRA-SNIP                                         PAINTINGS                                          MULTI-ADAPTER Figure 12: More image samples for single and multi-adapter fusion. Koala is not included in the training set of either of the Bluefire and Paintings Adapter styles. We observe that for this class, LoRA has significant artifacts whereas SHiRA produces exceptional images. 23LoRA man in mythical forest  Koala Bear  Bird  Car  Fox  House on prairie,storms,fire SHiRA-Struct SHiRA-Rand SHiRA-WM SHiRA-Grad SHiRA-SNIP Figure 13: More results for multi-adapter fusion. Koala is not included in the training set of either of the Bluefire and Paintings Adapter styles. We observe that for this class, LoRA has significant artifacts whereas SHiRA produces exceptional images. 24NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The manuscript discusses and reports detailed results accurately reflecting the claims and the scope of the work. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, discussed in section M. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] 25Justification: Discussed in section Appendix F. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All experimentation details for training and inference are included in the main and supplementary materials. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 26Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We plan to open source the code and datasets pending legal approval. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experimentation details required for understanding the results are included in the main and supplementary materials. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Yes, mean and standard deviation of the performance metrics are reported across various seed values. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). 27• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details of compute used for training and inference are included. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform to NeurIPS code of ethics. • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes, discussed in section Appendix L. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 28• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable since our models do not have high risk of misuse. • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We follow the license terms for every model and dataset we use. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 2913. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, details of the datasets are provided in the Appendix G.1.2. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not Applicable Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not Applicable Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 30",
      "meta_data": {
        "arxiv_id": "2406.13175v2",
        "authors": [
          "Kartikeya Bhardwaj",
          "Nilesh Prasad Pandey",
          "Sweta Priyadarshi",
          "Viswanath Ganapathy",
          "Shreya Kadambi",
          "Rafael Esteves",
          "Shubhankar Borse",
          "Paul Whatmough",
          "Risheek Garrepalli",
          "Mart Van Baalen",
          "Harris Teague",
          "Markus Nagel"
        ],
        "published_date": "2024-06-19T03:13:11Z",
        "pdf_url": "https://arxiv.org/pdf/2406.13175v2.pdf",
        "github_url": "https://github.com/Qualcomm-AI-research/SHiRA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Sparse High Rank Adapters (SHiRA) to address limitations of Low Rank Adaptation (LoRA), specifically inference overhead, slow adapter switching, and concept loss during multi-adapter fusion. SHiRA enables rapid switching and reduces concept loss by directly tuning only 1-2% of the base model weights, resulting in a highly sparse adapter that can be switched in fused mode without inference overhead. It empirically and theoretically demonstrates that SHiRA significantly outperforms LoRA on LVMs and LLMs, and introduces a PEFT-based implementation that trains at similar speeds to LoRA while consuming up to 16% less peak GPU memory. SHiRA also achieves 5x-16x faster adapter loading on a CPU compared to LoRA fusion.",
        "methodology": "SHiRA trains a small percentage (1-2%) of existing weights in a pretrained model by masking gradients during backpropagation, rather than adding new low-rank weights like LoRA. Various sparse mask strategies are employed: SHiRA-Struct (structured rows/columns + diagonal), SHiRA-Rand (random selection), SHiRA-WM (top-K weight magnitudes), SHiRA-Grad (top-K gradient magnitudes), and SHiRA-SNIP (combining weight and gradient magnitudes). For rapid switching, SHiRA stores sparse weights and their indices, overwriting base model weights using a `scatter_op` at inference time, avoiding the need for a separate fusion stage. For multi-adapter fusion, the high sparsity of SHiRA is shown to reduce interference between adapters, quantified by Adapter Weight Orthogonality Magnitude (AWOM) and Adapter Weight Orthogonality Ratio (AWOR). SHiRA's training is made memory- and latency-efficient through a backward hook-based gradient masking or a PEFT-based implementation that retains only sparse parameters in the optimizer.",
        "experimental_setup": "Experiments were conducted on Large Vision Models (LVMs) and Large Language Models (LLMs). LVMs include Stable Diffusion-v1.5 (RealisticVision-v3 checkpoint) and SDXL for style transfer and DreamBooth tasks, using custom Bluefire and Paintings datasets (public domain images) and DreamBooth datasets. LLMs include LLaMA-7B and LLaMA2-7B, evaluated on commonsense reasoning benchmarks like HellaSwag, PIQA, SIQA, BoolQ, Arc-easy, Arc-challenge, OpenBookQA, and Winogrande, using a combined 170K sample dataset. Additional experiments included Vision Transformer (ViT-Base) for image classification on CIFAR-10, CIFAR-100, Food101, and DTD, and DeBERTa-V3-base for GLUE tasks (QNLI, COLA, SST2, MRPC). Evaluation metrics include Human Preference Score-V2 (HPSv2) for vision, accuracy for language tasks, and computational metrics like peak GPU memory consumption, training steps per second, and CPU adapter loading times. Training was performed on a single NVIDIA A100 GPU.",
        "limitations": "The paper identifies several limitations: full mobile deployment may require hardware-software co-design techniques (e.g., lookup-table based approaches) for optimal edge device implementation. The process of building optimal sparse masks (i.e., selecting which specific parameters to train for a given task) warrants further investigation. SHiRA-Struct, while effective for vision tasks, performed poorly on complex language tasks (commonsense reasoning) likely due to its rank 1 + diagonal nature, indicating potential limitations in expressive power for certain domains. The theoretical orthogonality analysis is confined to adapter weights and does not extend to the orthogonality of subspaces. Finally, similar to other parameter-efficient finetuning techniques, SHiRA carries the potential risk of digital forgery due to finetuning generative models on smaller datasets, which could lead to overfitting.",
        "future_research_directions": "Future work could explore hardware-software co-design techniques, such as lookup-table (LUT) based approaches, to optimize SHiRA's implementation for edge devices. Further investigation into building optimal sparse masks is needed to identify which specific parameters are most effective to train for various tasks. The authors also suggest a subspace analysis of SHiRA, extending beyond the current focus on adapter weight orthogonality. Additionally, balancing single adapter performance (expressive power) with multi-adapter fusion capabilities remains an open area for research."
      }
    },
    {
      "title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
      "abstract": "Despite the widespread adoption of prompting, prompt tuning and prefix-tuning\nof transformer models, our theoretical understanding of these fine-tuning\nmethods remains limited. A key question is whether one can arbitrarily modify\nthe behavior of pretrained model by prompting or prefix-tuning it. Formally,\nwhether prompting and prefix-tuning a pretrained model can universally\napproximate sequence-to-sequence functions. This paper answers in the\naffirmative and demonstrates that much smaller pretrained models than\npreviously thought can be universal approximators when prefixed. In fact, the\nattention mechanism is uniquely suited for universal approximation with\nprefix-tuning a single attention head being sufficient to approximate any\ncontinuous function. Moreover, any sequence-to-sequence function can be\napproximated by prefixing a transformer with depth linear in the sequence\nlength. Beyond these density-type results, we also offer Jackson-type bounds on\nthe length of the prefix needed to approximate a function to a desired\nprecision.",
      "full_text": "Prompting a Pretrained Transformer Can Be a Universal Approximator Aleksandar Petrov1 Philip H.S. Torr1 Adel Bibi 1 Abstract Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key ques- tion is whether one can arbitrarily modify the be- havior of pretrained model by prompting or prefix- tuning it. Formally, whether prompting and prefix- tuning a pretrained model can universally approxi- mate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when pre- fixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix- tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approx- imated by prefixing a transformer with depth lin- ear in the sequence length. Beyond these density- type results, we also offer Jackson-type bounds on the length of the prefix needed to approximate a function to a desired precision. 1. Introduction The scale of modern transformer architectures (Vaswani et al., 2017) is ever-increasing and training competitive models from scratch, even fine-tuning them, is often pro- hibitively expensive (Lialin et al., 2023). To that end, there has been a proliferation of research aiming at efficient train- ing in general and fine-tuning in particular (Rebuffi et al., 2017; Houlsby et al., 2019; Hu et al., 2021; 2023). Motivated by the success of few- and zero-shot learning (Wei et al., 2021; Kojima et al., 2022), context-based fine- tuning methods do not change the model parameters. In- stead, they modify the way the input is presented. For example, with prompting, one fine-tunes a string of tokens (a prompt) which is prepended to the user input (Shin et al., 2020; Liu et al., 2023). As optimizing over discrete tokens 1Department of Engineering Science, University of Oxford, UK. Correspondence to: A. Petrov <aleks@robots.ox.ac.uk>. is difficult, one can optimize the real-valued embeddings in- stead (soft prompting, prompt tuning, Lester et al. 2021). A generalization to this approach is the optimization over the embeddings of every attention layer (prefix-tuning, Li and Liang 2021). These methods are attractive as they require a few learnable parameters and allow for different prefixes to be used for different samples in the same batch which is not possible with methods that change the model param- eters. As every prompt and soft prompt can be expressed as prefix-tuning (Petrov et al., 2024), in this paper, we will focus primarily on prefix-tuning. While these context-based fine-tuning techniques have seen widespread adoption and are, in some cases, competitive to full fine-tuning (Liu et al., 2022), our understanding of their abilities and restrictions remains limited. How much can the behavior of a model be modified without changing any model parameter? Given a pretrained transformer and an arbitrary target function, how long should the prefix be so that the transformer approximates this function to an arbitrary precision? Differently put, can prefix-tuning of a pretrained transformer be a universal approximator? These are some of the questions we aim to address in this work. It is well-known that fully-connected neural networks with suitable activation functions can approximate any continu- ous function (Cybenko, 1989; Hornik et al., 1989; Barron, 1993; Telgarsky, 2015), while Recurrent Neural Networks (RNNs) can approximate dynamical system. The attention mechanism (Bahdanau et al., 2015) has also been studied in its own right. Deora et al. (2023) derived convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, and Mah- davi et al. (2023) showed that the memorization capacity increases linearly with the number of attention heads. On the other hand, it was shown that attention layers are not expressive enough as they lose rank doubly exponentially with depth if Multi-Layer Perceptrons (MLPs) and residual connections are not present (Dong et al., 2021). However, attention layers, with a hidden size that grows only loga- rithmically in the sequence lengths, were shown to be good approximators for sparse attention patterns (Likhosherstov et al., 2021), except for a few tasks that require a linear scal- ing of the size of the hidden layers in the sequence length (Sanford et al., 2023). 1 arXiv:2402.14753v1  [cs.LG]  22 Feb 2024Prompting a Pretrained Transformer Can Be a Universal Approximator Considering universal approximation using encoder-only transformers, Yun et al. (2019) showed that transformers are universal approximators of sequence-to-sequence func- tions by demonstrating that self-attention layers can com- pute contextual mappings of input sequences. Jiang and Li (2023) demonstrated universality by instead leveraging the Kolmogorov-Albert representation Theorem. Moreover, Al- berti et al. (2023) provided universal approximation results for architectures with non-standard attention mechanisms. Despite this interest in theoretically understanding the ap- proximation properties of the transformer architecture when being trained, much less progress has been made in under- standing context-based fine-tuning methods such as prompt- ing, soft prompting, and prefix-tuning. Petrov et al. (2024) have shown that the presence of a prefix cannot change the relative attention over the context and experimentally demonstrate that one cannot learn completely novel tasks with prefix-tuning. In the realm of in-context learning, where input-target pairs are part of the prompt (Brown et al., 2020), Xie et al. (2021) and Yadlowsky et al. (2023) show that the ability to generalize depends on the choice of pre- training tasks. However, these are not universal approx- imation results. The closest to our objective is the work of Wang et al. (2023). They quantize the input and output spaces allowing them to enumerate all possible sequence-to- sequence functions. All possible functions and inputs can then be hard-coded in a transformer using the constructions by Yun et al. (2019). As this approach relies on memo- rization, the depth of the model depends on the desired approximation precision ϵ. In this work, we demonstrate that prefix-tuning can be a uni- versal approximator much more efficiently than previously assumed. In particular: i. We show that attention heads are especially suited to model functions over hyperspheres, concretely, prefix- tuning a single attention head is sufficient to approx- imate any smooth continuous function on the hyper- sphere Sm to any desired precision ϵ; ii. We give a bound on the required prompt length to approximate a smooth target function to a precision ϵ; iii. We demonstrate how this result can be leveraged to approximate general sequence-to-sequence functions with transformers of depth linear in the sequence length and independent of ϵ; iv. We discuss how prefix-tuning may result in element- wise functions which, when combined with cross- element mixing from the pretrained model, may be able to explain the success behind prefix-tuning and prompting and why it works for some tasks and not others. 2. Background Material 2.1. Transformer Architecture In soft prompting and prefix-tuning, the focus of this work, the sequence fed to a transformer model is split into two parts: a prefix sequence P = ( p1, . . . ,pN ), which is to be learnt or hand-crafted, and an input sequence X = (x1, . . . ,xT ), where xi, pi ∈ Rd. A transformer operat- ing on a sequence consists of alternating attention blocks which operate on the whole sequence and MLPs that op- erate on individual elements. For a sequence of length N + T, an attention head of dimension d is a function ˜u : Rd×(N+T) → Rd×(N+T). Since we will only be in- terested in the output at the positions corresponding to the inputs X, we use u(· ; P) : Rd×(N+T) → Rd×T to denote the output of ˜u at the locations corresponding to the input X when prefixed with P. Therefore, the k-th output of u is defined as: [u(X;P)]k= NX i=1 exp(x⊤kHpi)WVpi+ TX j=1 exp(x⊤kHxj)WVxj NX i=1 exp(x⊤k Hpi) + TX j=1 exp(x⊤k Hxj) , (1) where WV , the value matrix, and H are in Rd×d. H is typically split into two lower-rank matrices H = W⊤ Q WK, query and key matrices. Multiple attention heads can be combined into an attention block but, for simplicity, we will only consider single head attention blocks. A transformer is then constructed by alternating attention heads and MLPs. We consider pretrained transformers but, in the context of this work, these are constructed rather than trained. We refer to the matrices WV , WQ, WK along with the parameters of the MLPs as pretrained parameters, and they are fixed throughout and not learnt. The prefix P is the only variable that can be modified to change the behavior of the model. 2.2. Universal Approximation Let X and Y be normed vector spaces. We consider a fam- ily of target functions which is a subset C of all mappings X→Y , i.e., C ⊆ YX , with C is often referred to as aconcept space. These are the relationships we wish to learn by some simpler candidate functions. Let us denote this set of can- didates by H ⊂ YX , called hypothesis space. The problem of approximation is concerned with how well functions in H approximate functions in C. There are two main ways to measure how well functions in H represent functions in C: density results and approximation rate results (Jiang et al., 2023). Density results show that, given an ϵ, one can find a hypothesis h ∈ Happroximating any f ∈ Cwith error at most ϵ. Approximation rate results, also called Jackson- type, are stronger as they offer a measure of complexity for h to reach a desired precision ϵ. Classically, a Jackson-type 2Prompting a Pretrained Transformer Can Be a Universal Approximator result would provide a minimum width or depth necessary for a neural network to reach a desired precision ϵ. In the context of the present work, the notion of complexity that we care about is the length N of the prefix P. Formally: Definition 1 (Universal Approximation (Density-Type)) . We say that H is a universal approximator for C over a compact set S ⊆ Xif for every f ∈ Cand every ϵ > 0 there exists an h ∈ Hsuch that supx∈S ∥f(x) −h(x)∥ ≤ϵ. One typically says that H is dense in C. Lemma 1 (Transitivity). If A is dense in B and B is dense in C, then A is dense in C. Definition 2 (Approximation Rate (Jackson-Type)). Fix a hypothesis space H. Let {HN : N ∈ N+} be a collection of subsets of H such that HN ⊂ HN+1 and S N∈N+ HN = H. Here, N is a measure of the complexity of the approxima- tion candidates, and HN is the subset of hypotheses with complexity at most N. Then, the approximation rate esti- mate for C over a compact S ⊆ Xis a bound ZH: N ≥ ZH(f, ϵ) =⇒ inf h∈HN sup x∈S ∥f(x)−h(x)∥ ≤ϵ, ∀f∈C. ZH gives an upper bound to the minimum hypothesis com- plexity necessary to reach the target precisionϵ and typically depends on the smoothness of f. Lemma 2. A Jackson bound for {HN | N ∈ N+} with finite ZH for all f ∈ C,ϵ > 0 immediately implies thatS N∈N+ HN = H is dense in C. Hence, Jackson bounds (Definition 2) are stronger than density results (Definition 1). The key hypothesis classes we consider in this work are the set of all prefixed attention heads and the set of pre- fixed transformers. This is very different from the classical universal approximation setting. The hypothesis classes in the classical universal approximation results consist of all possible parameter values of the model itself (Cybenko, 1989; Yun et al., 2019). When studying universal approxi- mation with prefixing, the model parameters are fixed where prefixes are what can be modified. Definition 3 (Prefixed Attention Heads Class) . This is the class of all attention heads as defined in Equation (1) of dimension d, input/output sequence of length T, pre- fix of length at most N, and fixed pretrained components H, WV ∈ Rd×d: HN,T −,d (H, WV ) = \u001a u : Rd×(N′+T)→Rd×T , [u]k as in (1), pi ∈ Rd, N′ ≤ N \u001b . For simplicity, we say that HN,T −,d is dense in C to imply that there exists a pair (H, WV ) such that HN,T −,d (H, WV ) is dense in C. When considering all possible prefix lengths, we drop the N: HT −,d = S N∈N HN,T −,d . Definition 4 (Prefixed Transformers Class). A transformer consists of L layers with each layer l consisting of an at- tention head with Hl and Wl V followed by an MLP con- sisting of kl linear layers, each parameterized as Ll k(x) = Al,kx+ bl,k interspersed with non-linear activation σ. This gives rise to the following hypothesis class when prefixed: HN,T ≡,d \u0012n Hl,Wl V ,{(Al,k,bl,k)}kl k=1 oL l=1 \u0013 = ( LL kL ◦...◦σ ◦ LL 1 ◦hL...◦h2 ◦ L1 k1 ◦...◦σ ◦ L1 1 ◦h1 withhl ∈ HN′,T −,d (Hl,Wl V ), l= 1, . . . , L, N′ ≤ N. ) , applying linear layers L element-wise. Again, we say HN,T ≡,d is dense in C, as a shorthand, to there exists {Hl,WlV,{Al,k,bl,k}kl k=1}Ll=1 such that HN,T ≡,d ({Hl, Wl V , Al,k, bl,k}L l=1) is dense in C. In this paper, we consider several different concept classes. For reasons that will become apparent in the following section, we focus on functions whose domain is a hyper- sphere Sm={y∈Rm+1 | ∥y∥2=1}⊂Rm+1. We consider both scalar and vector-valued functions on the hypersphere. Definition 5 (Scalar Functions on the Hypersphere). Define C(Sm) ⊂ RSm to be the space of all continuous functions defined on Sm with bounded norm, i.e., ∥f∥∞ = sup x∈Sm |f(x)| < ∞, f ∈ C(Sm). (2) This is the concept class Cs,m = C(Sm) ⊂ RSm . Definition 6 (Vector-valued Functions on the Hypersphere). The class of vector-valued functions on the hypersphere is: Cv,m = {f : Sm→Rm+1 | [f]i ∈ C(Sm), i= 1, ..., m+1}. Transformers are typically used to learn mappings over se- quences rather than individual inputs. Hence, we define several sequence-to-sequence concept classes. Definition 7 (General Sequence-to-sequence Functions) . Given a fixed sequence length T ∈ N>0, we define the sequence-to-sequence function class as: CT,m = {f : (Sm)T→(Rm+1)T | f continuous and bounded}. We will also consider the subset of element-wise functions: Definition 8 (Element-wise functions). Element-wise func- tions operate over sequences of inputs but apply the exact same function independently to all inputs: C∥,T,m=    there existsg ∈ Cv,m, such that f ∈ CT,m f(x1, ...,xT )=(g(x1), ..., g(xT )) for all (x1, ...,xT ) ∈ (Sm)T   . 3. Universal Approximation with a Single Attention Head In this section, we will restrict ourselves to the setting when the input sequence is of length T=1, i.e., X=(x). Gen- eral sequence-to-sequence functions will be discussed in 3Prompting a Pretrained Transformer Can Be a Universal Approximator C. Classic attention head A. Core attention head D. Classic attention head Core attention head control points centered at the KernelsControl points    and  B. N=100 N=200 N=1000 λ=100λ=50λ=10 Figure 1. Approximating functions on the hypersphere with a single attention head. A. We simplify the classical attention head into a core attention head. B. The exp(λ⟨x, pα k ⟩)pβ k terms act like kernels when x is restricted to a hypersphere. We can approximate a function f by placing N control points pα 1 , ...,pα N and centering a kernel at each of them. C. Increasing λ results in less smoothing, while increasing N results in more control points and hence better approximation. With large enough λ and N, we can approximate f to any desired accuracy. D. With the normalization term in classical attention close to a constant, and giving x, pα k and pβ k orthogonal subspaces, core attention can be represented as classical attention. Hence, a classical attention head can also approximate f with arbitrary precision. Section 4. We will show that a single attention head can approximate any continuous function on the hypersphere, or that H1 −,m+1 is dense in Cs,m. To do this, we first sim- plify the classical attention head in Equation (1), resulting in what we call a core attention head. Then, we show that each of the terms in the core attention act as a kernel, meaning that it can approximate any function in Cs,m. Finally, we show that any core attention head can be approximated by a classical attention head, hence, H1 −,m+1 is indeed dense in Cs,m. The complete pipeline is illustrated in Figure 1. To illuminate the approximation abilities of the attention head mechanism we relax it a bit. That is, we allow for different values of the prefix positions when computing the attention (the exp terms in Equation (1)) and when computing the value (the right multiplication withWV ). We will also drop the terms depending only on x, set H = λId, λ >0, and WV = Id. We refer to this relaxed version as a split attention head with its corresponding hypothesis class: h ⇋(x) = PN′ k=1 exp(λ⟨x, pα k ⟩)pβ kPN′ k=1 exp(λ⟨x, pα k ⟩) . (3) Definition 9 (Split Attention Head Class). HN⇋,d = n h ⇋as in (3), pα k , pβ k ∈ Rd, N′ ≤ N, λ >0 o . We will later show that a split head can be represented by a classical attention head. For now, let us simplify a bit further: we drop the denominator, resulting in our core attention head: h⊛(x) = PN k=1 exp(λ⟨x, pα k ⟩)pβ k, (4) which gives rise to the hypothesis class: HN ⊛,d = ( x 7→ PN′ k=1 exp(λ⟨x, pα k ⟩)pβ k, where pα k , pβ k ∈ Rd, N′ ≤ N, λ >0 ) . We also have their scalar-valued counterparts: h⊙(x) = PN k=1 exp(λ⟨x, pα k ⟩)pβ k, (5) HN ⊙,d = ( x 7→ PN′ k=1 exp(λ⟨x, pα k ⟩)pβ k, where pα k ∈ Rd, pβ k ∈ R, N′ ≤ N, λ >0 ) . As the dot product is a notion of similarity, one can interpret h⊛ in Equation (4) and h⊙ in Equation (5) as interpolators. The pα i vectors act as control points, while the pβ i vectors designate the output value at the location of the correspond- ing control point. The dot product with the input x controls how much each control point should contribute to the final result, with control points closer to x (larger dot product) contributing more. Unfortunately, it is not generally true that higher dot product means smaller distance, hence the above interpretation fails in Rm+1. To see this, consider two control points pα 1 , pα 2 ∈ Rm+1 such that pα 2 = tpα 1 , with t >1. Then for x = pα 1 4Prompting a Pretrained Transformer Can Be a Universal Approximator 1 1 θ θ = cos−1(⟨x, pα 1 ⟩) = cos−1(⟨x, pα 2 ⟩) x pα 2 pα 1 Figure 2. The dot product is a measure of closeness over the hypersphere. We want large dot product for points with lower distances. That is not the case for general pα 1, pα 2 ∈ Rm+1: above we show larger dot product for points which are further away, i.e., ⟨x, pα 1 ⟩<⟨x, pα 2 ⟩ despite ∥x-pα 1 ∥2<∥x-pα 1 ∥2. However, if we restrict x, pα i , and pα j to the hypersphere Sm, then the dot product measures the cosine between x and pi which is truly a measure of closeness: ⟨x, pα i ⟩<⟨x, pα j ⟩ ⇐⇒ ∥x-pα i ∥2>∥x-pα j ∥2. we would have ⟨x, pα 1 ⟩=∥pα 1 ∥2 2 < ⟨x, pα 2 ⟩=t∥pα 1 ∥2 2; the dot product is smaller forpα 1 , the control point that is closer tox, than for the much further awaypα 2 (see Figure 2). Therefore, the further away control point has a larger contribution than the closer point, which is at odds with the interpolation behaviour we desire. In general, the contribution of control points with larger norms will “dominate” the one of points with smaller norms. This has been observed for the attention mechanism in general by Demeter et al. (2020). Fortunately, the domination of larger norm control points pα i is not an issue if all control points have the same norm. In particular, if x and pα i lie on the unit hypersphere Sm={y∈Rm+1 | ∥y∥2=1} then ⟨x, pα i ⟩ = cos(∠(x, pα i )) and it has the desired property that the closer x is to pα i , the higher their dot product. By doing this, we restrict h⊛ to be a function from the hypersphere Sm to Rm+1. While this might seem artificial, modern transformer architectures do operate over hyperspheres as LayerNorm projects activa- tions to Sm (Brody et al., 2023). The central result of this section is that the functions in the form of Equation (5) can approximate any continu- ous function defined on the hypersphere, i.e., H⊙,m+1 =S∞ N=1 HN ⊙,m+1 is dense in Cs,m (Definition 5) and H⊛,m+1 = S∞ N=1 HN ⊛,m+1 is dense in Cv,m (Definition 6). Furthermore, we offer a Jackson-type approximation rate result which gives us a bound on the necessary prefix length N to achieve a desired approximation quality. Theorem 1 (Jackson-type Bound for Universal Approxima- tion on the Hypersphere). Let f ∈ C(Sm) be a continuous function on Sm, m ≥ 8 with modulus of continuity ω(f;t)= sup{|f(x)-f(y)| |x,y∈Sm,cos-1(⟨x,y⟩) ≤ t} ≤Lt, for some L > 0. Then, for any ϵ > 0, there exist pα 1 , . . . ,pα N ∈ Sm and pβ 1 , . . . , pβ N ∈ R such that sup x∈Sm \f\f\f\f\ff(x) − NX k=1 exp(λ⟨x, pα k ⟩)pβ k \f\f\f\f\f ≤ ϵ, where λ = Λ(ϵ/2) with Λ(σ) = (8LCR+mσ+σ) \u0010 1- σ2 8LCHCR+2σCH \u0011 σ 4LCR+σ σ \u0012 1- \u0010 1- σ2 8LCHCR+2σCH \u0011 2σ 4LCR+σ \u0013 = O \u0010 L3CH σ4 \u0011 , (6)and any N ≥ N(λ, ϵ) with N(λ, ϵ)=Φ(m) \u00103π(L+λ∥f∥∞)cm+1(λ) exp(λ) ϵ \u00112(m+1) =O(ϵ-10-14m-4m2 ), (7) with CH being a constant depending on the smoothness of f (formally defined in the proof), CR being a constant not depending on f or ϵ, Φ(m) = O(m log m) being a function that depends only on the dimension m and cm+1 being a normalization function. Corollary 1. H⊙,m+1 is dense in Cs,m Proof. Theorem 1 holds for all ϵ >0 and Lemma 2. Theorem 1 is a Jackson-type result as Equation (7) gives the number N of control points needed to approximate f with accuracy ϵ. This corresponds to the length of the prefix sequence. Moreover, the smoother the target f is, i.e., the smaller L, CH, the shorter the prefix length N. Thus, our construction uses only as much prefix positions as necessary. The proof of Theorem 1 follows closely (Ng and Kwong, 2022). While they only provide a density result, we offer a Jackson-type bound which is non-trivial and may be of an independent interest. The idea behind the proof is as following. We first approximate f with its convolution with a kernel having the form of the terms in Equation (5): (f ∗ KvMF λ )(x)=R Sm cm+1(λ) exp(λ⟨x,y⟩)f(y) dwm(y). (8) The larger the λ is, the closer f ∗KvMF λ is to f and hence the smaller the approximation error (Menegatto, 1997). Λ(ϵ/2) gives the smallest value forλ such that this error isϵ/2. Equa- tion (8) can then be approximated with sums: we partition Sm into N sets V1, ..., VN small enough that f does not vary too much within each set. Each control point pα k is placed in its corresponding Vk. Then, exp(λ⟨x, y⟩)f(y) can be approximated with exp(λ⟨x, pα k ⟩)f(pα k ) when y is in the k-th set Vk. Hence, Equation (8) can be approximated withPN k=1 exp(λ⟨x, pα k ⟩) Cf (pα k ) for some suitable constant C. By increasing N we can reduce the error of approxi- mating the convolution with the sum. Equation (7) gives us the minimum N needed so that this error is ϵ/2. Hence, we have error of at most ϵ/2 from approximating f with the convolution and ϵ/2 from approximating the convolution with the sum, resulting in our overall error being bounded by ϵ. The full proof is in Appendix B and is illustrated in Figure 4. The theorem can be extended to vector-valued functions in Cv,m with a multiplicative factor 1/√m+1: Corollary 2. Let f : Sm→Rm+1, m≥8 be such that each component fi satisfies the conditions in Theorem 1. De- fine ∥f∥∞= max1≤i≤m+1∥fi∥∞. Then, for any ϵ>0, there 5Prompting a Pretrained Transformer Can Be a Universal Approximator exist pα 1 , ...,pα N ∈ Sm and pβ 1 , ...,pβ N ∈ Rm+1 such that sup x∈Sm \r\r\r\r\rf(x) − NX k=1 exp(λ⟨x, pα k ⟩)pβ k \r\r\r\r\r 2 ≤ ϵ, with λ = Λ(ϵ/2√m+1) for any N ≥ N(λ, ϵ/√m+1). That is, H⊛,m+1 is dense in Cv,m with respect to the ∥ · ∥2 norm. Thanks to Theorem 1 and Corollary 2, we know that func- tions in Cv,m can be approximated by core attention (Equa- tion (4)). We only have to demonstrate that a core attention head can be represented as a classical attention head (Equa- tion (1)). We do this by reversing the simplifications we made when constructing the core attention head. Let’s start by bringing the normalization term back, resulting in HN⇋,d, the split attention head hypothesis (Definition 9). In- tuitively, Pn k=1 exp(λ⟨x, pα k ⟩) is almost constant when the pα k are uniformly distributed over the sphere as the distribu- tion of distances from x to pα k will be similar, regardless of where x lies. We can bound how far Pn k=1 exp(λ⟨x, pα k ⟩) is from being a constant and adjust the approximation error to account for it. Appendix C has the full proof. Theorem 2. Let f : Sm→Rm+1, m≥8 be such that each component fi satisfies the conditions in Theorem 1. Then, for any 0<ϵ<2∥f∥∞, there exist pα 1 , ...,pα N ∈Sm such that sup x∈Sm \r\r\r\r\rf(x) − PN k=1 exp(λ⟨x, pα k ⟩)pβ kPN k=1 exp(λ⟨x, pα k ⟩) \r\r\r\r\r 2 ≤ ϵ, with λ = Λ \u0012 2ϵL 2L + ∥f∥∞ \u0013 pβ k = f(pα k ), ∀k = 1, . . . , N, for any N ≥ N(λ, ϵ/√m+1). That is, H ⇋,m+1 is dense in Cv,m with respect to the ∥ · ∥2 norm. An interesting observation is that adding the normalization term has not affected the asymptotic behavior ofλ and hence also of the prefix length N. Furthermore, notice how the value pβ i at the control point pα i is simply f(pα k ), the target function evaluated at this control point. We ultimately care about the ability of the classical attention head (Definition 3) to approximate functions in Cv,m by prefixing. Hence, we need to bring back the terms depending only on the input x, combine pα k and pβ k parts into a single prefix pk and bring back the H and WV matrices. One can do this by considering an attention head with a hidden dimension 3(m+1) allowing us to place x, pα k and pβ k in different subspaces of the embedding space. To do this, define a pair of embedding and projection operations: Π : Sm → R3(m+1) Π−1 : R3(m+1) → Rm+1 x 7→   Im+1 0m+1 0m+1  x x 7→   Im+1 0m+1 0m+1   ⊤ x. Lemma 3. Π−1 ◦ H1 −,3(m+1) ◦ Π is dense in H ⇋,m+1, with the composition applied to each function in the class. Proof. We can prove something stronger. For all f ∈ H ⇋,m+1 there exists a g ∈ H1 −,3(m+1) such that f = Π−1 ◦ g ◦ Π. If f ∈ H ⇋,m+1, then f(x) = PN k=1 pβ k exp(λ⟨x, pα k ⟩) PN k=1 exp(λ⟨x, pα k ⟩) , ∀x ∈ Sm for some N, λ, pα i , pβ i . Define: pk=   0 λpα k pβ k   ∈ R3(m+1), H=   MI I 0 0 0 0 0 0 0  , WV =   0 0 I 0 0 0 0 0 0  ∈R3(m+1)×3(m+1), With M a negative constant tending to −∞. Then: g(x) = NX i=1 exp(x⊤Hpi)WV pi+ exp(x⊤Hx)WV x NX i=1 exp(x⊤Hpi) + exp(x⊤Hx) , is in H1 −,3(m+1) and f = Π−1 ◦ g ◦ Π. As this holds for all f ∈ H ⇋,m+1, it follows that H ⇋,m+1 ⊂ Π−1 ◦H1 −,3(m+1) ◦ Π. Hence, Π−1 ◦ H1 −,3(m+1) ◦ Π is dense in H ⇋,m+1. Lemma 3 shows that every split attention head can be ex- actly represented as 3 times bigger classical attention head. Note that our choice for H and WV is not unique. Equiva- lent constructions are available by multiplying each compo- nent by an invertible matrix, effectively changing the basis. Finally, the embedding and projection operations can be represented as MLPs and hence can be embedded in a trans- former architecture. Now, we can provide the final result of this section, namely that the standard attention head of a transformer can approximate any vector-valued function on the hypersphere: Theorem 3. Let f : Sm → Rm+1, m ≥ 8 be such that each component fi satisfies the conditions in Theorem 1. Then, for any 0 < ϵ ≤ 2∥f∥∞, there exists an attention head h ∈ HN,1 −,3(m+1) such that supx∈Sm∥f(x) − (Π−1 ◦ h ◦ Π)(x)∥2 ≤ ϵ, (9) 6Prompting a Pretrained Transformer Can Be a Universal Approximator for any N ≥ N(λ, ϵ/√m+1). That is, Π−1 ◦H1 −,3(m+1) ◦Π is dense in Cv,m with respect to the ∥ · ∥2 norm. Proof. The density result follows directly from Theorem 2 and Lemma 3 and transitivity (Lemma 1). The Jackson bound is the same as in Theorem 2 as transforming the split attention head to a classical attention head is exact and does not contribute further error. Therefore, we have shown that a single attention head with a hidden dimension 3(m+1) can approximate any continuous function f : C(Sm)→Rm+1 to an arbitrary accuracy. This is for fixed pretrained components, that is, H and WV are as given in the proof of Lemma 3 and depend neither on the input x nor on the target function f. Therefore, the behavior of the attention head is fully controlled by the prefix. This is a Jackson-type result, with the length N of the prefix given in Theorem 2. To the best of our knowledge, Theorem 3 is the first bound on the necessary prefix length to achieve a desired accuracy of function approximation using an attention head. Most critically, Theorem 3 demonstrates that attention heads are more expressive than commonly thought. A single attention head with a very simple structure can be a universal approximator. 4. Universal Approximation of Sequence-to-Sequence Functions The previous section showed how we can approximate any continuous f : Sm → Rm+1 with a single attention head. Still, one typically uses the transformer architecture for operations over sequences rather than over single inputs (the case with T ≥ 1). We will now show how we can leverage Theorem 3 to model general sequence-to-sequence functions. First, we show the simpler case of functions that apply the exact same mapping to all inputs. We then show how to model general sequence-to-sequence functions using a variant of the Kolmogorov–Arnold theorem. Element-wise functions Theorem 3 can be extended to element-wise functions where the exact same func- tion is applied to each element in the input sequence, i.e., the concept class C∥,T,m from Definition 8. If f ∈ C∥,T,m, then there exists a g ∈ Cv,m such that f(x1, . . . ,xT )=(g(x1), . . . , g(xT )). By Theorem 3, there exists a prefix p1, ...,pN that approximates g. As the con- struction in Lemma 3 prevents interactions between two dif- ferent inputs xi and xj, an attention head hT ∈ HN,T −,3(m+1) for a T-long input (Equation (1)) with the exact same prefix p1, . . . ,pN approximates f: Corollary 3. Π−1 ◦ HT −,3(m+1) ◦Π is dense in C∥,T,m with respect to the ∥ · ∥2 norm applied element-wise. That is, for every ϵ >0, there exists hT ∈ HN,T −,3(m+1) such that: sup {xi}∈(Sm)T max 1≤k≤T \r\r\u0002f({xi}) −(Π−1◦hT◦Π)({xi})\u0003 k \r\r 2 ≤ ϵ, with Π and Π−1 applied element-wise, [·]k selecting the k-th element, and approximate rate bound onN as in Theorem 3. General sequence-to-sequence functions Ultimately, we are interested in modeling arbitrary functions from se- quences of inputs (x1, . . . ,xT ) to sequences of outputs (y1, . . . ,yT ), that is, the CT,m. We will use a version of the Kolmogorov–Arnold representation Theorem. The Theo- rem is typically defined on functions over the unit hypercube [0, 1]m. As there exists a homeomorphism between [0, 1]m and a subset of Sm (Lemma D.1), for simplicity, we will ignore this technical detail. Our construction requires only T + 2 attention layers, each with a single head. The original Kolmogorov-Arnold representation theorem (Kolmogorov, 1957) identifies every continuous function f : [0, 1]d → R with univariate functions gq, ψp,q such that: f(x1, . . . , xd) = P2d q=0 gq \u0010Pd p=1 ψp,q(xp) \u0011 . In other words, multivariate functions can be represented as sums and compositions of univariate functions. As trans- formers are good at summing and attention heads are good at approximating functions, they can approximate functions of this form. However, gq and ψp,q are generally not well- behaved (Girosi and Poggio, 1989), so we will use the con- struction by Schmidt-Hieber (2021) instead. Lemma 4 (Theorem 2 in (Schmidt-Hieber, 2021)). For a fixed d, there exists a monotone functions ψ : [0, 1] → C (the Cantor set) such that for any function f : [0, 1]d → R, we can find a function g : C → R such that i. f(x1, . . . , xd) = g \u0010 3 Pd p=1 3−p ψ(xp) \u0011 , (10) ii. if f is continuous, then g is also continuous, iii. if |f(x) − f(y)| ≤Q∥x − y∥∞, for all x, y ∈ [0, 1]d and some Q, then |g(x) − g(y)| ≤2Q, ∀x, y∈ C. In comparison with the original Kolmogorov–Arnold the- orem, we need a single inner function ψ which does not depend on the target function f and only one outer function g. Furthermore, both ψ and g are Lipschitz. Hence, we can approximate them with our results from Section 3. We need to modify Lemma 4 a bit to make it fit the sequence- to-sequence setting. First, flatten a sequence of T (m + 1)- dimensional vectors into a single vector in [0, 1](m+1)T . Second, define Ψd : [0, 1]d → Rd to be the element-wise application of ψ: Ψd({xi}d i=1) = ( ψ(xi))d i=1. We can 7Prompting a Pretrained Transformer Can Be a Universal Approximator also define Gi : C → Rm+1, i = 1 , . . . , Tand extend Equation (10) for our setting: f(x1, ...,xT ) = (G1(R), . . . , GT (R)), with R = 3 TX i=1 3-(i-1)(m+1) m+1X p=1 3-pψ(xi,p) (11) = 3 TX i=1 3-(i-1)(m+1)   3-1 ... 3-(m+1)   ⊤ Ψm+1(xi). Equation (11) can now be represented with a transformer with T + 2attention layers. Ψm+1 is applied element-wise, hence, all Ψm+1(xi) can be computed in parallel with a single attention head (Corollary 3). The dot product with the \u0002 3-1 ··· 3-(m+1)\u0003 vector can be computed using a single MLP. The product with the 3-(i-1)(m+1) scalar is a bit more challenging as it depends on the position in the sequence. However, if we concatenate position encodings to the input, another MLP can use them to compute this factor and the multiplication. The outer sum over the T inputs and the multiplication by 3 can be achieved with a single attention head. Hence, using only 2 attention layers, we have compressed the whole sequence in a single scalar R. 1 The only thing left is to apply G1, . . . , GT to R to compute each of the T outputs. As each one of these is Lipschitz, we can approximate each with a single attention head using Theorem 3. Each Gi is different and would need its own set of prefixes, requiring T attention heads arranged in T attention layers. Using the positional encodings, each layer can compute the output for its corresponding position and pass the input unmodified for the other positions. The over- all prefix size would be the longest of the prefixes necessary to approximate Ψm+1, G1, . . . , GT . Hence, we have constructed an architecture that can approx- imate any sequence-to-sequence function f ∈ CT,m with only T+2 attention layers. Thus, HT ≡,d is dense in CT,m. 5. Discussion and Conclusions Comparison with prior work Just like us, Wang et al. (2023) show that prefix-tuning can be a universal approxi- mator. Their approach relies on discretizing the input space and the set of sequence-to-sequence functions to a given precision depending on ϵ, resulting in a finite number of pairs of functions and inputs, each having a unique cor- responding output. Then, using the results of Yun et al. (2019), they construct a meta-transformer which maps each of the function-input pairs to their corresponding output. 1Yun et al. (2019) use a similar approach but use discretization to enumerate all possible sequences and require O(ϵ−m) attention layers. In our continuous setting, R is computed with 2 layers. This approach has several limitations: i) the model has ex- ponential depth O(T ϵ−m); ii) reducing the approximation error ϵ requires increasing the model depth; iii) the prefix length is fixed, hence a constant function and a highly non- smooth function would have equal prefix lengths, and iv) it effectively has memorized all possible functions and inputs, explaining the exponential size of their constructions. In contrast, we show that memorization is not needed: atten- tion heads are naturally suited for universal approximation. Section 4 showed that T + 2 layers are enough, we require shorter prefixes for more smooth functions and reducing the approximation error ϵ can be done by increasing the prefix length, without modifying the pretrained model. Petrov et al. (2024) have shown that prefix-tuning cannot change the relative attention patterns over the input tokens and hence cannot learn tasks with new attention patterns. This appears to be a limitation but V on Oswald et al. (2023) and Aky¨urek et al. (2022) proved that there exist attention heads that can learn any linear model, samples of which are given as a prefix. In this work, we showed the existence of a “universal” attention head (H and WV in Lemma 3) that can be used to emulate any new function defined as a prefix. Prefixes have been observed to have larger norms than token embeddings (Bailey et al., 2023). Our results provide an explanation to that. While the control points pα k are in Sm and hence have norm 1, in Lemma 3 we fold λ into them. Recall that the less smooth f is, the higher the concentration parameter λ has to be in order to reduce the influence of one control point on the locations far from it. Hence, the less smooth f is, the larger the norm of the prefixes. Connection to prompting and safety implications While this work focused on prefix-tuning, the results can extend to prompting. Observe that prefix tuning (where we have a distinct prefix) can be reduced to soft prompting (where only the first layer is prefixed) by using an appropri- ate attention mechanism and position embeddings. Hence, if a function f ∈ CT,m requires N prefixes to be approx- imated to precision ϵ with prefix-tuning, it would require O(T N) soft tokens to be approximated with soft prompt- ing. Finally, observe that a soft token can be encoded with a sequence of hard tokens, the number of hard tokens per soft token depends on the required precision and the vo- cabulary size V . Hence, f could be approximated with O(logV (ϵ−1)mT N) hard tokens. Therefore, our univer- sal approximation results may translate to prompting. This raises concerns as to whether it is at all possible to prevent a transformer model from exhibiting undesirable behaviors (Zou et al., 2023; Wolf et al., 2023; Chao et al., 2023). Fur- thermore, this means that transformer-based agents might have the technical possibility to collude in undetectable and uninterpretable manner (de Witt et al., 2023). Still, our results require specific form of the attention and value ma- 8Prompting a Pretrained Transformer Can Be a Universal Approximator trices and, hence, it is not clear whether these risk translate to real-world models. Prefix-Tuning and Prompting a Pretrained Transformer might be Less efficient than Training it Typically, with neural networks one expects that the number of trainable parameters would grow as O(ϵ−m) (Schmidt-Hieber, 2021). Indeed that is the case for universal approximation with a transformer when one learns the key, query and value matrices and the MLP parameters as shown by Yun et al. (2019). However, as Equation (7) shows, our construction results in the trainable parameters (prefix length in our case) growing as O(ϵ−10−14m−4m2 ). That the m2 term indicates worse asymptotic efficiency of prefix-tuning and prompting compared to training a transformer. However, our approach may not be tight. Thus, it remains an open question if a tighter Jackson bound exists or if prefix-tuning and prompt- ing inherently require more trainable parameters to reach the same approximation accuracy as training a transformer. Prefix-tuning and prompting may work by combining prefix-based element-wise maps with pretrained cross- element mixing The construction for general sequence-to- sequence functions in Section 4 is highly unlikely to occur in transformers pretrained on real data as it requires very specific parameter values. While the element-wise setting (Corollary 3) is more plausible, it cannot approximate gen- eral sequence-to-sequence functions. Hence, neither result explains why prefix-tuning works in practice. To this end, we hypothesise that prompting and prefix-tuning, can mod- ify how single tokens are processed (akin to fine-tuning only MLPs), while the cross-token information mixing happens with pretrained attention patterns. Therefore, prompting and prefix-tuning can easily learn novel tasks as long as no new attention patterns are required. Our findings suggest a method for guaranteeing that a pretrained model possesses the capability to act as a token-wise universal approxima- tor. This can be achieved by ensuring each layer of the model includes at least one attention head conforming to the structure in Lemma 3. Limitations. We assume a highly specific pretrained model which is unlikely to occur in practice when pretrain- ing with real-world data. Hence, the question of, given a real-world pretrained transformer, which is the class of functions it can approximate with prefix-tuning is still open. This is an inverse (Bernstein-type, Jiang et al. 2023) bound and is considerably more difficult to derive. Impact Statement This paper presents theoretical understanding about how the approximation abilities of the transformer architecture. Our results show that, under some conditions, prompting and prefix-tuning can arbitrarily modify the behavior of a model. This may have implications on how we design safety and security measures for transformer-based systems. However, whether these theoretical risks could manifest in realistic pretrained models remains an open problem. Acknowledgements We would like to thank Tom Lamb for spotting several mistakes and helping us rectify them. This work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1) and the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (EP/S024050/1). AB has received funding from the Amazon Research Awards. We also thank the Royal Academy of Engineering and FiveAI. References Ekin Aky¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? Investigations with linear models. In International Conference on Learning Representations. Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Ku- tyniok. 2023. Sumformer: Universal approximation for efficient transformers. In Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in Ma- chine Learning (TAG-ML). Donald E Amos. 1974. Computation of modified Bessel functions and their ratios. Mathematics of Computation, 28(125):239–251. Kendall Atkinson and Weimin Han. 2012. Spherical Har- monics and Approximations on the Unit Sphere: An In- troduction. Yogesh J Bagul and Satish K Panchal. 2018. Certain in- equalities of Kober and Lazarevi´c type. Research Group in Mathematical Inequalities and Applications Research Report Collection, 21(8). Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations. Luke Bailey, Gustaf Ahdritz, Anat Kleiman, Siddharth Swa- roop, Finale Doshi-Velez, and Weiwei Pan. 2023. Soft prompting might be a bug, not a feature. In Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning. Alex Barnett. 2021. Lower bounds on the modified Bessel function of the first kind. Mathematics Stack Exchange. Andrew R Barron. 1993. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Trans- actions on Information Theory, 39(3):930–945. 9Prompting a Pretrained Transformer Can Be a Universal Approximator Shaked Brody, Uri Alon, and Eran Yahav. 2023. On the expressivity role of LayerNorm in transformers’ attention. arXiv preprint arXiv:2305.02582. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems. Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. 2023. Jail- breaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419. George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303–314. Feng Dai and Yuan Xu. 2013. Approximation Theory and Harmonic Analysis on Spheres and Balls. Christian Schroeder de Witt, Samuel Sokota, J. Zico Kolter, Jakob Foerster, and Martin Strohmeier. 2023. Perfectly se- cure steganography using minimum entropy coupling. In International Conference on Learning Representations. David Demeter, Gregory Kimmel, and Doug Downey. 2020. Stolen probability: A structural weakness of neural lan- guage models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. 2023. On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680. Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning. Ricardo Estrada. 2014. On radial functions and distributions and their Fourier transforms. Journal of Fourier Analysis and Applications, 20(2):301–320. Uriel Feige and Gideon Schechtman. 2002. On the opti- mality of the random hyperplane rounding technique for MAX CUT. Random Structures & Algorithms, 20(3):403– 440. Paul Funk. 1915. Beitr¨age zur Theorie der Kugelfunktionen. Mathematische Annalen, 77:136–152. Federico Girosi and Tomaso Poggio. 1989. Representa- tion properties of networks: Kolmogorov’s theorem is irrelevant. Neural Computation, 1(4):465–469. E Hecke. 1917. ¨Uber orthogonal-invariante Integralgle- ichungen. Mathematische Annalen, 78:398–404. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter- efficient transfer learning for NLP. In International Con- ference on Machine Learning. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-rank adaptation of large lan- guage models. In International Conference on Learning Representations. Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023. LLM-Adapters: An adapter family for parameter- efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933. Haotian Jiang and Qianxiao Li. 2023. Approximation theory of transformer networks for sequence modeling. arXiv preprint arXiv:2305.18475. Haotian Jiang, Qianxiao Li, Zhong Li, and Shida Wang. 2023. A brief survey on the approximation theory for sequence modelling. arXiv preprint arXiv:2302.13752. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural In- formation Processing Systems. Andrei Nikolaevich Kolmogorov. 1957. On the represen- tation of continuous functions of many variables by su- perposition of continuous functions of one variable and addition. In Doklady Akademii Nauk, volume 114, pages 953–956. Russian Academy of Sciences. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing. Shengqiao Li. 2010. Concise formulas for the area and volume of a hyperspherical cap. Asian Journal of Mathe- matics & Statistics, 4(1):66–70. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Opti- mizing continuous prompts for generation. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers). 10Prompting a Pretrained Transformer Can Be a Universal Approximator Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023. Scaling down to scale up: A guide to parameter- efficient fine-tuning. arXiv preprint arXiv:2303.15647. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. 2021. On the expressive power of self-attention matrices. arXiv preprint arXiv:2106.03764. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- roaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-Tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. 2023. Memorization capacity of multi-head attention in transformers. arXiv preprint arXiv:2306.02010. Valdir Antˆonio Menegatto. 1997. Approximation by spher- ical convolution. Numerical Functional Analysis and Optimization, 18(9-10):995–1012. Tin Lok James Ng and Kwok-Kun Kwong. 2022. Universal approximation on the hypersphere. Communications in Statistics – Theory and Methods, 51(24):8694–8704. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. 2024. When do prompting and prefix-tuning work? A theory of capabilities and limitations. In International Conference on Learning Representations. David L Ragozin. 1971. Constructive polynomial approxi- mation on spheres and projective spaces. Transactions of the American Mathematical Society, 162:157–170. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems. C. A. Rogers. 1963. Covering a sphere with spheres. Math- ematika, 10(2):157–164. Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2023. Representational strengths and limitations of transform- ers. arXiv preprint arXiv:2306.02896. Johannes Schmidt-Hieber. 2021. The Kolmogorov–Arnold representation theorem revisited. Neural Networks , 137:119–126. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting knowledge from language models with automatically gen- erated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Matus Telgarsky. 2015. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zh- moginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning. Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. 2023. Universality and limitations of prompt tuning. In Advances in Neural Information Processing Systems. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learn- ing Representations. Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. 2023. Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit Bayesian inference. In International Confer- ence on Learning Representations. Steve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni. 2023. Pretraining data mixtures enable narrow model se- lection capabilities in transformer models. arXiv preprint arXiv:2311.00871. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. 2019. Are trans- formers universal approximators of sequence-to-sequence functions? In International Conference on Learning Rep- resentations. Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrik- son. 2023. Universal and transferable adversarial at- tacks on aligned language models. arXiv preprint arXiv:2307.15043. 11Prompting a Pretrained Transformer Can Be a Universal Approximator A. Background on Analysis on the Sphere As mentioned in the main text, the investigation of the properties of attention heads naturally leads to analysing functions over the hypersphere. To this end, our results require some basic facts about the analysis on the hypersphere. We will review them in this appendix. For a comprehensive reference, we recommend (Atkinson and Han, 2012) and (Dai and Xu, 2013). Define Pk(Rm+1) to be the space of polynomials of degree at most k. The restriction of a polynomial p ∈ Pk(Rm+1) to the unit hypersphere Sm = {x ∈ Rm+1 | ∥x∥2 = 1} is called a spherical polynomial. We can thus define the space of spherical polynomials: Pk(Sm) = {p|Sm for p ∈ Pk(Rm+1)}. Define by Hk(Rm+1) the space of polynomials of degree k that are homogeneous: Hk(Rm+1) = span ( (x1, . . . , xm+1) 7→ xα1 1 × ··· ×xαm+1 m+1 | m+1X i=1 αi = k ) . Its restriction to the sphere Hk(Sm) is defined analogously to Pk(Sm). Finally, we can define the space Y(Rm+1) of harmonic homogeneous polynomials: Yk(Rm+1) = \u001a p ∈ Hk(Rm+1) | ∂2 ∂x2 p(x) = 0, ∀x ∈ Rm+1 \u001b . Yk(Sm) which is the restriction of Yk(Rm+1) to Sm is the set of spherical harmonics of degree k. Spherical harmonics are the higher-dimensional extension of Fourier series. Notably, even though Yk(Sm) ⊂ Hk(Sm) ⊂ Pk(Sm), the restriction of any polynomial on Sm is a sum of spherical harmonics: Pk(Sm) = Y0(Sm) ⊕ ··· ⊕Yk(Sm), with ⊕ being the direct sum (Atkinson and Han, 2012, Corollary 2.19). We define C(Sm) to be the space of all continuous functions defined on Sm with the uniform norm ∥f∥∞ = sup x∈Sm |f(x)|, f ∈ C(Sm). (12) Similarly, Lp(Sm), 1 ≤ p <∞ is the space of all functions defined on Sm which are integrable with respect to the standard surface measure dwm. The norm in this space is: ∥f∥p = \u0012 1 wm Z Sm |f(x)|p dwm(x) \u00131/p , f ∈ Lp(Sm), (13) with the surface area being wm = Z Sm dwm = 2π (m+1)/2 Γ((m+1)/2). (14) We will use Vm to denote any of these two spaces and ∥ · ∥m the corresponding norm. A key property of spherical harmonics is that sums of spherical harmonics can uniformly approximate the functions in C(Sm). In other words, the span of S∞ k=0 Yk(Sm) is dense in C(Sm) with respect to the uniform norm ∥ · ∥∞. Hence, any f ∈ C(Sm) can be expressed as a series of spherical harmonics: f(x) = ∞X k=0 Y m k (x), with Y m k ∈ Yk(Sm), ∀k. We will also make a heavy use of the concept of spherical convolutions. Define the space of kernelsL1,m to consist of all measurable functions K on [−1, 1] with norm ∥K∥1,m = wm−1 wm Z 1 −1 |K(t)|(1 − t2)(m−2)/2 dt < ∞. 12Prompting a Pretrained Transformer Can Be a Universal Approximator Log Value 0.0 0.2 0.4 0.6 0.8 1 .0 Figure 3.Plots of the von Mises-Fisher kernel KvMF λ (⟨x, y⟩) for λ = 1, 5, 10 and fixed y in three dimensions (m = 2). The larger λ is, the more concentrated the kernel is around y. Definition A.1 (Spherical convolution). The spherical convolution K ∗ f of a kernel K in L1,m with a function f ∈ Vm is defined by: (K ∗ f)(x) = 1 wm Z Sm K(⟨x, y⟩)f(y) dwm(y), x ∈ Sm. Spherical convolutions map functions f ∈ Vm to functions in Vm. Furthermore, the spherical harmonics are eigenfunctions of the function generated by a kernel in L1,m: Lemma A.1 (Funk and Hecke’s formula (Funk, 1915; Hecke, 1917; Estrada, 2014)). K ∗ Y m k = am k (K)Y m k , when K ∈ L1,m, Ym k ∈ Yk(Sm), k= 0, 1, . . . , where am k (K) are the coefficients in the series expansion in terms of Gegenbauer polynomials associated with the kernel K: am k (K) = wm−1 wm Z 1 −1 K(t) Q(m−1)/2 k (t) Q(m−1)/2 k (1) (1 − t2)(m−2)/2 dt, k= 0, 1, . . . . (15) Here, Q(m−1)/2 k is the Gegenbauer polynomial of degreek. Note also that with a change of variables we have: Z Sm K(⟨x, y⟩) dwm(y) = wm−1 Z 1 −1 K(t)(1 − t2)(m−2)/2 dt. (16) Ideally, we would like a kernel that acts as an identity for the convolution operation. In this case, we would have ∥K ∗ f − f∥∞ = 0, f∈ C(Sm) which would be rather convenient. However, there is no such kernel in the spherical setting (Menegatto, 1997). The next best thing is to construct a sequence of kernels {Kn} ∈ L1,m such that ∥Kn ∗ f − f∥m → 0 as n → ∞for all f ∈ Vm. This sequence of kernels is called an approximate identity. The specific such sequence of kernels we will use is based on the von Mises-Fisher distribution as this gives us the exp(⟨x, y⟩) form that we also observe in the transformer attention mechanism. Definition A.2 (von Mises-Fisher kernels, (Ng and Kwong, 2022)). We define the sequence of von Mises-Fisher kernels as: KvMF λ (t) = cm+1(λ) exp(λt), t∈ [−1, 1], where cm+1(λ) = wmλ m+1 2 −1 (2π) m+1 2 Im+1 2 −1(λ) , with Iv being the modified Bessel function at order v. Note that a von Mises-Fisher kernel can also be expressed in terms of points onSm. In particular, for a fixedy ∈ Sm we have KvMF λ (⟨x, y⟩), x ∈ Sm. The parameter λ is a “peakiness” parameter: the large λ is, the closer KvMF λ (⟨x, y⟩) approximates 13Prompting a Pretrained Transformer Can Be a Universal Approximator A. B. C. D. E. F. Target function Convolution with von Mises-Fisher kernel Partition of    is approximately constant in each cell Approximating dot product with a point in the partition when Sum over the partition Figure 4. Intuition behind the proof of our Jackson-type bound for universal approximation on the hypersphere. A. We want to approximate a function f over the hypersphere Sm. This illustration is in three-dimensional space, so m = 2. B. In order to get the exp(λ⟨·, y⟩) form that we want, we convolve f with the KvMF λ (t) = cm+1(λ) exp(λt) kernel. C. We partition Sm into N cells V1,. . . ,VN . D. Our choice of N is such that f does not vary too much in each cell and hence can be approximated by a function that is constant in each Vk. E. As each cell is small, the dot product of x with any point in the cell Vk can be approximated by the dot product of x with a fixed point bk ∈ Vk. F. This allows us to approximate the integral in the convolution KvMF λ with a finite sum. the delta function centered at y, as can be seen in Figure 3. It is easy to check that ∥KvMF λ ∥1,m = 1, ∀λ >1, m >1 and hence the sequence is in L1,m, meaning they are valid kernels. Ng and Kwong (2022, Lemma 4.2) show that {KvMF λ } is indeed an approximate identity, i.e., ∥KvMF λ ∗ f − f∥m → 0 as λ → ∞for all f ∈ Vm.2 As we want a Jackson-type result however, we will need to upper bound the error ∥KvMF λ ∗ f − f∥m as a function of λ, that is a non-asymptotic result on the quality of the approximation by spherical convolutions with KvMF λ . We do that in Lemma B.5. B. A Jackson-type Bound for Universal Approximation on the Unit Hypersphere The overarching goal in this section is to provide a Jackson-type (Definition 2) bound for approximating functions f : Sm → Rm+1 on the hypersphere Sm = {x ∈ Rm+1 | ∥x∥2 = 1} by functions of the form h(x) = NX k=1 ξk exp(λ⟨x, bk⟩) (17) To this end, we will leverage results from approximation on the hypersphere using spherical convolutions by Menegatto (1997) and recent results on the universal approximation on the hypersphere by Ng and Kwong (2022). While these two works inspire the general proof strategy, they only offer uniform convergence (i.e., density-type results, Definition 1). Instead, we offer a non-asymptotic analysis and develop the first approximation rate results on the sphere for functions of the form of Equation (17), i.e., Jackson-type results (Definition 2). The high-level idea of the proof is to split the goal into approximatingf with the convolutionf ∗KvMF λ and approximating the convolution f ∗KvMF λ with a sum of terms that have theexp(t⟨x, bk⟩) structure resembling the kernel KvMF λ (Definition A.2): sup x∈Sm \r\r\r\r\rf(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \r\r\r\r\r ≤ \r\rf − f ∗ KvMF λ \r\r ∞| {z } Equation (19) / Lemma B.5 + \r\r\r\r\rf ∗ KvMF λ − NX k=1 ξk exp(λ⟨·, bk⟩) \r\r\r\r\r ∞| {z } Lemma B.7 . (18) This is also illustrated in Figure 4. Let’s focus on the first term in Equation (18). It can be further decomposed into three terms by introducing Wq ∈ Pq(Sm), 2The wm term in the normalization constant cm+1(λ) is not in (Ng and Kwong, 2022). However, without it KvMF λ are not an approximate identity. 14Prompting a Pretrained Transformer Can Be a Universal Approximator the best approximation of f with a spherical polynomial of degree q: ∥KvMF λ ∗ f − f∥∞ ≤ ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥∞| {z } Lemma B.2 + ∥KvMF λ ∗ Wq − Wq∥∞| {z } Lemma B.4 + ∥Wq − f∥∞| {z } Lemma B.1 . (19) There are a number of Jackson-type results for how well finite sums of spherical polynomials approximate functionsf ∈ Vm (the last term in Equation (19)). In particular, they are interested in bounding min Wq∈Pq(Sm) ∥f − Wq∥p, 1 ≤ p ≤ ∞. (20) We will use a simple bound by Ragozin (1971): Lemma B.1 (Ragozin bound). For f ∈ C(Sm) and q ∈ N>0 it holds that: min Wq∈Pq(Sm) ∥f − Wq∥∞ ≤ CR ω \u0012 f; 1 q \u0013 , (21) for some constant CR that does not depend on f or q and ω being the first modulus of continuity of f defined as: ω(f; t) = sup{|f(x) − f(y)| |x, y ∈ Sm, cos−1(x⊤y) ≤ t}. We recommend Atkinson and Han (2012, Chapter 4) and Dai and Xu (2013, Chapter 4) for an overview of the various bounds proposed for Equation (20) depending on the continuity properties of f and its derivatives. In particular, the above bound could be improved with a term 1/nk if f has k continuous derivates (Ragozin, 1971). We can upper-bound the first term in Equation (19) by recalling that the norm of the kernel KvMF λ is 1: Lemma B.2. ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥m ≤ ∥f − Wq∥m. Hence: ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥∞ ≤ ∥f − Wq∥∞ ≤ CR ω \u0012 f; 1 q \u0013 . Proof. Convolution is linear so ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥m = ∥KvMF λ ∗ (f − Wq)∥m. Using the H ¨older inequality (Dai and Xu, 2013, Theorem 2.1.2) we get ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥m ≤ ∥KvMF λ ∥1,m∥f − Wq∥m. As ∥KvMF λ ∥1,m = 1 for all λ >0, m >1, we obtain the inequality in the lemma. For the uniform norm, we also use the Ragozin bound from Lemma B.1. Only the second term in Equation (19) is left. However, before we tackle it, we will need a helper lemma that bounds the eigenvalues of the von Mises-Fisher kernel (Equation (15)): Lemma B.3 (Bounds on the eigenvalues am k (KvMF λ )). The eigenvalues am k , as defined in Equation (15), for the sequence of von Mises-Fisher kernels (Definition A.2) are bounded from below and above as: 0 <   λ \u0000m−1 2 + k \u0001 + q λ2 + \u0000m−1 2 + k \u00012   k ≤ am k (KvMF λ ) ≤ 1 Proof. We have am k (KvMF λ ) = wm−1 wm Z 1 −1 KvMF λ (t) Q(m−1)/2 k (t) Q(m−1)/2 k (1) (1 − t2)(m−2)/2dt = wm−1 wm Z 1 −1 cm+1(λ) exp(λt) Q(m−1)/2 k (t) Q(m−1)/2 k (1) (1 − t2)(m−2)/2dt 15Prompting a Pretrained Transformer Can Be a Universal Approximator 1 0 20 30 40 5 0 0.0 0.2 0.4 0.6 0.8 1 .0 True parameter Lower bound Figure 5.The coefficients am k for the von Mises-Fisher kernels KvMF λ for m = 2and k ∈ {1, 2, 3} as well as the lower bound from Lemma B.3. ⋆ = Im−1 2 +k(λ) Im−1 2 (λ) , with ⋆ solved using Mathematica. From here we can see that am 0 (KvMF λ ) = 1 for all m >1, λ >1. Furthermore, for v >1 and λ >0 the modified Bessel function of the first kind Iv(λ) is monotonically decreasing as v increases. Therefore, am k (KvMF λ ) ≤ am 0 (KvMF λ ) = 1, which gives us the upper bound in the lemma. For the lower bound, we will use the following bound on the ratio of modified Bessel functions by Amos (1974, Eq. 9): Iv+1(x) Iv(x) ≥ x (v + 1) + p x2 + (v + 1)2 , v≥ 0, x≥ 0. As mentioned above, 0 ≤ Iv+1(x) Iv(x) ≤ 1. Furthermore, these ratios are decreasing as v increases, i.e., Iv+2(x) Iv+1(x) ≤ Iv+1(x) Iv(x) for all v ≥ 0 and x ≥ 0 (Amos, 1974, Eq. 10). Combining these facts gives us: Iv+k(x) Iv(x) ≥ \u0012 Iv+k(x) Iv+k−1(x) \u0013k =   x (v + k) + p x2 + (v + k)2 !k . (22) We can now give the lower bound foram k (KvMF n ) using Equation (22) : am k (KvMF λ ) = Im−1 2 +k(λ) Im−1 2 (λ) ≥   λ \u0000m−1 2 + k \u0001 + q λ2 + \u0000m−1 2 + k \u00012   k . The lower bound for m = 2 is plotted in Figure 5. We can now provide a bound for the second term in Equation (19): Lemma B.4. Take an f ∈ C(Sm). Furthermore, assume that there exists a constant CH ≥ 0 that upper-bounds the norms of the spherical harmonics of any best polynomial approximation Wq of f: for all q ≥ 1, Wq = qX k=0 Y m k , Ym k ∈ Y(Sm), ∥Y m k ∥∞ ≤ CH, ∀k = 0, . . . , q,where Wq = arg min h∈Pq(Sm) ∥f − h∥∞. Then ∥KvMF λ ∗ Wq − Wq∥∞ ≤ CH q  1 −   λ \u0000m−1 2 + q \u0001 + q λ2 + \u0000m−1 2 + q \u00012   q . 16Prompting a Pretrained Transformer Can Be a Universal Approximator Proof. Using that Wq is a spherical polynomial of degree q and hence can be expressed as a sum of spherical harmonics Wq = Pq k=0 Y m k , we get: ∥KvMF λ ∗ Wq − Wq∥∞ = \r\r\r\r\rKvMF λ ∗ qX k=0 Y m k (x) − qX k=0 Y m k (x) \r\r\r\r\r ∞ = \r\r\r\r\r qX k=0 \u0000 KvMF λ ∗ Y m k (x) − Y m k (x) \u0001 \r\r\r\r\r ∞ ≤ qX k=0 \r\r\u0000 KvMF λ ∗ Y m k (x) − Y m k (x) \u0001\r\r ∞ (Triangle inequality) = qX k=0 \r\r\u0000 am k (KvMF λ )Y m k (x) − Y m k (x) \u0001\r\r ∞ (from Lemma A.1) = qX k=0 \r\r(am k (KvMF λ ) − 1) Y m k (x) \r\r ∞ = qX k=0 |am k (KvMF λ ) − 1|∥Y m k (x)∥∞ ≤ qX k=0 |am k (KvMF λ ) − 1| CH ≤ CH qX k=0  1 −   λ \u0000m−1 2 + k \u0001 + q λ2 + \u0000m−1 2 + k \u00012   k  (from Lemma B.3) ≤ CH qX k=0   1 −   λ \u0000m−1 2 + q \u0001 + q λ2 + \u0000m−1 2 + q \u00012   | {z } B k   (as k ≤ q) = CH   q + 1 − qX k=0 Bk ! ≤ CH (q + 1 − (1 + qBq)) (using that 0 < B <1) ≤ CH q (1 − Bq) . We can finally combine Lemmas B.1, B.2 and B.4 in order to provide an upper bound to Equation (19): Lemma B.5 (Bound on ∥KvMF λ ∗ f − f∥∞). Take an f ∈ Vm with modulus of continuity ω(f; t) ≤ Lt. As in Lemma B.4, assume that there exists a constant CH ≥ 0 that upper-bounds the norms of the spherical harmonics of any best polynomial approximation Wq of f: for all q ≥ 1, Wq = qX k=0 Y m k , Ym k ∈ Y(Sm), ∥Y m k ∥∞ ≤ CH, ∀k = 0, . . . , q,where Wq = arg min h∈Pq(Sm) ∥f − h∥∞. If λ ≥ Λ(ϵ), where Λ(ϵ) = (8LCR + mϵ + ϵ) \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 ϵ 4LCR+ϵ ϵ \u0010 1 − \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 2ϵ 4LCR+ϵ \u0011 = O \u0012L3CHC3 R ϵ4 \u0013 . (23) then ∥f − KvMF λ ∗ f∥∞ ≤ ϵ. 17Prompting a Pretrained Transformer Can Be a Universal Approximator Proof. As we want to upper-bound Equation (19) with ϵ, we will split our ϵ budget over the three terms. For the first and the third terms, using the Ragozin bound from Lemma B.1 we have: ∥f − Wq∥∞ ≤ CR ω \u0012 f; 1 q \u0013 ≤ CRL q , q≥ 1. (24) We want to select an integer q large enough so that ∥f − Wq∥∞ ≤ ϵ/4. That is q = \u00064CRL ϵ \u0007 . This will be how we bound the first and last terms in Equation (19). Let’s focus on the second term. Pick Wq to be the best approximation from the Ragozin bound. From Lemma B.4 we have that ∥KvMF λ ∗ Wq − Wq∥∞ ≤ CH q (1 − Bq) , where B = λ \u0000m−1 2 + q \u0001 + q λ2 + \u0000m−1 2 + q \u00012 . The error budget we want to allocate for the ∥KvMF λ ∗ Wq − Wq∥∞ term is ϵ/2. Hence: B ≥ \u0012 1 − ϵ 2CHq \u00131/q = D =⇒ ∥KvMF n ∗ Wq − Wq∥∞ ≤ ϵ 2. (25) We just need to find the minimum value for λ such that Equation (25) holds. We have: B = λ \u0000m−1 2 + q \u0001 | {z } E + q λ2 + \u0000m−1 2 + q \u00012 = λ E + √ λ2 + E2 . (26) Then, combining Equations (25) and (26) we get: λ E + √ λ2 + E2 ≥ D λ ≥ 2DE 1 − D2 . Finally, replacing D and E with the expressions in Equations (25) and (26), upper-bounding q = \u00064CRL ϵ \u0007 as q ≥ 4CRL ϵ + 1 and simplifying the expression we get our final bound for λ. If λ ≥ Λ(ϵ), with Λ(ϵ) = (8LCR + mϵ + ϵ) \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 ϵ 4LCR+ϵ ϵ \u0010 1 − \u0010 1 − ϵ2 8LCHCR+2ϵCH \u0011 2ϵ 4LCR+ϵ \u0011 , then ∥KvMF λ ∗ Wq − Wq∥∞ ≤ ϵ/2. Hence, for any λ ≥ Λ(ϵ) we have: ∥KvMF λ ∗ f − f∥∞ ≤ ∥KvMF λ ∗ f − KvMF λ ∗ Wq∥∞ + ∥KvMF λ ∗ Wq − Wq∥∞ + ∥Wq − f∥∞ ≤ ∥f − Wq∥∞ + ϵ 2 + ∥Wq − f∥∞ ≤ ϵ 4 + ϵ 2 + ϵ 4 = ϵ. This concludes our bound on Equation (19). Finally, to give the asymptotic behavior of Λ(ϵ) as ϵ → 0 we observe that the Taylor series expansion of Λ around ϵ = 0 is: Λ(ϵ) = 128L3CHC3 R ϵ4 + 16L2(m − 1)CHC2 R ϵ3 + O \u0012 1 ϵ2 \u0013 , 18Prompting a Pretrained Transformer Can Be a Universal Approximator hence: Λ(ϵ) = O \u0012L3CHC3 R ϵ4 \u0013 . Lemma B.5 is our bound on Equation (19) which is also the first term of Equation (18). Recall that bounding Equation (18) is our ultimate goal. Hence, we are halfway done with our proof. Let’s focus now on the second term in Equation (18), that is, how well we can approximate the convolution of f with the von Mises-Fisher kernel using a finite sum: \r\r\r\r\rf ∗ KvMF λ − NX k=1 ξk exp(λ⟨·, bk⟩) \r\r\r\r\r ∞ . The basic idea behind bounding this term is that we can partition the hypersphere Sm into N sets ({V1, . . . , VN }), each small enough so that, for a fixed x ∈ Sm, the K(⟨x, y⟩)f(y) term in the convolution (K ∗ f)(x) = 1 wm Z Sm K(⟨x, y⟩)f(y) dwm(y) is almost the same for all values y ∈ Vk in that element of the partition. Hence we can approximate the integral over the partition with estimate over a single point bk: Z Vk K(⟨x, y⟩)f(y) dwm(y) ≈ |Vk|K(⟨x, bk⟩)f(bk). The rest of this section will make this formal. First, in order to construct our partition {V1, . . . , VN } of Sm we will first construct a cover of Sm. Then, our partition will be such that each element Vk is a subset of the corresponding element of the cover of Sm. In this way, we can control the maximum size of the elements of the cover. Lemma B.6. Consider a cover {Bδ(b1), . . . , Bδ(bNm δ )} of Sm by Nm δ hyperspherical caps Bδ(x) = {y ∈ Sm | ⟨x, y⟩ ≥ 1 − δ} for 0 < δ <1, centred at b1, . . . ,bNm δ ∈ Sm. By cover we mean that SNm δ i=1 Bδ(bi) = Sm, with Nm δ being the smallest number of hyperspherical caps to cover Sm (its covering number). Then, for m ≥ 8 we have: 2 I(δ(2−δ)) \u0000m 2 , 1 2 \u0001 ≤ Nm δ < Φ(m) (δ(2 − δ)) m+1 2 < Φ(m) δm+1 , with I being the regularized incomplete beta function and Φ(m) = O(m log m) being a function that depends only on the dimension m. Proof. Define ϕ = cos−1(1 − δ): ϕ Bδ(x) x Naturally, the area of the caps needs to be at least as much as the area of the hypersphere for the set of caps to be a cover. This gives us our lower bound. The area of a cap with colatitude angle ϕ as above is (Li, 2010): wϕ m = 1 2wmIsin2 ϕ \u0012m 2 , 1 2 \u0013 . 19Prompting a Pretrained Transformer Can Be a Universal Approximator As sin ϕ = sin \u0000 cos−1(1 − δ) \u0001 = p 1 − (1 − δ)2 = p δ(2 − δ), we have our lower bound: Nm δ ≥ wm wϕ m = 2 Isin2 ϕ \u0000m 2 , 1 2 \u0001 = 2 I(δ(2−δ)) \u0000m 2 , 1 2 \u0001 For the upper bound, we can use the observation that if a unit ball is covered with balls of radius r, then the unit sphere is also covered with caps of radius r. From (Rogers, 1963, intermediate result from the proof of theorem 3) we have that for m ≥ 8 and 1/r ≥ m + 1, a unit ball can be covered by less than e ((m + 1) log(m + 1) + (m + 1) log log(m + 1) + 5(m + 1)) 1 rm+1 = Φ(m) 1 rm+1 balls of radius r. For high m, this is a pretty good approximation since most of the volume of the hypersphere lies near its surface. Our caps Bδ can fit inside balls of radius r = sin ϕ. Hence, we have the upper bound: Nm δ < Φ(m) sinm+1 ϕ = Φ(m) (δ(2 − δ)) m+1 2 . Now we can use a partition resulting from this covering in order to bound the error between the integral and its Riemannian sum approximation: Lemma B.7 (Approximation via Riemann sums) . Let g(x, y) : Sm × Sm → R with m ≥ 8 be a continuous function with modulus of continuity for both arguments ω(g(·; y), t) ≤ Lt, ∀y ∈ Sm and ω(g(x, ·); t) ≤ Lt, ∀x ∈ Sm. Take any 0 < δ < 1. Then, there exists a partition {V1, . . . , VNm δ } of Sm into Nm δ = ⌈Φ(m)/δm+1⌉ subsets, as well as b1, . . . ,bNm δ ∈ Sm such that: max x∈Sm 1 wm \f\f\f\f\f\f Z Sm g(x, y) dwm(y) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f ≤ 3L cos−1(1 − δ). Here, Φ(m) = O(m log m) is a function that depends only on the dimension m. Proof. This proof is a non-asymptotic version of the proof of Lemma 4.3 from Ng and Kwong (2022). First, we can use Lemma B.6 to construct a covering {Bδ(b1), . . . , Bδ(bNm δ )} of Sm. If we have a covering of Sm it is trivial to construct a partition of it {V1, . . . , VNm δ }, SNm δ k=1 Vk = Sm, Vi ∩ Vj = ∅, i̸= j such that Vk ⊆ Bδ(bk), ∀k. This partition can also be selected to be such that all elements of it have the same measure wm(V1) = wm(Vi), ∀i (Feige and Schechtman, 2002, Lemma 21). While this is not necessary for this proof, we will use this equal measure partition in Lemma C.2 and Theorem C.1. We can then use the triangle inequality to split the term we want to bound in three separate terms: \f\f\f\f\f\f Z Sm g(x, y) dwm(y) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f ≤ \f\f\f\f Z Sm g(x, y) dwm(y) − Z Sm g(b⋆, y) dwm(y) \f\f\f\f + \f\f\f\f\f\f Z Sm g(b⋆, y) dwm(y) − Nm δX k=1 g(b⋆, bk) wm(Vk) \f\f\f\f\f\f (27) + \f\f\f\f\f\f Nm δX k=1 g(b⋆, bk) wm(Vk) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f , where b⋆ is the center of one of the caps whose corresponding partition contains x, i.e., b⋆ = bi ⇐⇒ x ∈ Vi. Due to {V1, . . . , VNm δ } being a partition, b⋆ is well-defined as x is in exactly one of the elements of the partition. 20Prompting a Pretrained Transformer Can Be a Universal Approximator Observe also that the modulus of continuity gives us a Lipschitz-like bound, i.e., if ⟨x, y⟩ ≥1 − δ for x, y ∈ Sm and ω(f; t) ≤ Lt, then |f(x) − f(y)| ≤ω(f; cos−1(⟨x, y⟩)) ≤ ω(f; cos−1(1 − δ)) ≤ L cos−1(1 − δ). (28) Let’s start with the first term in Equation (27). Using the fact that we selected b⋆ to be such that ⟨b⋆, x⟩ ≥1 − δ and Equation (28), we have: \f\f\f\f Z Sm g(x, y) dwm(y) − Z Sm g(b⋆, y) dwm(y) \f\f\f\f = \f\f\f\f Z Sm (g(x, y) − g(b⋆, y)) dwm(y) \f\f\f\f ≤ Z Sm |g(x, y) − g(b⋆, y)| dwm(y) ≤ Z Sm L cos−1(1 − δ) dwm(y) = L cos−1(1 − δ) Z Sm dwm(y) = L cos−1(1 − δ) wm. We can similarly upper-bound the second term of Equation (27) using also the fact that {Vk} is a partition of Sm: \f\f\f\f\f\f Z Sm g(b⋆, y) dwm(y) − Nm δX k=1 g(b⋆, bk) wm(Vk) \f\f\f\f\f\f = \f\f\f\f\f\f Nm δX k=1 Z Vk g(b⋆, y) dmw(y) − Nm δX k=1 g(b⋆, bk) wm(Vk) \f\f\f\f\f\f = \f\f\f\f\f\f Nm δX k=1 Z Vk (g(b⋆, y) − g(b⋆, bk)) dmw(y) \f\f\f\f\f\f ≤ Nm δX k=1 Z Vk |g(b⋆, y) − g(b⋆, bk)| dmw(y) ≤ Nm δX k=1 Z Vk L cos−1(1 − δ) dmw(y) = L cos−1(1 − δ) Nm δX k=1 Z Vk dmw(y) = L cos−1(1 − δ)wm. And analogously, for the third term we get: \f\f\f\f\f\f Nm δX k=1 g(b⋆, bk) wm(Vk) − Nm δX k=1 g(x, bk) wm(Vk) \f\f\f\f\f\f = \f\f\f\f\f\f Nm δX k=1 (g(b⋆, bk) − g(x, bk)) wm(Vk) \f\f\f\f\f\f ≤ Nm δX k=1 |g(b⋆, bk) − g(x, bk)| wm(Vk) ≤ L cos−1(1 − δ) Nm δX k=1 wm(Vk) = L cos−1(1 − δ)wm. Finally, observing that the above bounds do not depend on the choice of x ∈ Sm and combining the three results we obtain our desired bound. 21Prompting a Pretrained Transformer Can Be a Universal Approximator By observing that we can set g(x, y) = KvMF λ (⟨x, y⟩)f(y), it becomes clear how Lemma B.7 can be used to bound the second term in Equation (18). For that we will also need to know what is the modulus of continuity of the von Mises-Fisher kernels KvMF λ . Lemma B.8 (Modulus of continuity ofKvMF λ ). The von Mises-Fisher kernelsKvMF λ have modulus of continuityω(KvMF λ ; t) ≤ cm+1(λ) exp(λ). Proof. Recall that KvMF λ (u) is defined on u ∈ [−1, 1]. KvMF λ (u) and its derivative are both monotonically increasing in u. Hence: ω(KvMF λ ; t) = sup \b |KvMF λ (⟨z, x⟩) − KvMF λ (⟨z, y⟩) | x, y ∈ Sm, cos−1(⟨x, y⟩) ≤ t \t . Using the mean value theorem we know there exists a d ∈ Sm such that |KvMF λ (⟨z, x⟩) − KvMF λ (⟨z, y⟩)| = |(KvMF λ )′(⟨z, d⟩) (⟨z, x⟩ − ⟨z, y⟩)| = (KvMF λ )′(⟨z, d⟩) |⟨z, x⟩ − ⟨z, y⟩| = λcm+1(λ) exp(λ⟨z, d⟩) |⟨z, x⟩ − ⟨z, y⟩| ≤ λcm+1(λ) exp(λ) |⟨z, x⟩ − ⟨z, y⟩| = λcm+1(λ) exp(λ)∥z∥2∥x − y∥2 cos(angle(z, x − y)) ≤ λcm+1(λ) exp(λ)∥x − y∥2. Using the law of cosines and that the angle between x and y is less than t: ∥x − y∥2 = q ∥x∥2 2 + ∥y∥2 2 − 2∥x∥2 2∥y∥2 2 cos(angle(x, y)) = p 2 − 2 cos(angle(x, y)) ≤ √ 2 − 2 cost ≤ t. Hence: ω(KvMF λ ; t) ≤ λcm+1(λ) exp(λ)t. Our final result, a bound on Equation (18), combines Lemma B.5 and Lemma B.7, each bounding one of the two terms in Equation (18). Theorem B.1 (Jackson-type bound for universal approximation on the hypersphere, Theorem 1 in the main text) . Let f ∈ C(Sm) be a continuous function on Sm with modulus of continuity ω(f; t) ≤ Lt for some L ∈ R>0 and m ≥ 8. Assume that there exists a constant CH ≥ 0 that upper-bounds the norms of the spherical harmonics of any best polynomial approximation Wq of f: for all q ≥ 1, Wq = qX k=0 Y m k , Ym k ∈ Y(Sm), ∥Y m k ∥∞ ≤ CH, ∀k = 0, . . . , q,where Wq = arg min h∈Pq(Sm) ∥f − h∥∞. Then, for any ϵ >0, there exist ξ1, . . . , ξN ∈ R and b1, . . . ,bN ∈ Sm such that sup x∈Sm \f\f\f\f\ff(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ, where λ = Λ(ϵ/2) (Equation (23)) and for any N such that N ≥ N(λ, ϵ) = Φ(m) \u00123π(L + λ∥f∥∞)cm+1(λ) exp(λ) ϵ \u00132(m+1) = O(ϵ−10−14m−4m2 ). (29) 22Prompting a Pretrained Transformer Can Be a Universal Approximator Proof. Recall the decomposition in Equation (18). We will split our error budget ϵ in half. Hence, we first select λ such that approximating f with its convolution with KvMF λ results in an error at most λ/2. Then, using this λ, we find how finely we need to partition Sm in order to be able to approximate the convolution with a sum up to an error ϵ/2. Let’s select how “peaky” we need the kernelKvMF λ to be, that is, how big should λ be. From Lemma B.5 we have that if λ = Λ(ϵ/2), then we would have ∥f − f ∗ KvMF λ ∥∞ ≤ ϵ/2. Now, for the second term in Equation (18), consider Lemma B.7 with g(x, y) = KvMF λ (⟨x, y⟩)f(y). From Lemma B.8 we have that the modulus of continuity of KvMF λ is ω(KvMF λ ; t) ≤ t λ cm+1(λ) exp(λ). Hence, we have modulus of continuity for g(x, y) being bounded as: ω(g; t) ≤ ∥KvMF λ ∥∞ω(f; t) + ∥f∥∞ω(KvMF λ ; t) ≤ KvMF λ (1) Lt + ∥f∥∞λcm+1(λ) exp(λ) t = cm+1(λ) exp(λ) Lt + ∥f∥∞λcm+1(λ) exp(λ) t = cm+1(λ) exp(λ) (L + λ∥f∥∞) t. Take δ = \u0012 2ϵ 6π(L + λ∥f∥∞)cm+1(λ) exp(λ) \u00132 . Then, by Lemma B.7, there exists a partition {V1, . . . , VN } of Sm and b1, . . . ,bN ∈ Sm for N as in the lemma such that: max x∈Sm \f\f\f\f\f 1 wm Z Sm KvMF λ (⟨x,y⟩)f(y) dwm(y) − 1 wm NX k=1 KvMF λ (⟨x,bk⟩)f(bk) wm(Vk) \f\f\f\f\f ≤ 3(L+λ∥f∥∞)cm+1exp(λ) cos−1(1−δ). As (2x/π)2 < 1 − cos(x) (Bagul and Panchal, 2018, Theorem 1), we have: δ = \u0012 2ϵ 6π(L + λ∥f∥∞)cm+1(λ) exp(λ) \u00132 < 1 − cos \u0012 ϵ 6(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013 . (30) Hence: max x∈Sm \f\f\f\f\f 1 wm Z Sm KvMF λ (⟨x, y⟩)f(y) dwm(y) − 1 wm NX k=1 KvMF λ (⟨x, bk⟩)f(bk) wm(Vk) \f\f\f\f\f ≤ 3(L + λ∥f∥∞)cm+1 exp(λ) cos−1(1 − δ) < 3(L + λ∥f∥∞)cm+1 exp(λ) cos−1 \u0012 cos \u0012 ϵ 6(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013\u0013 = ϵ/2. Combining the two results we have: \r\r\r\r\rf- 1 wm NX k=1 KvMF λ (⟨x, bk⟩)f(y) wm(Vk) \r\r\r\r\r ∞ ≤ ∥f-f∗KvMF λ ∥∞ + \r\r\r\r\rf∗KvMF λ - 1 wm NX k=1 KvMF λ (⟨x, bk⟩)f(y) wm(Vk) \r\r\r\r\r ∞ ≤ ϵ/2 + ϵ/2 = ϵ. Now, the only thing left is to show that this expression can be expressed in the form of Equation (17). 1 wm Nm δX k=1 KvMF λ (⟨x, bk⟩) f(bk) wm(Vk) = Nm δX k=1 1 wm cm+1(λ) exp(λ⟨x, bk⟩) f(bk) wm(Vk) = Nm δX k=1 ξk exp(λ⟨x, bk⟩), 23Prompting a Pretrained Transformer Can Be a Universal Approximator with ξk = cm+1(λ)f(bk)wm(Vk) wm . (31) If we have chosen a partition of equal measure this further simplifies to ξk = cm+1(λ) N f(bk). Hence, for this choice of Λ, N and bk and ξk constructed as above, we indeed have sup x∈Sm \f\f\f\f\ff(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ. Finally, let’s study the asymptotic growth ofN as ϵ → 0. We have: N(λ, ϵ) = Φ(m) \u00123π(L + λ∥f∥∞)cm+1(λ) exp(λ) ϵ \u00132(m+1) . Φ(m) is constant in ϵ so we can ignore it. Expanding cm+1 and dropping the terms that do not depend on ϵ gives us: O   (L + λ∥f∥∞)λ m+1 2 −1 exp(λ) ϵ Im+1 2 −1(λ) !2(m+1) . (32) The asymptotics of the modified Bessel function of the first kind are difficult to analyse. However, as we care about an upper bound, we can simplify the expression by lower-bounding Iν(λ) using Equation (22) and that I0(λ) ≥ C exp(λ)/ √ λ for λ >1/2 (Barnett, 2021): Iν(λ) ≥ C  √ ν2 + λ2 − ν λ !ν+1 exp \u0000√ ν2 + λ2\u0001 √ λ , for some constant C. Plugging this in Equation (32), replacing λ with its asymptotic growth ϵ−4 and taking the Taylor series expansion at for ϵ → 0 gives us: O(ϵ−10−14m−4m2 ). We can easily extend Theorem B.1 to vector-valued functions: Corollary B.1 (Corollary 2 in the main text) . Let f : Sm → Rm+1, m ≥ 8 be such that each component fi is in C(Sm), i= 1, . . . , m+ 1 and satisfies the conditions in Theorem B.1. Furthermore, define ∥f∥∞ = max1≤i≤m+1 ∥fi∥∞. Then, for any ϵ >0, there exist ξ1, . . . ,ξN ∈ Rm+1 and b1, . . . ,bN ∈ Sm such that sup x∈Sm \r\r\r\r\rf(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \r\r\r\r\r 2 ≤ ϵ, with λ = Λ(ϵ/2√m+1) for any N ≥ N(λ, ϵ/√m+1). Proof. The proof is the same as for Theorem B.1. As the concentration parameter λ of the kernels KvMF λ depends only on the smoothness properties of the individual components and these are assumed to be the same, the same kernel choice can be used for all components fi. Furthermore, the choice of partition is independent of the function to be approximated and depends only on the concentration parameter of the kernel. Hence, we can also use the same partition for all components fi. We only need to take into account that: ∥x − y∥2 = vuut m+1X i=1 |xi − yi|2 ≤ p (m + 1)ϵ2 = √ m + 1ϵ, ∀x, y∈ Rm+1, |xi − yi| ≤ϵ, i= 1, . . . , m+ 1. which results in the factor of √m + 1. 24Prompting a Pretrained Transformer Can Be a Universal Approximator C. A Jackson-type Bound for Approximation with a Split Attention Head Lemma C.1. Let a, b: Rd → R, c, d: R → R, c(x), d(x) ̸= 0, ∀x ∈ R and ϵ1, ϵ2 ≥ 0 be such that: sup y∈Rd ∥a(y) − b(y)∥2 ≤ ϵ1 sup x∈R |c(x) − d(x)| = |c − d|∞ ≤ ϵ2. Then for all x ∈ R and y ∈ Rd: \r\r\r\r a(y) c(x) − b(y) d(x) \r\r\r\r 2 ≤ ϵ1|c|∞ + ϵ2 supy∈Rd ∥a(y)∥2 |c(x) d(x)| . Proof. For a fixed x ∈ R and y ∈ Rd, using the triangle inequality gives us \r\r\r\r a(y) c(x) − b(y) d(x) \r\r\r\r 2 = \r\r\r\r a(y) d(x) − b(y) c(x) c(x) d(x) \r\r\r\r 2 = ∥a(y) d(x) − b(y) c(x)∥2 |c(x) d(x)| = ∥a(y) d(x) − a(y) c(x) + a(y) c(x) − b(y) c(x)∥2 |c(x) d(x)| ≤ ∥a(y) (d(x) − c(x))∥2 + ∥c(x) (a(y) − b(y))∥2 |c(x) d(x)| ≤ ϵ2∥a(y)∥2 + ϵ1|c(x)| |c(x) d(x)| ≤ ϵ1|c|∞ + ϵ2 supy∈Rd ∥a(y)∥2 |c(x) d(x)| . And as this holds for all x ∈ R and y ∈ Rd, the inequality in the lemma follows. Lemma C.2. Let f : Sm → Rm+1, m ≥ 8 satisfy the requirements in Corollary B.1. Then, given an ϵ >0 and taking λ, N, and b1, . . . ,bN ∈ Sm as prescribed by the Corollary, we have thatPN k=1 exp(λ⟨x, bk⟩) is close to being a constant: sup x∈Sm \f\f\f\f\f1 − cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ 2√m + 1(L + λ∥f∥∞). (33) Proof. We can use Lemma B.7 by taking g(x, y) = KvMF λ (⟨x, y⟩). From Lemma B.8 we have that the modulus of continuity of KvMF λ is ω(KvMF λ ; t) ≤ t cm+1(λ) exp(λ). Observe that using Equation (16) we have Z Sm g(x, y)dwm(y) = Z Sm KvMF λ (⟨x, y⟩)dwm(y) = wm−1 Z 1 −1 KvMF λ (t)(1 − t2)(m−2)/2dt = wm. The value for δ has to be selected as in Corollary B.1 (Equation (30)): δ = \u0012 2ϵ 6π√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) \u00132 < 1 − cos \u0012 ϵ 6√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013 . Now, using the same partition from Lemma B.7, and recalling that we constructed it such that each element of the partition has the same measure wm(V1) = wm(Vi), ∀i, we have: max x∈Sm \f\f\f\f\f 1 wm Z Sm g(x, y)dwm(y) − 1 wm NX k=1 KvMF λ (⟨x, bk⟩)wm(Vk) \f\f\f\f\f 25Prompting a Pretrained Transformer Can Be a Universal Approximator = max x∈Sm \f\f\f\f\f1 − 1 wm NX k=1 cm+1(λ) exp(λ⟨x, bk⟩)wm(Vk) \f\f\f\f\f = max x∈Sm \f\f\f\f\f1 − cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ 3cm+1 exp(λ) cos−1(1 − δ) < 3cm+1 exp(λ) cos−1 \u0012 cos \u0012 ϵ 6√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) \u0013\u0013 = 3cm+1 exp(λ)ϵ 6√m + 1(L + λ∥f∥∞)cm+1(λ) exp(λ) = ϵ 2√m + 1(L + λ∥f∥∞). Theorem C.1. Let f : Sm → Rm+1, m ≥ 8 satisfies the conditions in Corollary B.1. Define∥f∥∞ = max1≤i≤m+1 ∥fi∥∞. For any ϵ >0, there exist b1, . . . ,bN ∈ Sm such that f can be uniformly approximated to an error at most ϵ: sup x∈Sm \r\r\r\r\rf(x) − PN k=1 ξk exp(λ⟨x, bk⟩) PN k=1 exp(λ⟨x, bk⟩) \r\r\r\r\r 2 ≤ ϵ, with: λ = Λ \u0012 2ϵL 2L + ∥f∥∞ \u0013 with Λ from Equation (23), N ≥ Φ(m) \u00123π(L + λ∥f∥∞)√m + 1 cm+1(λ) exp(λ) ϵ \u00132(m+1) , ξk = f(bk), ∀k = 1, . . . , N. Proof. From Corollary B.1 we know that PN k=1 ξk exp(λ⟨x, bk⟩) approximates f(x) and from Lemma C.2 we know that cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) approximates 1. Using Lemma C.1 we can combine the two results to bound how well PN k=1 ξk exp(λ⟨x, bk⟩) cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) approximates f(x)/1 = f(x). The fact that cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) is not identically 1 means that we will need to increase the precision of approximating the numerator by reducing ϵ in order to account for the additional error coming from the denominator. In particular, we have sup x∈Sm \r\r\r\r\rf(x) − NX k=1 ξk exp(λ⟨x, bk⟩) \r\r\r\r\r 2 ≤ ϵ′, sup x∈Sm \f\f\f\f\f1 − cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ ϵ′ 2√m + 1(L + λ∥f∥∞), sup x∈Sm ∥f(x)∥2 ≤ √ m + 1∥f∥∞, sup x∈Sm \f\f\f\f\f cm+1(λ) N NX k=1 exp(λ⟨x, bk⟩) \f\f\f\f\f ≤ 1 + ϵ′ 2√m + 1(L + λ∥f∥∞). Hence, applying Lemma C.1 gives us: \r\r\r\r\rf(x) − PN k=1 ξk exp(λ⟨x, bk⟩) cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) \r\r\r\r\r 2 = \r\r\r\r\r f(x) 1 − PN k=1 ξk exp(λ⟨x, bk⟩) cm+1(λ) N PN k=1 exp(λ⟨x, bk⟩) \r\r\r\r\r 2 26Prompting a Pretrained Transformer Can Be a Universal Approximator ≤ ϵ′ + ϵ′ 2√m+1(L+λ∥f∥∞) √m + 1∥f∥∞ 1 + ϵ′ 2√m+1(L+λ∥f∥∞) = ϵ′√m + 1(∥f∥∞ + 2L + 2λ∥f∥∞) 2√m + 1(λ∥f∥∞ + L) + ϵ′ ≤ ϵ′ 2 \u0012 ∥f∥∞ λ∥f∥∞ + L + 2λ∥f∥∞ + L λ∥f∥∞ + L \u0013 ≤ ϵ′ 2 \u0012 2 + ∥f∥∞ L \u0013 , where we used that ϵ′, ∥f∥∞L >0, which is the case in realistic scenarios. Therefore, if we want this error to be upper bounded by ϵ, we need to select ϵ′ ≤ 2ϵL 2L + ∥f∥∞ . From Corollary B.1 (Equation (23)) that can be achieved by selecting λ = Λ \u0012 2ϵL 2L + ∥f∥∞ \u0013 and N ≥ Φ(m) \u00123π(L + λ∥f∥∞)√m + 1 cm+1(λ) exp(λ) ϵ \u00132(m+1) . Finally, observe that the cm+1(λ)/N factor can be folded in the ξk terms (Equation (31)): ξk = N cm+1(λ)f(bk) cm+1(λ)wm(Vk) wm = f(bk), with ξk nicely reducing to be the evaluation of f at the corresponding control point bk. D. Additional Results Lemma D.1. Define the stereographic projection and its inverse: Σm : ¯Sm → Rm (x1, . . . , xm+1) 7→ \u0012 x1 1 − xm+1 , . . . ,xm 1 − xm+1 \u0013 Σ−1 m : Rm → Sm (y1, . . . , ym) 7→ \u0012 2y1Pm i=1 y2 i + 1, . . . ,2ymPm i=1 y2 i + 1, Pm i=1 y2 i − 1Pm i=1 y2 i + 1 \u0013 with ¯Sm the part of Sm that gets mapped to [0, 1]m, i.e., ¯Sm = Σ −1 m ([0, 1]m). Σm and Σ−1 m are continuous and inverses of each other and there exist LΣ m and LΣ−1 m such that ω(Σm; t) ≤ LΣ mt and ω(Σ−1 m ; t) ≤ LΣ−1 m t. Furthermore, Σm ◦ H1 H,3(m+1) ◦ Σ−1 m is dense in the set of continuous functions [0, 1]m → Rm. 27",
      "meta_data": {
        "arxiv_id": "2402.14753v1",
        "authors": [
          "Aleksandar Petrov",
          "Philip H. S. Torr",
          "Adel Bibi"
        ],
        "published_date": "2024-02-22T18:12:48Z",
        "pdf_url": "https://arxiv.org/pdf/2402.14753v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that prefix-tuning a pretrained transformer can universally approximate sequence-to-sequence functions, even with much smaller pretrained models than previously thought. It shows that a single attention head is sufficient to approximate any smooth continuous function on a hypersphere. Furthermore, it proves that any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length and independent of approximation precision. The work also offers Jackson-type bounds on the length of the prefix required to achieve a desired approximation precision.",
        "methodology": "The methodology involves simplifying the classical attention head into 'core attention' and 'split attention' heads, demonstrating their ability to approximate continuous functions on hyperspheres. This is achieved using von Mises-Fisher kernels for approximation and providing Jackson-type bounds on the necessary prefix length. The paper then shows that a split attention head can be exactly represented by a classical attention head by embedding inputs and prefixes into different subspaces with specifically constructed attention and value matrices. For approximating general sequence-to-sequence functions, the work leverages a variant of the Kolmogorov–Arnold representation theorem (Schmidt-Hieber, 2021) to construct a transformer architecture with T+2 attention layers, complemented by MLPs and positional encodings, to handle complex function decompositions.",
        "experimental_setup": "Not mentioned",
        "limitations": "The theoretical results rely on the assumption of a highly specific pretrained model structure, which is unlikely to be found in transformers pretrained on real-world data. Consequently, the class of functions a real-world pretrained transformer can approximate with prefix-tuning remains an open question. Additionally, the demonstrated asymptotic efficiency of prefix-tuning shows the required prefix length growing as O(ϵ−10−14m−4m2), which is asymptotically worse than training a transformer from scratch, suggesting it might be less efficient for high precision. The constructions for general sequence-to-sequence functions also require very specific parameter values that are improbable in naturally pretrained models.",
        "future_research_directions": "Future research should focus on investigating whether tighter Jackson bounds exist for prefix-tuning, as well as determining if prefix-tuning and prompting inherently require more trainable parameters than training a transformer for the same approximation accuracy. A crucial direction is to determine the class of functions that a real-world pretrained transformer can approximate through prefix-tuning, requiring the derivation of inverse (Bernstein-type) bounds."
      }
    },
    {
      "title": "HyperTuning:  Toward Adapting Large Language Models without Back-propagation",
      "abstract": "Fine-tuning large language models for different tasks can be costly and\ninefficient, and even methods that reduce the number of tuned parameters still\nrequire full gradient-based optimization. We propose HyperTuning, a novel\napproach to model adaptation that uses a hypermodel to generate task-specific\nparameters for a fixed downstream model. We demonstrate a simple setup for\nhypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or\nLoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5\nin two stages: first, hyperpretraining with a modified conditional language\nmodeling objective that trains a hypermodel to generate parameters; second,\nmulti-task fine-tuning (MTF) on a large number of diverse language tasks. We\nevaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and\nshow that it can effectively generate parameters for unseen tasks. Moreover, we\nshow that using hypermodel-generated parameters as initializations for further\nparameter-efficient fine-tuning improves performance. HyperTuning can thus be a\nflexible and efficient way to leverage large language models for diverse\ndownstream applications.",
      "full_text": "HyperTuning: Toward Adapting Large Language Models without Back-propagation Jason Phang1,2∗, Yi Mao3, Pengcheng He3, Weizhu Chen3 1New York University 2EleutherAI 3Microsoft Azure AI Abstract Fine-tuning large language models for different tasks can be costly and inefﬁcient, and even methods that reduce the number of tuned parameters still require full gradient-based optimization. We propose HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-speciﬁc parameters for a ﬁxed downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a T5-based hypermodel that produces soft preﬁxes or LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5 in two stages: ﬁrst, hyperpretraining with a modiﬁed conditional language modeling objective that trains a hypermodel to generate parameters; second, multi-task ﬁne-tuning (MTF) on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and show that it can effectively generate parameters for unseen tasks. Moreover, we show that using hypermodel-generated parameters as initializations for further parameter-efﬁcient ﬁne-tuning improves performance. HyperTuning can thus be a ﬂexible and efﬁcient way to leverage large language models for diverse downstream applications. 1 Introduction While language models (LMs) have achieved remarkable capabilities with increasing model size (Brown et al., 2020; Chowdhery et al., 2022), ﬁne-tuning them on speciﬁc downstream tasks in- troduces signiﬁcant engineering challenges and computational costs. Although large models can perform zero-shot, instruction-prompted, and few-shot learning (Sanh et al., 2022; Wei et al., 2022), they are usually outperformed by fully ﬁne-tuned models when sufﬁcient training data is available. To reduce the computational and memory overhead of ﬁne-tuning LMs, parameter-efﬁcient ﬁne- tuning (PEFT) methods have been proposed, such as adapters (Houlsby et al., 2019), preﬁx tuning (Li and Liang, 2021), and prompt tuning (Lester et al., 2021). These methods update only a small subset of (possibly new) parameters of the LM, and have achieved competitive performance with full ﬁne-tuning (Ding et al., 2022). However, PEFT methods still require full back-propagation through the LM during training, which is computationally expensive and memory intensive. Given that (1) only a small number of parameters need to be updated to adapt an LM to a given task, (2) very large LMs have demonstrated strong in-context learning capabilities on a forward pass, and (3) a forward pass for very large LMs already entails a substantial amount of computation, we hypothesize that it is possible to train a separate model to perform the optimization or adaptation procedure entirely, using only a forward pass. ∗Work done at Microsoft. Correspondence: jasonphang@nyu.edu Preprint. arXiv:2211.12485v1  [cs.CL]  22 Nov 2022To avoid the costly computation of back-propagating through the LM to produce the parameter updates, especially for thousands or millions of iterations during training, we propose a new paradigm of hypertuning: using a hypermodel to adapt a downstream LM to a desired application. As a concrete proof of concept, we explore a simple setup where hypermodels take as input a set of few-shot examples from a given task, and output the PEFT parameters corresponding to that task in a single forward pass. To demonstrate the feasibility of this approach, we trainHyperT5: a set of T5-based hypermodels that output soft preﬁxes (Li and Liang, 2021) or LoRA parameters (Hu et al., 2022), to be incorporated into a frozen downstream T5 LM. To train HyperT5, we introduce a two-stage procedure for training hypermodels: hyperpretraining, where we adapt a pretrained LM to generate PEFT parameters via a modiﬁed language modeling objective, followed by multi-task ﬁne-tuning (MTF) the hypermodel. After training, HyperT5 models can take few-shot examples from unseen tasks and generate the corresponding PEFT parameters, allowing us to adapt a downstream LM without back-propagation. We show in experiments across P3, Super-NaturalInstructions and MetaICL datasets that LMs can be hypertuned using just a small number of examples. Furthermore, we show that when the hypermodel- generated parameters are used as initializations for further parameter-efﬁcient ﬁne-tuning, we can achieve faster training convergence and better overall performance. This work serves as a ﬁrst step toward hypertuning, and we are are aware of certain limitations of this preliminary setup. Because our current formulation of hypermodels can only take a small number of examples as input, its performance cannot compare to full parameter-efﬁcient ﬁne-tuning or full ﬁne-tuning. HyperT5 also generally underperforms T5 explicitly trained for few-shot in-context learning with full attention across examples, although we note that the latter is more computationally expensive to use at inference time. Nevertheless, we believe that our results demonstrate a promising step toward model adaptation without the need for back-propagation. We plan to release the code and model weights for HyperT5, as well as the multi-task ﬁne-tuned versions for the three datasets listed above. 2 Related Work HyperNetworks Several works have explored the concept of \"hypernetworks,\" where an auxiliary network is used to generate parameters for a primary network. This terminology was ﬁrst introduced by Ha et al. (2017) and applied to LSTMs. Among Transformer-based language models, Karimi Ma- habadi et al. (2021) and He et al. (2022) incorporated hypernetworks into T5 models for knowledge sharing during multitask ﬁne-tuning. Peebles et al. (2022) utilized a Transformer with diffusion for generating full model parameters for image-recognition and Cartpole tasks. Similarly, Lester et al. (2022) trained models to generate soft prompts for transferring between downstream models. Our work is closely related to Deb et al. (2022), who also used a hypernetwork to modify downstream model parameters and incorporated Super-NaturalInstuctions (S-NI) in their experimental setting. They found that incorporating instructions via a hypernetwork trained with MAML (Finn et al., 2017) improved downstream performance. Multi-task Training and Transfer A crucial ingredient to hypertuning is the transferrability of task knowledge and generalization to novel tasks. Many past works (Phang et al., 2018; Pruksachatkun et al., 2020; Vu et al., 2020) have explored the effectiveness of single- and multi-task transfer learning. More recent work has shown that large-scale multi-task training tends allows models to generalize to unseen tasks (Sanh et al., 2022; Wei et al., 2022; Wang et al., 2022; Chung et al., 2022). Min et al. (2022) and Chen et al. (2022) show that few-shot learning also beneﬁts from multi-task training. Pfeiffer et al. (2020), Vu et al. (2021) and Gu et al. (2021) have also explored transfer learning among PEFT methods. 3 HyperTuning The impetus for using hypermodels for adapting downstream models derives from two recent developments in natural language processing: 2Figure 1: Overview of HyperTuning. (A) Fine-tuning, where all model parameters are updated (red). (B) Parameter-efﬁcient ﬁne-tuning (PEFT), where all model parameters are frozen (blue) and only a small number of parameters, φ, are updated. (C) HyperTuning, where a hypermodel is used to generate parameters φfor a frozen downstream model. For instance, a hypermodel may take a set of few-shot examples to determine what φto generate. Only the hypermodel’s parameters are updated during training. (D) At inference time, the parameters φonly need to be generated once, and thereafter only need to store φ, with no need to retain the few-shot examples. 1) Large language models can perform in-context learning effectively. Large language models have been shown to be able to learn from the context of a small number of examples or instructions for a task, without any prior training on that task (Brown et al., 2020; Min et al., 2022; Wang et al., 2022). This suggests that models can “understand” what the task is and how to tackle it based on a few samples or descriptions of the task. This capability appears to improve as the models get larger or are trained on more relevant data (Chowdhery et al., 2022; Ouyang et al., 2022; Bai et al., 2022). 2) Large language models can be adapted to downstream tasks by tuning a small set of param- eters. Along with the growth in model sizes, there have been signiﬁcant advances in ﬁne-tuning methods that only modify a small number of parameters (possibly adding some new ones) in a frozen language model to adapt it to a speciﬁc task (Houlsby et al., 2019; Li and Liang, 2021; Lester et al., 2021; Ding et al., 2022). These methods often achieve performance comparable to ﬁne-tuning all parameters in the model. Importantly, the number of parameters that need to be changed is small enough that it is feasible to train a model to generate them (Qin et al., 2021; Lester et al., 2022). Taken together, these ﬁndings suggest that we may be able to use an auxiliary model that can ﬁrst extract some task-relevant knowledge from some input that describes the task (e.g. instruction, few-shot examples), and then generate a small number of adaptive parameters, thereby changing the main model’s behavior to suit the task. This approach, if successful, would enable us to adapt models to downstream applications without using backpropagation, or storing the encoded representations of few-shot examples in memory. In other words, we can delegate the work of model adaptation to a separate model. We call this approach hypertuning, inspired by the work on hypernetworks by Ha et al. (2017). Hy- pertuning uses a hypermodel to adapt a downstream model to a target downstream task or application. This is differs from ﬁne-tuning, which uses backpropagation and a gradient descent algorithm to update model parameters. In this work, we present one possible formulation of hypertuning using few-shot examples and generating a small set of parameters with a single forward pass through the hypermodel. However, this is just one possible way of performing hypertuning, and the idea of adapt- ing models with hypermodels can be generalized to many other cases. For example, hypermodels could also be trained to predict gradients or generate parameter updates based on input-output pairs. This way, hypermodels could work with large training sets, not just a few examples. Ultimately, with sufﬁciently general and well-trained hypermodels, we may be able to replace gradient-descent- based ﬁne-tuning pipelines with hypertuning for many applications, while achieving similar or better performance. 33.1 HyperTuning with Fewshot Examples Let M be a model with parameters θ, initialized at θ0 from pretraining, and L a loss function. Given a dataset of size N with input-output pairs {(x,y)}, standard ﬁne-tuning minimizes the following objective over θ: arg min θ 1 N ∑ {(x,y)} L ( y,M(θ; x) ) (1) In the case of parameter-efﬁcient ﬁne-tuning (PEFT), we ﬁx θ = θ0 and introduce a small set of trainable parameters φ(e.g. adapter parameters, soft prompts) that are injected into M. We optimize only over φ: arg min φ 1 N ∑ {(x,y)} L ( y,M(θ0; x,φ) ) (2) For hypertuning, we further deﬁne a hypermodel Hwith parameters ξthat produces PEFT parameters ˆφbased on its input, which can be a set of few-shot examples or task instructions. For example, if the hypermodel input is a set of few-shot examples {(xi,yi)}K, we have: ˆφ= H ( ξ; {(xi,yi)}K ) (3) One way to train the hypermodel (H,ξ) is to perform PEFT on many tasks and use the resulting φ as targets. However, this is costly in computation, requiring many ﬁne-tuning runs, and does not leverage cross-task knowledge transfer. Instead, we propose to train the hypermodel end-to-end, optimizing through the frozen model (M,θ0). Hence, the hypermodel training objective is: arg min ξ 1 N ∑ {(x,y)},{{(xi,yi)}K} L ( y,M ( θ0; x,H(ξ; {(xi,yi)}K) )) (4) At each training step, we sample a target example (x,y) and non-overlapping few-shot examples {(xi,yi)}K. We generate ˆφfrom the few-shot examples and compute the loss with respect to (x,y) and ˆφ. We then back-propagate the gradients through both M and H to update ξ. Note that since ˆφdoes not depend on x, it can be computed once for a given set of few-shot examples and reused for downstream predictions. At inference time, we can use ˆφdirectly without storing or recomputing the representations for {(x,y)},{(xi,yi)}K, saving memory and computation.2 4 HyperT5: A T5-Based HyperModel 4.1 Architecture and Setup To demonstrate the feasibility of hypertuning, we proposeHyperT5, a hypermodel based on T5, where both the hypermodel and the downstream model share a T5 backbone (Figure 2A). We use a frozen LM-adapted T5 3 as the downstream model. The hypermodel is also initialized with LM-adapted T5 parameters, but with some architectural changes. As deﬁned in Equation 3, the hypermodel encoder takes the few-shot examples (and/or task deﬁnitions, in the case of S-NI) as input. The hypermodel decoder takes a ﬁxed set of newly learned token embeddings as input, and outputs a set of decoder token representations, which are then fed to a set of MLPs to generate the PEFT parameters φfor the downstream model. We also remove the causal masking from the decoder, since the hypermodel does not perform autoregressive generation. 2By construction, few-shot examples occupy at least K times the memory of the target input x. 3This is the model introduced by Lester et al. (2021). We use the T5 v1.1 architecture and initialize all experiments with the LM-adapted parameters, unless stated otherwise. 4Figure 2: Overview of HyperT5. (A) HyperT5 takes as input few-shot examples and outputs PEFT parameters φ. The model is initialized from an LM-adapted T5. (B) In HyperT5-Preﬁx, φare key and value preﬁxes for every attention layer. (C) In HyperT5-LoRA, φare additive low-rank modiﬁcations to the query and value linear maps. We experiment with two PEFT methods: preﬁx tuning (Li and Liang, 2021) and LoRA (Hu et al., 2022). Preﬁx tuning (Figure 2B) prepends a set of learned key and value representations within each attention layer, while LoRA (Figure 2C) learns a low-rank additive modiﬁcation to the query and value linear maps. Both PEFT methods have been shown to achieve good performance across a wide range of tasks (Ding et al., 2022). Chan et al. (2022) also suggest that modifying in-context representations and model weights can lead to different model behaviors, and we seek to demonstrate that hypertuning is applicable to very different PEFT methods. We name the respective hypermodels HyperT5-Preﬁx and HyperT5-LoRA. The number of decoder input tokens and the size of the MLPs depend on the choice of PEFT method and its hyperparameters. For example, for HyperT5-Preﬁx that generates soft preﬁxes corresponding to preﬁx tuning, φwill be of the shape [L,2,2,P,H ], where Lis the number of layers, 2 is for the encoder and decoder, 2 is for the key and value preﬁxes, P is the number of preﬁx tokens, and H is the hidden size. We set the number of decoder input tokens to be 2P. We provide pseudo-code for HyperT5-Preﬁx and HyperT5-LoRA models in the Figure 7 and Figure 8 in the Appendix. 4.2 HyperPretraining To train HyperT5, we ﬁrst undergo an additional stage of pretraining to adapt the hypermodel to generate parameters φfor the downstream model, which we call hyperpretraining. As we show in Section 5.5, hyperpretraining is crucial for good hypermodel performance. We propose a simple scheme for hyperpretraining using a Context-Augmented Conditional Language Modeling (CACLM) objective, which extends the conditional language-modeling (CLM) objective of T5 LM-adaptation. As shown in Figure 3, we sample a 512-token sequence from a pretraining corpus and split it into four consecutive segments A–D. The downstream model receives segment B as input and predicts segment C, following the CLM objective. The hypermodel receives segments A and D as input, which provide additional context from the same document, and outputs PEFT parameters for the downstream model.4 The hypermodel thus compresses contextual information to assist the downstream model in its CLM task. We also make segment B very short (32 tokens) to encourage the downstream model to depend on the hypermodel information for accurate prediction of tokens in C. During hyperpretraining, we freeze the downstream model and only update the hypermodel parame- ters, training for 100K steps on the C4 dataset (Raffel et al., 2020). We perform hyperpretraining separately for HyperT5-Preﬁx and HyperT5-LoRA models. Hyperparameters can be found in Appendix A. 5 Multi-Task Fine-Tuning with HyperT5 5.1 Multitask Fine-Tuning (MTF) After hyperpretraining, we conduct a second stage of training to train the hypermodel to generate task- speciﬁc PEFT parameters based on a small number of examples that we provide as input (Figure 1C). By performing multi-task ﬁne-tuning on a sufﬁciently large number of tasks, we hope to have the 4Segments A and D are marked by sentinel tokens. 5Figure 3: Overview of HyperPretraining using the Context-Augmented Conditional Language Modeling (CACLM) objective to train a hypermodel to predict PEFT parametersφ. (A) Sample a sequence of 512 tokens from a pretraining corpus, and splice into 4 segments A–D. (B) The frozen downstream model takes as input B and predicts continuation C. (C) The hypermodel is trained to encode additional context A and D into PEFT parameters φ, providing additional information to the downstream model to predict C. hypermodel learn to generalize to generate parameters for unseen tasks. We adopt a similar training setup to MetaICL (Min et al., 2022), which uses multi-task ﬁne-tuning (Sanh et al., 2022; Wei et al., 2022) with both a target input example (x) and a set of few-shot input-output pairs {(xi,yi)}K as inputs. The hypermodel takes the few-shot pairs as input, while the downstream model takes the target example as input, as shown in Equation 3. We ﬁne-tune only the hypermodel parameters and keep the downstream model parameters ﬁxed, unless otherwise stated. Appendix A.1 shows how we format the few-shot inputs. We compare our approach with two baselines: multi-task ﬁne-tuning of a T5 model without few-shot inputs, and MetaICL (multi-task ﬁne-tuning with few-shot inputs). In MetaICL, the few-shot pairs are concatenated with the target example as input, both during training and evaluation on new tasks. We also include baselines that use PEFT methods for multi-task ﬁne-tuning, i.e. learning a single set of preﬁx tuning or LoRA parameters. We perform multi-task ﬁne-tuning for 10,000 steps with a batch size of 256. For models that use few-shot inputs (MTF with fewshot, and hypermodels), we use up to 16 examples, and truncate tokens that exceed the maximum input length. Appendix B provides more details on the datasets. 5.2 Datasets To demonstrate the generality of our approach, we conduct experiments on three different multi-task training datasets, each with different held-out tasks and evaluation protocols. Public Pool of Prompts (P3)(Sanh et al., 2022) consists of 62 task datasets, and was used in training the T0 models. The prompt are formatted with 0-shot inference in mind, and often contain instructions or the possible answer options. For training our models, we use the T0-train subset. In order to ﬁt multiple examples into the hypermodel’s context, we further exclude dataset-prompt subsets with average input sequence lengths longer than 320 tokens. The list of included dataset-prompts can be found in Figure 6. Evaluation is performed on a ﬁxed set of held-out tasks, based on multiple-choice scoring with accuracy. We exclude StoryCloze from evaluation as the task is not distributed with training data. MetaICL (Min et al., 2022) introduced a few-shot multi-task training dataset, which is an extension of CrossFit (Ye et al., 2021) with UniﬁedQA (Khashabi et al., 2020) and the addition of training data. For brevity, we will refer to this dataset as MetaICL. Unlike P3 and S-NI, the task inputs are not formatted for 0-shot inference; for instance, the task inputs may give no clue as to the goal of the task, or what the output space is. They provide several different train-task splits for tasks, of which we run our experiments on three (HR→LR, Non-NLI→NLI, Non-Class→Class) to economize on computation costs. Evaluation is performed on held-out tasks, with ROUGE or Macro-F1 on model generations depending on the task. 6Super-NaturalInstructions (S-NI) (Wang et al., 2022) consists of over 1,600 task datasets, each with a task deﬁnition as well as a ﬁxed set of positive and negative demonstrations. Following their ﬁndings, we focus our experiments on two settings: using only the task deﬁnition as the hypermodel input, and using deﬁnitions alongside two ﬁxed positive examples. We only use the English tasks within the dataset. Evaluation is performed on a set of held-out tasks using ROUGE-L on model generations. 5.3 Results 5.3.1 P3 ANLI HSwag CB COPA RTE WiC WSC WGD A VG Full Fine-Tuning T5-MTF 33.4 28.0 63.0 77.9 71.1 50.8 61.0 53.4 54.8 T5-MTF-Few-shot 35.3 27.5 68.6 70.5 75.2 51.7 62.1 52.2 55.4 Parameter-Efﬁcient Fine-Tuning (PEFT) T5-MTF (Preﬁx) 33.1 26.1 53.9 67.8 60.5 49.8 54.7 51.4 49.7 T5-MTF (LoRA) 32.9 26.0 36.0 59.7 49.8 51.2 58.1 50.5 45.5 HyperTuning HyperT5-Preﬁx 33.4 32.3 60.1 73.9 71.5 51.1 63.0 51.1 54.6 HyperT5-LoRA 33.6 33.0 49.5 74.2 67.4 52.0 64.0 52.9 53.3 HyperTuning + Fine-Tuning HyperT5-Preﬁx+ 34.5 32.2 58.1 78.4 76.5 50.4 63.8 54.3 56.0 HyperT5-LoRA+ 33.9 30.7 62.1 75.8 72.3 50.8 64.6 54.5 55.6 Table 1: Results on P3 on held-out tasks (dev) with T5-Large models. T0 results taken from Sanh et al. (2022). ANLI HSwag CB COPA RTE WiC WSC WGD A VG Full Fine-Tuning T5-MTF 39.9 29.4 64.5 88.0 80.8 51.7 60.7 57.9 59.1 T5-MTF-Few-shot 37.9 30.9 67.6 90.5 76.6 51.2 63.3 61.1 59.9 Parameter-Efﬁcient Fine-Tuning (PEFT) T5-MTF (Preﬁx) 38.3 31.2 61.4 82.4 78.6 52.6 57.0 54.3 57.0 T5-MTF (LoRA) 33.9 26.4 47.1 67.2 53.3 50.8 51.5 50.3 47.6 HyperTuning HyperT5-Preﬁx 38.7 33.6 69.6 88.4 79.5 53.1 57.6 56.6 59.6 HyperT5-LoRA 35.3 30.8 66.4 83.3 68.5 50.3 60.0 56.1 56.4 Other results T0 33.4 27.3 45.4 73.1 64.5 50.7 65.0 51.0 51.3 Table 2: Results on P3 on held-out tasks (dev) with T5-XL models. T0 results taken from Sanh et al. (2022). Table 1 and Table 2 show the results of our experiments on the P3 dataset using T5-Large (∼770M parameters) and T5-XL (∼3B parameters), respectively. We compare our HyperT5-Preﬁx and HyperT5-LoRA, which use hypermodels to generate task- speciﬁc PEFT parameters based on few-shot examples, with several baselines: preﬁx tuning, LoRA tuning, T5-MTF, and T5-MTF-Few-shot. T5-MTF is a model that roughly corresponds to the T0 model, and we detail the differences in Appendix B.1. Our results show that both HyperT5-Preﬁx and HyperT5-LoRA signiﬁcantly improve over the preﬁx and LoRA tuning baselines, indicating the effectiveness of using hypermodels to adapt the frozen downstream T5 model to unseen tasks. HyperT5-Preﬁx achieves performance close to T5-MTF, while 7T5-MTF-Few-shot attains the highest scores, in line with the ﬁndings of Min et al. (2022). These patterns are consistent across T5-Large and T5-XL,5 demonstrating the scalability of hypertuning. We emphasize that HyperT5-Preﬁx/LoRA only introduces a very small number of PEFT parameters in the frozen downstream T5 model, whereas all parameters are tuned in the T5-MTF and T5-MTF- Few-shot models. Moreover, the P3 examples are written with prompt templates that are optimized for zero-shot inference, which is the ideal input format for T5-MTF. Furthermore, T5-MTF-Fewshot has full, bidirectional self-attention between the target input xand the few-shot examples, whereas HyperT5-Preﬁx and HyperT5-Lora only incorporate information from the few-shot examples via the respective PEFT parameters. To investigate whether the hypermodel beneﬁts are complementary to updating the downstream model parameters, we conduct an additional set of experiments where we jointly train both the hypermodel and the downstream model (HyperTuning + Fine-Tuning), with results shown at the bottom of Table 1. We observe that both HyperT5-Preﬁx+ and HyperT5-Lora+ slightly surpass T5-MTF-Fewshot, suggesting that the hypermodels can further enhance the performance of ﬁne-tuned downstream models. 5.3.2 MetaICL Table 3 presents the results on three MetaICL task splits. As in the previous experiments, both HyperT5 models surpass the PEFT models and T5-MTF in performance, except for T5-MTF-Few- shot, which outperforms them in all but one case: Non-NLI→NLI, where HyperT5-Preﬁx achieves a higher score. T5-MTF performs poorly in the MetaICL experiments, as it has to handle task examples zero-shot, and the MetaICL inputs are not suitable for zero-shot inference, as explained above. HR →LR Non-NLI →NLI Non-Class →Class Full Fine-Tuning T5-MTF 34.3 48.8 30.3 T5-MTF-Few-shot 41.0 56.7 40.6 Parameter-Efﬁcient Fine-Tuning (PEFT) T5-MTF (Preﬁx) 29.8 42.8 29.6 T5-MTF (LoRA) 31.5 41.3 28.7 HyperTuning HyperT5-Preﬁx 38.0 58.3 38.6 HyperT5-LoRA 35.4 54.2 34.8 Table 3: Results on MetaICL (Test) with T5-Large models. 5.3.3 Super-NaturalInstructions (S-NI) We report the results on the different S-NI settings in Table 4 for T5-Large and Table 5 for T5-XL, using both Def (deﬁnition-only) and Def+2Pos (deﬁnition and two ﬁxed positive examples) settings. The T5-MTF (Def) and T5-MTF (Def+2Pos) models are similar to the corresponding Tk-Instruct variants (Wang et al., 2022), with a slight difference in input formatting (see Appendix A.1). For the hypermodels, we prepend the task deﬁnitions to the few-shot examples and treat them as part of the hypermodel input. On average, the HyperT5 with Def+2Pos outperforms T5-MTF (Def) by a large margin, but still underperforms T5-MTF (Def+2Pos), in line with the above results. 5.4 Discussion Above, we evaluated hypermodels on three multi-task datasets, where they generate task-speciﬁc soft preﬁxes or LoRA parameters from a few examples or instructions. In general, HyperT5 matched or exceeded T5-MTF models, but lagged behind T5-MTF-Fewshot models (or Def+2Pos models, in the case of S-NI). This gap is expected, as T5-MTF-Fewshot uses full self-attention between the 5We note that T0-XL performs much worse than our trained T5-MTF, which is in agreement with other work (Anonymous, 2023; Wu et al., 2022) that have reported similar results in replicating T0. 8A VG Full Fine-Tuning T5-MTF (Def) 40.6 T5-MTF (Def+2Pos) 47.6 HyperTuning HyperT5-Preﬁx (Def) 37.1 HyperT5-Preﬁx (Def+2Pos) 43.5 HyperT5-LoRA (Def) 34.9 HyperT5-LoRA (Def+2Pos) 42.0 Other Results Tk-Instruct (Def+2Pos) 48.0 Table 4: Results on Super-NaturalInstuctions (S-NI; Test) with T5-Large models. T k- Instruct results taken from Wang et al. (2022). A VG Full Fine-Tuning T5-MTF (Def) 46.6 T5-MTF (Def+2Pos) 54.3 HyperTuning HyperT5-Preﬁx (Def) 38.9 HyperT5-Preﬁx (Def+2Pos) 48.6 HyperT5-LoRA (Def) 38.9 HyperT5-LoRA (Def+2Pos) 45.0 Other Results Tk-Instruct (Def+2Pos) 54.0 Table 5: Results on Super-NaturalInstuctions (S-NI; Test) with T5-XL models. Tk-Instruct results taken from Wang et al. (2022). examples and the target input x, while HyperT5 encodes the examples into PEFT parameters that are independent of x. We attribute some of the gap to this limitation. However, this limitation also confers efﬁciency advantages to HyperT5 at inference time compared to T5-MTF-Fewshot. In encoder-decoders such as T5, the full self-attention between the examples and xprevents the separation of their representations: a new forward pass is needed for each new x. In contrast, for hypermodels the examples can be encoded into PEFT parameters once, and reused for all subsequent inputs. Even for decoder-only models (e.g. MetaICL based on GPT-2), where the examples can be cached as key and value representations, the cache size is likely much larger than the PEFT parameters, as the cache stores all the representations for every token in the examples, which are several times longer than the input by deﬁnition. Thus, hypermodels in our setup sacriﬁce some performance for efﬁciency. Regarding T5-MTF, one might wonder what the concrete beneﬁt of HyperT5 is, given their similar performance. After all, unlike T5-MTF-Fewshot, T5-MTF only uses xas the input, requiring no extra computation or memory, and only one set of model weights. Firstly, we stress that the HyperT5 model can only affect the downstream model through a small number of modiﬁed parameters, while in T5-MTF all the parameters that process xare modiﬁed. Although HyperT5 and T5-MTF have roughly the same number of tuned parameters, the parameters modiﬁed in T5-MTF directly interact with the input x, which we expect to help performance. Secondly, we identify two separate, but possibly related, sources of performance improvement: better general task performance of the downstream model (which is usually the goal of MTF training), and adapting the downstream model to a new task based on few-shot examples, using hypermodels in our case. Our aim in this work is to show the feasibility of the latter. We argue that both sources are complementary, and we showed in Section 5.3.1 that when we use hypermodels without freezing the downstream model, thereby acquiring both beneﬁts, performance further improves. More generally, we expect that training a hypermodel against an already multi-task ﬁne-tuned model will lead to better performance than just using the model for zero-shot inference alone, and we plan to explore this in future work. We also observe a consistent trend where HyperT5-Preﬁx outperforms HyperT5-LoRA. We speculate that it is easier for hypermodels to learn to generate soft preﬁxes as compared to LoRA weights, since soft preﬁx are effectively model-internal hidden states, and the generated PEFT parameters are themselves transformations of the hypermodel hidden states. Incidentally, another possible interpretation of the HyperT5-Preﬁx model is that the combination of the hypermodel and the downstream model can be seen as a dual-encoder, single-decoder model with separate encoders for the few-shot examples and the target example. Lastly, the majority of the experiments were conducted with minimal hyperparameter-tuning, and the current results primarily serve as a proof-of-concept of hypertuning being a viable approach to adapt downstream models. We expect that further exploration of hyperpretraining and MTF hyperparameters as well as hypermodel architectures may lead to better results and overcome some of the limitations we identiﬁed. 90 20k 50k 70k 100k HyperPretraining Steps 50 51 52 53 54 55P3 Held-out Performance (a) HyperT5-Preﬁx 0 20k 50k 70k 100k HyperPretraining Steps 50 51 52 53 54 55P3 Held-out Performance  (b) HyperT5-LoRA Figure 4: Performance of HyperT5 models on P3 evaluation with different amounts of hyperpretrain- ing. HyperPretraining is crucial for good performance of the hypermodels. However, hyperpretraining for too many steps can also hurt performance (as see in the case of HyperT5-LoRA). 5.5 Is HyperPretraining Necessary? We demonstrate the beneﬁts of hyperpretraining for the hypermodels in this section. As mentioned in Section 3, we hyperpretrained the hypermodels for 100k steps before multi-task ﬁne-tuning them on P3 tasks. To examine the impact of hyperpretraining, we also multi-task ﬁne-tuned HyperT5-Preﬁx and HyperT5-LoRA from LM-adapted T5 without any hyperpretraining, and from intermediate checkpoints over the course of hyperpretraining. Figure 4 shows the average scores on the held-out tasks for these models. Both HyperT5 models perform very poorly without any hyperpretraining, achieving scores similar to PEFT-only (see Table 1). With hyperpretraining, the performance of both hypermodels signiﬁcantly improves. While HyperT5-Preﬁx appears to consistently improve over the course of 100k steps, we observe that HyperT5-LoRA performance slightly declines after 50k steps. Hypermodels targeting different PEFT methods may beneﬁt from different amounts of hyperpretraining, and we emphasize that our choice of the number of hyperpretraining steps is by no means considered to be optimal.6 We expect that better hyperpretraining conﬁgurations can be explored in future work. 6 HyperModels for Improved Parameter Initialization Thus far, we have discussed hypermodels in the context of generating PEFT parameters in a single forward pass through the hypermodel. We can also consider an alternative use of hypermodels: Instead of randomly initializing new parameters, we can use hypermodels to produce task-speciﬁc PEFT parameters based on a few examples from the task. This can be seen as using task knowledge acquired by the hypermodel during training to provide a ﬁrst approximation of PEFT parameters, and thereafter reﬁning the parmaeters via regular PEFT training. In conventional PEFT, wherever new parameters are introduced into the model, they are either initialized randomly, or with ﬁxed initial values (e.g. the up-projection weights in LoRA are initialized to 0)–for brevity, we will refer to this simply as random initialization. Beyond random initialization, Vu et al. (2021, SPoT) and Gu et al. (2021, PPT) have explored transfer-learning within PEFT, ﬁrst doing PEFT on one or more upstream tasks, and then using the learned PEFT parameters as an initialization for downstream PEFT. This approach has two advantages over conventional PEFT initializations. First, the hypermodel- generated parameters already perform well on the task, as shown in Section 5.3, so PEFT training can reach good performance faster. Second, the hypermodel can automatically transfer relevant knowledge from previous tasks to the new task, similar to SPoT and PPT, except we let the hypermodel determine what previously learned task knowledge is most applicable to the new task. For instance, a major challenge addressed in SPoT was searching for the set of upstream tasks whose PEFT parameters would be the most appropriate initialization for a downstream task–in our case, we can directly provide a hypermodel with few-shot examples to generate our desired initialization. 6We chose 100k steps based on the T5 LM-adaptation procedure (Lester et al., 2021). 100 5000 10000 15000 20000 Training Steps 50 60 70Accuracy Rand Init Shared Init Hyper Init (a) Preﬁx Tuning 0 5000 10000 15000 20000 Training Steps 50 60 70Accuracy Rand Init Shared Init Hyper Init (b) LoRA Figure 5: Average performance on P3 held-out tasks with preﬁx tuning and LoRA, using different parameter initializations. Using hypermodel-generated initializations starts with higher performance and continues to perform better on average over the course of training. To investigate the effectiveness of using hypermodels to generate PEFT initializations, we use the P3-trained models from Section 5.3.1, and perform preﬁx tuning and LoRA tuning on the held-out tasks individually.7 For each method-task pair, we sweep across learning rates {1e−3,1e−4,1e−5} and take the best average result over 3 random seeds. We consider two baselines for initializations: random initialization (Rand Init) and using the multi-task ﬁne-tuned PEFT parameters from Section 5.3.1 as initializations (Shared Init). The hypermodel- generated initialization (Hyper Init) is generated using a randomly sampled set of 16 examples from the respective training sets. We show the results of preﬁx tuning8 and LoRA tuning with different initialization schemes in Table 6. We observe that for both preﬁx tuning and LoRA tuning, shared initialization signiﬁcantly performs random initialization, while using a hypermodel-generated initialization outperforms both on average. We also show the average performance across tasks over the course of tuning in Figure 5. We observe that hypermodel-generated initializations start with much better performance compared to the other two initialization schemes, and continue to outperform them over the course of ﬁne-tuning. Hence, hypermodels can be complementary to a standard PEFT pipeline, providing both performance gains and computational cost savings. ANLI HSwg CB COPA RTE WiC WSC WGD A VG Preﬁx (Rand Init) 54.6 50.5 98.8 79.0 78.8 71.6 63.5 52.2 68.6 Preﬁx (Shared Init) 60.8 51.6 99.4 85.7 84.8 72.4 72.6 65.1 74.0 Preﬁx (Hyper Init) 61.4 51.5 97.6 84.3 87.1 71.2 76.5 71.6 75.2 LoRA (Rand Init) 59.5 51.3 93.5 78.0 82.6 73.5 77.9 65.1 72.7 LoRA (Shared Init) 57.9 51.6 99.4 83.0 83.8 73.1 73.3 67.9 73.7 LoRA (Hyper Init) 57.7 48.4 99.4 87.3 84.1 73.0 83.9 66.2 75.0 Table 6: Preﬁx tuning and LoRA ﬁne-tuning on T5-Large with different initializations on P3 held-out tasks. Using HyperT5-generated parameters as an initialization achieves better performance on average than using shared MTF PEFT parameters or random initialization. 7 Conclusion We introduce the concept of hypertuning, which leverages a hypermodel to adapt a downstream model to a speciﬁc downstream application. We present a basic framework for hypertuning, where a hypermodel is trained to produce parameters for a downstream model from few-shot examples in one forward pass, and we apply this framework to train HyperT5-Preﬁx and HyperT5-LoRA models that can adapt a ﬁxed downstream T5 model. We ﬁnd that a two-stage training procedure of hyperpretraining and multi-task ﬁne-tuning is effective for training hypermodels, and we evaluate the 7We use one speciﬁc prompt format for each task, listed in Appendix B.1. 8Preﬁx tuning is performed via a reparameterization, in line with standard practice. Refer to Appendix D for details. 11HyperT5 models on P3, MetaICL and S-NI datasets, showing that they can generate PEFT parameters that enable the downstream T5 models to perform well on unseen tasks. Furthermore, the parameters generated by hypertuning can also serve as improved parameter initializations for parameter-efﬁcient ﬁne-tuning. We regard these ﬁndings as an initial but encouraging indication of the potential of adapting large language models without back-propagation. 8 Acknowledgements We would like to thank Sam Bowman for their thoughtful feedback and Jonas Pfeiffer for early idea discussion. References Anonymous. 2023. Pretraining One Language Model for All With the Text-to-text Framework Using Model-generated Signals. In Submitted to The Eleventh International Conference on Learning Representations. Under review. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatﬁeld-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc. Stephanie C. Y . Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K. Lampinen, and Felix Hill. 2022. Transformers generalize differently from information stored in context vs in weights. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via Language Model In-context Tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719–730, Dublin, Ireland. Association for Computational Linguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. 2022. Scaling Instruction-ﬁnetuned Language Models. 12Budhaditya Deb, Guoqing Zheng, and Ahmed Hassan Awadallah. 2022. Boosting Natural Language Generation from Instructions with Meta-learning. Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. 8-bit Optimizers via Block- wise Quantization. 9th International Conference on Learning Representations, ICLR. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. 2022. Delta Tuning: A Comprehensive Study of Parameter Efﬁcient Methods for Pre-trained Language Models. Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic Meta-learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126–1135. PMLR. Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. PPT: Pre-trained Prompt Tuning for Few-shot Learning. David Ha, Andrew M. Dai, and Quoc V . Le. 2017. HyperNetworks. InInternational Conference on Learning Representations. Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. 2022. HyperPrompt: Prompt-based task-conditioning of transformers. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 8678–8690. PMLR. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efﬁcient Transfer Learning for NLP. CoRR, abs/1902.00751. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021. Parameter-efﬁcient Multi-task Fine-tuning for Transformers via Shared Hypernetworks. In Pro- ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 565–576, Online. Association for Computational Linguistics. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896–1907, Online. Association for Computational Linguistics. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efﬁcient Prompt Tuning. Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah Constant. 2022. Reducing Retraining by Recycling Parameter-Efﬁcient Prompts. Xiang Lisa Li and Percy Liang. 2021. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online. Association for Computational Linguistics. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to Learn In Context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791–2809, Seattle, United States. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. 13William Peebles, Ilija Radosavovic, Tim Brooks, Alexei A. Efros, and Jitendra Malik. 2022. Learning to learn with generative models of neural network checkpoints. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. CoRR, abs/2005.00247. Jason Phang, Thibault Févry, and Samuel R. Bowman. 2018. Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231–5247, Online. Association for Computational Linguistics. Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li, Maosong Sun, and Jie Zhou. 2021. Exploring Universal Intrinsic Task Subspace via Prompt Tuning. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1–67. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: Memory Optimizations toward Training Trillion Parameter Models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC ’20. IEEE Press. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Repre- sentations. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2021. SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer. CoRR, abs/2110.07904. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella- Micke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and Predicting Transferability across NLP Tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art 14natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online. Association for Computational Linguistics. Zhaofeng Wu, Robert L. Logan IV , Pete Walsh, Akshita Bhagia, Dirk Groeneveld, Sameer Singh, and Iz Beltagy. 2022. Continued Pretraining for Better Zero- and Few-Shot Promptability. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics. Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163–7189, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. 15A Training Details All experiments are trained with 1-bit Adam (Dettmers et al., 2022) and batch size of 256, a learning rate of 5e-5, and a linear decay schedule. Training was performed with ZeRO (Rajbhandari et al., 2020) and Transformers (Wolf et al., 2020). For hypermodels, the hypermodel’s max input sequence length is 1024 tokens and the downstream model’s max input sequence length is 384 tokens. Correspondingly, the max input sequence length for all non-few-shot models (e.g. T5-MTF, T5-MTF(Preﬁx)) is 384. The max input sequence length of few-shot models (e.g. T5-MTF-Few-shot) is thus conservatively set at 1024+384=1408 tokens. The max target sequence length is set to 128 for all experiments. A.1 Input Formatting Few-shot examples for hypermodels are formatted in the following manner: <x> Input 1 <y> Target 1 <x> Input 2 <y> Target 2 <x> Input 3 <y> Target 3 where <x> and <y> and special tokens. For S-NI, the task deﬁnitions are treated as just another example: <x> Instruction <x> Input 1 <y>Target 1 <x> Input 2 <y>Target 2 B Dataset-speciﬁc Details B.1 P3 / T0 We highlight some differences our T0 baselines and the T0 setup described in the original paper (Sanh et al., 2022). Besides the different optimizers and batch sizes listed above, we do not use packing to process our training data. Moreover, because our focus is on few-shot learning, we remove a number of tasks formulations with longer inputs from the T0-train dataset, listed in Section 6. For T0, we use an input sequence length of 384 and output length of 128, which matches the input and output lengths of the downstream model in our hypermodel setup. For T5-MTF-Few-shot, we use an input sequence length of 1024+384=1408, which is the combined input lengths of the hypermodel and downstream model. We believe that these changes can meaningfully modify the performance of the T0 models, but provide a fairer baseline to the hypermodel setup. 16adversarial_qa_dbert_answer_the_following_q, adversarial_qa_dbert_based_on, adversarial_qa_dbert_generate_question, adversarial_qa_dbert_question_context_answer, adversarial_qa_dbert_tell_what_it_is, adversarial_qa_dbidaf_answer_the_following_q, adversarial_qa_dbidaf_based_on, adversarial_qa_dbidaf_generate_question, adversarial_qa_dbidaf_question_context_answer, adversarial_qa_dbidaf_tell_what_it_is, adversarial_qa_droberta_answer_the_following_q, adversarial_qa_droberta_based_on, adversarial_qa_droberta_generate_question, adversarial_qa_droberta_question_context_answer, adversarial_qa_droberta_tell_what_it_is, ag_news_classify, ag_news_classify_question_ﬁrst, ag_news_classify_with_choices, ag_news_classify_with_choices_question_ﬁrst, ag_news_recommend, ag_news_which_section, ag_news_which_section_choices, amazon_polarity_Is_this_product_review_positive, amazon_polarity_Is_this_review, amazon_polarity_Is_this_review_negative, amazon_polarity_User_recommend_this_product, amazon_polarity_convey_negative_or_positive_sentiment, amazon_polarity_ﬂattering_or_not, amazon_polarity_negative_or_positive_tone, amazon_polarity_user_satisﬁed, amazon_polarity_would_you_buy, app_reviews_categorize_rating_using_review, app_reviews_convert_to_rating, app_reviews_convert_to_star_rating, app_reviews_generate_review, cnn_dailymail_3.0.0_generate_story, cnn_dailymail_3.0.0_spice_up_story, common_gen_Example_prompt, common_gen_Given_concepts_type_1, common_gen_Given_concepts_type_2, common_gen_Put_together, common_gen_choice_in_concept_centric_sentence_generation, common_gen_random_task_template_prompt, common_gen_sentence_to_concepts, common_gen_topic_to_sentence, common_gen_topics_from_the_sentence, cos_e_v1.11_aligned_with_common_sense, cos_e_v1.11_description_question_option_id, cos_e_v1.11_description_question_option_text, cos_e_v1.11_explain_why_human, cos_e_v1.11_generate_explanation_given_text, cos_e_v1.11_i_think, cos_e_v1.11_question_description_option_id, cos_e_v1.11_question_description_option_text, cos_e_v1.11_question_option_description_id, cos_e_v1.11_question_option_description_text, cos_e_v1.11_rationale, cosmos_qa_context_answer_to_question, cosmos_qa_context_description_question_answer_id, cosmos_qa_context_description_question_answer_text, cosmos_qa_context_description_question_text, cosmos_qa_context_question_description_answer_id, cosmos_qa_context_question_description_answer_text, cosmos_qa_context_question_description_text, cosmos_qa_description_context_question_answer_id, cosmos_qa_description_context_question_answer_text, cosmos_qa_description_context_question_text, cosmos_qa_no_prompt_id, cosmos_qa_no_prompt_text, cosmos_qa_only_question_answer, dbpedia_14_given_a_choice_of_categories_, dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to, dbpedia_14_given_list_what_category_does_the_paragraph_belong_to, dbpedia_14_pick_one_category_for_the_following_text, dream_answer_to_dialogue, dream_baseline, dream_generate_ﬁrst_utterance, dream_generate_last_utterance, dream_read_the_following_conversation_and_answer_the_question, duorc_ParaphraseRC_build_story_around_qa, duorc_SelfRC_build_story_around_qa, gigaword_TLDR, gigaword_ﬁrst_sentence_title, gigaword_generate_summary_for_this, gigaword_in_a_nutshell, gigaword_make_a_title, gigaword_reverse_writing, gigaword_write_a_title_for_this_sentence, gigaword_write_an_article, gigaword_write_its_sentence, glue_mrpc_equivalent, glue_mrpc_generate_paraphrase, glue_mrpc_generate_sentence, glue_mrpc_paraphrase, glue_mrpc_replace, glue_mrpc_same_thing, glue_mrpc_want_to_know, glue_qqp_answer, glue_qqp_duplicate, glue_qqp_duplicate_or_not, glue_qqp_meaning, glue_qqp_quora, glue_qqp_same_thing, imdb_Movie_Expressed_Sentiment, imdb_Movie_Expressed_Sentiment_2, imdb_Negation_template_for_positive_and_negative, imdb_Reviewer_Enjoyment, imdb_Reviewer_Enjoyment_Yes_No, imdb_Reviewer_Expressed_Sentiment, imdb_Reviewer_Opinion_bad_good_choices, imdb_Reviewer_Sentiment_Feeling, imdb_Sentiment_with_choices_, imdb_Text_Expressed_Sentiment, imdb_Writer_Expressed_Sentiment, kilt_tasks_hotpotqa_combining_facts, kilt_tasks_hotpotqa_complex_question, kilt_tasks_hotpotqa_ﬁnal_exam, kilt_tasks_hotpotqa_formulate, kilt_tasks_hotpotqa_straighforward_qa, paws_labeled_ﬁnal_Concatenation, paws_labeled_ﬁnal_Concatenation_no_label, paws_labeled_ﬁnal_Meaning, paws_labeled_ﬁnal_Meaning_no_label, paws_labeled_ﬁnal_PAWS_ANLI_GPT3, paws_labeled_ﬁnal_PAWS_ANLI_GPT3_no_label, paws_labeled_ﬁnal_Rewrite, paws_labeled_ﬁnal_Rewrite_no_label, paws_labeled_ﬁnal_context_question, paws_labeled_ﬁnal_context_question_no_label, paws_labeled_ﬁnal_paraphrase_task, paws_labeled_ﬁnal_task_description_no_label, qasc_is_correct_1, qasc_is_correct_2, qasc_qa_with_combined_facts_1, qasc_qa_with_separated_facts_1, qasc_qa_with_separated_facts_2, qasc_qa_with_separated_facts_3, qasc_qa_with_separated_facts_4, qasc_qa_with_separated_facts_5, quarel_choose_between, quarel_do_not_use, quarel_heres_a_story, quarel_logic_test, quarel_testing_students, quartz_answer_question_based_on, quartz_answer_question_below, quartz_given_the_fact_answer_the_q, quartz_having_read_above_passage, quartz_paragraph_question_plain_concat, quartz_read_passage_below_choose, quartz_use_info_from_paragraph_question, quartz_use_info_from_question_paragraph, ropes_background_new_situation_answer, ropes_background_situation_middle, ropes_given_background_situation, ropes_new_situation_background_answer, ropes_plain_background_situation, ropes_plain_bottom_hint, ropes_plain_no_background, ropes_prompt_beginning, ropes_prompt_bottom_hint_beginning, ropes_prompt_bottom_no_hint, ropes_prompt_mix, ropes_read_background_situation, rotten_tomatoes_Movie_Expressed_Sentiment, rotten_tomatoes_Movie_Expressed_Sentiment_2, rotten_tomatoes_Reviewer_Enjoyment, rotten_tomatoes_Reviewer_Enjoyment_Yes_No, rotten_tomatoes_Reviewer_Expressed_Sentiment, rotten_tomatoes_Reviewer_Opinion_bad_good_choices, rotten_tomatoes_Reviewer_Sentiment_Feeling, rotten_tomatoes_Sentiment_with_choices_, rotten_tomatoes_Text_Expressed_Sentiment, rotten_tomatoes_Writer_Expressed_Sentiment, samsum_Generate_a_summary_for_this_dialogue, samsum_Given_the_above_dialogue_write_a_summary, samsum_Sum_up_the_following_dialogue, samsum_Summarize_, samsum_Summarize_this_dialogue_, samsum_To_sum_up_this_dialog, samsum_Write_a_dialogue_that_match_this_summary, sciq_Direct_Question, sciq_Direct_Question_Closed_Book_, sciq_Multiple_Choice, sciq_Multiple_Choice_Closed_Book_, sciq_Multiple_Choice_Question_First, social_i_qa_Check_if_a_random_answer_is_valid_or_not, social_i_qa_Generate_answer, social_i_qa_Generate_the_question_from_the_answer, social_i_qa_I_was_wondering, social_i_qa_Show_choices_and_generate_answer, social_i_qa_Show_choices_and_generate_index, trec_ﬁne_grained_ABBR, trec_ﬁne_grained_ABBR_context_ﬁrst, trec_ﬁne_grained_DESC, trec_ﬁne_grained_DESC_context_ﬁrst, trec_ﬁne_grained_ENTY , trec_ﬁne_grained_HUM, trec_ﬁne_grained_HUM_context_ﬁrst, trec_ﬁne_grained_LOC, trec_ﬁne_grained_LOC_context_ﬁrst, trec_ﬁne_grained_NUM, trec_ﬁne_grained_NUM_context_ﬁrst, trec_ﬁne_grained_open, trec_ﬁne_grained_open_context_ﬁrst, trec_pick_the_best_descriptor, trec_trec1, trec_trec2, trec_what_category_best_describe, trec_which_category_best_describes, wiki_bio_comprehension, wiki_bio_guess_person, wiki_bio_key_content, wiki_bio_what_content, wiki_bio_who, wiki_qa_Decide_good_answer, wiki_qa_Direct_Answer_to_Question, wiki_qa_Generate_Question_from_Topic, wiki_qa_Is_This_True_, wiki_qa_Jeopardy_style, wiki_qa_Topic_Prediction_Answer_Only, wiki_qa_Topic_Prediction_Question_Only, wiki_qa_Topic_Prediction_Question_and_Answer_Pair, wiki_qa_automatic_system, wiki_qa_exercise, wiki_qa_found_on_google, wiqa_does_the_supposed_perturbation_have_an_effect, wiqa_effect_with_label_answer, wiqa_effect_with_string_answer, wiqa_what_is_the_ﬁnal_step_of_the_following_process, wiqa_what_is_the_missing_ﬁrst_step, wiqa_what_might_be_the_ﬁrst_step_of_the_process, wiqa_what_might_be_the_last_step_of_the_process, wiqa_which_of_the_following_is_the_supposed_perturbation, yelp_review_full_based_on_that, yelp_review_full_format_rating, yelp_review_full_format_score, yelp_review_full_format_star, yelp_review_full_on_a_scale, yelp_review_full_so_i_would, yelp_review_full_this_place Figure 6: List of P3 dataset-prompts used for training. We chose a subset of T0-train with average input lengths shorter than 320 tokens. 17For the hypermodel initialization/PEFT experiments, we do single-task parameter-efﬁcient ﬁne-tuning on each of the following dataset-prompts: 1. anli_GPT_3_style_r1 2. hellaswag_complete_ﬁrst_then 3. super_glue_cb_GPT_3_style 4. super_glue_copa_C1_or_C2_premise_so_because_ 5. super_glue_rte_GPT_3_style 6. super_glue_wic_GPT_3_prompt 7. super_glue_wsc.ﬁxed_GPT_3_Style 8. winogrande_winogrande_debiased_Replace B.2 S-NI / T-KI To standardize the preprocessing across our experiments, we do not use the input formatting provided in the original work (Wang et al., 2022). Instead, we use the format described in Appendix A.1 for all experiments. Given that the same format is used in multi-task ﬁne-tuning and evaluation, this should not unfairly advantage any model. However, because the format deviates from that of the original work, we do not directly evaluate the T-KI models. Additionally, the Super-NaturalInstructions dataset (previously known as NaturalInstructions-v2) has undergone some changes over time. In our experiments, we use the v2.5 version of the dataset. B.3 MetaICL C Model Details D Elaboration on Preﬁx Tuning Comparisons While preﬁx tuning is generally presented as learning a set of prepended key and value representations for each Transformer layer, in practice, the learned preﬁxes are not optimized directly. In the work that introduced preﬁx tuning (Li and Liang, 2021), Section 4.3 explains that directly optimizing the learned preﬁxes leads to unstable training and poorer performance, and instead recommend optimizing a set of learned embeddings and a parameterized MLP to generate the learned preﬁxes. (At inference time, the preﬁxes can be generated from the learned components–this only impacts the training process.) We conﬁrmed in our experiments that directly optimizing preﬁxes leads to poor perfomance, and other works involving preﬁx tuning have similarly used this preﬁx reparamterization Hence, we have two ﬂavors of preﬁx tuning to consider: directly optimizing over preﬁxes (“Preﬁx- Flat\"), and optimizing with reparamterization (“Preﬁx-MLP\"). The T5-MTF (Preﬁx) model uses Preﬁx-MLP, which is the appropriate approach to tuning preﬁxes in that setting. However, because HyperT5-Preﬁx only generates the ﬁnal preﬁxes, only Preﬁx-Flat tuning is possible. Hence, when we perform the preﬁx tuning with different initializations in Section 6, we cannot fairly compare the two methods directly–one which uses a reparameterization during training, and the other which uses direct optimization which we know performs worse in practice. Instead, we compare preﬁx tuning in the two different settings, Preﬁx-Flat and Preﬁx-MLP, completely separately. We describe each individual initialization scheme: Preﬁx-Flat 1. Preﬁx-Flat (Rand): Randomly initialize soft preﬁxes 2. Preﬁx-Flat (Shared): Run a forward pass through the preﬁx reparameterization to obtain the ﬂat preﬁxes, and use them as the initialization 3. Preﬁx-Flat (Hyper): Generate preﬁxes with HyperT5-Preﬁx 18# B = batch_size # T = input_length # P = number of prompt tokens # H = hidden_dim # L = num layers in encoder/decoder # Shape: [B, T] fewshot_input_ids = ... # Shape: [B, T, H] hyper_enc_out = hypermodel.encoder(fewshot_input_ids) # Shape: [B, 2P, H] # Decoder implicitly uses a fixed set of input embeddings of size 2P hyper_dec_out = hypermodel.decoder(hyper_enc_out) # Shape: [B, P, LH] downstream_enc_k_prefix = hypermodel.enc_k_head(hyper_dec_out[:, :P, :]) downstream_enc_v_prefix = hypermodel.enc_v_head(hyper_dec_out[:, :P, :]) downstream_dec_k_prefix = hypermodel.dec_k_head(hyper_dec_out[:, P:, :]) downstream_dec_v_prefix = hypermodel.dec_v_head(hyper_dec_out[:, P:, :]) # Shape: [B, P, L H] downstream_enc_k_prefix = downstream_enc_k_prefix.reshape(B, P, L, H) downstream_enc_v_prefix = downstream_enc_v_prefix.reshape(B, P, L, H) downstream_dec_k_prefix = downstream_dec_k_prefix.reshape(B, P, L, H) downstream_dec_v_prefix = downstream_dec_v_prefix.reshape(B, P, L, H) # These correspond to the per-layer learned prefixes for K and V # where each of the heads is defined (e.g.): hypermode.enc_k_head = nn.Sequential([ nn.LayerNorm(), nn.Linear(H), nn.TanH(), nn.Linear(L*H), ]) Figure 7: Pseudo-code for HyperT5-Preﬁx Preﬁx-MLP 1. Preﬁx-MLP (Rand): Randomly initialize the preﬁx reparameterization embeddings and MLPs (i.e. conventional preﬁx tuning) 2. Preﬁx-MLP (Shared): Directly reuse the preﬁx reparameterization from T5-MTF (Preﬁx) 3. Preﬁx-MLP (Hyper): We train an entirely new HyperT5-Preﬁx-MLP model, where the parameter generation heads directly correspond to the preﬁx tuning reparameterization MLPs. The encoder-decoder in the hypermodel will output the “embeddings\", and we directly reuse the parameter generation heads during tuning. The results for Preﬁx-MLP are presented in the body of the paper in Section 6. We believe that this approach provides the fairest comparison of initializations. Importantly, both Preﬁx-MLP (Shared) and Preﬂix-MLP (Hyper) have been trained on the same number of labeled examples (not including the few-shot examples, which are inputs), but where the Preﬁx-MLP uses a single set of learned embeddings, HyperT5-Preﬁx-MLP generates the embeddings based on few-shot examples. We present the full set of preﬁx tuning results in Table 7, the performance of Preﬁx-Flat Figure 9. 19# B = batch_size # T = input_length # R = LoRA rank # H = hidden_dim # L = num layers in encoder/decoder # Shape: [B, T] fewshot_input_ids = ... # Shape: [B, T, H] hyper_enc_out = hypermodel.encoder(fewshot_input_ids) # Shape: [B, 3L, H] # Decoder implicitly uses a fixed set of input embeddings of size 3L hyper_dec_out = hypermodel.decoder(hyper_enc_out) # Shape: [B, L, H] enc_repr = hyper_dec_out[:, :L, :] dec_repr = hyper_dec_out[:, L:2*L, :] cross_repr = hyper_dec_out[:, 2*L:, :] # Repeat for dec_repr, cross_repr for decoder self- and cross-attention # Shape: [B, L, 2RH] enc_q_repr = hypermodel.enc_q_head(enc_repr) enc_v_repr = hypermodel.enc_v_head(enc_repr) # Shape: [B, L, 2RH] enc_q_repr = enc_q_repr.reshape(B, L, 2, R, H) enc_v_repr = enc_v_repr.reshape(B, L, 2, R, H) # raw_enc_q_gate and raw_enc_v_gate are learned parameters of size [L] # Shape: [1, L, 1, 1, 1] enc_q_gate = torch.tanh(raw_enc_q_gate)[None, :, None, None, None] enc_v_gate = torch.tanh(raw_enc_v_gate)[None, :, None, None, None] # Shape: List of [B, R, H] enc_lora_q_up_list = [enc_q_repr[:, l, 0, :, :] for l in range(L)] enc_lora_q_down_list = [enc_q_repr[:, l, 1, :, :] for l in range(L)] enc_lora_v_up_list = [enc_v_repr[:, l, 0, :, :] for l in range(L)] enc_lora_v_down_list = [enc_v_repr[:, l, 1, :, :] for l in range(L)] # These correspond to up- and down-map deltas in LoRA in Q and V # attention linear maps # where each of the heads is defined (e.g.): hypermode.enc_q_head = nn.Sequential([ nn.LayerNorm(), nn.Linear(H), nn.TanH(), nn.Linear(2*R*H), ]) Figure 8: Pseudo-code for HyperT5-LoRA 20ANLI HSwag CB COPA RTE WiC WSC WGD A VG Preﬁx-Flat (Rand Init) 43.6 36.3 82.7 74.0 72.9 64.4 64.2 53.0 61.4 Preﬁx-Flat (Shared Init) 54.3 40.4 98.8 82.7 83.9 71.0 67.4 57.1 69.4 Preﬁx-Flat (Hyper Init) 56.6 43.5 91.7 84.3 85.3 69.3 73.0 67.6 71.4 Preﬁx-MLP (Rand Init) 54.6 50.5 98.8 79.0 78.8 71.6 63.5 52.2 68.6 Preﬁx-MLP (Shared Init) 60.8 51.6 99.4 85.7 84.8 72.4 72.6 65.1 74.0 Preﬁx-MLP (Hyper Init) 61.4 51.5 97.6 84.3 87.1 71.2 76.5 71.6 75.2 LoRA (Rand Init) 59.5 51.3 93.5 78.0 82.6 73.5 77.9 65.1 72.7 LoRA (Shared Init) 57.9 51.6 99.4 83.0 83.8 73.1 73.3 67.9 73.7 LoRA (Hyper Init) 57.7 48.4 99.4 87.3 84.1 73.0 83.9 66.2 75.0 Table 7: Preﬁx tuning (Flat and MLP) and LoRA ﬁne-tuning on T5-Large with different initializations on P3 held-out tasks. 210 5000 10000 15000 20000 Training Steps 50 60 70Accuracy Rand Init Shared Init Hyper Init Figure 9: Average performance on P3 held-out tasks with preﬁx tuning (ﬂat). 22",
      "meta_data": {
        "arxiv_id": "2211.12485v1",
        "authors": [
          "Jason Phang",
          "Yi Mao",
          "Pengcheng He",
          "Weizhu Chen"
        ],
        "published_date": "2022-11-22T18:52:25Z",
        "pdf_url": "https://arxiv.org/pdf/2211.12485v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces HyperTuning, a novel approach to large language model (LLM) adaptation that avoids back-propagation during adaptation by using a hypermodel to generate task-specific parameters. It proposes HyperT5, a T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen T5 model from few-shot examples. The key contributions include a two-stage training procedure (hyperpretraining and multi-task fine-tuning) for hypermodels, demonstrating its effectiveness in generating parameters for unseen tasks across diverse datasets (P3, MetaICL, Super-NaturalInstructions), and showing that hypermodel-generated parameters can serve as improved initializations for further parameter-efficient fine-tuning, leading to faster convergence and better performance.",
        "methodology": "The core methodology involves training a hypermodel (HyperT5) to generate parameter-efficient fine-tuning (PEFT) parameters (soft prefixes or LoRA parameters) for a frozen downstream LLM in a single forward pass, based on few-shot examples or task instructions. HyperT5 is built on a T5 backbone, with its encoder processing few-shot inputs and its decoder generating representations that are fed to MLPs to produce the PEFT parameters. Training proceeds in two stages: first, hyperpretraining using a Context-Augmented Conditional Language Modeling (CACLM) objective to teach the hypermodel to generate parameters from contextual information; second, multi-task fine-tuning (MTF) of the hypermodel on a large number of diverse language tasks, where the hypermodel learns to generalize by taking few-shot pairs as input to generate task-specific PEFT parameters.",
        "experimental_setup": "The research used LM-adapted T5 models (T5-Large and T5-XL) as the frozen downstream model. Experiments were conducted on three multi-task datasets: P3 (Public Pool of Prompts) for zero-shot instruction-following, MetaICL for few-shot learning, and Super-NaturalInstructions (S-NI) for generalization via declarative instructions. Baselines included T5-MTF (multi-task fine-tuning, similar to T0), T5-MTF-Few-shot (MTF with few-shot inputs concatenated), and parameter-efficient fine-tuning methods like prefix tuning and LoRA. Hyperpretraining was performed for 100K steps on the C4 dataset. Multi-task fine-tuning for HyperT5 and baselines ran for 10,000 steps with a batch size of 256, utilizing up to 16 few-shot examples. The Adam optimizer with a learning rate of 5e-5 and linear decay schedule was used, with ZeRO for memory optimization. Evaluation was based on accuracy for P3, ROUGE or Macro-F1 for MetaICL, and ROUGE-L for S-NI on held-out tasks. The effectiveness of hypermodel-generated parameters as initializations for further PEFT was also evaluated on P3 held-out tasks.",
        "limitations": "The current hypermodel formulation is constrained to taking a small number of examples as input, which limits its performance compared to full parameter-efficient fine-tuning or full fine-tuning approaches. HyperT5 generally underperforms T5 models explicitly trained for few-shot in-context learning with full attention across examples (like T5-MTF-Fewshot), as HyperT5 encodes examples into PEFT parameters independently of the target input, while full self-attention allows for richer interaction. The experiments were conducted with minimal hyperparameter tuning, making the results primarily a proof-of-concept. Optimal hyperpretraining duration varies for different PEFT methods, as observed with HyperT5-LoRA's performance declining after 50k steps. Additionally, HyperT5-Preﬁx consistently outperformed HyperT5-LoRA, suggesting potential differences in learning complexity for generating different PEFT methods.",
        "future_research_directions": "Future research directions include exploring whether hypermodels can be trained to predict gradients or generate parameter updates, thereby enabling them to work with large training sets beyond just few-shot examples. The authors also envision eventually replacing gradient-descent-based fine-tuning pipelines with hypertuning for various applications. Further investigation into hyperpretraining and multi-task fine-tuning hyperparameters, as well as alternative hypermodel architectures, is suggested to improve results and address current limitations. Another potential area is training a hypermodel against an already multi-task fine-tuned model to achieve better performance than zero-shot inference alone."
      }
    },
    {
      "title": "Prefix Conditioning Unifies Language and Label Supervision",
      "abstract": "Image-classification datasets have been used to pretrain image recognition\nmodels. Recently, web-scale image-caption datasets have emerged as a source of\npowerful pretraining alternative. Image-caption datasets are more\n``open-domain'', containing a wider variety of scene types and vocabulary words\nthan traditional classification datasets, and models trained on these datasets\nhave demonstrated strong performance on few- and zero-shot recognition tasks.\nWhen naively unifying image-classification and -caption dataset, we show that\nsuch dataset biases negatively affect pre-training by reducing the\ngeneralizability of learned representations and thus jeopardizing zero-shot\nperformance since the unification can tailor the model for the classification\ndataset, making it vulnerable to the distribution shift from the dataset. In\nthis work, we address the problem by disentangling the dataset bias using\nprefix tokens that inform a language encoder of the type of the input dataset\n(e.g., image-classification or caption) at training time. This approach allows\nthe language encoder to share the knowledge from two datasets as well as switch\nthe mode of feature extraction, i.e., image-classification dataset or\nimage-caption dataset tailored mode, where we use image-caption mode in the\nzero-shot evaluation. Our method is generic and can be easily integrated into\nexisting VL pre-training objectives such as CLIP or UniCL. In experiments, we\nshow that this simple technique improves the performance in zero-shot image\nrecognition accuracy and robustness to the image-level distribution shift.",
      "full_text": "Preﬁx Conditioning Uniﬁes Language and Label Supervision Kuniaki Saito1,2*, Kihyuk Sohn 3 , Xiang Zhang 2 , Chun-Liang Li 2 , Chen-Yu Lee2 , Kate Saenko1,4 , Tomas Pﬁster2 {keisaito, saenko}@bu.edu {kihyuks,fancyzhx,chunliang,chenyulee,tpfister}@google.com 1Boston University, 2Google Cloud AI Research, 3Google Research, 4MIT-IBM Watson AI Lab Abstract Pretraining visual models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pretraining on image classiﬁcation data. Image-caption datasets are more “open-domain”, contain- ing broader scene types and vocabulary words, and re- sult in models that have strong performance in few- and zero-shot recognition tasks. However large-scale classi- ﬁcation datasets can provide ﬁne-grained categories with a balanced label distribution. In this work, we study a pretraining strategy that uses both classiﬁcation and cap- tion datasets to unite their complementary beneﬁts. First, we show that naively unifying the datasets results in sub- optimal performance in downstream zero-shot recognition tasks, as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem with novel Preﬁx Condi- tioning, a simple yet effective method that helps disentangle dataset biases from visual concepts. This is done by intro- ducing preﬁx tokens that inform the language encoder of the input data type (e.g., classiﬁcation vs caption) at training time. Our approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Preﬁx conditioning is generic and can be eas- ily integrated into existing VL pretraining objectives, such as CLIP or UniCL. In experiments, we show that it improves zero-shot image recognition and robustness to image-level distribution shift. 1. Introduction Supervised classiﬁcation datasets (e.g., ImageNet [7]) have traditionally been used to pretrain image representa- tions for use in downstream tasks. However, web-scale image-caption datasets have recently emerged as a pow- erful pretraining alternative [13, 20, 31]. Such datasets *Work done during internship at Google Cloud AI Research. Training DatasetIN21KCC12MIN21K + CC12MIN21K + CC12M CLIPUniCL CLIP+Ours UniCL+Ours Accuracy (%) on 11 standard zero-shot benchmarkAccuracy (%) on  11 standard zero-shot benchmark Training Dataset Figure 1. We propose Preﬁx Conditioning to unify image-caption (e.g., CC12M [5]) and image classiﬁcation datasets (e.g., Ima- geNet21K (IN21K) [7]) for training better zero-shot models. Pre- ﬁx conditioning improves zero-shot recognition performance by more than 6% on average when training on ImageNet21K and CC12M. are more “open-domain”, containing a wider variety of scene types and vocabularies than traditional classiﬁcation datasets, which are biased towards speciﬁc categories in their ﬁxed label sets. Consequently, models trained on web- scale image-caption datasets have shown stronger gener- alization in novel tasks [4, 31] and demonstrated remark- able performance on few and zero-shot image classiﬁcation tasks [31]. Nevertheless, classiﬁcation datasets are still use- ful for pre-training as they have a more balanced coverage of categories, including rare and ﬁne-grained categories, and a better focus on the labeled objects in each image. Recent works [43,45] therefore propose to combine cap- tion and classiﬁcation datasets for pre-training. [43] convert classiﬁcation labels to “label-prompts” by inserting the la- bel into a template sentence, e.g., “a photo of a <label>.”1 1We use the term prompt to indicate a template sentence ﬁlled with a class name. arXiv:2206.01125v2  [cs.CV]  15 May 2023Although training on the caption and label-prompt data achieves promising results, it does not fully resolve distribu- tion differences between the open-domain caption data and the classiﬁcation data. In particular, it produces a language embedding entangled with the classiﬁcation dataset “bias”. We note that classiﬁcation datasets tend to be biased in at least two ways: 1) the images mostly contain single ob- jects from restricted domains, and 2) the vocabulary is lim- ited and lacks the linguistic ﬂexibility required for zero-shot learning. Therefore, the class embedding of “a photo of a dog” optimized for ImageNet may really mean a photo of a dog from ImageNet instead, which is biased to ImageNet and does not generalize well to other datasets. We empiri- cally show that such dataset biases negatively affect uniﬁed pretraining by reducing the generalization of learned repre- sentations and thus jeopardizing zero-shot performance. To recognize diverse concepts in the open domain, the language model needs to disentangle the dataset bias from the visual concepts and extract language embeddings gen- eralizable to the open domain, e.g., the language embed- ding representing a photo of a dog from an open-domain dataset, such as image-caption dataset , instead of a photo of a dog from ImageNet. Given this intuition, we propose to learn dataset-speciﬁc language embeddings, while sharing knowledge from both datasets during training. We achieve this by a simple yet effective approach we call Preﬁx Con- ditioning. The idea is to learn a dataset-speciﬁc text token (preﬁx) for each dataset so that the bias of the dataset can be absorbed into this token, and in return the remaining text tokens can focus on learning visual concepts. Speciﬁcally, we prepend a different token for each dataset (e.g., image classiﬁcation or caption dataset) to the text input token se- quence during pre-training. The idea is in part inspired by preﬁx or prompt tun- ing [18, 21, 46], which showed that learnable tokens prepended to the input token sequences of the pre-trained language models are able to learn task-speciﬁc knowledge and thus can be used to solve downstream tasks by com- bining the knowledge of pre-trained large language models and task-speciﬁc preﬁx tokens. Our approach differs from prompt tuning in two ways: 1) the proposed preﬁx condi- tioning is designed to unify image-caption and classiﬁcation datasets by disentangling the dataset bias, which is a unique distinction to prompt-tuning works, 2) our approach is ap- plied for VL pre-training while the standard prompt tuning is used in ﬁne-tuning. In experiments, the proposed simple technique achieves superior performance on zero-shot evaluation if we use the preﬁx of the caption dataset to get the language embedding at test time as shown in Fig. 1. Meanwhile, inserting the pre- ﬁx of the classiﬁcation dataset leads to better performance on classiﬁcation data. We also observe a drastic perfor- mance improvement when combining our preﬁx condition- ing with the UniCL [43] objective because of their comple- mentarity. Our contributions are summarized as follows: • We propose novel Preﬁx Conditioning at pre-training time to unify image-label and image-caption supervision. It is the ﬁrst mechanism to use preﬁxes to condition the source of the dataset during vision language contrastive pre-training, rather than post pre-training. • This simple approach improves zero-shot recognition per- formance by more than 6% on average in experiments on ImageNet21K [7] and CC12M [5]. • Our comprehensive ablation study shows that preﬁx con- ditioning enables the model to switch its approach to ex- tracting language features, e.g., attend to different words. 2. Related Work Vision-Language Contrastive Learning. Zero-shot recognition is conventionally solved by learning the rela- tionship between visual representations and word embed- dings of the class names [1, 9, 12, 26, 38, 40, 41]. Vision- language contrastive learning models, such as CLIP [31], pre-train a model with a large-scale image-caption data (400M) and achieve a remarkable improvement in zero-shot recognition. ALIGN [13] demonstrated the effect of scaling up the size of image-caption data. Various techniques have been proposed to improve the data efﬁciency given a rela- tively small amount of image-caption data (order of 10M). ALBEF [20] employs model distillation and masked lan- guage modeling. DeCLIP [22], SLIP [29] and TCL [42] harness self-supervised contrastive learning. FILIP [44] uses token-to-token contrastive learning rather than the global contrastive learning used in CLIP. BLIP [19] gener- ates pseudo captions to diversify the language modality for each image. Unlike these works that handle only caption- style supervision, we focus on the use of label supervision in vision-language pre-training. Our approach brings or- thogonal improvement to the aforementioned works as they seek to improve training on image-caption data. UniCL [43] and K-Lite [34] unite the image-caption and image-label supervision by converting labels into text with pre-deﬁned template sentences. UniCL leverages a supervised contrastive loss [15] for image-label pairs. K- Lite [34] utilizes external knowledge from WordNet [28] and Wikitionary [27]. The input noun is augmented with the class hierarchy and deﬁnition to enrich the supervision. Our method is complementary to these approaches since both UniCL and K-Lite do not consider the domain shift between datasets. In experiments, we observe a signiﬁcant performance boost when UniCL is combined with the preﬁx conditioning. Learning with Prompts. Prompt tuning is a popu- lar technique to adapt a large language model to a spe- ciﬁc task with few training data and low computationalFigure 2. Left: Preﬁx conditioning at training time. Dataset-speciﬁc token is added to the input tokens with a contrastive learning objective applied. Right: Preﬁx conditioning at test time. Given a class name, we construct a class prompt with pre-deﬁned templates and add a token used to condition real caption during training considering that image-caption dataset covers much wider range of image domains and vocabulary words than image classiﬁcation dataset. cost [10,18,21,23,30]. To avoid tuning all parameters of the model and using hand-crafted prompts, preﬁx embeddings are added to the training input and are the only parame- ters optimized during ﬁne-tuning. The preﬁx embedding can be viewed as the knowledge of the downstream task. In this paper, since the target task is the zero-shot classi- ﬁcation, the bias of the language embedding needs to be from the dataset covering a wide range of domains rather than a speciﬁc domain. Therefore, we choose to use the preﬁx embedding learned for image-caption dataset during test time. This technique is also effective in adapting a pre- trained vision-language model [46, 47] to few-shot classi- ﬁcation by tuning the prompts of the language encoder to adapt to a downstream task. Additionally, prompt-tuning is effective in adapting a pre-trained vision model to a target task [14]. While these works aim to tailor a large pre-trained model to a speciﬁc downstream task with a small amount of data or low computational cost, our goal here is to condi- tion a model with the preﬁx during the pre-training stage by distinguishing between the image label and image caption data. This allows a model to effectively share the knowl- edge obtained from two different types of data sources. Dataset bias in image recognition. A large-scale image recognition dataset such as ImageNet [7] is known to be bi- ased towards a speciﬁc image domain. Therefore, a model trained on such a dataset shows vulnerability to the distri- bution shift, e.g., shift in object pose [3] and style of the images [37]. Nevertheless, [16,39] show that adapting only a linear layer on the pre-trained models can improve per- formance on the downstream tasks with distribution shifts. This indicates the importance of having a good classiﬁer on top of image encoders, such as linear classiﬁers generated by language encoders with preconditioning in our work. [8] propose a method for domain generalization. They condi- tion image recognition models with the domain embedding, which discriminates the input image domains, and demon- strate the importance of the domain-speciﬁc image classi- ﬁer. Our preﬁx conditioning can be seen as an attempt to de-bias the linear classiﬁer to obtain a domain-speciﬁc clas- siﬁer and adapt it from the classiﬁcation to the captioning domain. Also, [2, 17] approach the dataset bias in image classiﬁcation by de-biasing image representations. By con- trast, we tackle the problem in the framework of vision- language learning, disentangle the dataset bias in the lan- guage embedding and utilize the classiﬁer obtained by the caption domain. We note that while captioning datasets can also have data biases, they tend to be more open-domain than existing classiﬁcation datasets. 3. Method In this section, we introduce the Preﬁx Conditioning technique for pretraining a deep learning model on both image-caption and image-label (classiﬁcation) data. In Sec. 3.1, we discuss our problem setting and the back- ground of contrastive learning with image-caption data. In Sec. 3.2, we explain the details of our training approach, and in Sec. 3.3 our inference procedure. 3.1. Preliminaries Setup. Suppose we have access to two datasets: (i) an image label datasetSL = {(xn,tP n ,yn)}NL n=1, where x∈X is the image and tP ∈ Pis a prompt-style language de- scription based on its class label y∈Y, and (ii) a dataset of image-caption pairs SC = {(xn,tC n )}NC n=1, where tC ∈T is a caption. We assume thattis the tokenized language de- scription. For each imagex, an image encoder modelfθpa- rameterized by θextracts a visual representation ˜v∈Rd×1:˜v = fθ(x). For each caption or prompt t∈T , a text en- coder fφ parameterized by φextracts a language represen- tation ˜u∈Rd×1 : ˜u= fφ(t). Contrastive Loss. CLIP [31] is designed to ﬁnd repre- sentations that match an image to its paired caption while separating unpaired ones. For i-th image xi and j-th lan- guage description tj in a batch B, their features are nor- malized using vi = ˜vi ∥˜vi∥ and uj = ˜uj ∥˜uj∥. Finally, CLIP op- timizes the symmetric multi-class N-pair loss [35]: min {θ,φ} Lcon =Lt2i + Li2t, (1) which includes two contrastive terms (a temperature hyper- parameter τ controls the strength of penalties on hard nega- tive samples): Lt2i = −1 |B| ∑ i∈B log exp(τuT i vi)∑ j∈Bexp(τuT i vj), (2) Li2t = −1 |B| ∑ i∈B log exp(τvT i ui)∑ j∈Bexp(τvT i uj). (3) UniCL [43] composes each mini-batch with samples from both SL and SC. Then, for pairs from SL, they regard all samples from the same class as positive pairs while a sam- ple from SC has a unique pair. Except for the number of positive pairs, no special treatment is given to differentiate between the image-caption and image-label data. 3.2. Preﬁx Conditioned Contrastive Learning Fig. 2 describes the overview of our approach. We aim to enable the language encoder to learn embedding strategies conditioned on the type of input dataset. The conditioning can then be used to manipulate the bias at inference time. Preﬁx-tuning [10, 18, 21, 23, 30] shares the intuition that the preﬁx tokens are responsible for switching the context of a language model from the pre-trained task to the down- stream task. These approaches leverage the preﬁx to tailor a model to a single task during ﬁne-tuning and construct dif- ferent preﬁxes for different natural language tasks [18]. In our problem setting, there is no task distinction between the image-caption and image-prompt matching since both are formulated as contrastive learning. However, we focus on the fact that the two datasets have different biases in the im- age distributions and vocabulary words. The label-prompt sentences are embedded closer to the image classiﬁcation data, even though we may want to use them to match a new label to an image from the open-domain image distribution during zero-shot classiﬁcation. To solve this problem, we propose to inform the model of the type of dataset at the input level to switch the fea- ture extraction. Speciﬁcally, to make the model aware of the dataset type, preﬁx-conditioning prepends a preﬁx to- ken to an input sentence to obtain ¯tP = [ PREFIXP ; tP ], ¯tC = [ PREFIXC; tC]. The brackets indicate the con- catenation of two lists of discrete tokens; PREFIX P and PREFIXC denote a prompt-style and caption-style token re- spectively. In this way, we prepend the token to learn the dataset-speciﬁc bias, which enables us to disentangle the bias in language representations and utilize the embedding learned on the image-caption dataset at test time even with- out an input caption. In prompt-tuning, the number of preﬁx tokens can af- fect the performance of the model [18, 21, 46]. However, we do not see the performance difference by the number of preﬁx tokens. This is probably because adding one token is enough to distinguish the domain of input sentences. To avoid signiﬁcantly increasing the training cost, we set the number of preﬁxes to one in all experiments. Then, the lan- guage representations for each data source are extracted as ˜uP ,˜uC = fφ(¯tP ),fφ(¯tC). This input design is indepen- dent from the training objectives, and therefore we can eas- ily apply the technique to optimize Eq. 1 or UniCL’s loss. Data Sampling. [6] argue that the data sampling matters when learning from multiple data sources in a contrastive learning framework, as the model may learn to distinguish the samples by exploiting the dataset bias. As such, we need to take data sampling into consideration in our problem set- ting as we learn from two different data sources. One op- tion is a debiased sampling [6], which constructs each mini batch to contain samples from a single data source. Alterna- tively, as done in UniCL [43], we can compose each mini- batch with samples from both data sources (image-caption and image-label) with equal probability. In experiments, we choose the debiased sampling, but ﬁnd that the choice of sampling does not signiﬁcantly affect the performance. 3.3. Inference with Preﬁx Conditioning During inference (the right side of Fig. 2), an input im- age is classiﬁed as one of K classes by embedding the cor- responding label-prompts and choosing the one most sim- ilar to the image embedding. Following [31], we obtain class prompts by ﬁlling the default prompt templates with class names, and add a preﬁx. Considering the wider cov- erage of domains in the image-caption dataset, the caption- style preﬁx conditioning may work better to classify novel downstream data. In our experiments, we empirically ﬁnd that the caption-style preﬁx indeed outperforms the prompt- style preﬁx with a large margin in zero-shot recognition while prompt preﬁx performs better on the image classiﬁca- tion dataset used to train the model. We provide a detailed analysis of different conditioning in Section 4.3. 4. Experiments The goal of experiments is twofold: comparing our ap- proach with baselines in zero-shot recognition, and analyz- ing the behavior of preﬁx conditioning. We describe the experimental setup in Sec. 4.1, show the main results inTraining Data Objective Preﬁx Conditioning Metric Classiﬁcation Caption Size IN-1K Zero-shot 11 datasets – CC-3M 3M CLIP 18.1 28.7 – CC-12M 12M CLIP 33.4 41.2 ImageNet-1K – 1M CLIP 72.1 20.2 ImageNet-21K – 12M CLIP 47.1 39.6 ImageNet-1K CC-12M 13M CLIP 68.7 43.3 ImageNet-1K CC-12M 13M CLIP ✓ 71.5 45.5 ImageNet-1K CC-12M 13M UniCL 68.8 43.1 ImageNet-1K CC-12M 13M UniCL ✓ 71.7 44.5 ImageNet-21K CC-12M 25M CLIP 56.8 49.5 ImageNet-21K CC-12M 25M CLIP ✓ 67.3 57.8 ImageNet-21K CC-12M 25M UniCL 58.2 51.7 ImageNet-21K CC-12M 25M UniCL ✓ 66.5 58.4 ImageNet-21K w/o IN-1K CC-12M 24M CLIP 29.1 46.9 ImageNet-21K w/o IN-1K CC-12M 24M CLIP ✓ 47.8 56.4 Table 1. Performance comparison among different training datasets and training objectives. Note that we use caption preﬁx to obtain these results. The proposed preﬁx conditioning shows improved zero-shot recognition accuracy across models trained with different combinations of image-classiﬁcation and image-caption datasets and training objectives. Sec. 4.2, and analyze the properties of preﬁx-conditioning in Sec. 4.3. 4.1. Setup Training Datasets. We conduct experiments on the set- ting where we have a large source of image-caption and image-label datasets. Following UniCL [43], we utilize CC3M [33] and CC12M [5] as image-caption data. For the image classiﬁcation dataset, we utilize ImageNet21K and ImageNet1K [7]. While ImageNet1k contains 1,000 classes, ImageNet21K has more than 20,000 categories that include ﬁne-grained and general objects. To observe the behavior in diverse image classiﬁcation data, we also run experiments on ImageNet21K while excluding the classes of ImageNet1K. Details are explained in each section. Training. We use the same prompt strategy and 80 prompt templates as used in CLIP [31]. During training, we randomly sample one prompt template and ﬁll it with the class names, followed by a tokenization step before feeding into the text encoder. We average language embeddings ex- tracted from all 80 templates in validation. We use the same language encoder as CLIP [31] and Swin-Tiny transformer [24] as the vision encoder following UniCL [43]. All mod- els are optimized with AdamW [25] where the learning rate is set to 0.001, and weight decay to 0.1. All models are trained with a batch size of 1024. Considering the amount of training data, we train the models for 15 and 50 epochs in the experiments on ImageNet21K and ImageNet1K re- spectively.2 For all training, we used a cosine learning rate 2When training a model on two different datasets, e.g., IN21K and schedule with a warm-up of 10,000 iterations. Baselines. We train CLIP [31] and UniCL [43] as our baselines. For comparison, we present results on CLIP trained only on image-caption or image classiﬁcation data, as well as CLIP and UniCL trained on both image-caption and IN21K data. Unless otherwise stated, CLIP and UniCL are trained with equal sampling (ES) strategy as in [43], while our preﬁx conditioning model is trained with debiased sampling (DS) [6]. We provide an analysis of the sampling in Sec. 4.2 and ﬁnd that DS itself does not have a noticeable advantage over ES. Evaluation. We evaluate the learned representations on supervised and zero-shot image classiﬁcation on Ima- geNet1K3 and on 11 datasets chosen from the ones used in CLIP [31] including object classiﬁcation (e.g., CIFAR10, CIFAR100), ﬁne-grained classiﬁcation (e.g., Oxford-IIIT Pets, Oxford Flowers 102, and Food-101), and aerial images (e.g., EuroSAT and Resisc45). Although our main focus is at the zero-shot generalization, we also provide an analysis of a linear-probe evaluation of the image encoder. CC12M, we count the epochs based on how many samples are used from the image classiﬁcation dataset. For instance, in UniCL, each mini-batch consists of approximately the same number of samples from IN21K and CC12M. Then, to train a model for 15 epochs, we train for N/1024 ×2 ×15 iterations, where N indicates the number of samples in IN21K. 3While we follow the same zero-shot evaluation protocol when evalu- ating on ImageNet1K, we note that it is zero-shot only where we explicitly exclude ImageNet1K from the training, last two rows of Table 1Train Preﬁx Sampling IN-1K Cal CF100 CF10 ESTAT Food Flower Pets Patch R45 VOC DTD A VG ES 56.8 70.2 55.0 79.4 21.1 46.0 60.3 57.2 51.2 24.8 57.7 21.4 49.5 ✓ ES 65.4 81.2 62.6 88.9 30.4 51.7 61.8 71.9 50.0 28.2 78.1 27.7 57.5 DS 58.7 65.9 55.0 85.7 22.8 40.8 55.7 60.2 50.0 20.6 45.2 23.8 47.8 ✓ DS 67.3 79.7 63.8 87.9 31.5 53.4 58.8 69.6 50.6 31.5 80.5 28.4 57.8 Table 2. Ablation study for sampling in IN21K + CC12M. Equal sampling (ES) composes a mini-batch with roughly equal number of samples from two datasets. Debiased sampling (DS) samples a mini-batch of either IN21K or CC12M with equal probability. Train Data Preﬁx ConditioningIN-1K Cifar10 Cifar100 Caltech Food Pet Patch VOC DTD ImageNet-21K 71.5 94.3 79.1 83.5 79.1 86.3 82.3 88.9 61.3 ImageNet-21K + CC12M 69.2 93.0 76.4 82.4 78.4 82.2 81.4 88.7 61.4 ImageNet-21K + CC12M ✓ 69.4 93.5 77.3 83.2 78.8 83.6 82.0 88.8 62.5 Table 3. Linear evaluation accuracy on models trained with and without preﬁx conditioning. Preﬁx conditioning slightly improves the performance upon a model without it (second row vs. last row). 4.2. Main Results We describe our main results in Table 1, followed by the analysis of preﬁx conditioning in Sec. 4.3. There are three observations. First, the improvements upon a model trained only with image-caption or image- label data are obvious in almost all cases. As the previ- ous work indicates [43], the effectiveness of combining two types of supervision is clear from these results. Second, in all cases, our preﬁx conditioning signiﬁcantly improves performance on both ImageNet-1K (supervised recognition) and 11 zero-shot recognition tasks. When training on ImageNet-21K, the conditioning improves the baseline by more than 8% in ImageNet-1K and more than 6% in zero-shot recognition on average. In training with ImageNet-1K, the margin from the baseline is smaller than training with ImageNet-21K, probably because the size of ImageNet-1K (1M) is much smaller than that of ImageNet- 21K (12M). Also, preﬁx conditioning is effective in both UniCL and CLIP objectives. Due to its simplicity, our ap- proach can be easily integrated with various objectives. Finally, our method is less affected by ablating a part of categories. The classes of ImageNet-1K are excluded from ImageNet-21K in the last two rows of Table 1. There- fore, both approaches signiﬁcantly drop performance on ImageNet-1K, whose task now becomes true zero-shot recognition, compared to other settings. Even in this set- ting, preﬁx conditioning maintains high accuracy and out- performs a CLIP baseline model by a large margin. Sampling Method. We analyze the data sampling scheme to construct a mini-batch in Table 2. We apply debiased sampling (DS) in our method, namely, sampling one data source with equal probability and getting a mini- batch of it. The other option is mixing two data sources with equal probability (ES). The table indicates that pre- ﬁx conditioning works well with ES sampling and the sam- pling strategy itself is not advantageous. Ablating preﬁx conditioning during training clearly drops the performance in both sampling strategies, and the performance is worse than ES on average in zero-shot results (49.5 vs. 47.8). ES sampling should allow the model to differentiate sen- tences by using the prepended preﬁx. Interestingly, this re- sult implies that differentiating sentences by preﬁx infor- mation does not much degrade the performance. The dis- tinguished sentences enable the model to associate images from different datasets. Since images of two datasets are different with respect to the categories and the locations of objects in images, distinguishing the two kinds of images may not harm generalizability of the representations. Linear-probe Evaluation. We evaluate the linear-probe performance in Table 3 to see the quality of learned image representations. Although the accuracy is better than the model trained without preﬁx conditioning (second line), the improvements are not substantial. This result indicates that the zero-shot performance gain obtained by our method is not due to the image representations. We investigate the learned language and image features in the next subsection. 4.3. Analysis of Preﬁx-Conditioning We present a detailed analysis of preﬁx conditioning. We ﬁrst study how different preﬁxes impact the zero-shot recognition performance and analyze their behaviors by looking into the attention weights of the language trans- former encoder. We also demonstrate improved robustness with respect to the image-level domain shift. Unless other- wise stated, we employ a model trained with CLIP objec- tive on ImageNet-21K and CC12M in this analysis. Finally, this section concludes that preﬁx conditioning enables the language encoder to switch its role during training, which eases learning from different types of datasets, e.g., image classiﬁcation and image caption dataset.Data Test-time Preﬁx IN-1K Cal C100 C10 ESTAT Food Flower Pets Patch R45 VOC DTD A VG IN-1K + CC12M N/A 68.7 68.7 38.4 69.5 24.4 31.9 13.3 66.6 50.2 25.4 65.6 22.3 43.3 Prompt 75.4 71.7 35.5 63.9 24.2 20.0 8.1 72.2 50.4 24.2 61.1 15.3 40.6 Caption 71.5 75.1 39.4 70.5 26.7 33.9 13.9 72.3 50.5 25.8 67.8 25.4 45.5 IN-21K + CC12M N/A 56.8 70.2 55.0 79.4 21.1 46.0 60.3 57.2 51.2 24.8 57.7 21.4 49.5 Prompt 71.4 76.5 59.0 86.0 20.1 45.7 62.3 69.1 52.4 26.3 76.8 21.4 54.1 Caption 67.3 79.7 63.8 87.9 31.5 53.4 58.8 69.6 50.6 31.5 80.5 28.4 57.8 IN-21K w/o 1K + CC12M N/A 29.1 67.4 45.9 80.0 28.6 40.8 56.9 39.2 50.2 21.9 64.9 19.8 46.9 Prompt 40.8 74.9 61.0 84.6 31.2 48.1 58.7 45.2 51.2 23.5 67.5 21.4 51.6 Caption 47.8 81.9 63.3 87.3 32.4 52.9 62.8 57.0 50.6 25.6 80.1 26.2 56.4 Table 4. Ablation study for test-time preﬁx conditioning. Note that the difference between two results come from the preﬁx used in test time and we use the same model for this evaluation. A model trained without conditioning is shown at the top of each block. Caption PrefixPromptPrefix Unconditional Model Figure 3. An example of attention weights for an end token. Best viewed in color. The sentence shown here is one of class prompts in the VOC 2007 dataset. Different rows show the weights of different transformer layers. With a prompt preﬁx (leftmost), the model focuses on a class name (airplane) while caption preﬁx (middle) allows a model to pay attention to another noun, sculpture. By preﬁx conditioning, the attention of the model changes as intended. Test Time Preﬁx. We analyze the role of the preﬁx to- ken in Table 4, where the table describes the comparison in the choice of test time preﬁx conditioning. As explained in Sec. 3, the choice of preﬁx during test time should change the behavior of the model since the preﬁx should tailor the language encoder for classiﬁcation-style or caption-style feature extraction. Except for the IN-1K results of a model trained with the entire IN21K or IN-1K, conditioning with the caption preﬁx shows much better results. The superi- ority of the caption preﬁx is noticeable in several datasets. This means caption preﬁx works better if the target comes from outside the image classiﬁcation data, indicating that the class-prompt preﬁx conditioning makes the model tai- lored for the image classiﬁcation dataset. Class-prompts preﬁx works better to categorize IN-1K data because the preﬁx is trained to specialize in classifying it. Note that caption-style preﬁx performs better than prompt-style preﬁx in IN-1K for a model trained with IN21K excluding IN1k classes. This indicates that the caption-style preﬁx works better when the vocabulary of the class name comes from outside the image classiﬁcation data since the caption data covers much more diverse words. Preﬁx controls attention. Fig. 3 visualizes the atten- tion weights for an end token in different preﬁx conditions and models. The input sentence, a sculpture of an airplane, is one of the class-prompts. When a prompt preﬁx (left- most) is employed, the language model pays attention to the class name at the ﬁrst layer, it does not focus on the noun in other layers. The only noun the encoder focuses on is airplane. By contrast, the model attends to both sculpture and airplane in the case of the caption preﬁx and uncon- ditional model. Note that this behavior does not mean that the prompt-preﬁx performs better in zero-shot recognition as shown in experiments due to the effect of the bias in im- age classiﬁcation dataset. While we visualize only one example in the main text due to the space limit and defer more examples to the ap- pendix, this highlights a general trend that the prompt preﬁx guides the language encoder to focus on a single word (e.g., class name), whereas the caption preﬁx makes the model at- tend to multiple words. In other words, preﬁx conditioning allows the language encoder to “switch gears” to represent sentences from different datasets (i.e., image-classiﬁcation vs image-caption). On the other hand, the baseline model without preﬁx conditioning attends to multiple words (e.g., Fig. 3 rightmost) even though the input sentence is a class prompt. This indicates that it is hard to switch the gears without explicitly informing of the type of dataset. Language Feature Visualization. Fig. 4 visualizes ex- tracted language features conditioned with different pre-Train Data Preﬁx Conditioning Test-Time Preﬁx IN IN-V2 IN-R IN-S ImageNet-1K N/A 72.1 59.3 19.9 17.8 ImageNet-1K + CC12M N/A 68.7 57.4 27.7 27.8 ImageNet-1K + CC12M ✓ Caption 71.5 60.2 31.8 30.7 ImageNet-1K + CC12M ✓ Prompt 75.4 63.3 29.2 27.9 ImageNet-21K N/A 47.1 41.1 20.1 16.1 ImageNet-21K + CC12M N/A 56.8 48.6 29.4 30.6 ImageNet-21K + CC12M ✓ Caption 67.3 57.5 35.2 34.6 ImageNet-21K + CC12M ✓ Prompt 71.4 61.1 32.1 32.2 Table 5. Evaluation on the robustness to the image-level domain shift. Preﬁx conditioned training achieves better robustness, and caption- preﬁx outperforms prompt-preﬁx in the images distinct from those used in training (IN-R and IN-S). (a) Different conditions  (b) Prompt condition (c) Caption condition  (d) No condition Figure 4. T-SNE [36] visualization of the class-prompt features of 20 classes of VOC 2007 with different preﬁx conditions. (a): Language embeddings with prompt (red) and caption (blue) pre- ﬁxes, respectively. (b)(c)(d): Different colors indicate language embeddings of different classes. ﬁxes. As seen in Fig. 4a, language features extracted with caption-preﬁx (blue) and prompt-preﬁx (red) are clearly separated. In addition, prompt-preﬁx (Fig. 4b) has lower intra-class and higher inter-class variance, whereas caption- preﬁx (Fig. 4c) shows higher intra-class variance across prompts. Interestingly, results in Table 4 suggest that the caption-preﬁx conditioned language features result in a bet- ter zero-shot recognition performance than those condi- tioned on the prompt-preﬁx. Although the prompt-preﬁx mode extracts discriminative language embeddings, the em- beddings do not perform well on the zero-shot recogni- tion because the embeddings contain signiﬁcant bias from image-classiﬁcation dataset. Robustness in image domain shift. Test samples can be unseen with respect to image classiﬁcation data in two ways (or combinations of two): 1) The image is similar to the training distribution, but the class name is different from the seen image classiﬁcation labels. 2) Although the class label is the same, the image data comes from a different distribution. Datasets evaluated in the zero-shot recognition include both two cases since the vocabularies and image are from different domains. To understand them, we analyze the test-time preﬁx by using ImageNet-1K and evaluate the performance on image-level domain shift using variants of ImageNet, i.e., ImageNet-V2 [32], ImageNet-R [11], and ImageNet-S [37]. Table 5 describes the results of ablat- ing preﬁx-conditioned training and the test-time preﬁx. The preﬁx-conditioned training outperforms all baselines. This reveals that the preﬁx-conditioned training achieves class embeddings that are generalizable across image domains. The prompt-style preﬁx performs the best in IN, IN-V2, both of which have image styles similar to ImageNet. By contrast, the caption-style preﬁx performs the best in IN-R and IN-S, which has art-style and sketch-style images re- spectively. Thus, the caption-style preﬁx generates more generalizable class embeddings for the domain dissimilar from the ImageNet training data. This observation is con- sistent with the results in the paragraph Test time Preﬁx. 5. Conclusion In this paper, we explore a simple yet effective mecha- nism for uniﬁed pre-training on image-caption and image classiﬁcation data. We propose to learn preﬁx tokens at training time to condition the language encoder to switch the input source. Specifying the preﬁx allows the model to switch the manner of feature extraction and can control which visual domain the embedding is projected to. This approach boosts the performance of zero-shot recognition accuracy of the contrastive learning models. Our analysis suggests that the trained language encoder provides robust- ness to the image-level domain shift. Although we limit our scope to unifying image-caption and image-label supervi- sion, incorporating other supervision such as object detec- tion or semantic segmentation is an interesting next step. Acknowledgment. We thank Zizhao Zhang for their help- ful feedback on the manuscript. This work was supported in part by DARPA LwLL.Abbreviation Dataset #Concepts Train size Test size Source link Food Food-101 102 75,750 25,250 Tensorﬂow CF10 CIFAR-10 10 50,000 10,000 Tensorﬂow CF100 CIFAR-100 100 50,000 10,000 Tensorﬂow VOC VOC2007 classiﬁcation 20 5,011 4,952 Tensorﬂow DTD Describable Textures 47 3,760 1,880 Tensorﬂow Pets Oxford-IIIT Pets 37 3,680 3,669 Tensorﬂow Cal Caltech-101 102 3,060 6084 Tensorﬂow Flower Oxford Flowers 102 102 1,020 6,149 Tensorﬂow Patch PatchCamelyon 2 294,912 32,768 Tensorﬂow ESTAT EuroSAT 10 N/A 27,000 Tensorﬂow R45 Resisc45 45 N/A 31,500 Tensorﬂow Table 6. Statistics of datasets used in zero-shot and linear probe. A. Experimental Details Dataset. Table 6 describes the statistics of dataset used for evaluation. We pick the test datasets based on UniCL [43] and availability in Tensorﬂow dataset. We use the test set to evaluate zero-shot recognition and linear probe while the train set is used to train a linear classiﬁer. Note that since EuroSAT and Resisc45 utilize the training split for evaluation, we exclude the two datasets from linear probe evaluation. Also, since Oxford Flowers do not have many training samples (10 samples per class), we exclude the dataset from the evaluation too. Data Augmentation. Following UniCL [43], only ran- dom cropping is applied to train all models for a fair com- parison. Computation. We use 32 Nvidia Tesla V100 GPUs to train all models. 4 nodes, where each node has 8 GPUs, are used to run experiments. B. Additional Results Attention Visualization. Fig. 5 visualizes attention weights for the class forest area , where a prompt tem- plate, a tatto of , is employed. The model focuses on a word, forest when prompt preﬁx is employed. In other two cases, the model also pays much attention to tatoo proba- bly because the word should provide useful information to distinguish a sentence from others for image-caption con- trastive learning. Fig. 6 represents attention for a real cap- tion from CC3M. While the model conditioned with cap- tion preﬁx and unconditional model attend to several words through many layers, the model conditioned with prompt preﬁx shows clear attention only in the ﬁrst layer. Since the prompt-conditioned model has never seen the real caption during training, it fails in attending to discriminative words. Class Name Shift. Test samples can be unseen with re- spect to image classiﬁcation data in two ways (or combina- tions of two): 1) The image is similar to training distribu- tion, but the class name used for testing is different from the image classiﬁcation label. 2) Although the class label is the same, the image data comes from the different distri- butions. Datasets evaluated in the zeros-shot recognition in- clude both two cases since class names and images are from different domains. 2) is analyzed in Subsection 4.3 of the main paper, Robustness in image domain shift. We analyze 1) by evaluating the recognition performance of ImageNet- 1K by changing its class name from the one used during training. We ﬁnd a synonym for each class with Word- Net [28], where we exclude synonyms substantially simi- lar to the original class name and obtain synonyms for 525 classes. Then, we use the synonym to classify images dur- ing evaluation. Since the input image distribution does not vary, we can evaluate the performance on the class name shift. If the model is robust to the change in the class name, the degrade in the performance should be small. The ﬁrst 6 rows of Table 7 describe the models trained with the original class names and evaluated on both original ones and synonyms, and the last two rows represent a model trained with synonyms, where the original class names are replaced with synonyms. Prompt preﬁx outperforms cap- tion preﬁx with a large margin in testing with class names used in training time. Generally, caption preﬁx performs better when tested with the class names different from the ones used during training. Prompt preﬁx is tailored to han- dle class names employed during training time while cap- tion preﬁx enables the language encoder to extract more general representations. Interestingly, the choice of class names seems to signif- icantly change the generalization as shown in the compar- ison between a model trained with synonyms and original class names. The original model decreases the accuracy more than 30% by changing the class name while the model trained with synonym decreases less than 20%. Image-Caption Retrieval. In Table 8, we evaluate the performance of image-caption retrieval using the sub-Caption PrefixPrompt Prefix Unconditional Model Figure 5. Attention visualization for a class prompt. Note that the attention weights are for and end token. Best viewed in color. The class name shown here is one of class prompts in the EUROSAT dataset. Different rows show the weights of different transformer layers. With a prompt preﬁx (leftmost), the model focuses on a class name ( forest area) while caption preﬁx (middle) allows a model to pay attention to another noun, tattoo. By preﬁx conditioning, the attention of the model changes as intended. Caption PrefixPrompt Prefix Unconditional Model Figure 6. Attention visualization for a real caption. Note that the attention weights are for and end token. Best viewed in color. The sentence shown here is from CC3M. Different rows show the weights of different transformer layers. Caption preﬁx conditioning helps to attend to many words while prompt conditioning fails to do that. set of CC3M (12288 pairs of image and caption) and COCO validation set (5000 pairs of image and caption), where all models are trained with CC12M and ImageNet- 21K. First, our model (last row) slightly performs better than the model without conditioning (ﬁrst row). Second, prompt preﬁx conditioning (second row) signiﬁcantly per- forms worse than caption preﬁx conditioning (last row). Since the prompt preﬁx conditioning specializes a model for the class name prompts of ImageNet21K, the condition- ing does not generalize well to real captions. Larger Batch-size and Training Epochs. We exam- ine the effect of increasing batch-size and training epochs in Table 9. In CLIP, increasing the batch-size and train- ing epochs improves the performance of both ImageNet-1K and zero-shot recognition. On the other hand, the zero-shot performance of UniCL is not beneﬁted from training with longer epochs (compare last and second to last row). UniCL attempts to ensure the invariance of images from the same classes by supervised contrastive loss while CLIP does not consider it. However, such invariance is not necessarily re- quired in zero-shot recognition, which leads to the degraded performance. Comparison to Reported UniCL’s Results. In the main paper, we provide our reproduced results of UniCL,Train Data Train on Synonym Preﬁx Training Test-Time Preﬁx Original Synonym top-1 top-5 top-1 top-5 IN1K + CC12M N/A 69.3 89.3 31.2 49.5 IN1K + CC12M ✓ Prompt 75.0 92.9 38.3 54.8 IN1K + CC12M ✓ Caption 71.4 91.6 36.6 56.7 IN21K + CC12M N/A 54.5 83.2 23.1 43.9 IN21K + CC12M ✓ Prompt 69.9 92.4 32.1 53.7 IN21K + CC12M ✓ Caption 65.3 90.6 33.5 56.9 IN21K + CC12M ✓ ✓ Prompt 54.4 78.6 70.8 92.8 IN21K + CC12M ✓ ✓ Caption 54.5 82.6 59.0 86.1 Table 7. Evaluation on the robustness to the class name shift using ImageNet-1K. Original refers to the subset of ImageNet-1K classes while synonym refers to their synonyms taken from Wordnet. The last two rows indicate the models trained with the synonyms, thus showing superior performance on synonym whereas degrading performance on Original. Preﬁx Training Test-time Preﬁx CC3M COCO I2T@1 I2T@5 T2I@1 T2I@5 I2T@1 I2T@5 T2I@1 T2I@5 N/A 21.8 47.4 21.0 45.7 23.9 49.5 18.7 43.2 ✓ Prompt 13.1 31.3 8.1 21.8 17.2 38.1 16.8 37.7 ✓ Caption 22.6 47.5 21.6 46.1 24.7 49.7 19.7 43.9 Table 8. Image-text retrieval results on CC3M and COCO. The performance is evaluated on the subset of CC3M and validation set of COCO. All models are trained on CC12M and ImageNet-21K. Caption conditioning (last row) slightly improves retrieval performance compared to the unconditional model (ﬁrst row). Since prompt conditioning (middle) tailors a model for class-prompt, it fails to extract discriminative information from real captions. which is based on our implementation, since the authors have not released the code and did not report the numer- ical accuracy of each zero-shot recognition. In this para- graph, we compare our approach and the reported perfor- mance of UniCL [43] and K-Lite [34] by aligning sev- eral hyper-parameters, e.g., batch-size and training epochs, using ImageNet-1K. When using ImageNet-22K and CC- 15M for training, our method (batch-size:4096, training epochs: 30) shows 73.9 while UniCL (batch-size:4096, training epochs 32) reports 71.5. When using ImageNet- 21K excluding ImageNet-1K and CC-15M, our method (batch-size:1024, training epochs 30) shows 49.7 whereas UniCL (batch-size: 4096, training epochs: 32) and K- Lite (batch-size: 4096, training epochs: 32) perform 46.6 and 48.7 respectively according to K-Lite results (See last two rows of Table 3 in [34]). These results suggest that our method performs better than the reported numbers of UniCL and K-Lite in ImageNet-1K. Also, the knowledge augmentation technique proposed by K-Lite can be comple- mentary to our approach, thus combining two approaches is an interesting research direction. T-SNE visualization for language features. Fig. 7 vi- sualizes extracted language features (ImageNet-1K) condi- tioned with different preﬁxes. The prompt-preﬁx (left) has lower intra-class and higher inter-class variance, whereas caption-preﬁx (right) shows higher intra-class variance across prompts. T-SNE visualization for image features. Fig. 8 visu- alizes image features from ImageNet-1K (blue) and CC3M (red). Since ImageNet-1K is object-centered while CC3M covers more diverse scenes, the distributions are separated. This is consistent across baseline (w/o conditioning) and our method (with conditioning). Comparison between unconditioned and conditioned model by language features. Fig. 9 visualizes language features of ImageNet-1K class prompts (Blue) and CC3M captions (Red) for unconditioned (left) and conditioned (right) respectively. Note that the conditioned model uti- lizes prompt preﬁx for class prompts and caption preﬁx for real captions respectively. As seen from the visualiza- tion, unconditioned model cannot distinguish some prompts from captions of CC3M. This is probably because some captions are similar to class prompts of ImageNet. By contrast, the conditioned model differentiate class prompts from captions better than unconditioned model due to the preﬁx conditioning. References [1] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for ﬁne-grained image classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2927–2936, 2015. 2Training Data Objective Batch-size Epochs Metric Classiﬁcation Caption IN-1K Zero-shot 11 datasets ImageNet-21K CC-12M CLIP 1024 15 67.3 57.8 ImageNet-21K CC-12M CLIP 1024 30 69.1 58.3 ImageNet-22K CC-15M CLIP 1024 15 69.3 58.5 ImageNet-22K CC-15M CLIP 4096 15 71.1 59.5 ImageNet-22K CC-15M CLIP 4096 30 72.2 59.8 ImageNet-22K CC-15M UniCL 1024 15 69.7 58.5 ImageNet-22K CC-15M UniCL 4096 15 70.3 60.4 ImageNet-22K CC-15M UniCL 4096 30 73.9 58.9 Table 9. Performance comparison among different batch-size and training epochs. ImageNet-22K denotes the combination of ImageNet- 21K and ImageNet-1K, CC-15M indicates that of CC-12M and CC-3M. (a) Prompt conditioned (b) Caption conditioned Figure 7. T-SNE [36] visualization of the class-prompt features of ImageNet-1K with different preﬁx conditions. Different colors indicate language embeddings of different classes. Prompt con- ditioning extracts more class discriminative representations than caption conditioning. (a) W/O conditioning (b) With conditioning Figure 8. T-SNE [36] visualization of the image features of ImageNet-1K (blue) and CC3M (red). Since ImageNet-1K is object-centered while CC3M covers more diverse scenes, the dis- tributions are separated. This is consistent across baseline (w/o conditioning) and our method (with conditioning).(a) Unconditioned model (b) Conditioned model Figure 9. T-SNE [36] visualization of language features of ImageNet-1K class prompts (Blue) and CC3M captions (Red) for unconditioned (left) and conditioned (right) respectively. Our pro- posed condition better differentiates prompts from real captions. [2] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Ma- chine Learning, pages 528–539. PMLR, 2020. 3 [3] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Ad- vances in neural information processing systems , 32, 2019. 3 [4] Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay Mc- Clelland, and Felix Hill. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022. 1 [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 1, 2, 5 [6] Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu, and Osamu Yoshie. Zerovl: A strong baseline for align- ing vision-language representations with limited resources. arXiv preprint arXiv:2112.09331, 2021. 4, 5 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 1, 2, 3, 5 [8] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world do- main generalization. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 14340–14349, 2021. 3 [9] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. De- vise: A deep visual-semantic embedding model. Advances in neural information processing systems, 26, 2013. 2 [10] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre- trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. 3, 4 [11] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada- vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robust- ness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340–8349, 2021. 8 [12] Dinesh Jayaraman and Kristen Grauman. Zero-shot recogni- tion with unreliable attributes. Advances in neural informa- tion processing systems, 27, 2014. 2 [13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representa- tion learning with noisy text supervision. In International Conference on Machine Learning, pages 4904–4916. PMLR, 2021. 1, 2 [14] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- sual prompt tuning. arXiv preprint arXiv:2203.12119, 2022. 3 [15] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661–18673, 2020. 2 [16] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained fea- tures and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022. 3 [17] Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo. Learning debiased representation via disentan- gled feature augmentation. Advances in Neural Information Processing Systems, 34:25123–25133, 2021. 3 [18] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning.arXiv preprint arXiv:2104.08691, 2021. 2, 3, 4 [19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni- ﬁed vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022. 2 [20] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi.Align before fuse: Vision and language representation learn- ing with momentum distillation. Advances in Neural Infor- mation Processing Systems, 34, 2021. 1, 2 [21] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimiz- ing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 2, 3, 4 [22] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su- pervision exists everywhere: A data efﬁcient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021. 2 [23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi- roaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in nat- ural language processing. arXiv preprint arXiv:2107.13586, 2021. 3, 4 [24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [25] Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations, 2019. 5 [26] Thomas Mensink, Efstratios Gavves, and Cees GM Snoek. Costa: Co-occurrence statistics for zero-shot classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2441–2448, 2014. 2 [27] Christian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography. na, 2012. 2 [28] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995. 2, 9 [29] Norman Mu, Alexander Kirillov, David Wagner, and Sain- ing Xie. Slip: Self-supervision meets language-image pre- training. arXiv preprint arXiv:2112.12750, 2021. 2 [30] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021. 3, 4 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning , pages 8748–8763. PMLR, 2021. 1, 2, 4, 5 [32] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize to im- agenet? In International Conference on Machine Learning, pages 5389–5400. PMLR, 2019. 8 [33] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, im- age alt-text dataset for automatic image captioning. In Pro- ceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2556–2565, 2018. 5 [34] Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jian- wei Yang, Pengchuan Zhang, Anna Rohrbach, Zhe Gan, Lijuan Wang, Lu Yuan, et al. K-lite: Learning transfer- able visual models with external knowledge. arXiv preprint arXiv:2204.09222, 2022. 2, 11 [35] Kihyuk Sohn. Improved deep metric learning with multi- class n-pair loss objective. Advances in neural information processing systems, 29, 2016. 4 [36] Laurens Van der Maaten and Geoffrey Hinton. Visualiz- ing data using t-sne. Journal of machine learning research, 9(11), 2008. 8, 12, 13 [37] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506–10518, 2019. 3, 8 [38] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6857–6866, 2018. 2 [39] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon- tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust ﬁne-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959–7971, 2022. 3 [40] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latent embed- dings for zero-shot classiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 69–77, 2016. 2 [41] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. InProceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4582–4591, 2017. 2 [42] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. arXiv preprint arXiv:2202.10401, 2022. 2 [43] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Uniﬁed contrastive learning in image-text-label space. arXiv preprint arXiv:2204.03610, 2022. 1, 2, 4, 5, 6, 9, 11 [44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021. 2 [45] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo- jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 1 [46] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021. 2, 3, 4 [47] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi- wei Liu. Conditional prompt learning for vision-language models. arXiv preprint arXiv:2203.05557, 2022. 3",
      "meta_data": {
        "arxiv_id": "2206.01125v2",
        "authors": [
          "Kuniaki Saito",
          "Kihyuk Sohn",
          "Xiang Zhang",
          "Chun-Liang Li",
          "Chen-Yu Lee",
          "Kate Saenko",
          "Tomas Pfister"
        ],
        "published_date": "2022-06-02T16:12:26Z",
        "pdf_url": "https://arxiv.org/pdf/2206.01125v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Prefix Conditioning, a novel pre-training strategy that unifies image-classification and image-caption datasets to leverage their complementary benefits while addressing dataset biases. It is the first mechanism to use prefixes to condition the source of the dataset during vision-language contrastive pre-training. This simple approach improves zero-shot recognition performance by more than 6% on average in experiments on ImageNet21K and CC12M. The work also demonstrates that prefix conditioning enables the language encoder to adapt its feature extraction strategy, leading to improved robustness against image-level distribution shifts.",
        "methodology": "The core methodology is Prefix Conditioning, where dataset-specific prefix tokens are prepended to the text input token sequence during pre-training. These prefixes inform the language encoder of the input data type (e.g., classification vs. caption), helping to disentangle dataset biases from visual concepts. The approach is generic and can be integrated into existing Vision-Language (VL) pre-training objectives like CLIP or UniCL. The model is trained using a contrastive learning objective (e.g., symmetric multi-class N-pair loss). Debiased data sampling, where mini-batches are constructed from a single data source, is primarily used, though equal sampling is also explored. During inference for zero-shot classification, a caption-style prefix is added to class prompts to leverage the wider domain coverage learned from image-caption data, which empirically outperforms a prompt-style prefix for novel data.",
        "experimental_setup": "Experiments were conducted using CC3M and CC12M as image-caption datasets, and ImageNet21K and ImageNet1K as image-classification datasets. Baselines include CLIP and UniCL. A Swin-Tiny transformer was used as the vision encoder, and the language encoder was the same as CLIP. Training involved 80 prompt templates, with one randomly sampled during training and averaged for validation. Models were optimized with AdamW (learning rate 0.001, weight decay 0.1, batch size 1024) for 15 or 50 epochs depending on the dataset size, using a cosine learning rate schedule with 10,000 warm-up iterations. Evaluation included supervised and zero-shot image classification on ImageNet1K and 11 standard zero-shot benchmarks (e.g., CIFAR10/100, Oxford-IIIT Pets, Food-101, EuroSAT, Resisc45). Linear-probe evaluation, robustness to image-level domain shifts (ImageNet-V2, ImageNet-R, ImageNet-S), class name shift using WordNet synonyms, and image-caption retrieval on CC3M and COCO validation sets were also performed. All models were trained on 32 Nvidia Tesla V100 GPUs.",
        "limitations": "The scope of the research is limited to unifying image-caption and image-label supervision, suggesting other forms of supervision are not explored. Classification datasets are acknowledged to be biased towards specific image domains and have limited vocabulary, which prefix conditioning aims to mitigate but does not eliminate. While captioning datasets are more open-domain, they can also have data biases. The choice of data sampling method (debiased vs. equal) did not significantly affect performance. Furthermore, while caption-style prefixes generally improve zero-shot recognition, prompt-style prefixes perform better on the original image classification dataset, indicating a potential trade-off or specialization.",
        "future_research_directions": "Future work could involve incorporating other forms of supervision beyond image-caption and image-label, such as object detection or semantic segmentation. Another promising direction is to combine prefix conditioning with external knowledge augmentation techniques, similar to K-Lite, to further enrich the learned representations."
      }
    },
    {
      "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
      "abstract": "Recent works have demonstrated great success in pre-training large-scale\nautoregressive language models on massive GPUs. To reduce the wall-clock\ntraining time, a common practice is to increase the batch size and learning\nrate. However, such practice is often brittle and leads to a so-called\nstability-efficiency dilemma: increasing the batch sizes and learning rates\nleads to better training efficiency but can also result in training\ninstability, leading to poor generalization accuracy or failed runs. To better\nunderstand this phenomenon, we conduct an in-depth analysis on large-scale\npre-training experiments replicating the GPT-2 model. We find that there is a\nstrong correlation between training instability and extreme values of gradient\nvariance, and that samples with long sequence lengths contribute to these\nextreme gradient variance values, especially at the beginning of the training,\nindicating that long sequence length can be a main source of training\ninstability. Based on the analysis, we present a Sequence Length Warmup method\nthat aims to solve the training stability-efficiency dilemma. Experiments\nreplicating GPT-2 models show that our approach enables stable training with 8x\nlarger batch size and 4x larger learning rate, whereas the baseline approach\nstruggles with training instability. To achieve the same or better zero-shot\nevaluation results, our method reduces the required number of training tokens\nand wall clock time by up to 2.2x and 3.7x, respectively. Experiments\nreplicating GPT-3 model (125M) show that our approach enables stable training\nwith 8x larger batch size and 40x larger learning rate, and retains 99% of the\nzero-shot accuracy on 11 tasks using 10x less data and 17x less time compared\nto the original GPT-3 training recipe, while the baseline diverges under the\nsame settings and only retain 95% of accuracy under lower learning rate.",
      "full_text": "The Stability-Efﬁciency Dilemma: Investigating Sequence Length Warmup for Training GPT Models Conglong Li Microsoft conglong.li@microsoft.com Minjia Zhang Microsoft minjiaz@microsoft.com Yuxiong He Microsoft yuxhe@microsoft.com Abstract Recent works have demonstrated great success in pre-training large-scale autore- gressive language models (e.g., GPT-3) on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efﬁciency dilemma: increasing the batch sizes and learning rates leads to better training efﬁciency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model with public dataset. We ﬁnd that there is a strong correlation between training instability and extreme values of gradient variance. We further identify that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a simple yet effective Sequence Length Warmup method that aims to solve the training stability-efﬁciency dilemma by avoiding extreme gradient variance values. Moreover, we present a lightweight tuning strategy that allows us to tune our method with just a small portion of the expensive full training. Experiments replicating GPT-2 models (117M and 1.5B) show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate. 1 Introduction Large-scale Transformer-based language models have achieved great success in many natural lan- guage processing tasks [46, 11]. Among them, large-scale autoregressive models, such as GPT-3 [6], have attracted lots of attention due to their superior performance on zero-shot generalization, i.e., they can perform a wide range of tasks that they are not explicitly trained on. However, pre-training GPT models raises huge challenges on training efﬁciency and less-discussed training instability issues. On the efﬁciency side, as the model size continues to grow from a few hundreds of millions (e.g., GPT [31]), to billion-scale parameters (1.5B GPT-2 [33]), and to more recent hundreds of billions of parameters (175B GPT-3 [6]), the training cost also increases exponentially: it requires 9.2 days This paper was previously titled “Curriculum Learning: A Regularization Method for Efﬁcient and Stable Billion-Scale GPT Model Pre-Training” in an early arxiv preprint version[22]. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2108.06084v4  [cs.LG]  16 Oct 2022on 512 V100 GPUs to train a 8.3B GPT-2 [40], and 47.8 days on 2240 A100 GPUs to train a 530B GPT-3-style model [26]. Such a long training time makes it almost infeasible for most research labs and practitioners to reproduce these models. Various solutions have been proposed to reduce the training wall clock time of these large models [40, 25, 35]. However, many solutions require using more GPUs or sophisticated system techniques. In this work, we investigate speeding up the pre-training of GPT-style models via exploiting data efﬁciency, not at the cost of excessive hardware resources. In particular, in a distributed training environment, increasing the batch sizes and/or using more aggressive learning rates can make the model converge faster [41]. However, it has been observed that larger batch sizes and learning rates can make large-scale GPT model training more difﬁcult, e.g., causing training instability that leads to divergence or slow convergence [49, 9]. To investigate this training instability issue, we conduct a thorough study of the GPT-2 pre-training task [33, 40] with different models sizes under various batch sizes, learning rates, and sequence lengths. We ﬁnd a stability-efﬁciency dilemma: • A larger batch size (and larger learning rate) increases the per-iteration computational efﬁciency but with increasing likelihood of training instability and even divergence. • A smaller batch size makes the training more stable but decreases the per-step computation efﬁciency signiﬁcantly. We ﬁnd it difﬁcult to overcome this dilemma by existing techniques such as extra gradient clipping. More recent proposed techniques such as batch size warmup proposed in [6] does not provide stability beneﬁt in our evaluations. Recently, Shortformer[30] shows that by adding an additional ﬁrst training stage with a shorter sequence length, language models can achieve the same dev. set perplexity with shorter total training time. However, (1) its main focus is to purely improve training efﬁciency instead of solving the efﬁciency-stability dilemma, and our evaluations show that Shortformer’s 2-stage approach is insufﬁcient for overcoming the training instability issue for large models (Section 5.1), (2) it is primarily evaluated on small scale transformer models (247M) on WikiText datasets (103M tokens) without considering large-scale generative models like GPT with hundreds or even billions of parameters, and (3) it does not discuss how to choose some of the important hyperparameters, which is very expensive to ﬁgure out for large-scale model training. Inspired by Shortformer, we investigate the importance of sequence length in training GPT models and ﬁnd that it plays an important role in both training stability and efﬁciency. Based on our investigation, we propose a simple yet effective method called Sequence Length Warmup (SLW), which starts training with short sequences and gradually increases the length. We observe that our approach enables stable and efﬁcient training with much larger batch sizes and learning rates than baseline approaches. Speciﬁcally, we make the following contributions: (1) We conduct an extensive study of the GPT-2 pre-training task, which provides detailed insights about the training stability-efﬁciency dilemma, the correlation between instability and gradient variance outliers, and how sequence length plays a critical role (Section 3). (2) Based on the study, we present a simple yet effective sequence length warmup method for GPT-style model (and autoregressive model in general) that enables stable training with improved training efﬁciency. We also identify a lightweight hyperparameter tuning strategy for the approach, which identiﬁes promising hyperparameters by only incurring a small fraction of the expensive total pre-training cost (Section 4). The implementation of our approach as well as the necessary changes to the GPT-2/3 pre-training framework has been open sourced in a deep learning optimization library called DeepSpeed1. (3) We conduct large-scale experiments to demonstrate the proposed work’s ability to provide superior training stability and efﬁciency at the same time (Section 5). Our empirical results show that: • SLW enables stable and efﬁcient training with 8x larger batch size and 4x larger learning rate on GPT-2 (117M and 1.5B) models with public datasets, while the baseline and related works struggle with instability under the same settings. To achieve the same or better zero-shot WikiText- 103/LAMBADA evaluation results at the end of training, SLW reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. • On GPT-3 model (125M) pre-training we study an even more aggressive training scenario where only 10% of data can be used. Our method, with 8x larger batch size and 40x larger learning rate than the original GPT-3 training recipe, is able to maintain the training stability, retaining 99% of the zero-shot accuracy on 11 evaluation tasks, and use 10x less data and 17x less time. Without our 1https://github.com/microsoft/DeepSpeed, https://www.deepspeed.ai/ 2method, the baseline has unrecoverable divergence under the same settings, and can only retain 95% of the zero-shot accuracy after lowering learning rate to 30x. 2 Background and Related Work Language Model Pre-training: The accuracy of transformer-based language models grows substan- tially with its model size [31, 33, 6]. Today, a large language model such as GPT-3 [6] contains up to 175B parameters, and recent studies show that model accuracy can continue to improve with even larger model sizes [16]. However, training these large models often incurs excessively long training time and training difﬁculties [6]. Therefore, there are a lot of demands of performing efﬁcient and stable training for these models. To have the pre-training ﬁnished in a reasonable amount of time, the most common way is to leverage data parallelism to train models on multiple GPUs. However, the speedup gains often saturate beyond a few tens of GPUs, because communication becomes the major bottleneck, i.e., the workers will spend more time communicating gradients than computing them, as the number of GPUs increases. To mitigate this bottleneck, recent works such as 1-bit Adam [43] have studied gradient compression and demonstrate their effectiveness against auto-encoding models such as BERT [11]. An alternative approach to alleviate these overheads is to use large batch sizes. For example, LAMB [52] and 1-bit LAMB [21] enable stable and efﬁcient distributed BERT pre-training with batch size up to 64K/32K (for sequence length 128/512, i.e., 8M/16M tokens per batch) while maintaining the sample-wise convergence speed. For encoder-decoder models, T5 [34] uses batch size up to 2K (for sequence length 512, i.e., 1M tokens per batch). For autoregressive models such as the GPT family [ 31, 33, 6], existing works use batch size up to 1.6K (for sequence length 2K, i.e, 3.2M tokens per batch). Despite the beneﬁt of reduced communication overhead, large-batch training is sensitive to hyperparameters and often leads to issues such as slow convergence, training instabilities, and model divergence. For example, recently a research project shared that they are dealing with challenging training instability issues when pre-training a 104B GPT-style model with batch size 2K [49], and another work on a 540B model with batch size 2K observed spikes in the loss roughly 20 times during training, despite the fact that gradient clipping was enabled [9]. Curriculum Learning: Our method can be viewed as a kind of curriculum learning (CL) [12, 39, 2], which presents easier/simpler examples earlier during training and gradually increases the sample difﬁculties2. Comparing with traditional CL works which focus on solely improving the convergence speed under the same batch size, learning rate and other hyperparameters, our work is motivated by the stability-efﬁciency dilemma and we aim to achieve both efﬁcient convergence and better stability by enabling stable training with more aggressive hyperparameters. To our knowledge, we are the ﬁrst to investigate and conﬁrm that certain curriculum learning method can provide a dual stability-efﬁciency beneﬁt. In the NLP area, most of the curriculum learning works focus on small-scale one-stage tasks and downstream ﬁne-tuning tasks, such as neural machine translation (NMT) [ 18, 5, 56, 29, 57] and natural language understanding (NLU) [36, 37, 44, 50]. There are also a few works explore curriculum learning for language model pre-training [ 30, 55, 7]. These works show that curriculum learning can improve convergence speed, reduce training time, and improve accuracy under the same training hyperparameters as baseline. In these works, the curriculum difﬁculty metrics for each training sample are usually deﬁned as the sentence length, vocabulary frequency, the inference loss on smaller/cheaper models, or based on self-paced learning [19]. For the pacing function (i.e., to decide the curriculum difﬁculty range when sampling next training data batch), these works usually use ﬁxed predeﬁned functions (e.g., gradually increase difﬁculty upper bound by linear, root, and exponential functions), bucketing heuristics (group data with similar difﬁculties, and sample from a subset of buckets every time), or based on self-paced learning. 3 GPT-2 Pre-training Stability-Efﬁciency Analysis In this section we perform an in-depth analysis of pre-training tasks (without our method) replicating the GPT-2 models with public data. We follow the training pipeline from the NVIDIA Megatron-LM work [40]3. All of the experiments are performed on 128 NVIDIA V100 GPUs (32GB memory). There are 16 nodes and 8 GPUs per node. GPUs inside the same node are connected by NVLink 2.0, and nodes are connected by a 100 Gigabit InﬁniBand EDR inter-node network. We evaluate 2The shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. 3https://github.com/NVIDIA/Megatron-LM 30 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 1: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. In Appendix A.1 we plot the same ﬁgure but zoom in the ﬁrst 30B tokens. Table 1: Measuring training instability by the ratio between the current step training loss and the minimum loss among all previ- ous steps. Larger ratios (esp. those greatly larger than 1.0) indicate larger training insta- bility/divergence. Proposed work (SLW) and related works (last two rows) are discussed in Section 5. #steps with max Pre-training loss ratio > 1.2 loss Case parameters (% of total steps) ratio 117M: 1: Baseline bsz512-seqlen1K 0 (0.0%) 1.05 2: SLW 60K bsz512-seqlen1K 0 (0.0%) 1.06 3: Baseline bsz4K-seqlen1K 22 (0.06%) 1.42 4: SLW 20K bsz4K-seqlen1K 0 (0.0%) 1.02 5: Baseline bsz512-seqlen2K 0 (0.0%) 1.04 6: SLW 110K bsz512-seqlen2K 0 (0.0%) 1.04 1.5B: 7: Baseline bsz512-seqlen1K 114 (0.04%) 2.10 8: SLW 270K bsz512-seqlen1K 0 (0.0%) 1.06 9: Baseline bsz4K-seqlen1K 1381 (3.68%) 5.65 10: SLW 45K bsz4K-seqlen1K 0 (0.0%) 1.02 11: Shortformer bsz4K-seqlen1K 219 (0.4%) 2.86 12: Bsz Warmup bsz4K-seqlen1K 1179 (2.01%) 4.32 two GPT-2 model sizes from the original GPT-2 work [33]: 117M parameters (12 layers, 768 hidden size, 12 attention heads) and 1.5B parameters (48 lay- ers, 1600 hidden size, 25 attention heads). For training data, we collect and use the same dataset blend as the Megatron-LM work: Wikipedia [11], CC-Stories [45], RealNews [54], and OpenWebtext [32]. We evaluate two sets of training parameters. The ﬁrst set follows the Megatron-LM work: batch size 512, 300K total training steps (157B tokens), and learning rate 1.5 ×10−4 with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 297K steps (1 ×10−5 min. learning rate). The second parameter set tests a more aggressive training strategy: batch size 4K (8×larger), 37.5K total training steps (157B tokens4), and learning rate6×10−4 (4×larger) with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 34.5K steps (same min. learning rate). For sequence length/context size, we mainly use 1K which is the default for GPT-2. But we also test 2K (on the 117M model with batch size 512 and 157B tokens) which is the default for GPT-3. All experiments are performed with mixed precision/FP16 training, Adam optimizer (β1 = 0.9, β2 = 0.999, ϵ= 1×10−8) [17], 0.01 weight decay, same random seed, and gradient clipping at 1.0. For both batch sizes we use the same number of gpus (128). It is true that under fewer nodes, smaller batch sizes can also achieve good computation efﬁciency. However, in practice, the goal of a training task is usually \"given a ﬁxed number of hardwares, how to train the model in the fastest wall clock time\". And given the increasing model sizes, pre-training on hundreds of GPUs is not uncommon. Thus we believe that using the same hardware resources is a fair comparison. The stability-efﬁciency dilemma: Figure 1(a) and 1(b) present the training loss curves of 5 baseline cases under different model sizes, batch sizes (and LR), and sequence lengths. At 117M, the baseline has a few training loss spikes at batch size 4K. At 1.5B, the baseline has many loss spikes when training with either batch size 512 or 4K. As an indicative measurement to quantitatively study training instability, we deﬁne “loss ratio” which measures the ratio between the current step training loss and the minimum loss among all previous steps. A ratio larger than 1.0 means that current step’s loss is larger than the previous minimum loss, thus larger ratio indicates a larger loss spike and training instability. Table 1 summarizes the number of steps with loss ratio larger than 1.2, and the 4For pre-training it is common to keep the number of training tokens the same for fair comparison. 4Table 2: Zero-shot evaluation of the trained models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case 2 to 9 are compared with case 1, and case 11 to 17 are compared with case 10. Proposed work (SLW) and related works (16, 17) are discussed in Section 5. Pre-training Training Training Training WikiText LAMBADA Case parameters steps tokens time PPL ↓ accuracy ↑ 117M: 1: Baseline bsz512-seqlen1K 300K 157B 37Hr 27.78 33.19% 2: SLW 60K bsz512-seqlen1K 200K 89B (1.8x) 20Hr (1.9x) 27.74 34.78% 3: SLW 60K bsz512-seqlen1K 330K 157B (1x) 33Hr (1.1x) 27.01 34.41% 4: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 16Hr (2.3x) 28.09 32.54% 5: SLW 30K bsz4K-seqlen1K 37K 92B (1.7x) 10Hr ( 3.7x) 27.77 33.40% 6: SLW 30K bsz4K-seqlen1K 52.5K 157B (1x) 16Hr (2.3x) 27.15 34.16% 7: Baseline bsz512-seqlen2K 150K 157B (1x) 32Hr (1.2x) 28.19 32.99% 8: SLW 110K bsz512-seqlen2K 122.5K 71B ( 2.2x) 15Hr (2.5x) 27.06 33.24% 9: SLW 110K bsz512-seqlen2K 205K 157B (1x) 31Hr (1.2x) 26.03 34.58% 1.5B: 10: Baseline bsz512-seqlen1K 300K 157B 341Hr 13.89 57.29% 11: SLW 270K bsz512-seqlen1K 360K 122B (1.3x) 286Hr (1.2x) 13.89 57.38% 12: SLW 270K bsz512-seqlen1K 428K 157B (1x) 364Hr (0.9x) 13.88 57.89% 13: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 151Hr (2.3x) 14.76 55.06% 14: SLW 45K bsz4K-seqlen1K 50K 121B (1.3x) 121Hr (2.8x) 13.88 58.20% 15: SLW 45K bsz4K-seqlen1K 58.8K 157B (1x) 155Hr (2.2x) 13.72 58.47% 16: Shortformer bsz4K-seqlen1K 55K 157B (1x) 162Hr (2.1x) 14.14 57.23% 17: Bsz Warmup bsz4K-seqlen1K 58.8K 157B (1x) 165Hr (2.1x) 14.21 56.36% Reference 18: Original GPT-2 117M [33], different data 37.50 45.99% works: 19: Original GPT-2 1.5B [33], different data 17.48 63.24% 20: Megatron-LM GPT-2 355M [33], same data 19.31 45.18% 21: Megatron-LM GPT-2 2.5B [33], same data 12.76 61.73% maximum loss ratio during the training. At 117M model size only the baseline with batch size 4K has high loss ratios up to 1.421. At 1.5B model size the baseline with both batch size 512 and 4K has much more steps with large loss ratios, and with the maximum loss ratio as high as 5.65. Baseline with batch size 4K is less stable than baseline with batch size 512, indicating that larger batch sizes (combined with larger learning rates) could lead to more training instability risks. In Appendix A.3.1 we show that larger learning rates under the same batch size could also increase training instability. Training instability are undesirable because (1) it could lead to divergence that never recover as in [49] and our GPT-3 experiments (Section 5.2); (2) in our GPT-2 case it leads to worse convergence, validation loss, and zero-shot downstream task accuracy. Table 2 summarizes the zero-shot WikiText- 103/LAMBADA evaluation results. For both 117M and 1.5B models, increasing baseline’s batch size (and LR) or sequence length leads to training instabilities and loss spikes, and it requires a nontrivial number of training steps/tokens to recover the training loss back to a normal level (e.g., Figure 1(b)). These training “detours” slow down the learning and eventually lead to worse evaluation results (e.g., case 13 vs case 10 in Table 2). On the other hand, increasing batch size (and LR) or sequence length improves training efﬁciency, reducing the training time by up to 2.3x under the same number of training tokens (case 1, 4, 10, 13). Overall, the above observations demonstrate the stability-efﬁciency dilemma for baseline pre-training: the training is more stable and can achieve better ﬁnal generalization, but presumably with poorer training efﬁciency under smaller batch size/learning rate/sequence length; increasing them leads to better training efﬁciency, but with lower stability and worse generalization. The correlation between instability and gradient variance outliers: For stochastic gradient opti- mization, when the gradient variance is large, the algorithm might spend much time bouncing around, leading to slower convergence and potential divergence [48]. Previous studies show that variance reduction methods improve training stability in areas such as reinforcement learning [ 24, 8, 1]. Figure 1(c), 1(d), 1(e) and 1(f) plot the l1 norm and max element of Adam’s variance state (√vt, where vt = β2vt−1 + (1−β2)(gt)2)5. When baseline’s batch size increases, the max variance norm decreases but the max element increases. Comparing GPT-2 117M and 1.5B cases, larger model size leads to larger variance norm and max element. When sequence length increases for the GPT-2 117M case, the variance norm stays the same but the max element increases. To further study the link between instability and gradient variance, Figure 1(g) and 1(h) plot the loss ratio (deﬁned earlier in this section) and gradient variance norm/max element (all normalized by max value) for the most unstable 1.5B baseline with 4K batch size. Results show that when training loss spike happens and loss ratio increases, the gradient variance norm/max also increase (especially the max outliers). Table 3 presents the Pearson correlation coefﬁcient calculations, which demonstrate 5We use l1 norm to avoid outlier ampliﬁcation. 5Table 3: Pearson correlation coefﬁcient (with range (-1, 1)) between loss ratio and gradient variance norm/max. Low p-value indicates that the correlation is statistically signiﬁcant. Pearson correlation coefﬁcient p-value Loss ratio vs Gradient variance norm 0.23 0.0 Loss ratio vs Gradient variance max 0.26 0.0 0 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Steps 2 4 6 8 10 12 14Training loss Seqlen1K, baseline Seqlen128 Seqlen128(90%)+1K(10%) Figure 2: Step-wise training loss during GPT-2 1.5B pre-training (ﬁrst 10K steps only) with batch size 4K, comparing seqlen 1K (baseline), seqlen 128, and mixed seqlen of 128+1K (1K seqlen used at the cyan areas). a statistically signiﬁcant positive correlation between loss ratio and gradient variance norm/max. Overall, our analysis shows that training instability has a strong correlation with gradient variance norm and (especially) max element outliers. Length of early data sequences is critical to training stability: Aiming to solve the stability- efﬁciency dilemma we ﬁrst tried traditional methods such as increasing gradient clipping, but it does not fully resolve the instability issue (Appendix A.3.2). Seeing that in Figure 1 the training instability mostly happens at the ﬁrst half of training, we then explored whether we can solve the issue by gradually increasing any of the batch size/learning rate/sequence length during training. We already employed the same learning rate warmup mechanism used by existing GPT-2 and GPT-3 works [33, 40, 6]. We tried the batch size warmup method proposed in GPT-3 work [ 6], but the instability issue still appears when increasing the batch size (Section 5.1). Our investigation on the sequence length leads to interesting ﬁndings, where we ﬁnd that sequence lengths play a critical role in training instability. Figure 2 presents the training loss curve of the most unstable GPT-2 1.5B pre-training with batch size 4K and seqlen 1K, together with another two artiﬁcial settings: one with seqlen 128, the other with mixed seqlen where we feed 900 steps of seqlen 128 then 100 steps of seqlen 1K in every 1K steps. The seqlen 128 case has no instability issue, even with large model size/batch size/learning rate. The mixed seqlen case has instability issues, and (1) they mostly happen when we switch to seqlen 1K (e.g., at step 900, 1900, 2900...); (2) they mostly happen during the ﬁrst 5K steps, and after that it becomes more stable than the seqlen 1K case. These observations indicate that training instability is strongly correlated with early long sequence lengths, which motivates us to explore the sequence length warmup method described in the next section, and evaluations in Section 5 will demonstrate how this method provides a gradient variance reduction effect and solves the stability-efﬁciency dilemma in our experiments. 4 The Sequence Length Warmup Method The analysis in last section about training instability and sequence lengths motivates us to explore sequence length warmup methods: the model needs to start learning with short sequence length for more stable training, then gradually increase the length when training becomes more stable so that the model can still learn from longer contextual information to achieve better ﬁnal model accuracy. The sequence length warmup strategy depends on two factors: how to support variable sequence length during training and how to adaptively decide the sequence length for each iteration (the pacing function). For the ﬁrst component, we develop an efﬁcient truncation-based implementation: For the baseline GPT-2 pre-training, the raw text inputs are indexed into sequences with the same length before training, so that the model can efﬁciently retrieve a batch of ﬁxed-length sequences regardless of the actual sentence boundaries. It’s possible to index the text inputs based on all possible sequence lengths, but that adds signiﬁcant amount of overhead due to the massive pre-training data. To avoid the large indexing overhead, we take a lightweight approach: our implementation still lets the dataloader index the raw text into only the full sequence length. At each training step, our method uses pacing function to determine the sequence length and then truncates the full-length sequences to obtain a modiﬁed version of the mini-batch for training. It is true that this truncation-based implementation will drop some data in the current step. However, with some implementation changes, it’s possible to record the index of dropped data and use them in future steps. We deﬁne the pacing function as a step-wise linear function with the following properties: Given a starting sequence length seqlens, an ending sequence length seqlene (full sequence length), and a total duration T (number of steps), the sequence length used for the training batch at step tis seqlent = seqlens + (seqlene −seqlens) ×min( t T ,1). Besides step-wise linear, we also explored 3 other pacing functions: i) We tried a discrete 2-stage pacing function from [ 30], but it leads 6to unstable training and worse convergence (Section 5.1). ii) We tried a step-wise root function (seqlent = seqlens + (seqlene −seqlens) ×min(( t T )r,1), where r is the root degree), which performs similar to linear but requires one extra hyperparameter. iii) We tried an adaptive pacing function based on training/validation losses, which also performs similar and requires extra tuning. 0 2K 4K 6K 8K 10K Steps 0 250 500 750 1000 1250 1500 1750 2000Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (a) Step-wise validation per- plexity (beginning of training) 0 50B 100B 150B Tokens 20.0 20.2 20.4 20.6 20.8 21.0Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (b) Token-wise validation per- plexity (end of training) Figure 3: Validation perplexity during GPT-2 117M seqlen 1K pre-training with batch size 512 and different duration T. (“SLW 20K” means proposed approach withT=20K steps). Pacing function analysis and tuning strategy: To study the impact of our approach’s pacing function, we set the starting sequence length (seqlens) ﬁxed at 8 and perform a grid search for the pacing function duration ( T) on the GPT-2 117M case full training (de- tails in Appendix A.4). After full train- ings we perform evaluation on pretrain data test set and WikiText/LAMBADA zero-shot tasks to conﬁrm which du- ration T provides the best accuracy performance. All the cases have quite comparable evaluation results, indicat- ing that the performance is not very sensitive to the duration T within a reasonable range. This grid search sheds light on a low-cost tuning strategy: we ﬁnd that (for GPT-2 117M training with batch size 512 and 4K) the best duration T is the longest duration that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst 10K steps (i.e., a few multiples of the LR warmup steps). In on our study, this “signiﬁcant ﬂuctuation” can be deﬁned as “whether the perplexity value becomes larger than 1.3x of the previous best perplexity”. In Figure 3(a) the SLW 60K is the longest duration we tested that does not have signiﬁcant validation ﬂuctuation during the ﬁrst 10K steps. In Figure 3(b) and Appendix A.4 SLW 60K does provide the best ﬁnal validation perplexity, best ﬁnal test perplexity, and second best eval results. Since it does not require training the model until full convergence, this heuristic greatly reduces the hyperparameter tuning cost of our approach. Another grid search on the starting sequence length seqlens shows that it’s generally better to set it as small as possible, to maximize the stability and convergence speedup beneﬁt. However, if the validation perplexity has signiﬁcant ﬂuctuation near the starting sequence length, increasing seqlens would lead to better convergence. Overall, the low-cost tuning strategy can be summarized as: (1) Start with seqlens = 8and T = a few multiples of LR warmup steps. (2) Increase seqlens until the validation perplexity no longer has signiﬁcant ﬂuctuation at the very beginning. (3) Perform a binary search to ﬁnd the largest T that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst few multiples of LR warmup steps. This tuning strategy relies only on the validation set and does not require test set or downstream task evaluation. For the GPT-2 1.5B and GPT-3 125M models, we used this strategy to tune T and seqlens for the pacing function, and results show that this low-cost tuning strategy could provide similar stability-efﬁciency beneﬁt as grid search on full training runs (GPT-2 117M case). 5 Evaluation 5.1 GPT-2 experiments For GPT-2 model, dataset, and hardware, we follow the same methodology in Section 3. For proposed work’s pacing function conﬁgurations (Section 4), we useseqlens = 8/64 (for 117M/1.5B model based on tuning) and seqlene = 1K/2K(full sequence length). To fully utilize the NVIDIA Tensor Core acceleration, we add a seqlent = seqlent −(seqlent mod 8) postprocessing to make sure the sequence length is always a multiple of 8. For the total duration T, we tune this parameter (grid search for 117M and low-cost tuning for 1.5B) for each case. For the training parameters, for our approach we use the same shared parameters as the baseline except two parameters: 1) Because during sequence length warmup the number of tokens in a data batch is smaller, we modify the training termination condition so that all cases stop when reaching the same 157B training tokens. 2) Because of 1), proposed approach now has more training steps, which make it necessary to modify the learning rate decay schedule to have a fair comparison with the baseline. We change the learning rate decay to token-wise over the 157B tokens (still cosine decay) instead of step-wise over the total number of steps. We describe the underlying rationale in Appendix A.2. 70 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (a) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2500 5000 7500 10000 12500 15000 17500Variance norm Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (e) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2000 4000 6000 8000 10000 12000 14000Variance norm Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (h) Step-wise Adam vari- ance max element Figure 4: Validation perplexity and Adam variance norm/max element during GPT-2 1.5B seqlen 1K pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR. Also compare with related works (“Shortformer” [30] and “Bsz Warmup” [6]) at 2nd row. Each row of subﬁgures share the same legend (“SLW 45K” means our work withT=45K steps). Based on the following observations, we demonstrate that our approach resolves the dilemma and simultaneously improves the stability and efﬁciency. We will mainly present the GPT-2 1.5B results and leave some GPT-2 117M results in Appendix. Signiﬁcant stability gain: In Section 3 Table 1 we discussed how we measure the training instability based on the “loss ratio” metric, which shows that the baseline becomes less stable under larger model size/batch size/learning rate/sequence length. Comparing with baseline and proposed work in this table shows that our work reduces this instability measurement to zero in all cases, together with max ratio close to 1.0 (no spike). This demonstrates the signiﬁcant stability gain by our method. Faster token-wise and time-wise convergence: Figure 4(a) and 4(b) present the validation perplex- ity curves during GPT-2 1.5B seqlen 1K pre-training, comparing baseline and our approach. When the batch size increases from 512 to 4K for baseline, the time-wise convergence becomes faster but the token-wise convergence becomes slower and poorer. On the other hand, our approach at batch size 4K provides faster and better convergence both token-wise and time-wise comparing with the best baseline curve in each case. The shape of SLW’s curves is different (worse than baseline in early stage) because SLW cases initially only learn from shorter sequences, which limit the validation perplexity it can reach (since validation data is always full-length). On the other hand, when SLW cases start to learn from longer sequences, the validation perplexity drops faster than baseline (and related works) and eventually surpasses them. Our approach with batch size 512 provides smaller convergence speedup because (1) Baseline with batch size 512 has less instability issue, limiting the gain from the proposed approach; (2) At batch size 512 the communication overhead is very high, and our approach takes more steps (i.e., communication rounds) than baseline to reach the same 157B training tokens. This extra communication cost “cancelled” part of the time saving from our approach. For GPT-2 117M, our approach provides similar token-wise and time-wise convergence speedup (Appendix A.4). Advancing cost-quality Pareto curve: In Section 3 Table 2 we discussed about baseline’s zero-shot evaluation results. For proposed work eval results in this table, we present them in two ways: one evaluated at the earliest checkpoint that provides better eval results than baseline (batch size 512 and seqlen 1K); the other one evaluated at the end of full training. Results show that our approach is able to advance the cost-quality Pareto curve in two ways: (1) To reach the same eval result quality as baseline, our approach reduces the required number of pre-training tokens and wall clock time by up to 2.2x and 3.7x, respectively; (2) Under the same 157B training tokens, our approach can further improve the eval result quality. In (1) the time-wise saving is higher than the token-wise because (a) For each Transformer block, the self-attention and intermediate layers have time complexity of O(B×L2 ×H) and O(B×L×H2), respectively6. The proposed method uses shorter sequences at 6B, L, Hrepresent batch size, sequence length, hidden size. 8the beginning, reducing the time complexity quadratically for the self-attention sub-layer and linearly for the intermediate sub-layer of Transformer blocks; (b) By enabling stable training at larger batch size, our approach achieves additional time-wise saving by reducing the communication overhead. Variance reduction helps stabilize training: In Section 3 we discussed the strong correlation between training instability and gradient variance norm/max. Figure 4(c) and 4(d) demonstrate that proposed approach stabilizes training and reduces both the Adam variance norm and the variance max element. Importantly, it avoids all the spikes of the variance max element, which all happen to be where the baseline has training loss spikes. One may wonder why gradient clipping cannot help avoid these extreme gradient variance outliers. Although gradient clipping can avoid large gradient at every single step, it cannot avoid the gradient variance getting accumulated from multiple steps (Appendix A.3.2). Comparing with related works: We now compare the proposed work with two related works on the most challenging “1.5B model + batch size 4K” case. The ﬁrst work is the Shortformer where the ﬁrst stage uses shorter sequences and the second stage uses full-length sequences [30]. Following the grid search in the paper, we use seqlen 128 for the ﬁrst stage and set its duration at about half of the baseline duration (20K steps). The second work is the batch size warmup technique used by GPT-3 [6], where we set the starting batch size at 128 and then gradually increase it to 4K, and set the warmup duration same as the proposed work. Other training hyperparameters are unchanged. Figure 4(e) to 4(h) present the results. Both related works provide convergence speedup but it is less than our work. More importantly, they still have training instability issues. The Shortformer has an obvious training divergence at step 20K when the sequence length switches from 128 to 1K (the spike at 20K in Figure 4(h)). This shows when staying at the same shorter sequence length for too long, the model becomes heavily overﬁtted for that length which leads to divergence risk when/after switching to full length. Although both batch size warmup and our method reduce the number of tokens per batch in a similar fashion, batch size warmup does not provide any training stability beneﬁt compared to the baseline. This indicates that providing the same number of shorter (simpler) sequences leads to better training stability than providing fewer number of same length (same difﬁculty) sequences. In addition, batch size warmup has a limitation that the batch size must be multiple of data-parallel size. On the other hand, for our method the sequence length only needs to be multiple of 8 to enable Tensor Core acceleration. These two limitations are different: for the proposed SLW method, the “multiple of 8” limitation is ﬁxed and unrelated to data-parallel size. For batch size warmup it’s a dynamic \"multiple of data-parallel size\" limitation: since nowadays pre-training tasks are performed on up to thousands of GPUs, the data-parallel size can easily go beyond 100, prohibiting ﬂexible conﬁguration of the method (or requires reducing the number of GPUs when using smaller batches, increasing the training clock time). Last but not least, both related works provide non-zero “loss ratio” in Table 1 and worse zero-shot evaluation results in Table 2. 5.2 GPT-3 experiments For experiments replicating the GPT-3 125M model [6] using the Pile public dataset [13], ﬁrst we reproduce the original GPT-3 training recipe: 300B training tokens, seqlen 2K, batch size 256 with batch size warmup (start with 16 then gradually increase to 256 in ﬁrst 4B tokens), learning rate 6 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (6 ×10−5 min. learning rate) 7. Then we explore an aggressive training scenario where only 30B tokens (10%) are allowed. This is because (1) GPT-3 paper admits that it has poor training sample efﬁciency and it sees much more text during pre-training than a human sees in the their lifetime [6, 23]. (2) There could exist cases where the total amount of data/computation resource is limited. We adjust several hyperparameters in this 30B-token training: 8x batch size (2K) for better training efﬁciency, learning rate decay reduced to 30B tokens (based on study that LR schedule should match total training tokens [14], warmup stays at 375M), min. learning rate reduced to 0 (based on recent study on GPT-3 [51]). For baseline we keep using 4B-token batch size warmup, but when our method is used (seqlens = 72, T = 11.5Kbased on tuning) we disable it since both methods reduce tokens per batch. And for both cases we tune the learning rate and use the highest one that provides stable training, which is 30x (1.8×10−2) for baseline and 40x (2.4×10−2) for our method. All experiments are performed on 128 V100 GPUs. Figure 5 and 6 present the training loss and gradient variance max for the GPT-3 pre-training experiments. When applying 40x learning rate to the baseline (batch size warmup), it quickly 7Different from GPT-2, GPT-3 uses token-based learning rate schedule and we follow it. 9Table 4: Zero-shot evaluation of the trained GPT-3 125M models on 11 tasks used by the original GPT-3 work [6]. Per- task eval results in Appendix A.5. Batch Training Training Average Case size tokens time accuracy ↑ 1: Original [6] 256 300B 33.6 2: Baseline repro 256 300B (1x) 61Hr 31.4 3: Baseline 30x LR 2K 30B (10x) 7Hr (9x) 29.8 (95%) 4: SLW 40x LR 2K 30B (10x) 3.5Hr (17x) 31.1 (99%) 0 250M 500M 750M 1B Tokens 2 3 4 5 6 7 8 9 10Training loss Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 5: Training loss during GPT-3 125M pre-training (ﬁrst 1B to- kens). 0 250M 500M 750M 1B Tokens 0.00 0.05 0.10 0.15 0.20 0.25Variance max Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 6: Gradient vari- ance max during GPT-3 125M pre-training (ﬁrst 1B tokens). diverges and cannot continue to train due to NaN losses. The corresponding gradient variance max element becomes a ﬂat line after divergence because the gradients on all dimensions are so large that all gradients get clipped including the max element. After lowering the learning rate to 30x, the baseline is able to ﬁnish the whole training, but it can only retain 95% of average zero-shot accuracy on 11 tasks (HellaSwag [53], LAMBADA [28], TriviaQA [15], WebQs [3], Winogrande [38], PIQA [4], ARC Challenge/Easy [10], ANLI R1/R2/R3 [27]) compared with the case that reproduces the original GPT-3 training recipe8 as shown in Table 4. In contrast, our approach enables stable training with 40x learning rate, demonstrates lower gradient variance max outliers than baseline with 30x learning rate, retains 99% of the original training recipe’s average zero-shot accuracy, and achieves 10x data saving and 17x time saving.9 This demonstrates that the proposed method not only solves the stability-efﬁciency dilemma, but also opens a promising direction of signiﬁcantly reducing total training cost in a different data efﬁciency dimension. Finally, in Appendix A.6 we evaluate the proposed method on a larger GPT-3 1.3B model, including not only zero-shot but also few-shot evaluation. Results show that under the same 300B training tokens the proposed SLW method provides better average accuracy than the baseline for both zero- shot (from 41.6 to 41.9) and few-shot (from 44.8 to 45.3) tasks, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) can provide better accuracy performance. Similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup and proposed SLW method. 6 Limitation and Future Work Although our paper discovers the correlation between long sequences and training instability (and proposes a method to alleviate the issue), the root cause of this causal relationship is not fully deciphered and would be an interesting future work direction. One assumption we have is that the shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. We think encoding shorter sequences (especially at the early training stage when weight is still relatively random) will generate less \"noisy\" gradients thus leading to higher stability. This work presents correlation analysis on an empirical connection between training instability and gradient variance norm/max element, but it is not sufﬁcient to prove a causal relationship, and training instability could be caused by other factors. Overall, our work is one preliminary attempt to study the topic of training instability in large-scale model training, and we hope it could inspire future works to further study this important issue in both theory and practice. 7 Conclusion This paper presents the Sequence Length Warmup method, which solves a stability-efﬁciency dilemma inside GPT-style model pre-training, an critical issue that hinders efﬁcient language model pre-training as explained in our in-depth analysis. By enabling stable training on more aggressive training recipe, this method also motivates a new dimension of training cost reduction by improving the data efﬁciency, as demonstrated by the 10x data and 17x time saving in our GPT-3 experiments. We believe that the effectiveness, simplicity, and easy-to-use/tune make the proposed method a must-try for deep learning practitioners, and we hope this work could motivate more studies on improving training data efﬁciency. 8Our reproduced GPT-3 has 2.2 point lower average accuracy than the original GPT-3, which is because of the different training data and OpenAI employed many data processing techniques [6] 9We want to emphasize here that the SLW method “only retains 99% of accuracy” because this experiment is an extreme case: only 10% of original training data is used during training. 10References [1] Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. In International Conference on Machine Learning, pages 176–185. PMLR, 2017. [2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48, 2009. [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artiﬁcial intelligence, pages 7432–7439, 2020. [5] Ondˇrej Bojar, Jindˇrich Helcl, Tom Kocmi, Jindˇrich Libovick`y, and Tomáš Musil. Results of the wmt17 neural mt training task. In Proceedings of the second conference on machine translation, pages 525–533, 2017. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020. [7] Daniel Campos. Curriculum learning for language modeling. arXiv preprint arXiv:2108.02170, 2021. [8] Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick. Control regularization for reduced variance reinforcement learning. In International Conference on Machine Learning, pages 1141–1150. PMLR, 2019. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. [12] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99, 1993. [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. [17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. 11[18] Tom Kocmi and Ondˇrej Bojar. Curriculum learning and minibatch bucketing in neural machine translation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 379–386, 2017. [19] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS, volume 1, page 2, 2010. [20] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [21] Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit LAMB: Communication Efﬁcient Large-Scale Large-Batch Training with LAMB’s Convergence Speed. arXiv preprint arXiv:2104.06069, 2021. [22] Conglong Li, Minjia Zhang, and Yuxiong He. Curriculum learning: A regularization method for efﬁcient and stable billion-scale gpt model pre-training. arXiv preprint arXiv:2108.06084, 2021. [23] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020. [24] Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh. Variance reduction for reinforcement learning in input-driven environments. In International Conference on Learning Representations, 2018. [25] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. [26] Microsoft and Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to- train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful- generative-language-model/, 2021. [27] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019. [28] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [29] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, and Tom M Mitchell. Competence-based curriculum learning for neural machine translation. In NAACL- HLT, 2019. [30] Oﬁr Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832, 2020. [31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [32] Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, and Ilya Sutskever. Better language models and their implications. OpenAI Blog, 2019. [33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza- tions toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020. [36] Mrinmaya Sachan and Eric Xing. Easy questions ﬁrst? a case study on curriculum learning for question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 453–463, 2016. 12[37] Mrinmaya Sachan and Eric Xing. Self-training for jointly learning to ask and answer questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 629–640, 2018. [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 8732–8740, 2020. [39] Terence D Sanger. Neural network learning control of robot manipulators using gradually increasing task difﬁculty. IEEE transactions on Robotics and Automation, 10(3):323–333, 1994. [40] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [41] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V . Le. Don’t decay the learning rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [42] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. [43] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit Adam: Communication Efﬁcient Large-Scale Training with Adam’s Convergence Speed. InProceedings of the 38th International Conference on Machine Learning, pages 10118–10129, 2021. [44] Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C Phan, Xingdi Yuan, Jinfeng Rao, Siu Cheung Hui, and Aston Zhang. Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4922–4931, 2019. [45] Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information processing systems, pages 5998–6008, 2017. [47] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019. [48] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. In NIPS, 2013. [49] Thomas Wolf. The engineering group in @bigsciencew ﬁghting training instabilities over +100b parameters. https://twitter.com/Thom_Wolf/status/1447565680384032776, 2021. [50] Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Curriculum learning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6095–6104, 2020. [51] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. [52] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. [53] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019. 13[54] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9054–9065, 2019. [55] Wei Zhang, Wei Wei, Wen Wang, Lingling Jin, and Zheng Cao. Reducing bert computation by padding removal and curriculum learning. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 90–92. IEEE, 2021. [56] Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. An empirical exploration of curriculum learning for neural machine translation. arXiv preprint arXiv:1811.00739, 2018. [57] Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. Curriculum learning for domain adaptation in neural machine translation. In NAACL-HLT, 2019. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] We have a section describing limitation and future work. (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed 10, and Section 3 and 5 include all instructions needed to reproduce the experiments. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Section 3 and 5 include all the training details. (c) Did you report error bars (e.g., with respect to the random seed after running ex- periments multiple times)? [N/A] Our results are based on a single seed given the pre-training is expensive. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Section 3 and 5 include the information. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 10https://github.com/microsoft/DeepSpeed 14(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 150 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 10B 20B 30B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 10B 20B 30B Tokens 0.00 0.02 0.04 0.06 0.08 0.10Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 7: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. A Appendix A.1 Zoom in of Figure 1 Figure 7 zoom in the ﬁrst 30B token in main paper Figure 1, where the training is the most unstable. A.2 Learning rate decay for proposed approach As discussed in main paper Section 5.1 GPT-2 experiments, proposed approach needs more training steps than baseline in order to reach the same 157B training tokens. This makes it necessary to modify the learning rate decay schedule for proposed approach. We ﬁrst tried to increase the number of learning rate decay steps by half of the proposed approach’s pacing function durationT (since the proposed approach roughly needs T/2 additional steps to reach 157B tokens). However, we ﬁnd that simply increasing decay steps still leads to faster learning rate decay than baseline. At last we change the learning rate decay to token-wise (same cosine decay over the 157B tokens) instead of step-wise. This is because for the proposed approach there are fewer tokens per step at the beginning. So even if we increase the LR decay steps, it still cannot avoid decaying faster token-wise at the beginning compared to baseline. As shown in Figure 8, the proposed approach with step-wise LR decay (with T/2 extra decay steps) has faster LR decay token-wise compared to baseline, which leads to a worse validation perplexity curve. On the other hand, the same proposed approach case with token-wise LR decay has the same token-wise LR decay schedule as baseline, which leads to better convergence. A.3 Additional analysis about training hyperparameters In main paper Section 4 we demonstrate that proposed approach’s two hyperparameters can be tuned with very low cost only running the very beginning of the training (the third hyperparameter, ending sequence length, does not require tuning since it will always be the full length). To understand more about how proposed approach affects the choice and tuning of normal training hyperparameters, this section provides additional analysis about learning rates and gradient clipping. Results demonstrate that (a) Compared to baseline, proposed approach requires less tuning effort on these hyperparameters to provide a stable training; (b) By enabling stable training on larger learning rates, proposed approach could provide better training efﬁciency and convergence (as demonstrated in main paper Section 5); (c) Tuning gradient clipping for baseline could not provide the same training stability as proposed approach. A.3.1 Learning rate In Section 5.1 we demonstrate that proposed approach can provide stable and more efﬁcient training at larger batch size and learning rate, where baseline suffers from training instability. We increased both batch size and learning rate at the same time because (a) Large-batch training is more efﬁcient 160 40B 80B 120B 160B Tokens 11.5 12.0 12.5 13.0 13.5 14.0Validation PPL Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (a) Token-wise validation perplexity 0 100K 200K 300K 400K Steps 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (b) Step-wise learning rate 0 50B 100B 150B Tokens 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (c) Token-wise learning rate Figure 8: Validation perplexity and learning rate during GPT-2 1.5B seqlen 1K pre-training with batch size 512, comparing the baseline and proposed approach under different learning rate decay schedules. “SLW 270K” means proposed approach withT=270K steps. 0 1K 2K 3K Steps 3 4 5 6 7 8 9 10 11 12Training loss Baseline LR=6e-4 Baseline LR=12e-4 SLW 8K LR=6e-4 SLW 8K LR=12e-4 Figure 9: Step-wise training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, seed 1236, and different learning rates for baseline and proposed approach (“SLW 8K” means proposed approach with T=8K steps). for large-scale distributed training, so larger batch was necessary in our study (b) In order to maintain the same convergence speed, it is necessary to simultaneously increase the learning rate under larger batch size. A well-known rule of thumb is that the learning rate should at least increase by the square root of the batch size’s increase ratio. As a controlled experiment, here we perform additional studies about what if we keep the batch size the same and only tune learning rate for baseline and proposed approach. We do not consider the case of “same learning rate, different batch sizes” due to the reason (b) above. Table 5 presents the number of steps with training loss ratios (deﬁned in main paper Section 3 as an indicative measurement of training instability) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K11, 5 different seeds, and different learning rates for baseline and proposed approach. And Figure 9 illustrates some of the cases with seed 1236 to show how the loss spikes look 11Batch size 2K is used here because this analysis was performed at an early stage of this work, and we do not have enough resource to rerun the same analysis with batch size 4K. 17Table 5: Number of steps with training loss ratios (deﬁned in Section 3) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, 5 different seeds, and different learning rates for baseline and proposed approach (SLW). Left/right number in each cell is for baseline/SLW, respectively. Baseline/SLW LR = LR = LR = LR = #loss ratio > 1.5 1.5 × 10−4 3 × 10−4 6 × 10−4 12 × 10−4 Seed 1234 0/0 296/0 359/0 179/74 Seed 1235 0/0 302/0 408/0 555/459 Seed 1236 0/0 0/0 569/0 626/414 Seed 1237 7/0 0/0 548/0 614/139 Seed 1238 0/0 0/0 121/0 394/29 Total 7/0 598/0 2005/0 2368/1115 like. Results show that proposed approach provides stable training during this ﬁrst 3K steps for all ﬁve seeds at learning rates up to 6 ×10−4, while baseline with seed 1237 still has 7 large loss ratios at learning rate as low as 1.5 ×10−4. At learning rate 12 ×10−4 both cases have large loss ratios, but proposed approach reduces the frequency by 2.1x. This demonstrates that (a) Larger learning rates lead to higher training instability risk for both cases. (b) With the same amount of tuning effort, proposed approach has a higher probability of providing a stable training because of the wider range of learning rates it enables; (c) Since proposed approach enables stable training at larger learning rate, it could provide better and faster training convergence as shown in main paper Section 5. A.3.2 Gradient clipping In main paper Section 5 we used gradient clipping at 1.0 (global gradient l2 norm is clipped to 1.0) following the previous work [40]. Here we perform additional studies about what if we apply more gradient clipping to baseline. Figure 10(a) presents the training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K (the same hyperparameters as the second set in Section 3), comparing the baseline and proposed approach under different gradient clipping levels12. Results show that when applying more gradient clipping to baseline, the training has less and smaller loss spikes. And the Adam varaince norm is also reduced as shown in Figure 10(c). However, more gradient clipping does not fully resolve the training instability issue. Even baseline with the lowest gradient clipping norm cannot avoid all training loss spikes, while proposed approach with default gradient clipping has no loss spike. As described in main paper, we believe that this is a limitation of common gradient clipping technique: Although gradient clipping can avoid too large gradient at every single step, it cannot avoid the gradient variance getting accumulated at certain dimensions (as shown in Figure 10(d)), especially for large batch sizes. Another concern about applying more gradient clipping is that the momentum norm is also reduced due to more clipping (Figure 10(b)). This indicates that when later the training reaches a more stable stage, more gradient clipping could hurt the convergence speed. On the other hand, proposed approach will not affect the convergence speed after the full sequence length is reached. Another thing to note is that proposed approach relies less on gradient clipping: at gradient clipping norm 1.0, baseline has 798 clippings in the ﬁrst 5K steps while proposed approach has 628 clippings (21% less). Overall, this analysis demonstrates that proposed approach requires less or no tuning on gradient clipping, while baseline still has training stability issue with more gradient clipping. It is possible that more complex and adaptive gradient/variance/activation clipping techniques could potentially achieve the same level of training stability as proposed approach. However, inventing and applying such techniques would require an effort no lower than the proposed approach, which is both easy to integrate and low-cost to tune. A.4 GPT-2 117M evaluation results Figure 11 presents the validation perplexity and Adam variance norm/max element during GPT-2 117M pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. Table 6 presents the zero-shot evaluation of the trained 117M models on 12We also tried less than 0.25 gradient clipping, which triggered a silent crash without error messages after around 100 steps. We did not have enough time to ﬁnd the root cause, but it could be caused by the too extreme gradient clipping. 180 1K 2K 3K 4K 5K Steps 2 4 6 8 10 12 14 16 18Training loss Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (a) Step-wise training loss 0 1K 2K 3K 4K 5K Steps 0 500 1000 1500 2000 2500 3000Momentum norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (b) Step-wise Adam momentum l1 norm 0 1K 2K 3K 4K 5K Steps 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (c) Step-wise Adam variance l1 norm 0 1K 2K 3K 4K 5K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06Variance max Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (d) Step-wise Adam variance max element Figure 10: Training loss, Adam momentum l1 norm, and Adam variance l1 norm/max element during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K, comparing the baseline and proposed approach under different gradient clipping levels. Grad clip 1.0 indicates that the global gradient l2 norm is clipped to 1.0. ‘SLW 45K” means proposed approach withT=45K steps. 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (a) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (e) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (h) Step-wise Adam vari- ance max element Figure 11: Validation perplexity and Adam variance norm/max element during GPT-2 117M pre- training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. “SLW 60K” means proposed work withT=60K steps. the WikiText-103 and LAMBADA datasets for baseline and proposed work with different pacing function duration. A.5 GPT-3 125M evaluation results Table 7 presents the zero-shot evaluation of the trained GPT-3 125M models on the 11 tasks used by the original GPT-3 work [6]. 19Table 6: Zero-shot evaluation of the GPT-2 117M models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case Pre-training Pre-training Pre-training WikiText-103 LAMBADA parameters steps, tokens, time test perplexity ↓ perplexity ↓ accuracy ↑ 1: Baseline bsz512-seqlen1K 300K, 157B, 37Hr 20.75 27.78 33.19% 2: SLW 20K bsz512-seqlen1K 310K, 157B, 30Hr 20.49 27.43 34.60% 3: SLW 60K bsz512-seqlen1K 330K, 157B, 33Hr 20.11 27.01 34.41% 4: SLW 100K bsz512-seqlen1K 350K, 157B, 35Hr 20.16 26.91 34.21% 5: SLW 140K bsz512-seqlen1K 370K, 157B, 35Hr 20.17 27.17 33.92% 6: Baseline bsz4K-seqlen1K 37.5K, 157B, 16Hr 20.99 28.09 32.54% 7: SLW 10K bsz4K-seqlen1K 42.5K, 157B, 16Hr 20.34 27.22 33.98% 8: SLW 20K bsz4K-seqlen1K 47.5K, 157B, 16Hr 20.25 27.13 34.54% 9: SLW 30K bsz4K-seqlen1K 52.5K, 157B, 16Hr 20.22 27.15 34.16% 10: SLW 40K bsz4K-seqlen1K 57.5K, 157B, 16Hr 20.26 27.11 33.53% 13: Baseline bsz512-seqlen2K 150K, 157B, 32Hr 20.87 28.19 32.99% 15: SLW 70K bsz512-seqlen2K 185K, 157B, 31Hr 19.82 26.04 33.46% 17: SLW 110K bsz512-seqlen2K 205K, 157B, 31Hr 19.64 26.03 34.58% 18: SLW 150K bsz512-seqlen2K 215K, 157B, 32Hr 19.64 25.99 33.32% 15: SLW 190K bsz512-seqlen2K 245K, 157B, 33Hr 19.64 26.09 33.09% Table 7: GPT-3 125M zero-shot evaluation results Baseline Baseline SLW Case Original [6] repro 30x LR 40x LR Model size 125M 125M 125M 125M Train tokens 300B 300B 30B 30B Batch size 256 256 2K 2K Bsz warmup 4B 4B 4B N/A LR 6e-4 6e-4 1.8e-2 2.4e-2 min LR 6e-5 6e-5 0 0 LR warmup 375M 375M 375M 375M LR decay 260B 260B 30B 30B decay style cosine cosine cosine cosine SLW N/A N/A N/A 11.5K steps Avg. accuracy 33.6 31.4 29.8 31.1 (0) HellaSwag 33.7 30.4 28.2 28.9 (1) LAMBADA 42.7 39.3 30.4 34.2 (2) TriviaQA 4.15 1.72 0.76 1.45 (3) WebQs 1.77 0.197 0 0.394 (4) Winogrande 52.0 49.3 50.9 51.9 (5) PIQA 64.6 61.9 59.8 62.7 (6) ARC Challenge 26.6 23.3 21.7 22.3 (7) ARC Easy 43.6 39.9 36.0 39.1 (8) ANLI R1 33.4 32.8 33.1 33.4 (9) ANLI R2 33.2 33.3 33.3 33.6 (10) ANLI R3 33.6 33.3 33.2 34.7 A.6 GPT-3 1.3B evaluation results In this section we evaluate the proposed SLW method on the larger GPT-3 1.3B model. Compared to the GPT-3 125M evaluation in main paper section 5.2 there are two differences on the setup: (1) The GPT-3 125M evaluation aims to explore whether the proposed method can retain the accuracy performance while greatly reducing the training tokens, while this GPT-3 1.3B evaluation aims to explore that, under same amount of training tokens, does proposed method provides better training stability and better accuracy performance. (2) To improve the training data quality, for GPT-3 1.3B pre-training we added two additional sub-datasets (CC-Stories [45] and RealNews [54]), together with additional data cleaning on all data following the process in [42]. Similar to previous experiments, we test two set of hyperparameters on both baseline and proposed method: The ﬁrst set follows the original GPT-3 setup: 300B training tokens, seqlen 2K, batch size 512 (baseline case includes batch size warmup that starts with 16 then gradually increase to 512 in ﬁrst 8B tokens), learning rate 2 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (2 ×10−5 min. learning rate). The second set changes the batch size to 4K (8x) and learning rate to 8 ×10−4 (4x). 20Table 8: GPT-3 1.3B zero-shot evaluation results Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 44.4 41.6 41.9 (0) LAMBADA 63.6 63.7 65.0 (1) TriviaQA 19.7 10.1 11.3 (2) WebQs 4.63 3.25 2.36 (3) PIQA 75.1 73.4 73.8 (4) RACE-h 40.9 35.6 37.1 (5) BoolQ 62.4 63.4 61.8 The baseline case only enables stable training on the ﬁrst set of hyperparameters. Under larger batch size and learning rate, a training divergence (similar to main paper Figure 5 blue line) happened and the training cannot continue. On the other hand, the proposed SLW method is able to provide stable training under 8x larger batch size and 4x larger learning rate. Under the same number of training tokens, the 8x larger batch size leads to better training efﬁciency and 2x training time speedup, similar to what we obserbe in GPT-2 pre-training (main paper Table 2 case 10 vs. 15). This demonstrate the stability-efﬁciency beneﬁt of the proposed method. In addition, Table 8 and 9 present the zero-shot and few-shot evaluations of the trained GPT-3 1.3B models on 6 tasks used by the original GPT-3 work [6]: LAMBADA [28], TriviaQA [15], WebQs [3], PIQA [4], RACE-h [20], BoolQ [47]. Results show that similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup (from 41.6 to 44.8) and proposed SLW method (from 41.9 to 45.3).13 The change on each task also follows the same pattern: TriviaQA and WebQs accuracy improve a lot under few-shot; PIQA, RACE-h, and BoolQ have similar accuracy under zero and few-shot; LAMBADA accuracy becomes worse under few-shot. More importantly, under the same 300B training tokens the proposed SLW method provides better average accuracy (zero-shot from 41.6 to 41.9, few-shot from 44.8 to 45.3) than the baseline, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) is able to provide better accuracy performance. 13Similar to main paper section 5.2, our reproduced GPT-3 baseline has 2.9/3.3 point lower average zero/few- shot accuracy than the original GPT-3, which is because of the different training data and OpenAI employed special data processing techniques [6] 21Table 9: GPT-3 1.3B few-shot evaluation results. k denotes the number of shots following the original GPT-3 work [6]. Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 48.1 44.8 45.3 (0) LAMBADA (k=15) 57.0 58.8 59.7 (1) TriviaQA (k=64) 32.1 19.2 19.0 (2) WebQs (k=64) 19.6 18.4 19.4 (3) PIQA (k=50) 74.3 74.2 72.8 (4) RACE-h (k=10) 41.4 35.0 37.6 (5) BoolQ (k=32) 64.1 63.2 63.2 22",
      "meta_data": {
        "arxiv_id": "2108.06084v4",
        "authors": [
          "Conglong Li",
          "Minjia Zhang",
          "Yuxiong He"
        ],
        "published_date": "2021-08-13T06:32:53Z",
        "pdf_url": "https://arxiv.org/pdf/2108.06084v4.pdf",
        "github_url": "https://github.com/NVIDIA/Megatron-LM"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the stability-efficiency dilemma in pre-training large-scale autoregressive language models (GPT-style), where increasing batch sizes and learning rates for efficiency often leads to training instability. It identifies a strong correlation between training instability and extreme gradient variance values, particularly linked to long sequence lengths at the beginning of training. Based on this, the authors propose a novel Sequence Length Warmup (SLW) method, which starts training with short sequences and gradually increases their length. This method is shown to enable stable and efficient training, allowing for 8x larger batch size and 4x larger learning rate for GPT-2 models, and 8x larger batch size and 40x larger learning rate for GPT-3 (125M) while retaining 99% accuracy. It reduces required training tokens by up to 2.2x and wall-clock time by up to 3.7x for GPT-2, and achieves 10x data saving and 17x time saving for GPT-3. A lightweight hyperparameter tuning strategy for SLW is also presented, and the implementation is open-sourced in DeepSpeed.",
        "methodology": "The study began with an in-depth analysis of GPT-2 pre-training tasks to understand the stability-efficiency dilemma. It quantitatively measured training instability using a 'loss ratio' metric (current loss / minimum previous loss) and correlated it with Adam's gradient variance state (l1 norm and max element of sqrt(vt)). The proposed Sequence Length Warmup (SLW) method involves an efficient truncation-based implementation where the dataloader initially indexes full-length sequences, which are then truncated to a shorter length determined by a step-wise linear pacing function: seqlent = seqlens + (seqlene - seqlens) * min(t/T, 1). A lightweight tuning strategy for SLW's hyperparameters (starting sequence length 'seqlens' and duration 'T') is also described, which relies on monitoring validation perplexity fluctuations during the initial training steps rather than full convergence.",
        "experimental_setup": "Experiments were conducted on GPT-2 (117M and 1.5B parameters) and GPT-3 (125M and 1.3B parameters) models. The hardware used consisted of 128 NVIDIA V100 GPUs across 16 nodes, connected by NVLink 2.0 within nodes and 100 Gigabit InfiniBand EDR between nodes. For GPT-2, training data included Wikipedia, CC-Stories, RealNews, and OpenWebtext. For GPT-3, the Pile public dataset was used, with additional CC-Stories and RealNews for the 1.3B model. Training employed mixed precision/FP16, Adam optimizer (β1=0.9, β2=0.999, ϵ=1e-8), 0.01 weight decay, and gradient clipping at 1.0. Baselines included standard training configurations, Shortformer, and Batch Size Warmup. Evaluation metrics included 'loss ratio' for stability, validation perplexity for convergence speed/quality, and zero-shot/few-shot accuracy on various downstream NLP tasks such as WikiText-103, LAMBADA, HellaSwag, TriviaQA, WebQs, Winogrande, PIQA, ARC Challenge/Easy, ANLI R1/R2/R3, RACE-h, and BoolQ.",
        "limitations": "The paper acknowledges that the root cause of the causal relationship between long sequences and training instability has not been fully deciphered, though an assumption is made that shorter sequences are 'simpler' examples leading to less noisy gradients. The correlation analysis between training instability and gradient variance outliers is empirical and not sufficient to prove a causal relationship, as other factors could also contribute to instability. Due to the high computational cost of pre-training, experiments were primarily conducted using a single random seed, which might limit the generalizability of results regarding error bars or sensitivity to initialization.",
        "future_research_directions": "Future research could focus on thoroughly deciphering the root cause of the causal relationship between long sequences and training instability. Further theoretical and practical studies into the broader topic of training instability in large-scale model training are encouraged. Additionally, exploring more complex and adaptive gradient, variance, or activation clipping techniques could be a promising direction.",
        "experimental_code": "def __getitem__(self, idx: Optional[int]) -> Dict[str, torch.Tensor]:\n        if idx is None:\n            # Batch padding sequence so the index does not matter\n            text, _ = self._query_document_sample_shuffle_indices(0)\n        else:\n            text, _ = self._query_document_sample_shuffle_indices(idx)\n        text = torch.from_numpy(text).long()\n        if self.config.add_extra_token_to_sequence:\n            tokens = text[:-1].contiguous()\n            labels = text[1:].contiguous()\n        else:\n            tokens = text\n            labels = torch.roll(text, shifts=-1, dims=0)\n            labels[-1] = self._pad_token_id\n        if (\n            not self.masks_and_position_ids_are_cacheable\n            or not self.masks_and_position_ids_are_cached\n        ):\n            attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(\n                tokens,\n                self.config.tokenizer.eod,\n                self.config.reset_position_ids,\n                self.config.reset_attention_mask,\n                self.config.eod_mask_loss,\n                self.config.create_attention_mask,\n            )\n            if self.masks_and_position_ids_are_cacheable:\n                self.cached_attention_mask = attention_mask\n                self.cached_loss_mask = loss_mask\n                self.cached_position_ids = position_ids\n                self.masks_and_position_ids_are_cached = True\n        else:\n            attention_mask = self.cached_attention_mask\n            loss_mask = self.cached_loss_mask.clone()\n            position_ids = self.cached_position_ids\n        # For padded sequences, mask the loss\n        loss_mask[labels == self._pad_token_id] = 0.0\n        # For padded sequences, ensure the embedding layer can map the token ID\n        tokens[tokens == self._pad_token_id] = 0\n        labels[labels == self._pad_token_id] = 0\n        # Batch padding sequence so we mask the loss\n        if idx is None:\n            loss_mask = torch.zeros_like(loss_mask)\n        if self.config.create_attention_mask:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"attention_mask\": attention_mask,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\n        else:\n            return {\n                \"tokens\": tokens,\n                \"labels\": labels,\n                \"loss_mask\": loss_mask,\n                \"position_ids\": position_ids,\n            }\ndef _build_document_sample_shuffle_indices(\n        self,\n    ) -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n        path_to_cache = self.config.path_to_cache\n        if path_to_cache is None and not self.config.mock:\n            path_to_cache = os.path.join(\n                self.dataset.path_prefix, \"cache\", f\"{type(self).__name__}_indices\"\n            )\n        if path_to_cache:\n            base = f\"{self.unique_description_hash}-{type(self).__name__}-{self.index_split.name}\"\n            get_path_to = lambda affix: os.path.join(path_to_cache, f\"{base}-{affix}\")\n            path_to_description = get_path_to(\"description.txt\")\n            path_to_document_index = get_path_to(\"document_index.npy\")\n            path_to_sample_index = get_path_to(\"sample_index.npy\")\n            path_to_shuffle_index = get_path_to(\"shuffle_index.npy\")\n            cache_hit = all(\n                map(\n                    os.path.isfile,\n                    [\n                        path_to_description,\n                        path_to_document_index,\n                        path_to_sample_index,\n                        path_to_shuffle_index,\n                    ],\n                )\n            )\n        else:\n            cache_hit = False\n        if not path_to_cache or (\n            not cache_hit\n            and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0)\n        ):\n            log_single_rank(\n                logger,\n                logging.INFO,\n                f\"Build and save the {type(self).__name__} {self.index_split.name} indices\",\n            )\n            t_beg = time.time()\n            sequence_length = self.config.sequence_length\n            num_tokens_per_epoch = self._get_num_tokens_per_epoch()\n            num_epochs = self._get_num_epochs(num_tokens_per_epoch)\n            if num_epochs == 1:\n                separate_final_epoch = False\n            else:\n                # Get the number of samples for the last epoch\n                num_samples_sans_final_epoch = (\n                    (num_epochs - 1) * num_tokens_per_epoch\n                    - self.config.add_extra_token_to_sequence\n                ) // sequence_length\n                num_samples_from_final_epoch = self.num_samples - num_samples_sans_final_epoch\n                num_samples_per_epoch = (\n                    num_tokens_per_epoch - self.config.add_extra_token_to_sequence\n                ) // sequence_length\n                # num_samples_from_final_epoch should be non-negative\n                assert num_samples_from_final_epoch >= 0\n                # num_samples_from_final_epoch should not exceed max value\n                assert num_samples_from_final_epoch <= num_samples_per_epoch + 1\n                # Separate the final epoch if it falls below the threshold\n                threshold = 0.80\n                separate_final_epoch = num_samples_from_final_epoch < int(\n                    threshold * num_samples_per_epoch\n                )\n                log_single_rank(\n                    logger,\n                    logging.DEBUG,\n                    f\"> num_samples_from_final_epoch: {num_samples_from_final_epoch}\",\n                )\n                log_single_rank(logger, logging.DEBUG, f\"> threshold: {threshold}\")\n                log_single_rank(\n                    logger, logging.DEBUG, f\"> num_samples_per_epoch: {num_samples_per_epoch}\",\n                )\n            log_single_rank(\n                logger, logging.DEBUG, f\"> separate_final_epoch: {separate_final_epoch}\",\n            )\n            numpy_random_state = numpy.random.RandomState(self.config.random_seed)\n            # Build the document index\n            document_index = _build_document_index(\n                self.indices, num_epochs, numpy_random_state, separate_final_epoch\n            )\n            # Build the sample index\n            from megatron.core.datasets import helpers\n            if self.index_split == Split.valid:\n                drop_last_partial_sequence = self.config.drop_last_partial_validation_sequence\n            else:\n                drop_last_partial_sequence = True\n            assert document_index.dtype == numpy.int32\n            assert self.dataset.sequence_lengths.dtype == numpy.int32\n            if len(document_index) * 2 > len(self.dataset.sequence_lengths):\n                # If \"access density\" of sequence_lengths is high, force load the mmap-ed array\n                # into memory by making a copy.\n                #\n                # System performance benefits come from two aspects:\n                #   1. We sequentially pre-load the whole file, most of which we expect to read\n                #   2. The GIL is held when entering the c++ program, improving the speed of which\n                #      improves parallelism\n                sequence_lengths_for_cpp = self.dataset.sequence_lengths.copy()\n            else:\n                sequence_lengths_for_cpp = self.dataset.sequence_lengths\n            sample_index = helpers.build_sample_idx(\n                sequence_lengths_for_cpp,\n                document_index,\n                sequence_length,\n                num_epochs,\n                num_tokens_per_epoch,\n                drop_last_partial_sequence,\n                self.config.add_extra_token_to_sequence,\n            )\n            # Build the shuffle index\n            if separate_final_epoch:\n                shuffle_index = _build_shuffle_index(\n                    num_samples_sans_final_epoch, sample_index.shape[0] - 1, numpy_random_state\n                )\n            else:\n                shuffle_index = _build_shuffle_index(\n                    sample_index.shape[0] - 1, sample_index.shape[0] - 1, numpy_random_state\n                )\n            if path_to_cache:\n                os.makedirs(path_to_cache, exist_ok=True)\n                # Write the description\n                with open(path_to_description, \"wt\") as writer:\n                    writer.write(self.unique_description)\n                numpy.save(path_to_document_index, document_index, allow_pickle=True)\n                numpy.save(path_to_sample_index, sample_index, allow_pickle=True)\n                numpy.save(path_to_shuffle_index, shuffle_index, allow_pickle=True)\n            else:\n                log_single_rank(\n                    logger,\n                    logging.WARNING,\n                    f\"Unable to save {type(self).__name__} indexes because path_to_cache is None\",\n                )\n            t_end = time.time()\n            log_single_rank(logger, logging.DEBUG, f\"\\t> time elapsed: {t_end - t_beg:4f} seconds\")\n            log_single_rank(\n                logger, logging.INFO, f\"> total number of samples: {sample_index.shape[0] - 1}\"\n            )\n            log_single_rank(logger, logging.INFO, f\"> total number of epochs: {num_epochs}\")\n            return document_index, sample_index, shuffle_index",
        "experimental_info": "The study quantitatively measured training instability using a 'loss ratio' metric (current loss / minimum previous loss) and correlated it with Adam's gradient variance state (l1 norm and max element of sqrt(vt)). A lightweight tuning strategy for SLW's hyperparameters (starting sequence length 'seqlens' and duration 'T') is also described, which relies on monitoring validation perplexity fluctuations during the initial training steps rather than full convergence. "
      }
    },
    {
      "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models",
      "abstract": "Recent works have demonstrated great success in pre-training large-scale\nautoregressive language models on massive GPUs. To reduce the wall-clock\ntraining time, a common practice is to increase the batch size and learning\nrate. However, such practice is often brittle and leads to a so-called\nstability-efficiency dilemma: increasing the batch sizes and learning rates\nleads to better training efficiency but can also result in training\ninstability, leading to poor generalization accuracy or failed runs. To better\nunderstand this phenomenon, we conduct an in-depth analysis on large-scale\npre-training experiments replicating the GPT-2 model. We find that there is a\nstrong correlation between training instability and extreme values of gradient\nvariance, and that samples with long sequence lengths contribute to these\nextreme gradient variance values, especially at the beginning of the training,\nindicating that long sequence length can be a main source of training\ninstability. Based on the analysis, we present a Sequence Length Warmup method\nthat aims to solve the training stability-efficiency dilemma. Experiments\nreplicating GPT-2 models show that our approach enables stable training with 8x\nlarger batch size and 4x larger learning rate, whereas the baseline approach\nstruggles with training instability. To achieve the same or better zero-shot\nevaluation results, our method reduces the required number of training tokens\nand wall clock time by up to 2.2x and 3.7x, respectively. Experiments\nreplicating GPT-3 model (125M) show that our approach enables stable training\nwith 8x larger batch size and 40x larger learning rate, and retains 99% of the\nzero-shot accuracy on 11 tasks using 10x less data and 17x less time compared\nto the original GPT-3 training recipe, while the baseline diverges under the\nsame settings and only retain 95% of accuracy under lower learning rate.",
      "full_text": "The Stability-Efﬁciency Dilemma: Investigating Sequence Length Warmup for Training GPT Models Conglong Li Microsoft conglong.li@microsoft.com Minjia Zhang Microsoft minjiaz@microsoft.com Yuxiong He Microsoft yuxhe@microsoft.com Abstract Recent works have demonstrated great success in pre-training large-scale autore- gressive language models (e.g., GPT-3) on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efﬁciency dilemma: increasing the batch sizes and learning rates leads to better training efﬁciency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model with public dataset. We ﬁnd that there is a strong correlation between training instability and extreme values of gradient variance. We further identify that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a simple yet effective Sequence Length Warmup method that aims to solve the training stability-efﬁciency dilemma by avoiding extreme gradient variance values. Moreover, we present a lightweight tuning strategy that allows us to tune our method with just a small portion of the expensive full training. Experiments replicating GPT-2 models (117M and 1.5B) show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate. 1 Introduction Large-scale Transformer-based language models have achieved great success in many natural lan- guage processing tasks [46, 11]. Among them, large-scale autoregressive models, such as GPT-3 [6], have attracted lots of attention due to their superior performance on zero-shot generalization, i.e., they can perform a wide range of tasks that they are not explicitly trained on. However, pre-training GPT models raises huge challenges on training efﬁciency and less-discussed training instability issues. On the efﬁciency side, as the model size continues to grow from a few hundreds of millions (e.g., GPT [31]), to billion-scale parameters (1.5B GPT-2 [33]), and to more recent hundreds of billions of parameters (175B GPT-3 [6]), the training cost also increases exponentially: it requires 9.2 days This paper was previously titled “Curriculum Learning: A Regularization Method for Efﬁcient and Stable Billion-Scale GPT Model Pre-Training” in an early arxiv preprint version[22]. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2108.06084v4  [cs.LG]  16 Oct 2022on 512 V100 GPUs to train a 8.3B GPT-2 [40], and 47.8 days on 2240 A100 GPUs to train a 530B GPT-3-style model [26]. Such a long training time makes it almost infeasible for most research labs and practitioners to reproduce these models. Various solutions have been proposed to reduce the training wall clock time of these large models [40, 25, 35]. However, many solutions require using more GPUs or sophisticated system techniques. In this work, we investigate speeding up the pre-training of GPT-style models via exploiting data efﬁciency, not at the cost of excessive hardware resources. In particular, in a distributed training environment, increasing the batch sizes and/or using more aggressive learning rates can make the model converge faster [41]. However, it has been observed that larger batch sizes and learning rates can make large-scale GPT model training more difﬁcult, e.g., causing training instability that leads to divergence or slow convergence [49, 9]. To investigate this training instability issue, we conduct a thorough study of the GPT-2 pre-training task [33, 40] with different models sizes under various batch sizes, learning rates, and sequence lengths. We ﬁnd a stability-efﬁciency dilemma: • A larger batch size (and larger learning rate) increases the per-iteration computational efﬁciency but with increasing likelihood of training instability and even divergence. • A smaller batch size makes the training more stable but decreases the per-step computation efﬁciency signiﬁcantly. We ﬁnd it difﬁcult to overcome this dilemma by existing techniques such as extra gradient clipping. More recent proposed techniques such as batch size warmup proposed in [6] does not provide stability beneﬁt in our evaluations. Recently, Shortformer[30] shows that by adding an additional ﬁrst training stage with a shorter sequence length, language models can achieve the same dev. set perplexity with shorter total training time. However, (1) its main focus is to purely improve training efﬁciency instead of solving the efﬁciency-stability dilemma, and our evaluations show that Shortformer’s 2-stage approach is insufﬁcient for overcoming the training instability issue for large models (Section 5.1), (2) it is primarily evaluated on small scale transformer models (247M) on WikiText datasets (103M tokens) without considering large-scale generative models like GPT with hundreds or even billions of parameters, and (3) it does not discuss how to choose some of the important hyperparameters, which is very expensive to ﬁgure out for large-scale model training. Inspired by Shortformer, we investigate the importance of sequence length in training GPT models and ﬁnd that it plays an important role in both training stability and efﬁciency. Based on our investigation, we propose a simple yet effective method called Sequence Length Warmup (SLW), which starts training with short sequences and gradually increases the length. We observe that our approach enables stable and efﬁcient training with much larger batch sizes and learning rates than baseline approaches. Speciﬁcally, we make the following contributions: (1) We conduct an extensive study of the GPT-2 pre-training task, which provides detailed insights about the training stability-efﬁciency dilemma, the correlation between instability and gradient variance outliers, and how sequence length plays a critical role (Section 3). (2) Based on the study, we present a simple yet effective sequence length warmup method for GPT-style model (and autoregressive model in general) that enables stable training with improved training efﬁciency. We also identify a lightweight hyperparameter tuning strategy for the approach, which identiﬁes promising hyperparameters by only incurring a small fraction of the expensive total pre-training cost (Section 4). The implementation of our approach as well as the necessary changes to the GPT-2/3 pre-training framework has been open sourced in a deep learning optimization library called DeepSpeed1. (3) We conduct large-scale experiments to demonstrate the proposed work’s ability to provide superior training stability and efﬁciency at the same time (Section 5). Our empirical results show that: • SLW enables stable and efﬁcient training with 8x larger batch size and 4x larger learning rate on GPT-2 (117M and 1.5B) models with public datasets, while the baseline and related works struggle with instability under the same settings. To achieve the same or better zero-shot WikiText- 103/LAMBADA evaluation results at the end of training, SLW reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. • On GPT-3 model (125M) pre-training we study an even more aggressive training scenario where only 10% of data can be used. Our method, with 8x larger batch size and 40x larger learning rate than the original GPT-3 training recipe, is able to maintain the training stability, retaining 99% of the zero-shot accuracy on 11 evaluation tasks, and use 10x less data and 17x less time. Without our 1https://github.com/microsoft/DeepSpeed, https://www.deepspeed.ai/ 2method, the baseline has unrecoverable divergence under the same settings, and can only retain 95% of the zero-shot accuracy after lowering learning rate to 30x. 2 Background and Related Work Language Model Pre-training: The accuracy of transformer-based language models grows substan- tially with its model size [31, 33, 6]. Today, a large language model such as GPT-3 [6] contains up to 175B parameters, and recent studies show that model accuracy can continue to improve with even larger model sizes [16]. However, training these large models often incurs excessively long training time and training difﬁculties [6]. Therefore, there are a lot of demands of performing efﬁcient and stable training for these models. To have the pre-training ﬁnished in a reasonable amount of time, the most common way is to leverage data parallelism to train models on multiple GPUs. However, the speedup gains often saturate beyond a few tens of GPUs, because communication becomes the major bottleneck, i.e., the workers will spend more time communicating gradients than computing them, as the number of GPUs increases. To mitigate this bottleneck, recent works such as 1-bit Adam [43] have studied gradient compression and demonstrate their effectiveness against auto-encoding models such as BERT [11]. An alternative approach to alleviate these overheads is to use large batch sizes. For example, LAMB [52] and 1-bit LAMB [21] enable stable and efﬁcient distributed BERT pre-training with batch size up to 64K/32K (for sequence length 128/512, i.e., 8M/16M tokens per batch) while maintaining the sample-wise convergence speed. For encoder-decoder models, T5 [34] uses batch size up to 2K (for sequence length 512, i.e., 1M tokens per batch). For autoregressive models such as the GPT family [ 31, 33, 6], existing works use batch size up to 1.6K (for sequence length 2K, i.e, 3.2M tokens per batch). Despite the beneﬁt of reduced communication overhead, large-batch training is sensitive to hyperparameters and often leads to issues such as slow convergence, training instabilities, and model divergence. For example, recently a research project shared that they are dealing with challenging training instability issues when pre-training a 104B GPT-style model with batch size 2K [49], and another work on a 540B model with batch size 2K observed spikes in the loss roughly 20 times during training, despite the fact that gradient clipping was enabled [9]. Curriculum Learning: Our method can be viewed as a kind of curriculum learning (CL) [12, 39, 2], which presents easier/simpler examples earlier during training and gradually increases the sample difﬁculties2. Comparing with traditional CL works which focus on solely improving the convergence speed under the same batch size, learning rate and other hyperparameters, our work is motivated by the stability-efﬁciency dilemma and we aim to achieve both efﬁcient convergence and better stability by enabling stable training with more aggressive hyperparameters. To our knowledge, we are the ﬁrst to investigate and conﬁrm that certain curriculum learning method can provide a dual stability-efﬁciency beneﬁt. In the NLP area, most of the curriculum learning works focus on small-scale one-stage tasks and downstream ﬁne-tuning tasks, such as neural machine translation (NMT) [ 18, 5, 56, 29, 57] and natural language understanding (NLU) [36, 37, 44, 50]. There are also a few works explore curriculum learning for language model pre-training [ 30, 55, 7]. These works show that curriculum learning can improve convergence speed, reduce training time, and improve accuracy under the same training hyperparameters as baseline. In these works, the curriculum difﬁculty metrics for each training sample are usually deﬁned as the sentence length, vocabulary frequency, the inference loss on smaller/cheaper models, or based on self-paced learning [19]. For the pacing function (i.e., to decide the curriculum difﬁculty range when sampling next training data batch), these works usually use ﬁxed predeﬁned functions (e.g., gradually increase difﬁculty upper bound by linear, root, and exponential functions), bucketing heuristics (group data with similar difﬁculties, and sample from a subset of buckets every time), or based on self-paced learning. 3 GPT-2 Pre-training Stability-Efﬁciency Analysis In this section we perform an in-depth analysis of pre-training tasks (without our method) replicating the GPT-2 models with public data. We follow the training pipeline from the NVIDIA Megatron-LM work [40]3. All of the experiments are performed on 128 NVIDIA V100 GPUs (32GB memory). There are 16 nodes and 8 GPUs per node. GPUs inside the same node are connected by NVLink 2.0, and nodes are connected by a 100 Gigabit InﬁniBand EDR inter-node network. We evaluate 2The shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. 3https://github.com/NVIDIA/Megatron-LM 30 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 50B 100B 150B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 50B 100B 150B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 50B 100B 150B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 50B 100B 150B Tokens 0.0 0.2 0.4 0.6 0.8 1.0Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 1: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. In Appendix A.1 we plot the same ﬁgure but zoom in the ﬁrst 30B tokens. Table 1: Measuring training instability by the ratio between the current step training loss and the minimum loss among all previ- ous steps. Larger ratios (esp. those greatly larger than 1.0) indicate larger training insta- bility/divergence. Proposed work (SLW) and related works (last two rows) are discussed in Section 5. #steps with max Pre-training loss ratio > 1.2 loss Case parameters (% of total steps) ratio 117M: 1: Baseline bsz512-seqlen1K 0 (0.0%) 1.05 2: SLW 60K bsz512-seqlen1K 0 (0.0%) 1.06 3: Baseline bsz4K-seqlen1K 22 (0.06%) 1.42 4: SLW 20K bsz4K-seqlen1K 0 (0.0%) 1.02 5: Baseline bsz512-seqlen2K 0 (0.0%) 1.04 6: SLW 110K bsz512-seqlen2K 0 (0.0%) 1.04 1.5B: 7: Baseline bsz512-seqlen1K 114 (0.04%) 2.10 8: SLW 270K bsz512-seqlen1K 0 (0.0%) 1.06 9: Baseline bsz4K-seqlen1K 1381 (3.68%) 5.65 10: SLW 45K bsz4K-seqlen1K 0 (0.0%) 1.02 11: Shortformer bsz4K-seqlen1K 219 (0.4%) 2.86 12: Bsz Warmup bsz4K-seqlen1K 1179 (2.01%) 4.32 two GPT-2 model sizes from the original GPT-2 work [33]: 117M parameters (12 layers, 768 hidden size, 12 attention heads) and 1.5B parameters (48 lay- ers, 1600 hidden size, 25 attention heads). For training data, we collect and use the same dataset blend as the Megatron-LM work: Wikipedia [11], CC-Stories [45], RealNews [54], and OpenWebtext [32]. We evaluate two sets of training parameters. The ﬁrst set follows the Megatron-LM work: batch size 512, 300K total training steps (157B tokens), and learning rate 1.5 ×10−4 with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 297K steps (1 ×10−5 min. learning rate). The second parameter set tests a more aggressive training strategy: batch size 4K (8×larger), 37.5K total training steps (157B tokens4), and learning rate6×10−4 (4×larger) with a linear warmup of 3K steps and a single cycle cosine decay over the remaining 34.5K steps (same min. learning rate). For sequence length/context size, we mainly use 1K which is the default for GPT-2. But we also test 2K (on the 117M model with batch size 512 and 157B tokens) which is the default for GPT-3. All experiments are performed with mixed precision/FP16 training, Adam optimizer (β1 = 0.9, β2 = 0.999, ϵ= 1×10−8) [17], 0.01 weight decay, same random seed, and gradient clipping at 1.0. For both batch sizes we use the same number of gpus (128). It is true that under fewer nodes, smaller batch sizes can also achieve good computation efﬁciency. However, in practice, the goal of a training task is usually \"given a ﬁxed number of hardwares, how to train the model in the fastest wall clock time\". And given the increasing model sizes, pre-training on hundreds of GPUs is not uncommon. Thus we believe that using the same hardware resources is a fair comparison. The stability-efﬁciency dilemma: Figure 1(a) and 1(b) present the training loss curves of 5 baseline cases under different model sizes, batch sizes (and LR), and sequence lengths. At 117M, the baseline has a few training loss spikes at batch size 4K. At 1.5B, the baseline has many loss spikes when training with either batch size 512 or 4K. As an indicative measurement to quantitatively study training instability, we deﬁne “loss ratio” which measures the ratio between the current step training loss and the minimum loss among all previous steps. A ratio larger than 1.0 means that current step’s loss is larger than the previous minimum loss, thus larger ratio indicates a larger loss spike and training instability. Table 1 summarizes the number of steps with loss ratio larger than 1.2, and the 4For pre-training it is common to keep the number of training tokens the same for fair comparison. 4Table 2: Zero-shot evaluation of the trained models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case 2 to 9 are compared with case 1, and case 11 to 17 are compared with case 10. Proposed work (SLW) and related works (16, 17) are discussed in Section 5. Pre-training Training Training Training WikiText LAMBADA Case parameters steps tokens time PPL ↓ accuracy ↑ 117M: 1: Baseline bsz512-seqlen1K 300K 157B 37Hr 27.78 33.19% 2: SLW 60K bsz512-seqlen1K 200K 89B (1.8x) 20Hr (1.9x) 27.74 34.78% 3: SLW 60K bsz512-seqlen1K 330K 157B (1x) 33Hr (1.1x) 27.01 34.41% 4: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 16Hr (2.3x) 28.09 32.54% 5: SLW 30K bsz4K-seqlen1K 37K 92B (1.7x) 10Hr ( 3.7x) 27.77 33.40% 6: SLW 30K bsz4K-seqlen1K 52.5K 157B (1x) 16Hr (2.3x) 27.15 34.16% 7: Baseline bsz512-seqlen2K 150K 157B (1x) 32Hr (1.2x) 28.19 32.99% 8: SLW 110K bsz512-seqlen2K 122.5K 71B ( 2.2x) 15Hr (2.5x) 27.06 33.24% 9: SLW 110K bsz512-seqlen2K 205K 157B (1x) 31Hr (1.2x) 26.03 34.58% 1.5B: 10: Baseline bsz512-seqlen1K 300K 157B 341Hr 13.89 57.29% 11: SLW 270K bsz512-seqlen1K 360K 122B (1.3x) 286Hr (1.2x) 13.89 57.38% 12: SLW 270K bsz512-seqlen1K 428K 157B (1x) 364Hr (0.9x) 13.88 57.89% 13: Baseline bsz4K-seqlen1K 37.5K 157B (1x) 151Hr (2.3x) 14.76 55.06% 14: SLW 45K bsz4K-seqlen1K 50K 121B (1.3x) 121Hr (2.8x) 13.88 58.20% 15: SLW 45K bsz4K-seqlen1K 58.8K 157B (1x) 155Hr (2.2x) 13.72 58.47% 16: Shortformer bsz4K-seqlen1K 55K 157B (1x) 162Hr (2.1x) 14.14 57.23% 17: Bsz Warmup bsz4K-seqlen1K 58.8K 157B (1x) 165Hr (2.1x) 14.21 56.36% Reference 18: Original GPT-2 117M [33], different data 37.50 45.99% works: 19: Original GPT-2 1.5B [33], different data 17.48 63.24% 20: Megatron-LM GPT-2 355M [33], same data 19.31 45.18% 21: Megatron-LM GPT-2 2.5B [33], same data 12.76 61.73% maximum loss ratio during the training. At 117M model size only the baseline with batch size 4K has high loss ratios up to 1.421. At 1.5B model size the baseline with both batch size 512 and 4K has much more steps with large loss ratios, and with the maximum loss ratio as high as 5.65. Baseline with batch size 4K is less stable than baseline with batch size 512, indicating that larger batch sizes (combined with larger learning rates) could lead to more training instability risks. In Appendix A.3.1 we show that larger learning rates under the same batch size could also increase training instability. Training instability are undesirable because (1) it could lead to divergence that never recover as in [49] and our GPT-3 experiments (Section 5.2); (2) in our GPT-2 case it leads to worse convergence, validation loss, and zero-shot downstream task accuracy. Table 2 summarizes the zero-shot WikiText- 103/LAMBADA evaluation results. For both 117M and 1.5B models, increasing baseline’s batch size (and LR) or sequence length leads to training instabilities and loss spikes, and it requires a nontrivial number of training steps/tokens to recover the training loss back to a normal level (e.g., Figure 1(b)). These training “detours” slow down the learning and eventually lead to worse evaluation results (e.g., case 13 vs case 10 in Table 2). On the other hand, increasing batch size (and LR) or sequence length improves training efﬁciency, reducing the training time by up to 2.3x under the same number of training tokens (case 1, 4, 10, 13). Overall, the above observations demonstrate the stability-efﬁciency dilemma for baseline pre-training: the training is more stable and can achieve better ﬁnal generalization, but presumably with poorer training efﬁciency under smaller batch size/learning rate/sequence length; increasing them leads to better training efﬁciency, but with lower stability and worse generalization. The correlation between instability and gradient variance outliers: For stochastic gradient opti- mization, when the gradient variance is large, the algorithm might spend much time bouncing around, leading to slower convergence and potential divergence [48]. Previous studies show that variance reduction methods improve training stability in areas such as reinforcement learning [ 24, 8, 1]. Figure 1(c), 1(d), 1(e) and 1(f) plot the l1 norm and max element of Adam’s variance state (√vt, where vt = β2vt−1 + (1−β2)(gt)2)5. When baseline’s batch size increases, the max variance norm decreases but the max element increases. Comparing GPT-2 117M and 1.5B cases, larger model size leads to larger variance norm and max element. When sequence length increases for the GPT-2 117M case, the variance norm stays the same but the max element increases. To further study the link between instability and gradient variance, Figure 1(g) and 1(h) plot the loss ratio (deﬁned earlier in this section) and gradient variance norm/max element (all normalized by max value) for the most unstable 1.5B baseline with 4K batch size. Results show that when training loss spike happens and loss ratio increases, the gradient variance norm/max also increase (especially the max outliers). Table 3 presents the Pearson correlation coefﬁcient calculations, which demonstrate 5We use l1 norm to avoid outlier ampliﬁcation. 5Table 3: Pearson correlation coefﬁcient (with range (-1, 1)) between loss ratio and gradient variance norm/max. Low p-value indicates that the correlation is statistically signiﬁcant. Pearson correlation coefﬁcient p-value Loss ratio vs Gradient variance norm 0.23 0.0 Loss ratio vs Gradient variance max 0.26 0.0 0 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Steps 2 4 6 8 10 12 14Training loss Seqlen1K, baseline Seqlen128 Seqlen128(90%)+1K(10%) Figure 2: Step-wise training loss during GPT-2 1.5B pre-training (ﬁrst 10K steps only) with batch size 4K, comparing seqlen 1K (baseline), seqlen 128, and mixed seqlen of 128+1K (1K seqlen used at the cyan areas). a statistically signiﬁcant positive correlation between loss ratio and gradient variance norm/max. Overall, our analysis shows that training instability has a strong correlation with gradient variance norm and (especially) max element outliers. Length of early data sequences is critical to training stability: Aiming to solve the stability- efﬁciency dilemma we ﬁrst tried traditional methods such as increasing gradient clipping, but it does not fully resolve the instability issue (Appendix A.3.2). Seeing that in Figure 1 the training instability mostly happens at the ﬁrst half of training, we then explored whether we can solve the issue by gradually increasing any of the batch size/learning rate/sequence length during training. We already employed the same learning rate warmup mechanism used by existing GPT-2 and GPT-3 works [33, 40, 6]. We tried the batch size warmup method proposed in GPT-3 work [ 6], but the instability issue still appears when increasing the batch size (Section 5.1). Our investigation on the sequence length leads to interesting ﬁndings, where we ﬁnd that sequence lengths play a critical role in training instability. Figure 2 presents the training loss curve of the most unstable GPT-2 1.5B pre-training with batch size 4K and seqlen 1K, together with another two artiﬁcial settings: one with seqlen 128, the other with mixed seqlen where we feed 900 steps of seqlen 128 then 100 steps of seqlen 1K in every 1K steps. The seqlen 128 case has no instability issue, even with large model size/batch size/learning rate. The mixed seqlen case has instability issues, and (1) they mostly happen when we switch to seqlen 1K (e.g., at step 900, 1900, 2900...); (2) they mostly happen during the ﬁrst 5K steps, and after that it becomes more stable than the seqlen 1K case. These observations indicate that training instability is strongly correlated with early long sequence lengths, which motivates us to explore the sequence length warmup method described in the next section, and evaluations in Section 5 will demonstrate how this method provides a gradient variance reduction effect and solves the stability-efﬁciency dilemma in our experiments. 4 The Sequence Length Warmup Method The analysis in last section about training instability and sequence lengths motivates us to explore sequence length warmup methods: the model needs to start learning with short sequence length for more stable training, then gradually increase the length when training becomes more stable so that the model can still learn from longer contextual information to achieve better ﬁnal model accuracy. The sequence length warmup strategy depends on two factors: how to support variable sequence length during training and how to adaptively decide the sequence length for each iteration (the pacing function). For the ﬁrst component, we develop an efﬁcient truncation-based implementation: For the baseline GPT-2 pre-training, the raw text inputs are indexed into sequences with the same length before training, so that the model can efﬁciently retrieve a batch of ﬁxed-length sequences regardless of the actual sentence boundaries. It’s possible to index the text inputs based on all possible sequence lengths, but that adds signiﬁcant amount of overhead due to the massive pre-training data. To avoid the large indexing overhead, we take a lightweight approach: our implementation still lets the dataloader index the raw text into only the full sequence length. At each training step, our method uses pacing function to determine the sequence length and then truncates the full-length sequences to obtain a modiﬁed version of the mini-batch for training. It is true that this truncation-based implementation will drop some data in the current step. However, with some implementation changes, it’s possible to record the index of dropped data and use them in future steps. We deﬁne the pacing function as a step-wise linear function with the following properties: Given a starting sequence length seqlens, an ending sequence length seqlene (full sequence length), and a total duration T (number of steps), the sequence length used for the training batch at step tis seqlent = seqlens + (seqlene −seqlens) ×min( t T ,1). Besides step-wise linear, we also explored 3 other pacing functions: i) We tried a discrete 2-stage pacing function from [ 30], but it leads 6to unstable training and worse convergence (Section 5.1). ii) We tried a step-wise root function (seqlent = seqlens + (seqlene −seqlens) ×min(( t T )r,1), where r is the root degree), which performs similar to linear but requires one extra hyperparameter. iii) We tried an adaptive pacing function based on training/validation losses, which also performs similar and requires extra tuning. 0 2K 4K 6K 8K 10K Steps 0 250 500 750 1000 1250 1500 1750 2000Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (a) Step-wise validation per- plexity (beginning of training) 0 50B 100B 150B Tokens 20.0 20.2 20.4 20.6 20.8 21.0Validation PPL SLW 20K SLW 60K SLW 100K SLW 140K (b) Token-wise validation per- plexity (end of training) Figure 3: Validation perplexity during GPT-2 117M seqlen 1K pre-training with batch size 512 and different duration T. (“SLW 20K” means proposed approach withT=20K steps). Pacing function analysis and tuning strategy: To study the impact of our approach’s pacing function, we set the starting sequence length (seqlens) ﬁxed at 8 and perform a grid search for the pacing function duration ( T) on the GPT-2 117M case full training (de- tails in Appendix A.4). After full train- ings we perform evaluation on pretrain data test set and WikiText/LAMBADA zero-shot tasks to conﬁrm which du- ration T provides the best accuracy performance. All the cases have quite comparable evaluation results, indicat- ing that the performance is not very sensitive to the duration T within a reasonable range. This grid search sheds light on a low-cost tuning strategy: we ﬁnd that (for GPT-2 117M training with batch size 512 and 4K) the best duration T is the longest duration that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst 10K steps (i.e., a few multiples of the LR warmup steps). In on our study, this “signiﬁcant ﬂuctuation” can be deﬁned as “whether the perplexity value becomes larger than 1.3x of the previous best perplexity”. In Figure 3(a) the SLW 60K is the longest duration we tested that does not have signiﬁcant validation ﬂuctuation during the ﬁrst 10K steps. In Figure 3(b) and Appendix A.4 SLW 60K does provide the best ﬁnal validation perplexity, best ﬁnal test perplexity, and second best eval results. Since it does not require training the model until full convergence, this heuristic greatly reduces the hyperparameter tuning cost of our approach. Another grid search on the starting sequence length seqlens shows that it’s generally better to set it as small as possible, to maximize the stability and convergence speedup beneﬁt. However, if the validation perplexity has signiﬁcant ﬂuctuation near the starting sequence length, increasing seqlens would lead to better convergence. Overall, the low-cost tuning strategy can be summarized as: (1) Start with seqlens = 8and T = a few multiples of LR warmup steps. (2) Increase seqlens until the validation perplexity no longer has signiﬁcant ﬂuctuation at the very beginning. (3) Perform a binary search to ﬁnd the largest T that does not have signiﬁcant validation perplexity ﬂuctuation during the ﬁrst few multiples of LR warmup steps. This tuning strategy relies only on the validation set and does not require test set or downstream task evaluation. For the GPT-2 1.5B and GPT-3 125M models, we used this strategy to tune T and seqlens for the pacing function, and results show that this low-cost tuning strategy could provide similar stability-efﬁciency beneﬁt as grid search on full training runs (GPT-2 117M case). 5 Evaluation 5.1 GPT-2 experiments For GPT-2 model, dataset, and hardware, we follow the same methodology in Section 3. For proposed work’s pacing function conﬁgurations (Section 4), we useseqlens = 8/64 (for 117M/1.5B model based on tuning) and seqlene = 1K/2K(full sequence length). To fully utilize the NVIDIA Tensor Core acceleration, we add a seqlent = seqlent −(seqlent mod 8) postprocessing to make sure the sequence length is always a multiple of 8. For the total duration T, we tune this parameter (grid search for 117M and low-cost tuning for 1.5B) for each case. For the training parameters, for our approach we use the same shared parameters as the baseline except two parameters: 1) Because during sequence length warmup the number of tokens in a data batch is smaller, we modify the training termination condition so that all cases stop when reaching the same 157B training tokens. 2) Because of 1), proposed approach now has more training steps, which make it necessary to modify the learning rate decay schedule to have a fair comparison with the baseline. We change the learning rate decay to token-wise over the 157B tokens (still cosine decay) instead of step-wise over the total number of steps. We describe the underlying rationale in Appendix A.2. 70 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (a) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2500 5000 7500 10000 12500 15000 17500Variance norm Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512 Bsz 512, SLW 270K Bsz 4K Bsz 4K, SLW 45K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (e) Token-wise valida- tion perplexity 0 60 120 180 240 300 360 Wall-clock time (hr) 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0Validation PPL Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 0 2000 4000 6000 8000 10000 12000 14000Variance norm Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 4K Bsz 4K, SLW 45K Bsz 4K, Shortformer Bsz 4K, Bsz Warmup (h) Step-wise Adam vari- ance max element Figure 4: Validation perplexity and Adam variance norm/max element during GPT-2 1.5B seqlen 1K pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR. Also compare with related works (“Shortformer” [30] and “Bsz Warmup” [6]) at 2nd row. Each row of subﬁgures share the same legend (“SLW 45K” means our work withT=45K steps). Based on the following observations, we demonstrate that our approach resolves the dilemma and simultaneously improves the stability and efﬁciency. We will mainly present the GPT-2 1.5B results and leave some GPT-2 117M results in Appendix. Signiﬁcant stability gain: In Section 3 Table 1 we discussed how we measure the training instability based on the “loss ratio” metric, which shows that the baseline becomes less stable under larger model size/batch size/learning rate/sequence length. Comparing with baseline and proposed work in this table shows that our work reduces this instability measurement to zero in all cases, together with max ratio close to 1.0 (no spike). This demonstrates the signiﬁcant stability gain by our method. Faster token-wise and time-wise convergence: Figure 4(a) and 4(b) present the validation perplex- ity curves during GPT-2 1.5B seqlen 1K pre-training, comparing baseline and our approach. When the batch size increases from 512 to 4K for baseline, the time-wise convergence becomes faster but the token-wise convergence becomes slower and poorer. On the other hand, our approach at batch size 4K provides faster and better convergence both token-wise and time-wise comparing with the best baseline curve in each case. The shape of SLW’s curves is different (worse than baseline in early stage) because SLW cases initially only learn from shorter sequences, which limit the validation perplexity it can reach (since validation data is always full-length). On the other hand, when SLW cases start to learn from longer sequences, the validation perplexity drops faster than baseline (and related works) and eventually surpasses them. Our approach with batch size 512 provides smaller convergence speedup because (1) Baseline with batch size 512 has less instability issue, limiting the gain from the proposed approach; (2) At batch size 512 the communication overhead is very high, and our approach takes more steps (i.e., communication rounds) than baseline to reach the same 157B training tokens. This extra communication cost “cancelled” part of the time saving from our approach. For GPT-2 117M, our approach provides similar token-wise and time-wise convergence speedup (Appendix A.4). Advancing cost-quality Pareto curve: In Section 3 Table 2 we discussed about baseline’s zero-shot evaluation results. For proposed work eval results in this table, we present them in two ways: one evaluated at the earliest checkpoint that provides better eval results than baseline (batch size 512 and seqlen 1K); the other one evaluated at the end of full training. Results show that our approach is able to advance the cost-quality Pareto curve in two ways: (1) To reach the same eval result quality as baseline, our approach reduces the required number of pre-training tokens and wall clock time by up to 2.2x and 3.7x, respectively; (2) Under the same 157B training tokens, our approach can further improve the eval result quality. In (1) the time-wise saving is higher than the token-wise because (a) For each Transformer block, the self-attention and intermediate layers have time complexity of O(B×L2 ×H) and O(B×L×H2), respectively6. The proposed method uses shorter sequences at 6B, L, Hrepresent batch size, sequence length, hidden size. 8the beginning, reducing the time complexity quadratically for the self-attention sub-layer and linearly for the intermediate sub-layer of Transformer blocks; (b) By enabling stable training at larger batch size, our approach achieves additional time-wise saving by reducing the communication overhead. Variance reduction helps stabilize training: In Section 3 we discussed the strong correlation between training instability and gradient variance norm/max. Figure 4(c) and 4(d) demonstrate that proposed approach stabilizes training and reduces both the Adam variance norm and the variance max element. Importantly, it avoids all the spikes of the variance max element, which all happen to be where the baseline has training loss spikes. One may wonder why gradient clipping cannot help avoid these extreme gradient variance outliers. Although gradient clipping can avoid large gradient at every single step, it cannot avoid the gradient variance getting accumulated from multiple steps (Appendix A.3.2). Comparing with related works: We now compare the proposed work with two related works on the most challenging “1.5B model + batch size 4K” case. The ﬁrst work is the Shortformer where the ﬁrst stage uses shorter sequences and the second stage uses full-length sequences [30]. Following the grid search in the paper, we use seqlen 128 for the ﬁrst stage and set its duration at about half of the baseline duration (20K steps). The second work is the batch size warmup technique used by GPT-3 [6], where we set the starting batch size at 128 and then gradually increase it to 4K, and set the warmup duration same as the proposed work. Other training hyperparameters are unchanged. Figure 4(e) to 4(h) present the results. Both related works provide convergence speedup but it is less than our work. More importantly, they still have training instability issues. The Shortformer has an obvious training divergence at step 20K when the sequence length switches from 128 to 1K (the spike at 20K in Figure 4(h)). This shows when staying at the same shorter sequence length for too long, the model becomes heavily overﬁtted for that length which leads to divergence risk when/after switching to full length. Although both batch size warmup and our method reduce the number of tokens per batch in a similar fashion, batch size warmup does not provide any training stability beneﬁt compared to the baseline. This indicates that providing the same number of shorter (simpler) sequences leads to better training stability than providing fewer number of same length (same difﬁculty) sequences. In addition, batch size warmup has a limitation that the batch size must be multiple of data-parallel size. On the other hand, for our method the sequence length only needs to be multiple of 8 to enable Tensor Core acceleration. These two limitations are different: for the proposed SLW method, the “multiple of 8” limitation is ﬁxed and unrelated to data-parallel size. For batch size warmup it’s a dynamic \"multiple of data-parallel size\" limitation: since nowadays pre-training tasks are performed on up to thousands of GPUs, the data-parallel size can easily go beyond 100, prohibiting ﬂexible conﬁguration of the method (or requires reducing the number of GPUs when using smaller batches, increasing the training clock time). Last but not least, both related works provide non-zero “loss ratio” in Table 1 and worse zero-shot evaluation results in Table 2. 5.2 GPT-3 experiments For experiments replicating the GPT-3 125M model [6] using the Pile public dataset [13], ﬁrst we reproduce the original GPT-3 training recipe: 300B training tokens, seqlen 2K, batch size 256 with batch size warmup (start with 16 then gradually increase to 256 in ﬁrst 4B tokens), learning rate 6 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (6 ×10−5 min. learning rate) 7. Then we explore an aggressive training scenario where only 30B tokens (10%) are allowed. This is because (1) GPT-3 paper admits that it has poor training sample efﬁciency and it sees much more text during pre-training than a human sees in the their lifetime [6, 23]. (2) There could exist cases where the total amount of data/computation resource is limited. We adjust several hyperparameters in this 30B-token training: 8x batch size (2K) for better training efﬁciency, learning rate decay reduced to 30B tokens (based on study that LR schedule should match total training tokens [14], warmup stays at 375M), min. learning rate reduced to 0 (based on recent study on GPT-3 [51]). For baseline we keep using 4B-token batch size warmup, but when our method is used (seqlens = 72, T = 11.5Kbased on tuning) we disable it since both methods reduce tokens per batch. And for both cases we tune the learning rate and use the highest one that provides stable training, which is 30x (1.8×10−2) for baseline and 40x (2.4×10−2) for our method. All experiments are performed on 128 V100 GPUs. Figure 5 and 6 present the training loss and gradient variance max for the GPT-3 pre-training experiments. When applying 40x learning rate to the baseline (batch size warmup), it quickly 7Different from GPT-2, GPT-3 uses token-based learning rate schedule and we follow it. 9Table 4: Zero-shot evaluation of the trained GPT-3 125M models on 11 tasks used by the original GPT-3 work [6]. Per- task eval results in Appendix A.5. Batch Training Training Average Case size tokens time accuracy ↑ 1: Original [6] 256 300B 33.6 2: Baseline repro 256 300B (1x) 61Hr 31.4 3: Baseline 30x LR 2K 30B (10x) 7Hr (9x) 29.8 (95%) 4: SLW 40x LR 2K 30B (10x) 3.5Hr (17x) 31.1 (99%) 0 250M 500M 750M 1B Tokens 2 3 4 5 6 7 8 9 10Training loss Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 5: Training loss during GPT-3 125M pre-training (ﬁrst 1B to- kens). 0 250M 500M 750M 1B Tokens 0.00 0.05 0.10 0.15 0.20 0.25Variance max Baseline repro Baseline 40x LR Baseline 30x LR SLW 40x LR Figure 6: Gradient vari- ance max during GPT-3 125M pre-training (ﬁrst 1B tokens). diverges and cannot continue to train due to NaN losses. The corresponding gradient variance max element becomes a ﬂat line after divergence because the gradients on all dimensions are so large that all gradients get clipped including the max element. After lowering the learning rate to 30x, the baseline is able to ﬁnish the whole training, but it can only retain 95% of average zero-shot accuracy on 11 tasks (HellaSwag [53], LAMBADA [28], TriviaQA [15], WebQs [3], Winogrande [38], PIQA [4], ARC Challenge/Easy [10], ANLI R1/R2/R3 [27]) compared with the case that reproduces the original GPT-3 training recipe8 as shown in Table 4. In contrast, our approach enables stable training with 40x learning rate, demonstrates lower gradient variance max outliers than baseline with 30x learning rate, retains 99% of the original training recipe’s average zero-shot accuracy, and achieves 10x data saving and 17x time saving.9 This demonstrates that the proposed method not only solves the stability-efﬁciency dilemma, but also opens a promising direction of signiﬁcantly reducing total training cost in a different data efﬁciency dimension. Finally, in Appendix A.6 we evaluate the proposed method on a larger GPT-3 1.3B model, including not only zero-shot but also few-shot evaluation. Results show that under the same 300B training tokens the proposed SLW method provides better average accuracy than the baseline for both zero- shot (from 41.6 to 41.9) and few-shot (from 44.8 to 45.3) tasks, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) can provide better accuracy performance. Similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup and proposed SLW method. 6 Limitation and Future Work Although our paper discovers the correlation between long sequences and training instability (and proposes a method to alleviate the issue), the root cause of this causal relationship is not fully deciphered and would be an interesting future work direction. One assumption we have is that the shorter sequences are not necessarily easier but can be viewed as simpler examples since there are less context to embed. We think encoding shorter sequences (especially at the early training stage when weight is still relatively random) will generate less \"noisy\" gradients thus leading to higher stability. This work presents correlation analysis on an empirical connection between training instability and gradient variance norm/max element, but it is not sufﬁcient to prove a causal relationship, and training instability could be caused by other factors. Overall, our work is one preliminary attempt to study the topic of training instability in large-scale model training, and we hope it could inspire future works to further study this important issue in both theory and practice. 7 Conclusion This paper presents the Sequence Length Warmup method, which solves a stability-efﬁciency dilemma inside GPT-style model pre-training, an critical issue that hinders efﬁcient language model pre-training as explained in our in-depth analysis. By enabling stable training on more aggressive training recipe, this method also motivates a new dimension of training cost reduction by improving the data efﬁciency, as demonstrated by the 10x data and 17x time saving in our GPT-3 experiments. We believe that the effectiveness, simplicity, and easy-to-use/tune make the proposed method a must-try for deep learning practitioners, and we hope this work could motivate more studies on improving training data efﬁciency. 8Our reproduced GPT-3 has 2.2 point lower average accuracy than the original GPT-3, which is because of the different training data and OpenAI employed many data processing techniques [6] 9We want to emphasize here that the SLW method “only retains 99% of accuracy” because this experiment is an extreme case: only 10% of original training data is used during training. 10References [1] Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. In International Conference on Machine Learning, pages 176–185. PMLR, 2017. [2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48, 2009. [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533–1544, 2013. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- ical commonsense in natural language. In Proceedings of the AAAI conference on artiﬁcial intelligence, pages 7432–7439, 2020. [5] Ondˇrej Bojar, Jindˇrich Helcl, Tom Kocmi, Jindˇrich Libovick`y, and Tomáš Musil. Results of the wmt17 neural mt training task. In Proceedings of the second conference on machine translation, pages 525–533, 2017. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020. [7] Daniel Campos. Curriculum learning for language modeling. arXiv preprint arXiv:2108.02170, 2021. [8] Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick. Control regularization for reduced variance reinforcement learning. In International Conference on Machine Learning, pages 1141–1150. PMLR, 2019. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. [12] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99, 1993. [13] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. [17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. 11[18] Tom Kocmi and Ondˇrej Bojar. Curriculum learning and minibatch bucketing in neural machine translation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 379–386, 2017. [19] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS, volume 1, page 2, 2010. [20] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [21] Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit LAMB: Communication Efﬁcient Large-Scale Large-Batch Training with LAMB’s Convergence Speed. arXiv preprint arXiv:2104.06069, 2021. [22] Conglong Li, Minjia Zhang, and Yuxiong He. Curriculum learning: A regularization method for efﬁcient and stable billion-scale gpt model pre-training. arXiv preprint arXiv:2108.06084, 2021. [23] Tal Linzen. How can we accelerate progress towards human-like linguistic generalization? arXiv preprint arXiv:2005.00955, 2020. [24] Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh. Variance reduction for reinforcement learning in input-driven environments. In International Conference on Learning Representations, 2018. [25] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. [26] Microsoft and Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to- train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful- generative-language-model/, 2021. [27] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599, 2019. [28] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [29] Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabás Póczos, and Tom M Mitchell. Competence-based curriculum learning for neural machine translation. In NAACL- HLT, 2019. [30] Oﬁr Press, Noah A Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832, 2020. [31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [32] Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, and Ilya Sutskever. Better language models and their implications. OpenAI Blog, 2019. [33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza- tions toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020. [36] Mrinmaya Sachan and Eric Xing. Easy questions ﬁrst? a case study on curriculum learning for question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 453–463, 2016. 12[37] Mrinmaya Sachan and Eric Xing. Self-training for jointly learning to ask and answer questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 629–640, 2018. [38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, pages 8732–8740, 2020. [39] Terence D Sanger. Neural network learning control of robot manipulators using gradually increasing task difﬁculty. IEEE transactions on Robotics and Automation, 10(3):323–333, 1994. [40] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [41] Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V . Le. Don’t decay the learning rate, increase the batch size. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [42] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. [43] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit Adam: Communication Efﬁcient Large-Scale Training with Adam’s Convergence Speed. InProceedings of the 38th International Conference on Machine Learning, pages 10118–10129, 2021. [44] Yi Tay, Shuohang Wang, Anh Tuan Luu, Jie Fu, Minh C Phan, Xingdi Yuan, Jinfeng Rao, Siu Cheung Hui, and Aston Zhang. Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4922–4931, 2019. [45] Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information processing systems, pages 5998–6008, 2017. [47] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019. [48] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization. In NIPS, 2013. [49] Thomas Wolf. The engineering group in @bigsciencew ﬁghting training instabilities over +100b parameters. https://twitter.com/Thom_Wolf/status/1447565680384032776, 2021. [50] Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, and Yongdong Zhang. Curriculum learning for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6095–6104, 2020. [51] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. [52] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations, 2020. [53] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019. 13[54] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9054–9065, 2019. [55] Wei Zhang, Wei Wei, Wen Wang, Lingling Jin, and Zheng Cao. Reducing bert computation by padding removal and curriculum learning. In 2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 90–92. IEEE, 2021. [56] Xuan Zhang, Gaurav Kumar, Huda Khayrallah, Kenton Murray, Jeremy Gwinnup, Marianna J Martindale, Paul McNamee, Kevin Duh, and Marine Carpuat. An empirical exploration of curriculum learning for neural machine translation. arXiv preprint arXiv:1811.00739, 2018. [57] Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. Curriculum learning for domain adaptation in neural machine translation. In NAACL-HLT, 2019. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] We have a section describing limitation and future work. (c) Did you discuss any potential negative societal impacts of your work? [N/A] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main ex- perimental results (either in the supplemental material or as a URL)? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed 10, and Section 3 and 5 include all instructions needed to reproduce the experiments. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Section 3 and 5 include all the training details. (c) Did you report error bars (e.g., with respect to the random seed after running ex- periments multiple times)? [N/A] Our results are based on a single seed given the pre-training is expensive. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Section 3 and 5 include the information. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We open sourced the code in a deep learning optimization library called DeepSpeed. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 10https://github.com/microsoft/DeepSpeed 14(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 150 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (a) 117M training loss 0 10B 20B 30B Tokens 2 3 4 5 6 7 8Training loss Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (b) 1.5B training loss 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (c) 117M variance norm 0 10B 20B 30B Tokens 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (d) 1.5B variance norm 0 10B 20B 30B Tokens 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K Bsz 512, seqlen 2K (e) 117M variance max 0 10B 20B 30B Tokens 0.00 0.02 0.04 0.06 0.08 0.10Variance max Bsz 512, seqlen 1K Bsz 4K, seqlen 1K (f) 1.5B variance max 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance norm (g) Var norm correlation 0 10B 20B 30B Tokens 0.0 0.2 0.4 0.6 0.8 1.0 1.2Normalized value 1.5B,Bsz 4K Loss ratio Variance max (h) Var max correlation Figure 7: Training loss, Adam variance norm/max element, and correlations between loss spikes and variance norm/max during GPT-2 pre-training (without the proposed method) under different model sizes, batch sizes (and LR), and sequence lengths. A Appendix A.1 Zoom in of Figure 1 Figure 7 zoom in the ﬁrst 30B token in main paper Figure 1, where the training is the most unstable. A.2 Learning rate decay for proposed approach As discussed in main paper Section 5.1 GPT-2 experiments, proposed approach needs more training steps than baseline in order to reach the same 157B training tokens. This makes it necessary to modify the learning rate decay schedule for proposed approach. We ﬁrst tried to increase the number of learning rate decay steps by half of the proposed approach’s pacing function durationT (since the proposed approach roughly needs T/2 additional steps to reach 157B tokens). However, we ﬁnd that simply increasing decay steps still leads to faster learning rate decay than baseline. At last we change the learning rate decay to token-wise (same cosine decay over the 157B tokens) instead of step-wise. This is because for the proposed approach there are fewer tokens per step at the beginning. So even if we increase the LR decay steps, it still cannot avoid decaying faster token-wise at the beginning compared to baseline. As shown in Figure 8, the proposed approach with step-wise LR decay (with T/2 extra decay steps) has faster LR decay token-wise compared to baseline, which leads to a worse validation perplexity curve. On the other hand, the same proposed approach case with token-wise LR decay has the same token-wise LR decay schedule as baseline, which leads to better convergence. A.3 Additional analysis about training hyperparameters In main paper Section 4 we demonstrate that proposed approach’s two hyperparameters can be tuned with very low cost only running the very beginning of the training (the third hyperparameter, ending sequence length, does not require tuning since it will always be the full length). To understand more about how proposed approach affects the choice and tuning of normal training hyperparameters, this section provides additional analysis about learning rates and gradient clipping. Results demonstrate that (a) Compared to baseline, proposed approach requires less tuning effort on these hyperparameters to provide a stable training; (b) By enabling stable training on larger learning rates, proposed approach could provide better training efﬁciency and convergence (as demonstrated in main paper Section 5); (c) Tuning gradient clipping for baseline could not provide the same training stability as proposed approach. A.3.1 Learning rate In Section 5.1 we demonstrate that proposed approach can provide stable and more efﬁcient training at larger batch size and learning rate, where baseline suffers from training instability. We increased both batch size and learning rate at the same time because (a) Large-batch training is more efﬁcient 160 40B 80B 120B 160B Tokens 11.5 12.0 12.5 13.0 13.5 14.0Validation PPL Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (a) Token-wise validation perplexity 0 100K 200K 300K 400K Steps 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (b) Step-wise learning rate 0 50B 100B 150B Tokens 0.0e+00 2.0e-05 4.0e-05 6.0e-05 8.0e-05 1.0e-04 1.2e-04 1.4e-04 1.6e-04 Learning Rate Baseline SLW 270K, token-wise LR decay SLW 270K, step-wise LR decay (c) Token-wise learning rate Figure 8: Validation perplexity and learning rate during GPT-2 1.5B seqlen 1K pre-training with batch size 512, comparing the baseline and proposed approach under different learning rate decay schedules. “SLW 270K” means proposed approach withT=270K steps. 0 1K 2K 3K Steps 3 4 5 6 7 8 9 10 11 12Training loss Baseline LR=6e-4 Baseline LR=12e-4 SLW 8K LR=6e-4 SLW 8K LR=12e-4 Figure 9: Step-wise training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, seed 1236, and different learning rates for baseline and proposed approach (“SLW 8K” means proposed approach with T=8K steps). for large-scale distributed training, so larger batch was necessary in our study (b) In order to maintain the same convergence speed, it is necessary to simultaneously increase the learning rate under larger batch size. A well-known rule of thumb is that the learning rate should at least increase by the square root of the batch size’s increase ratio. As a controlled experiment, here we perform additional studies about what if we keep the batch size the same and only tune learning rate for baseline and proposed approach. We do not consider the case of “same learning rate, different batch sizes” due to the reason (b) above. Table 5 presents the number of steps with training loss ratios (deﬁned in main paper Section 3 as an indicative measurement of training instability) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K11, 5 different seeds, and different learning rates for baseline and proposed approach. And Figure 9 illustrates some of the cases with seed 1236 to show how the loss spikes look 11Batch size 2K is used here because this analysis was performed at an early stage of this work, and we do not have enough resource to rerun the same analysis with batch size 4K. 17Table 5: Number of steps with training loss ratios (deﬁned in Section 3) larger than 1.5 during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 3K steps only) with batch size 2K, 5 different seeds, and different learning rates for baseline and proposed approach (SLW). Left/right number in each cell is for baseline/SLW, respectively. Baseline/SLW LR = LR = LR = LR = #loss ratio > 1.5 1.5 × 10−4 3 × 10−4 6 × 10−4 12 × 10−4 Seed 1234 0/0 296/0 359/0 179/74 Seed 1235 0/0 302/0 408/0 555/459 Seed 1236 0/0 0/0 569/0 626/414 Seed 1237 7/0 0/0 548/0 614/139 Seed 1238 0/0 0/0 121/0 394/29 Total 7/0 598/0 2005/0 2368/1115 like. Results show that proposed approach provides stable training during this ﬁrst 3K steps for all ﬁve seeds at learning rates up to 6 ×10−4, while baseline with seed 1237 still has 7 large loss ratios at learning rate as low as 1.5 ×10−4. At learning rate 12 ×10−4 both cases have large loss ratios, but proposed approach reduces the frequency by 2.1x. This demonstrates that (a) Larger learning rates lead to higher training instability risk for both cases. (b) With the same amount of tuning effort, proposed approach has a higher probability of providing a stable training because of the wider range of learning rates it enables; (c) Since proposed approach enables stable training at larger learning rate, it could provide better and faster training convergence as shown in main paper Section 5. A.3.2 Gradient clipping In main paper Section 5 we used gradient clipping at 1.0 (global gradient l2 norm is clipped to 1.0) following the previous work [40]. Here we perform additional studies about what if we apply more gradient clipping to baseline. Figure 10(a) presents the training loss during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K (the same hyperparameters as the second set in Section 3), comparing the baseline and proposed approach under different gradient clipping levels12. Results show that when applying more gradient clipping to baseline, the training has less and smaller loss spikes. And the Adam varaince norm is also reduced as shown in Figure 10(c). However, more gradient clipping does not fully resolve the training instability issue. Even baseline with the lowest gradient clipping norm cannot avoid all training loss spikes, while proposed approach with default gradient clipping has no loss spike. As described in main paper, we believe that this is a limitation of common gradient clipping technique: Although gradient clipping can avoid too large gradient at every single step, it cannot avoid the gradient variance getting accumulated at certain dimensions (as shown in Figure 10(d)), especially for large batch sizes. Another concern about applying more gradient clipping is that the momentum norm is also reduced due to more clipping (Figure 10(b)). This indicates that when later the training reaches a more stable stage, more gradient clipping could hurt the convergence speed. On the other hand, proposed approach will not affect the convergence speed after the full sequence length is reached. Another thing to note is that proposed approach relies less on gradient clipping: at gradient clipping norm 1.0, baseline has 798 clippings in the ﬁrst 5K steps while proposed approach has 628 clippings (21% less). Overall, this analysis demonstrates that proposed approach requires less or no tuning on gradient clipping, while baseline still has training stability issue with more gradient clipping. It is possible that more complex and adaptive gradient/variance/activation clipping techniques could potentially achieve the same level of training stability as proposed approach. However, inventing and applying such techniques would require an effort no lower than the proposed approach, which is both easy to integrate and low-cost to tune. A.4 GPT-2 117M evaluation results Figure 11 presents the validation perplexity and Adam variance norm/max element during GPT-2 117M pre-training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. Table 6 presents the zero-shot evaluation of the trained 117M models on 12We also tried less than 0.25 gradient clipping, which triggered a silent crash without error messages after around 100 steps. We did not have enough time to ﬁnd the root cause, but it could be caused by the too extreme gradient clipping. 180 1K 2K 3K 4K 5K Steps 2 4 6 8 10 12 14 16 18Training loss Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (a) Step-wise training loss 0 1K 2K 3K 4K 5K Steps 0 500 1000 1500 2000 2500 3000Momentum norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (b) Step-wise Adam momentum l1 norm 0 1K 2K 3K 4K 5K Steps 0 2500 5000 7500 10000 12500 15000 17500 20000Variance norm Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (c) Step-wise Adam variance l1 norm 0 1K 2K 3K 4K 5K Steps 0.00 0.01 0.02 0.03 0.04 0.05 0.06Variance max Baseline grad clip 1.0 Baseline grad clip 0.5 Baseline grad clip 0.25 SLW 45K grad clip 1.0 (d) Step-wise Adam variance max element Figure 10: Training loss, Adam momentum l1 norm, and Adam variance l1 norm/max element during GPT-2 1.5B seqlen 1K pre-training (ﬁrst 5K steps only) with batch size 4K, comparing the baseline and proposed approach under different gradient clipping levels. Grad clip 1.0 indicates that the global gradient l2 norm is clipped to 1.0. ‘SLW 45K” means proposed approach withT=45K steps. 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (a) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (b) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (c) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 4K, Seqlen 1K Bsz 4K, Seqlen 1K, SLW 30K (d) Step-wise Adam vari- ance max element 0 50B 100B 150B Tokens 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (e) Token-wise valida- tion perplexity 0 10 20 30 40 Wall-clock time (hr) 20 22 24 26 28 30Validation PPL Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (f) Time-wise validation perplexity 0 8K 16K 24K 32K 40K Steps 1000 2000 3000 4000 5000 6000Variance norm Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (g) Step-wise Adam vari- ance norm 0 8K 16K 24K 32K 40K Steps 0.005 0.010 0.015 0.020 0.025Variance max Bsz 512, Seqlen 1K Bsz 512, Seqlen 1K, SLW 60K Bsz 512, Seqlen 2K Bsz 512, Seqlen 2K, SLW 110K (h) Step-wise Adam vari- ance max element Figure 11: Validation perplexity and Adam variance norm/max element during GPT-2 117M pre- training, comparing the baseline and proposed work (SLW) under different batch sizes/LR and sequence lengths. “SLW 60K” means proposed work withT=60K steps. the WikiText-103 and LAMBADA datasets for baseline and proposed work with different pacing function duration. A.5 GPT-3 125M evaluation results Table 7 presents the zero-shot evaluation of the trained GPT-3 125M models on the 11 tasks used by the original GPT-3 work [6]. 19Table 6: Zero-shot evaluation of the GPT-2 117M models on the WikiText-103 and LAMBADA datasets, following the evaluation methodology from [40]. Case Pre-training Pre-training Pre-training WikiText-103 LAMBADA parameters steps, tokens, time test perplexity ↓ perplexity ↓ accuracy ↑ 1: Baseline bsz512-seqlen1K 300K, 157B, 37Hr 20.75 27.78 33.19% 2: SLW 20K bsz512-seqlen1K 310K, 157B, 30Hr 20.49 27.43 34.60% 3: SLW 60K bsz512-seqlen1K 330K, 157B, 33Hr 20.11 27.01 34.41% 4: SLW 100K bsz512-seqlen1K 350K, 157B, 35Hr 20.16 26.91 34.21% 5: SLW 140K bsz512-seqlen1K 370K, 157B, 35Hr 20.17 27.17 33.92% 6: Baseline bsz4K-seqlen1K 37.5K, 157B, 16Hr 20.99 28.09 32.54% 7: SLW 10K bsz4K-seqlen1K 42.5K, 157B, 16Hr 20.34 27.22 33.98% 8: SLW 20K bsz4K-seqlen1K 47.5K, 157B, 16Hr 20.25 27.13 34.54% 9: SLW 30K bsz4K-seqlen1K 52.5K, 157B, 16Hr 20.22 27.15 34.16% 10: SLW 40K bsz4K-seqlen1K 57.5K, 157B, 16Hr 20.26 27.11 33.53% 13: Baseline bsz512-seqlen2K 150K, 157B, 32Hr 20.87 28.19 32.99% 15: SLW 70K bsz512-seqlen2K 185K, 157B, 31Hr 19.82 26.04 33.46% 17: SLW 110K bsz512-seqlen2K 205K, 157B, 31Hr 19.64 26.03 34.58% 18: SLW 150K bsz512-seqlen2K 215K, 157B, 32Hr 19.64 25.99 33.32% 15: SLW 190K bsz512-seqlen2K 245K, 157B, 33Hr 19.64 26.09 33.09% Table 7: GPT-3 125M zero-shot evaluation results Baseline Baseline SLW Case Original [6] repro 30x LR 40x LR Model size 125M 125M 125M 125M Train tokens 300B 300B 30B 30B Batch size 256 256 2K 2K Bsz warmup 4B 4B 4B N/A LR 6e-4 6e-4 1.8e-2 2.4e-2 min LR 6e-5 6e-5 0 0 LR warmup 375M 375M 375M 375M LR decay 260B 260B 30B 30B decay style cosine cosine cosine cosine SLW N/A N/A N/A 11.5K steps Avg. accuracy 33.6 31.4 29.8 31.1 (0) HellaSwag 33.7 30.4 28.2 28.9 (1) LAMBADA 42.7 39.3 30.4 34.2 (2) TriviaQA 4.15 1.72 0.76 1.45 (3) WebQs 1.77 0.197 0 0.394 (4) Winogrande 52.0 49.3 50.9 51.9 (5) PIQA 64.6 61.9 59.8 62.7 (6) ARC Challenge 26.6 23.3 21.7 22.3 (7) ARC Easy 43.6 39.9 36.0 39.1 (8) ANLI R1 33.4 32.8 33.1 33.4 (9) ANLI R2 33.2 33.3 33.3 33.6 (10) ANLI R3 33.6 33.3 33.2 34.7 A.6 GPT-3 1.3B evaluation results In this section we evaluate the proposed SLW method on the larger GPT-3 1.3B model. Compared to the GPT-3 125M evaluation in main paper section 5.2 there are two differences on the setup: (1) The GPT-3 125M evaluation aims to explore whether the proposed method can retain the accuracy performance while greatly reducing the training tokens, while this GPT-3 1.3B evaluation aims to explore that, under same amount of training tokens, does proposed method provides better training stability and better accuracy performance. (2) To improve the training data quality, for GPT-3 1.3B pre-training we added two additional sub-datasets (CC-Stories [45] and RealNews [54]), together with additional data cleaning on all data following the process in [42]. Similar to previous experiments, we test two set of hyperparameters on both baseline and proposed method: The ﬁrst set follows the original GPT-3 setup: 300B training tokens, seqlen 2K, batch size 512 (baseline case includes batch size warmup that starts with 16 then gradually increase to 512 in ﬁrst 8B tokens), learning rate 2 ×10−4 with a linear warmup of 375M tokens and a single cycle cosine decay over 260B tokens (2 ×10−5 min. learning rate). The second set changes the batch size to 4K (8x) and learning rate to 8 ×10−4 (4x). 20Table 8: GPT-3 1.3B zero-shot evaluation results Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 44.4 41.6 41.9 (0) LAMBADA 63.6 63.7 65.0 (1) TriviaQA 19.7 10.1 11.3 (2) WebQs 4.63 3.25 2.36 (3) PIQA 75.1 73.4 73.8 (4) RACE-h 40.9 35.6 37.1 (5) BoolQ 62.4 63.4 61.8 The baseline case only enables stable training on the ﬁrst set of hyperparameters. Under larger batch size and learning rate, a training divergence (similar to main paper Figure 5 blue line) happened and the training cannot continue. On the other hand, the proposed SLW method is able to provide stable training under 8x larger batch size and 4x larger learning rate. Under the same number of training tokens, the 8x larger batch size leads to better training efﬁciency and 2x training time speedup, similar to what we obserbe in GPT-2 pre-training (main paper Table 2 case 10 vs. 15). This demonstrate the stability-efﬁciency beneﬁt of the proposed method. In addition, Table 8 and 9 present the zero-shot and few-shot evaluations of the trained GPT-3 1.3B models on 6 tasks used by the original GPT-3 work [6]: LAMBADA [28], TriviaQA [15], WebQs [3], PIQA [4], RACE-h [20], BoolQ [47]. Results show that similar to the original GPT-3, under few-shot prompts the average accuracy is better than zero-shot results for both models trained with baseline batch size warmup (from 41.6 to 44.8) and proposed SLW method (from 41.9 to 45.3).13 The change on each task also follows the same pattern: TriviaQA and WebQs accuracy improve a lot under few-shot; PIQA, RACE-h, and BoolQ have similar accuracy under zero and few-shot; LAMBADA accuracy becomes worse under few-shot. More importantly, under the same 300B training tokens the proposed SLW method provides better average accuracy (zero-shot from 41.6 to 41.9, few-shot from 44.8 to 45.3) than the baseline, demonstrating that the proposed method (in addition to the stability-efﬁciency beneﬁt) is able to provide better accuracy performance. 13Similar to main paper section 5.2, our reproduced GPT-3 baseline has 2.9/3.3 point lower average zero/few- shot accuracy than the original GPT-3, which is because of the different training data and OpenAI employed special data processing techniques [6] 21Table 9: GPT-3 1.3B few-shot evaluation results. k denotes the number of shots following the original GPT-3 work [6]. Baseline SLW Case Original [6] repro 8x Bsz Model size 1.3B 1.3B 1.3B Train tokens 300B 300B 300B Batch size 512 512 4K Bsz warmup 8B 8B N/A LR 2e-4 2e-4 8e-4 min LR 2e-5 2e-5 2e-5 LR warmup 375M 375M 375M LR decay 260B 260B 260B decay style cosine cosine cosine SLW N/A N/A 11K steps Avg. accuracy 48.1 44.8 45.3 (0) LAMBADA (k=15) 57.0 58.8 59.7 (1) TriviaQA (k=64) 32.1 19.2 19.0 (2) WebQs (k=64) 19.6 18.4 19.4 (3) PIQA (k=50) 74.3 74.2 72.8 (4) RACE-h (k=10) 41.4 35.0 37.6 (5) BoolQ (k=32) 64.1 63.2 63.2 22",
      "meta_data": {
        "arxiv_id": "2108.06084v4",
        "authors": [
          "Conglong Li",
          "Minjia Zhang",
          "Yuxiong He"
        ],
        "published_date": "2021-08-13T06:32:53Z",
        "pdf_url": "https://arxiv.org/pdf/2108.06084v4.pdf",
        "github_url": "https://github.com/NVIDIA/Megatron-LM"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper identifies and addresses the \"stability-efficiency dilemma\" in pre-training large-scale autoregressive language models (GPT-style models), where increasing batch sizes and learning rates for efficiency leads to training instability and poor generalization. They discover a strong correlation between training instability and extreme gradient variance values, particularly caused by long sequence lengths at the beginning of training. To solve this, they propose the Sequence Length Warmup (SLW) method, which starts training with short sequences and gradually increases the length, effectively avoiding extreme gradient variance. They also introduce a lightweight hyperparameter tuning strategy for SLW. Empirically, SLW enables stable training with 8x larger batch sizes and 4x larger learning rates for GPT-2 models, reducing required training tokens by up to 2.2x and wall clock time by up to 3.7x. For GPT-3 (125M), SLW allows 8x larger batch size and 40x larger learning rate, retaining 99% of zero-shot accuracy with 10x less data and 17x less time, where baseline training diverges.",
        "methodology": "The methodology involves an in-depth analysis of GPT-2 pre-training stability under varying batch sizes, learning rates, and sequence lengths. Training instability is quantitatively measured by a \"loss ratio\" (current step loss / minimum previous loss), and its correlation with the l1 norm and max element of Adam's variance state is analyzed. This analysis reveals that long sequence lengths, especially early in training, significantly contribute to extreme gradient variance and instability. Based on this, the Sequence Length Warmup (SLW) method is proposed. SLW uses an efficient truncation-based implementation where the dataloader initially provides full-length sequences, which are then truncated to a shorter length determined by a step-wise linear pacing function: `seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`. A low-cost tuning strategy is developed for SLW's hyperparameters (`seqlens` and `T`) by monitoring validation perplexity fluctuations during the initial training steps, avoiding expensive full training runs. The method's effectiveness is compared against baseline training, Shortformer's two-stage approach, and GPT-3's batch size warmup.",
        "experimental_setup": "Experiments were conducted on GPT-2 (117M and 1.5B parameters) and GPT-3 (125M and 1.3B parameters) models, using 128 NVIDIA V100 GPUs (32GB memory) configured in 16 nodes with NVLink 2.0 and InfiniBand EDR. For GPT-2, the NVIDIA Megatron-LM training pipeline was followed, utilizing a dataset blend of Wikipedia, CC-Stories, RealNews, and OpenWebtext. Training parameters included batch sizes of 512 and 4K, learning rates of 1.5e-4 and 6e-4, and sequence lengths of 1K and 2K. All experiments used mixed precision/FP16 training, Adam optimizer, 0.01 weight decay, and gradient clipping at 1.0. For GPT-3, experiments replicated the 125M and 1.3B models using the Pile public dataset (with additional data for 1.3B). An aggressive training scenario for GPT-3 125M used 10% of the original training data (30B tokens), batch size 2K, and increased learning rates (30x for baseline, 40x for SLW). Validation was based on perplexity, and zero-shot accuracy was evaluated on WikiText-103/LAMBADA for GPT-2, and 11 tasks (HellaSwag, LAMBADA, TriviaQA, WebQs, Winogrande, PIQA, ARC Challenge/Easy, ANLI R1/R2/R3) for GPT-3. GPT-3 1.3B also included few-shot evaluations on 6 tasks.",
        "limitations": "The paper acknowledges that while it discovers a correlation between long sequences and training instability, the precise root cause of this causal relationship is not fully deciphered. The authors hypothesize that shorter sequences are ",
        "future_research_directions": "Future research could delve deeper into deciphering the precise root cause of the causal relationship between long sequence lengths and training instability. The authors also hope this work inspires further studies on training instability in large-scale model training, both theoretically and practically. Additionally, investigating whether more complex and adaptive gradient/variance/activation clipping techniques could achieve a similar level of training stability as the proposed Sequence Length Warmup method is a potential area for exploration.",
        "experimental_code": "# File Path: examples/multimodal/dataset_helpers.py\ndef search_for_fit(numbers: List[int], capacity: int) -> int:\n    \"\"\"Finds the index of largest number that fits into the knapsack with the given capacity.\"\"\"\n    index = bisect.bisect(numbers, capacity)\n    return -1 if index == 0 else (index - 1)\n\n# File Path: examples/multimodal/dataset_helpers.py\ndef greedy_knapsack(item_sizes: List[int], samples: List, max_capacity: int) -> List:\n    \"\"\"Greedy algorithm with binary search for the knapsack problem.\n\n    Pack as many samples as possible given a maximum capacity and capacities of individual samples.\n    Used if sequence packing is enabled.\n    \"\"\"\n    assert len(item_sizes) == len(samples), \"sample lengths and samples must have the same length.\"\n\n    knapsacks = []\n\n    if len(item_sizes) == 0:\n        return knapsacks\n\n    # Sort sample lengths and samples together.\n    sorted_item_sizes, sorted_samples = zip(*sorted(zip(item_sizes, samples), key=lambda x: x[0]))\n    sorted_item_sizes = list(sorted_item_sizes)\n    sorted_samples = list(sorted_samples)\n\n    # Check if all samples fit in the knapsack capacity.\n    if sorted_item_sizes[-1] > max_capacity:\n        raise ValueError(f\"knapsack: A sample is larger {sorted_item_sizes[-1]} than the max_sequence_length {max_capacity}.\")\n\n    while sorted_item_sizes:\n        current_knapsack = []\n        remaining_capacity = max_capacity\n\n        while True:\n            idx = search_for_fit(sorted_item_sizes, remaining_capacity)\n            if idx == -1:\n                break   # Can't fit more samples.\n\n            remaining_capacity -= sorted_item_sizes[idx]\n\n            sorted_item_sizes.pop(idx)\n            sample = sorted_samples.pop(idx)\n            current_knapsack.append(sample)\n\n        knapsacks.append(current_knapsack)\n\n    return knapsacks\n\n# File Path: examples/multimodal/dataset_helpers.py\n    def _truncate_for_packing(self, input_ids, target, num_tiles):\n        \"\"\"Truncate tokens and labels if they exceed packing sequence length.\"\"\"\n        total_num_images = len(num_tiles)\n        total_num_tiles = sum(num_tiles)\n        total_img_embeddings_len = total_num_tiles * self.num_image_embeddings_per_tile\n        max_text_tokens = self.packing_seq_length - total_img_embeddings_len + total_num_images\n\n        input_ids = input_ids[:max_text_tokens]\n        target = target[:max_text_tokens]\n\n        # If truncate causes all labels to be ignored, then skip the sample\n        if (target == IGNORE_INDEX).all():\n            raise ValueError(f\"all targets will be ignored after truncation: {input_ids}\")\n\n        return input_ids, target\n\n# File Path: examples/multimodal/dataset_helpers.py\n    @stateless\n    def pack_selected_samples(self, samples: List[ImageTaskSample]) -> List[ImageTaskSamplePacked]:\n        \"\"\"\n        Function to pack a list of ImageTaskSample into a single ImageTaskSamplePacked.\n\n        NOTE: Energon dataloader calls this method internally if packing is used.\n        Please see https://nvidia.github.io/Megatron-Energon/advanced/packing.html\n\n        Args:\n            samples: List of ImageTaskSample instances to pack into one sample.\n\n        Returns:\n            ImageTaskSamplePacked instance.\n        \"\"\"\n        packing_seq_len = self.packing_seq_length\n\n        packed_tokens = []\n        packed_labels = []\n        packed_imgs = []\n\n        current_length = 0\n        max_length = 0\n        cu_lengths = [0]\n\n        # Process each sample and build lists that we will concatenate to create the packed sample.\n        for _, sample in enumerate(samples):\n            sample_len = sample.total_len\n\n            if sample_len > max_length:\n                max_length = sample_len\n\n            # If adding this sample exceeds the max length, stop.\n            # This should not happen. The select_samples_to_pack method should have already ensured that the samples fit.\n            if current_length + sample_len > packing_seq_len:\n                raise ValueError(f\"Packed sample exceeds the maximum sequence length of {packing_seq_len}: {samples}\")\n\n            # Add the sample's tokens and labels\n            packed_tokens.append(sample.tokens)\n            packed_labels.append(sample.labels)\n\n            # Add the images\n            packed_imgs += sample.imgs\n\n            current_length += sample_len\n            cu_lengths.append(current_length)\n\n        # Concatenate packed tokens and labels.\n        packed_tokens = torch.cat(packed_tokens, dim=0)\n        packed_labels = torch.cat(packed_labels, dim=0)\n\n        return ImageTaskSamplePacked(\n            __key__=\",\".join([s.__key__ for s in samples]),\n            __restore_key__=(),  # Will be set by energon based on `samples`\n            __subflavor__=None,\n            __subflavors__=samples[0].__subflavors__,\n            tokens=packed_tokens,\n            labels=packed_labels,\n            imgs=packed_imgs,\n            cu_lengths=torch.tensor(cu_lengths, dtype=torch.int32),\n            max_length=max_length,\n            num_tiles=[n for s in samples for n in s.num_tiles],\n        )\n\n# File Path: examples/multimodal/train.py\ndef scaled_loss_func(loss_mask, output_tensor):\n    \"\"\"\n    Scaled loss function\n\n    Scale the loss for each conversation turn using the formula:\n\n    1 / sum_j[ sqrt(length(loss_turn_j)) ] * sum_i[ sum(loss_turn_i) / sqrt(length(loss_turn_i)) ]\n\n    Where we use the loss mask to infer the start / end of the conversation turns.\n    \"\"\"\n    args = get_args()\n    losses = output_tensor.float()\n\n    loss_list = []\n    num_valid_labels_list = []\n    for idx in range(losses.shape[0]):\n        loss_this_sample = losses[idx]\n        turn_start_end_list = get_mask_start_and_end_idx(loss_mask[idx])\n        for turn_start, turn_end in turn_start_end_list:\n            # compute loss for each turn\n            loss_this_turn = loss_this_sample[turn_start:turn_end+1].sum()\n            assert (1 - loss_mask)[idx][turn_start:turn_end+1].sum() < 1.0\n            num_valid_labels_this_turn = turn_end - turn_start + 1\n            loss_this_turn = loss_this_turn / num_valid_labels_this_turn\n            loss_list.append(loss_this_turn)\n            # append num of valid labels for each turn\n            num_valid_labels_list.append(num_valid_labels_this_turn)\n    base_num = sum([math.sqrt(each) for each in num_valid_labels_list])\n    for idx in range(len(loss_list)):\n        # normalize loss for each turn\n        loss_list[idx] = loss_list[idx] * math.sqrt(num_valid_labels_list[idx]) / base_num\n\n    # Some ranks may not get loss tokens due to Context Parallel Sharding\n    if len(loss_list) > 0:\n        total_loss = torch.stack(loss_list).sum()\n        total_tokens = torch.ones_like(total_loss)\n    elif len(loss_list) == 0 and args.context_parallel_size > 1:\n        total_tokens = loss_mask.sum()\n        total_loss = torch.sum(losses.view(-1) * loss_mask)\n    else:\n        raise RuntimeError(\"loss_list for loss scaling per conversation unexpectedly got empty list\")\n\n    num_tokens = total_tokens.clone().detach().to(torch.int)\n    reporting_loss = torch.cat([total_loss.clone().detach().view(1), num_tokens.view(1)])\n\n    return (total_loss, num_tokens, {'lm loss': reporting_loss})\n\n# File Path: examples/multimodal/train.py\ndef get_batch(data_iterator, image_token_index, img_seq_len):\n    # ... (code omitted for brevity)\n    cu_lengths = tensor_parallel.broadcast_data([\"cu_lengths\"], data, torch.int32)[\"cu_lengths\"]\n    max_lengths = tensor_parallel.broadcast_data([\"max_lengths\"], data, torch.int32)[\"max_lengths\"]\n    # ... (code omitted for brevity)\n    if cu_lengths.shape != torch.Size([1, 1]):\n        assert (\n            cu_lengths.shape[0] == max_lengths.shape[0] == 1\n        ), \"micro-batch-size must be 1 for packing\"\n        cu_lengths = cu_lengths[0]\n        max_lengths = max_lengths[0]\n\n        packed_seq_params = PackedSeqParams(\n            qkv_format=\"thd\",\n            cu_seqlens_q=cu_lengths,\n            cu_seqlens_kv=cu_lengths,\n            max_seqlen_q=max_lengths,\n            max_seqlen_kv=max_lengths,\n        )\n    # ... (code omitted for brevity)\n    return (\n        tokens,\n        labels,\n        loss_mask,\n        attention_mask,\n        position_ids,\n        imgs,\n        num_tiles,\n        packed_seq_params,\n    )\n\n# File Path: examples/multimodal/train.py\ndef forward_step(data_iterator, model: LLaVAModel):\n    # ... (code omitted for brevity)\n    (\n        tokens,\n        labels,\n        loss_mask,\n        attention_mask,\n        position_ids,\n        images,\n        num_image_tiles,\n        packed_seq_params,\n    ) = get_batch(data_iterator, model.module.module.image_token_index, model.module.module.img_seq_len)\n    # ... (code omitted for brevity)\n    output_tensor, loss_mask = model(\n        images,\n        tokens,\n        position_ids,\n        attention_mask,\n        labels,\n        loss_mask,\n        num_image_tiles=num_image_tiles,\n        packed_seq_params=packed_seq_params,\n    )\n    args = get_args()\n    if args.use_loss_scaling:\n        loss_function = partial(scaled_loss_func, loss_mask)\n    else:\n        loss_function = partial(loss_func, loss_mask)\n\n    return output_tensor, loss_function",
        "experimental_info": "The implementation involves sequence packing, a truncation-based approach for managing varying sequence lengths. This is related to the Sequence Length Warmup (SLW) method, although an explicit step-wise linear pacing function (`seqlent = seqlens + (seqlene - seqlens) * min(t / T, 1)`) is not directly found in the provided code.\n\n**Experimental Settings:**\n-   `--packing-buffer-size`: This argument enables sample packing. Setting the buffer size to a value greater than 0 activates the packing mechanism, influencing how multiple short sequences are combined into longer ones to optimize GPU utilization.\n-   `--packing-seq-length`: When packing is enabled, this argument specifies the maximum target sequence length for the packed samples. This is the length to which sequences are truncated or padded during the packing process.\n-   `--use-loss-scaling`: A boolean flag that, when true, enables a `scaled_loss_func` instead of the standard `loss_func`. This `scaled_loss_func` is a mechanism to address training instability by scaling the loss for each conversation turn. The scaling formula is: `1 / sum_j[ sqrt(length(loss_turn_j)) ] * sum_i[ sum(loss_turn_i) / sqrt(length(loss_turn_i)) ]`.\n\n**Truncation-based Implementation Details:**\n-   The `_truncate_for_packing` method in `examples/multimodal/dataset_helpers.py` is responsible for truncating `input_ids` and `target` labels. It calculates `max_text_tokens` based on `self.packing_seq_length` and the length of image embeddings, then truncates the text tokens and labels accordingly to fit within the `packing_seq_length` limit."
      }
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DP-HyPO, a pioneering adaptive private hyperparameter optimization (HPO) framework designed to bridge the gap between private and non-private HPO. It addresses the significant privacy risks associated with HPO in differentially private (DP) machine learning models, a concern often overlooked. DP-HyPO allows practitioners to adaptively select hyperparameters based on previous runs, utilizing non-DP adaptive HPO methods (such as Gaussian process) for enhanced efficiency while avoiding the substantial privacy costs typically incurred by composition. The framework provides sharp DP guarantees using the R\n\nrnyi DP framework for arbitrary adaptive sampling distributions, strictly generalizing prior non-adaptive methods. Empirically, the Gaussian process-based DP-HyPO algorithm demonstrates superior performance compared to its uniform counterpart across diverse real-world datasets, and offers practitioners flexibility in allocating privacy budgets for adaptivity.",
        "methodology": "DP-HyPO is built upon an adaptive sampling distribution (π) that updates at each iteration, incorporating information gathered from the dataset and previous outputs. A core mechanism to ensure differential privacy is the requirement that the density of any posterior sampling distribution π(j+1) must be bounded relative to a prior distribution π(0) within constants c and C. To achieve this, the framework includes a general recipe for privatizing non-private HPO methods: projecting the non-private π(j) onto a convex space SC,c of bounded density functions by solving a convex functional programming problem. The paper provides rigorous R\n\nrnyi Differential Privacy (RDP) guarantees for the entire framework, particularly for scenarios where the total number of repetitions (T) follows a random distribution, such as a truncated negative binomial distribution. As an instantiation, DP-HyPO is demonstrated with Gaussian Process (GP), where GP is used to construct a surrogate model for model performance, assign scores (estimated Upper Confidence Bound - UCB) to hyperparameters, and convert these scores into a sampling distribution via a softmax function before applying the density projection technique.",
        "experimental_setup": "The empirical evaluation of DP-HyPO focused on comparing its Gaussian Process-based implementation (\"GP\") against a Uniform DP-HyPO baseline (\"Uniform\"), which is a non-adaptive special case where C=c=1. Experiments were conducted under two privacy configurations:\n1.  **White-box setting**: Assumes adaptive HPO incurs extra privacy cost, reducing the budget for the base algorithm.\n    *   **MNIST Simulation**: Involved training a standard CNN with DP-SGD, optimizing learning rate (η) and clipping norm (R). A semi-real simulation cached mean accuracy from 5 independently trained models for each discretized hyperparameter combination (16 η values, 20 R values, total 320 HPs), adding Gaussian noise (std dev 0.1) to sampled accuracy. Total privacy budget: ε = 15, δ = 1e-5.\n    *   **CIFAR-10 Simulation**: Utilized the same CNN model and hyperparameters. The hyperparameter landscape (mean and standard error of accuracy) was generated using BoTorch, and an oracle returned noisy scores from a normal distribution. Total privacy budget: ε = 12, δ = 1e-5. η and R were discretized into 50 observations each, leading to 2500 HP combinations.\n2.  **Black-box setting**: Assumes the base algorithm has a fixed privacy cost, allowing an extra privacy budget for HPO adaptivity.\n    *   **Federated Learning (FL) Task**: Explored an FL task on a proprietary dataset, aiming to optimize learning rates for the central server (AdaGrad) and individual users (SGD). The loss landscape (mean and standard error) was generated by BoTorch, and an oracle returned noisy scores from a normal distribution. Experiments investigated the impact of varying the C parameter (1.25, 1.33, 1.5) with c=1/C.\nPerformance was evaluated based on the \"actual accuracy\" for MNIST and CIFAR-10, and loss for the FL task. Results were obtained by averaging across hundreds or thousands of independent runs and reported with 95% confidence error bars.",
        "limitations": "Existing private hyperparameter optimization (HPO) approaches often neglect privacy costs or rely on unrealistic assumptions like prior knowledge of optimal parameters or the availability of perfectly matched public datasets. Traditional composition-based DP analysis for HPO results in overly loose privacy guarantees. Prior non-adaptive private HPO methods, while improving privacy bounds, are limited to uniform sampling and cannot adapt to past results, which is inefficient for large search spaces. The DP-HyPO framework itself requires the adaptive sampling distribution density to be bounded, which necessitates a projection technique for non-private HPO methods. This projection, for practical computational feasibility, often requires discretizing the continuous hyperparameter space. Empirically, the Gaussian process-based DP-HyPO may initially perform slightly worse than its uniform counterpart due to the privacy budget allocated for adaptivity.",
        "future_research_directions": "The paper suggests two key directions for future research:\n1.  **Alternative HPO Specification**: Further explore and integrate more advanced HPO methods from the extensive literature to develop alternative HPO specifications that could improve the empirical performance of DP-HyPO in practical scenarios.\n2.  **Theoretical Utility Guarantee**: Establish theoretical utility guarantees for the general DP-HyPO framework, or for specific configurations within it, potentially by adapting and extending proof methodologies similar to those found in related prior work (e.g., Theorem 3.3 in [26])."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research proposes Partitioned Neural Networks, a novel and efficient method for hyperparameter optimization (HPO) that is inspired by marginal likelihood. It addresses the computational expense and validation data dependency of traditional HPO and existing marginal likelihood approximations, particularly in deep learning and federated learning settings. The method allows for optimizing a variety of hyperparameters in a single training run without a separate validation set, significantly reducing computational overhead and communication costs in federated learning.",
        "methodology": "The core methodology involves partitioning the training data into C shards and the neural network parameters into C partitions. Each parameter partition is associated with and optimized on specific data shards. Subnetworks are formed by concatenating the first k learned parameter partitions and setting subsequent partitions to default values (e.g., initialization). Hyperparameters are optimized by minimizing an 'out-of-training-sample' loss objective (LML), which is an approximation to the marginal likelihood, calculated as the loss on data shards unseen by the respective subnetwork. The parameter updates and hyperparameter updates are interleaved using stochastic gradient descent. The primary partitioning scheme involves randomly assigning a fixed proportion of weights in each layer to a given partition.",
        "experimental_setup": "The method was evaluated on: 1) A toy input selection task, demonstrating its ability to identify informative features and learn a differentiable input mask. 2) Learning affine image augmentations on MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotMNIST, rotCIFAR10, rotTinyImagenet). Architectures included CNNs for MNIST, FixupResNet-8 and FixupResNet-14 for CIFAR10, and ResNet-50 with GroupNorm(2) for TinyImagenet. 3) Optimizing a feature extractor (first two stages of Wide ResNet-20) on CIFAR10. 4) Federated Learning (FL) with non-i.i.d. splits of MNIST and CIFAR10 (and their rotated variants) across 100 clients, optimizing data augmentation and dropout rates. Baselines included standard training (no augmentations), Augerino, Differentiable Laplace, Last-layer Marginal Likelihood, traditional training/validation split with finetuning, and FedAvg (with and without Augerino). Validation involved test accuracy and log-likelihood.",
        "limitations": "The method requires an additional forward-backward pass for hyperparameter updates, although this is computationally less expensive than existing alternatives. It may also require more training iterations to converge. Partitioning the network inherently constrains its capacity, which could lead to a performance loss compared to an optimally tuned full network if the optimal hyperparameters were known a priori. Furthermore, the method introduces an additional hyperparameter: the partitioning strategy (number of chunks, relative proportions of data and parameters per chunk), which might require some tuning for optimal performance.",
        "future_research_directions": "Future research could explore dynamic partitioning of network parameters during training, rather than a fixed partitioning prior to training. Investigating alternative partitioning schemes, such as node partitioning, could enable more efficient updates. Additionally, methods to alleviate the performance loss due to network partitioning, potentially by adjusting training rounds or increasing the initial network capacity, could be explored. Further optimization of hyperparameter update schedules, such as gradient accumulation or less frequent updates, is also a promising direction."
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of setting regularization parameters for Lasso-type estimators, which typically suffers from exponential complexity with grid-search or high memory/instability with gradient-based methods. The main contributions include an efficient implicit differentiation algorithm tailored for Lasso-type problems that avoids matrix inversion and scales to high-dimensional data by leveraging solution sparsity. Key findings demonstrate that forward iterative differentiation of block coordinate descent (BCD) converges to the true gradient, linearly once the support is identified. A novel algorithm (Algorithm 2) decouples Jacobian computation from regression coefficient estimation, circumventing the need to solve potentially ill-conditioned linear systems. Experiments show the proposed method outperforms existing hyperparameter optimization techniques for optimizing held-out error or the Stein Unbiased Risk Estimator (SURE) across both low (Lasso) and high (weighted Lasso) numbers of hyperparameters.",
        "methodology": "Hyperparameter optimization (HO) is formulated as a bi-level optimization problem, where an outer criterion `L(λ) = C(ˆβ(λ))` is optimized given an inner Lasso-type problem `ˆβ(λ) = arg min β ψ(β,λ)`. The regularization parameter is parametrized as `eλ`. The core challenge is computing the weak Jacobian `ˆJ(λ) = ∇λˆβ(λ)`. The proposed method leverages the fixed-point iteration property of proximal BCD algorithms for Lasso-type problems. By differentiating this fixed-point equation, closed-form, sparse solutions for the Jacobian are derived for Lasso and weighted Lasso (Proposition 1). The 'Implicit Forward Iterdiff' algorithm (Algorithm 2) is introduced, which decouples the computation of regression coefficients and the Jacobian. First, `ˆβ` is obtained using a standard solver, then forward differentiation recursion steps are applied to compute the Jacobian, restricted to the active support, which ensures efficiency and stability. For the SURE criterion, a weakly differentiable approximation based on Finite Differences Monte-Carlo is used.",
        "experimental_setup": "The Python code for the proposed method is open-sourced (`https://github.com/QB3/sparse-ho`), utilizing Numba for critical parts. For fair comparison across all methods, a vanilla BCD algorithm (Algorithm 5) was used as the inner solver, stopping when `(f(β(k+1)) - f(β(k))) / f(0) < 10^-5`. Gradient-based methods employed a line-search strategy. Lasso was initialized with `λ_init = λ_max - log(10)`, while weighted Lasso used a regularized version of the HO problem for initialization. Competitors included other hypergradient-based methods (Implicit, Forward Iterdiff, Backward Iterdiff) and non-hypergradient methods (Grid-search, Random-search, Lattice Hyp., Bayesian optimization). Experiments were conducted on both synthetic data (Toeplitz correlated matrices with `n=1000, p=2000` or `n=1000, p=10000`) and real high-dimensional datasets: `rcv1` (`p=19,959`), `20news` (`p=130,107`), and `finance` (`p=1,668,737`). Datasets were split into training, validation, and test sets for held-out loss evaluation, or simulated data used with the SURE criterion, varying `p` from 200 to 10,000 for MSE and time efficiency analysis.",
        "limitations": "The theoretical guarantees for the proposed method do not cover non-convex cases (e.g., MCP penalty), although empirical results show good performance. The proof for Jacobian convergence (Proposition 2) assumes a unique solution for the inner Lasso problem; while non-uniqueness is rare, its occurrence could lead to a non-continuous solution path, potentially undermining gradient-based methods. The overall HO problem `L(λ)` is generally non-convex, so gradient descent might converge only to a local minimum, emphasizing the importance of initialization. The implicit differentiation algorithm (Algorithm 1) can still suffer from ill-conditioned linear systems, especially with large support sizes, leading to numerical instabilities and slower resolution. Additionally, the SURE criterion requires prior knowledge of the noise variance.",
        "future_research_directions": "Future research directions include extending the theoretical guarantees to handle non-convex penalty functions like MCP. Further investigation is warranted into the behavior and theoretical implications of the algorithm in 'pathological' settings where the Lasso solution might not be unique. The authors also suggest combining the proposed algorithm with state-of-the-art Lasso solvers that employ techniques like active sets or screening rules, to leverage their efficiency in the first step of the decoupled process. Exploring other formulations beyond Lasso and weighted Lasso, such as Elastic-Net, is also mentioned as a potential avenue.",
        "experimental_code": "import numpy as np\nfrom scipy.linalg import norm\nimport scipy.sparse.linalg as slinalg\nfrom numba import njit\nfrom scipy.sparse.linalg import LinearOperator\n\nfrom sparse_ho.utils import init_dbeta0_new, ST\nfrom sparse_ho.utils import sparse_scalar_product\nfrom sparse_ho.models.base import BaseModel\n\n\nclass Lasso(BaseModel):\n    \"\"\"Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\n    The optimization objective for Lasso is:\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Parameters\n    ----------\n    estimator: sklearn estimator\n        Estimator used to solve the optimization problem. Must follow the\n        scikit-learn API.\n    \"\"\"\n\n    def __init__(self, estimator=None):\n        self.estimator = estimator\n\n    def _init_dbeta_ddual_var(self, X, y, mask0=None, jac0=None,\n                              dense0=None, compute_jac=True):\n        n_samples, n_features = X.shape\n        dbeta = np.zeros(n_features)\n        if jac0 is None or not compute_jac:\n            ddual_var = np.zeros(n_samples)\n        else:\n            dbeta[mask0] = jac0.copy()\n            ddual_var = - X[:, mask0] @ jac0.copy()\n        return dbeta, ddual_var\n\n    def _init_beta_dual_var(self, X, y, mask0=None, dense0=None):\n        beta = np.zeros(X.shape[1])\n        if dense0 is None or len(dense0) == 0:\n            dual_var = y.copy()\n            dual_var = dual_var.astype(np.float)\n        else:\n            beta[mask0] = dense0.copy()\n            dual_var = y - X[:, mask0] @ dense0\n        return beta, dual_var\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd(\n            X, y, beta, dbeta, dual_var, ddual_var,\n            alpha, L, compute_jac=True):\n        n_samples, n_features = X.shape\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n                # compute derivatives\n            zj = beta[j] + dual_var @ X[:, j] / (L[j] * n_samples)\n            beta[j] = ST(zj, alpha[j] / L[j])\n            # beta[j:j+1] = ST(zj, alpha[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + X[:, j] @ ddual_var / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alpha[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var -= X[:, j] * (dbeta[j] - dbeta_old)\n            dual_var -= X[:, j] * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_beta_jac_bcd_sparse(\n            data, indptr, indices, y, n_samples, n_features, beta,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n\n        non_zeros = np.where(L != 0)[0]\n\n        for j in non_zeros:\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero indices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            beta_old = beta[j]\n            if compute_jac:\n                dbeta_old = dbeta[j]\n            zj = beta[j] + dual_var[idx_nz] @ Xjs / (L[j] * n_samples)\n            beta[j:j+1] = ST(zj, alphas[j] / L[j])\n            if compute_jac:\n                dzj = dbeta[j] + Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n                dbeta[j:j+1] = np.abs(np.sign(beta[j])) * dzj\n                dbeta[j:j+1] -= alphas[j] * np.sign(beta[j]) / L[j]\n                # update residuals\n                ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n            dual_var[idx_nz] -= Xjs * (beta[j] - beta_old)\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward(X, alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        n_samples, n_features = X.shape\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n            v_t_jac[j] *= np.abs(sign_beta[j])\n            v_t_jac -= v_t_jac[j] / (L[j] * n_samples) * X[:, j] @ X\n\n        return grad\n\n    @staticmethod\n    @njit\n    def _update_bcd_jac_backward_sparse(\n            data, indptr, indices, n_samples, n_features,\n            alpha, grad, beta, v_t_jac, L):\n        sign_beta = np.sign(beta)\n        for j in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n            if L[j] != 0:\n                Xjs = data[indptr[j]:indptr[j+1]]\n                idx_nz = indices[indptr[j]:indptr[j+1]]\n                grad -= (v_t_jac[j]) * alpha * sign_beta[j] / L[j]\n                v_t_jac[j] *= np.abs(sign_beta[j])\n                cste = v_t_jac[j] / (L[j] * n_samples)\n                for i in (np.arange(sign_beta.shape[0] - 1, -1, -1)):\n                    Xis = data[indptr[i]:indptr[i+1]]\n                    idx = indices[indptr[i]:indptr[i+1]]\n                    product = sparse_scalar_product(Xjs, idx_nz, Xis, idx)\n                    v_t_jac[i] -= cste * product\n\n        return grad\n\n    @staticmethod\n    def _get_pobj0(dual_var, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return norm(y) ** 2 / (2 * n_samples)\n\n    @staticmethod\n    def _get_pobj(dual_var, X, beta, alphas, y=None):\n        n_samples = dual_var.shape[0]\n        return (\n            norm(dual_var) ** 2 / (2 * n_samples) +\n            np.abs(alphas * beta).sum())\n\n    @staticmethod\n    def _get_dobj(dual_var, X, beta, alpha, y=None):\n        # the dual variable is theta = (y - X beta) / (alpha n_samples)\n        n_samples = X.shape[0]\n        theta = dual_var / (alpha * n_samples)\n        norm_inf_XTtheta = np.max(np.abs(X.T @ theta))\n        if norm_inf_XTtheta > 1:\n            theta /= norm_inf_XTtheta\n        dobj = alpha * y @ theta\n        dobj -= alpha ** 2 * n_samples / 2 * (theta ** 2).sum()\n        return dobj\n\n    @staticmethod\n    def _get_jac(dbeta, mask):\n        return dbeta[mask]\n\n    @staticmethod\n    def get_full_jac_v(mask, jac_v, n_features):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        n_features: int\n            Number of features.\n        \"\"\"\n        # MM sorry I don't get what this does\n        return jac_v\n\n    @staticmethod\n    def get_mask_jac_v(mask, jac_v):\n        \"\"\"TODO\n\n        Parameters\n        ----------\n        mask: TODO\n        jac_v: TODO\n        \"\"\"\n        return jac_v\n\n    @staticmethod\n    def _init_dbeta0(mask, mask0, jac0):\n        size_mat = mask.sum()\n        if jac0 is not None:\n            dbeta0_new = init_dbeta0_new(jac0, mask, mask0)\n        else:\n            dbeta0_new = np.zeros(size_mat)\n        return dbeta0_new\n\n    @staticmethod\n    def _init_dbeta(n_features):\n        dbeta = np.zeros(n_features)\n        return dbeta\n\n    @staticmethod\n    def _init_ddual_var(dbeta, X, y, sign_beta, alpha):\n        return - X @ dbeta\n\n    @staticmethod\n    def _init_g_backward(jac_v0, n_features):\n        if jac_v0 is None:\n            return 0.0\n        else:\n            return jac_v0\n\n    @staticmethod\n    @njit\n    def _update_only_jac(Xs, y, dual_var, dbeta, ddual_var,\n                         L, alpha, sign_beta):\n        n_samples, n_features = Xs.shape\n        for j in range(n_features):\n            # dbeta_old = dbeta[j].copy()\n            dbeta_old = dbeta[j]\n            dbeta[j] += Xs[:, j].T @ ddual_var / (L[j] * n_samples)\n            dbeta[j] -= alpha * sign_beta[j] / L[j]\n            ddual_var -= Xs[:, j] * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _update_only_jac_sparse(\n            data, indptr, indices, y, n_samples, n_features,\n            dbeta, dual_var, ddual_var, alphas, L, compute_jac=True):\n        for j in range(n_features):\n            # get the j-st column of X in sparse format\n            Xjs = data[indptr[j]:indptr[j+1]]\n            # get the non zero idices\n            idx_nz = indices[indptr[j]:indptr[j+1]]\n            # store old beta j for fast update\n            dbeta_old = dbeta[j]\n            # update of the Jacobian dbeta\n            dbeta[j] += Xjs @ ddual_var[idx_nz] / (L[j] * n_samples)\n            dbeta[j] -= alphas[j] * np.sign(beta[j]) / L[j]\n            ddual_var[idx_nz] -= Xjs * (dbeta[j] - dbeta_old)\n\n    @staticmethod\n    @njit\n    def _reduce_alpha(alpha, mask):\n        return alpha\n\n    @staticmethod\n    def _get_grad(X, y, jac, mask, dense, alphas, v):\n        return alphas[mask] * np.sign(dense) @ jac\n\n    def proj_hyperparam(self, X, y, log_alpha):\n        \"\"\"Project hyperparameter on an admissible range of values.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float\n            Logarithm of hyperparameter.\n\n        Returns\n        -------\n        log_alpha: float\n            Logarithm of projected hyperparameter.\n        \"\"\"\n        if not hasattr(self, \"log_alpha_max\"):\n            alpha_max = np.max(np.abs(X.T @ y))\n            alpha_max /= X.shape[0]\n            self.log_alpha_max = np.log(alpha_max)\n        return np.clip(log_alpha, self.log_alpha_max - 12,\n                       self.log_alpha_max + np.log(0.9))\n\n    @staticmethod\n    def get_L(X):\n        \"\"\"Compute Lipschitz constant of datafit.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n\n        Returns\n        -------\n        L: float\n            The Lipschitz constant.\n        \"\"\"\n        if issparse(X):\n            return slinalg.norm(X, axis=0) ** 2 / (X.shape[0])\n        else:\n            return norm(X, axis=0) ** 2 / (X.shape[0])\n\n    def _use_estimator(self, X, y, alpha, tol):\n        if self.estimator is None:\n            raise ValueError(\"You did not pass a solver with sklearn API\")\n        self.estimator.set_params(tol=tol, alpha=alpha)\n        self.estimator.fit(X, y)\n        mask = self.estimator.coef_ != 0\n        dense = self.estimator.coef_[mask]\n        return mask, dense, None\n\n    @staticmethod\n    def reduce_X(X, mask):\n        \"\"\"Reduce design matrix to generalized support.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        mask : ndarray, shape (n_features,)\n            Generalized support.\n        \"\"\"\n        return X[:, mask]\n\n    @staticmethod\n    def reduce_y(y, mask):\n        \"\"\"Reduce observation vector to generalized support.\n\n        Parameters\n        ----------\n        y : ndarray, shape (n_samples,)\n            Observation vector.\n        mask : ndarray, shape (n_features,)  TODO shape n_samples right?\n            Generalized support.\n        \"\"\"\n        return y\n\n    def sign(self, x, log_alpha):\n        \"\"\"Get sign of iterate.\n\n        Parameters\n        ----------\n        x : ndarray, shape TODO\n        log_alpha : ndarray, shape TODO\n            Logarithm of hyperparameter.\n        \"\"\"\n        return np.sign(x)\n\n    def get_beta(self, X, y, mask, dense):\n        \"\"\"Return primal iterate.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        \"\"\"\n        return mask, dense\n\n    def get_jac_v(self, X, y, mask, dense, jac, v):\n        \"\"\"Compute hypergradient.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        jac: TODO\n        v: TODO\n        \"\"\"\n        return jac.T @ v(mask, dense)\n\n    @staticmethod\n    def get_mat_vec(X, y, mask, dense, log_alpha):\n        \"\"\"Returns a LinearOperator computing the matrix vector product\n        with the Hessian of datafit. It is necessary to avoid storing a\n        potentially large matrix, and keep advantage of the sparsity of X.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        mask: ndarray, shape (n_features,)\n            Mask corresponding to non zero entries of beta.\n        dense: ndarray, shape (mask.sum(),)\n            Non zero entries of beta.\n        log_alpha: ndarray\n            Logarithm of hyperparameter.\n        \"\"\"\n        X_m = X[:, mask]\n        n_samples, size_supp = X_m.shape\n\n        def mv(v):\n            return X_m.T @ (X_m @ v) / n_samples\n        return LinearOperator((size_supp, size_supp), matvec=mv)\n\n    def generalized_supp(self, X, v, log_alpha):\n        \"\"\"Generalized support of iterate.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Design matrix.\n        v : TODO\n        log_alpha : float\n            Log of hyperparameter.\n\n        Returns\n        -------\n        TODO\n        \"\"\"\n        return v\n\n    def get_jac_residual_norm(self, Xs, ys, n_samples, sign_beta, dbeta,\n                              dual_var, ddual_var, alpha):\n        return norm(ddual_var.T @ ddual_var +\n                    n_samples * alpha * sign_beta @ dbeta)",
        "experimental_info": "This `Lasso` model class provides the specific implementations for computing regression coefficients (`_update_beta_jac_bcd`) and their Jacobian (`_update_only_jac`) using proximal coordinate descent, for both dense and sparse design matrices. It also includes methods for objective function calculation, Lipschitz constant, and projection of hyperparameters. The `_update_only_jac` methods directly implement the differentiation of the fixed-point equation to derive sparse Jacobian solutions."
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Bayesian Optimization for Iterative Learning (BOIL), a novel Bayesian optimization (BO) approach designed for efficient hyperparameter tuning of deep (reinforcement) learning systems. It addresses the high computational cost and the limitation of traditional BO methods that only consider final performance, ignoring valuable intermediate training information. BOIL proposes to learn an evaluation function that compresses the entire learning progress at any stage of training into a single numeric score, accounting for both training success and stability. The framework balances the benefit of evaluating a hyperparameter setting over additional training steps against their computational cost. It also enhances efficiency by selectively including scores from different training steps (data augmentation). The algorithm demonstrates superior performance over existing baselines in identifying optimal hyperparameters in minimal wall-clock time for Deep Reinforcement Learning (DRL) agents and convolutional neural networks (CNNs).",
        "methodology": "The methodology is centered on a Bayesian Optimization framework that models the cost-sensitive black-box objective function f(x,t) (hyperparameters x and iterations t) using a Gaussian Process (GP) with a product kernel k([x,t],[x',t']). The training time cost c(x,t) is approximated using a linear regressor. Crucially, BOIL introduces a training curve compression technique, transforming the entire learning curve into a numeric utility score via a Sigmoid (Logistic) preference function. The parameters of this Sigmoid function (growth parameter g0 and middle point m0) are learned dynamically from the observed data by maximizing the GP's log marginal likelihood. To improve sample efficiency and prevent ill-conditioning of the GP covariance matrix, a selective data augmentation technique is used. This involves actively selecting a subset of points from the observed learning curve (at fixed hyperparameters x) by maximizing the GP predictive uncertainty, ensuring the natural log of the covariance matrix's condition number remains below a specified threshold. The next hyperparameter-iteration pair is chosen by maximizing a cost-aware acquisition function (a modified Expected Improvement criterion divided by the predicted cost).",
        "experimental_setup": "Experiments were conducted on an NVIDIA 1080 GTX GPU using TensorFlow. All results were averaged over 20 independent runs with distinct random seeds, with final performance estimated by evaluating hyperparameters over the maximum number of iterations. DRL tasks included Dueling DQN (DDQN) on the CartPole-v0 environment and Advantage Actor Critic (A2C) on InvertedPendulum-v2 and Reacher-v2 environments, utilizing OpenAI gym and Mujoco, with implementations based on OpenAI Baselines. CNN tuning involved a convolutional neural network on the SVHN and CIFAR10 datasets. The GP model employed square-exponential kernels, with parameters estimated by maximizing marginal likelihood. Data augmentation was set with a maximum of 15 augmented points and a threshold of 20 for the natural log of the GP covariance matrix's condition number. Baselines for comparison included Hyperband, Continuous Multi-Task/Fidelity BO (CM-T/F-BO), vanilla Bayesian Optimization (BO), and BO with curve compression (BO-L). Detailed hyperparameter search ranges for each task and specifications for DRL agents were provided.",
        "limitations": "The authors identify that a naive approach of augmenting the GP model with an entire learning curve's points can lead to redundancy and severe ill-conditioning of the GP covariance matrix, necessitating their proposed selective augmentation strategy. A more general limitation noted is that methods assuming an exponential decay in training performance, such as Freeze-thaw BO, are often unsuitable for Deep Reinforcement Learning (DRL) due to the unpredictable and noisy fluctuations typical of DRL reward curves. From a broader societal perspective, the paper acknowledges that an increasingly automated machine learning pipeline, while efficient, could potentially detach humans from the modeling process, making critical failures harder to identify and contributing to the growing opacity of ML models. Practitioners are encouraged to reflect on potential biases in data and models used in such automated processes.",
        "future_research_directions": "The paper suggests that the proposed BOIL framework is not limited to machine learning algorithms but can be broadly applied to any iterative process that depends on a set of parameters and whose iterative structure can be exploited. A specific example given is the optimization of manufacturing pipelines, where factory settings could be adjusted to enhance productivity. Furthermore, the authors implicitly suggest a direction related to broader impact, encouraging the integration of rapidly developing work on interpretability of machine learning models to rigorously analyze the outcomes of automatically trained models, helping to address concerns about model opacity.",
        "experimental_code": "import numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import MinMaxScaler\nimport scipy\n\n\nclass GaussianProcess(object):\n    def __init__ (self,SearchSpace,noise_delta=1e-8,verbose=0):\n        self.noise_delta=noise_delta\n        self.noise_upperbound=noise_delta\n        self.mycov=self.cov_RBF\n        self.SearchSpace=SearchSpace\n        scaler = MinMaxScaler()\n        scaler.fit(SearchSpace.T)\n        self.Xscaler=scaler\n        self.verbose=verbose\n        self.dim=SearchSpace.shape[0]\n        \n        self.hyper={}\n        self.hyper['var']=1 # standardise the data\n        self.hyper['lengthscale']=0.04 #to be optimised\n        self.noise_delta=noise_delta\n        return None\n        \n    def set_optimum_value(self,fstar_scaled):\n        self.fstar=fstar_scaled\n        \n    def fit(self,X,Y,IsOptimize=0):\n        \"\"\"\n        Fit a Gaussian Process model\n        X: input 2d array [N*d]\n        Y: output 2d array [N*1]\n        \"\"\"       \n        self.X_ori=X # this is the output in original scale\n        #self.X= self.Xscaler.transform(X) #this is the normalised data [0-1] in each column\n        self.X=X\n        self.Y_ori=Y # this is the output in original scale\n        self.Y=(Y-np.mean(Y))/np.std(Y) # this is the standardised output N(0,1)\n        \n        if IsOptimize:\n            self.hyper['lengthscale']=self.optimise()         # optimise GP hyperparameters\n            \n        self.KK_x_x=self.mycov(self.X,self.X,self.hyper)+np.eye(len(X))*self.noise_delta     \n        if np.isnan(self.KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x !\")\n      \n        self.L=scipy.linalg.cholesky(self.KK_x_x,lower=True)\n        temp=np.linalg.solve(self.L,self.Y)\n        self.alpha=np.linalg.solve(self.L.T,temp)\n        \n    def cov_RBF(self,x1, x2,hyper):        \n        \"\"\"\n        Radial Basic function kernel (or SE kernel)\n        \"\"\"\n        variance=hyper['var']\n        lengthscale=hyper['lengthscale']\n\n        if x1.shape[1]!=x2.shape[1]:\n            x1=np.reshape(x1,(-1,x2.shape[1]))\n        Euc_dist=euclidean_distances(x1,x2)\n\n        return variance*np.exp(-np.square(Euc_dist)/lengthscale)\n    \n\n    def log_llk(self,X,y,hyper_values):\n        \n        #print(hyper_values)\n        hyper={}\n        hyper['var']=1\n        hyper['lengthscale']=hyper_values[0]\n        noise_delta=self.noise_delta\n\n        KK_x_x=self.mycov(X,X,hyper)+np.eye(len(X))*noise_delta     \n        if np.isnan(KK_x_x).any(): #NaN\n            print(\"nan in KK_x_x !\")   \n\n        try:\n            L=scipy.linalg.cholesky(KK_x_x,lower=True)\n            alpha=np.linalg.solve(KK_x_x,y)\n\n        except: # singular\n            return -np.inf\n        try:\n            first_term=-0.5*np.dot(self.Y.T,alpha)\n            W_logdet=np.sum(np.log(np.diag(L)))\n            second_term=-W_logdet\n\n        except: # singular\n            return -np.inf\n\n        logmarginal=first_term+second_term-0.5*len(y)*np.log(2*3.14)\n        \n        #print(hyper_values,logmarginal)\n        return np.asscalar(logmarginal)\n    \n    def set_ls(self,lengthscale):\n        self.hyper['lengthscale']=lengthscale\n        \n    def optimise(self):\n        \"\"\"\n        Optimise the GP kernel hyperparameters\n        Returns\n        x_t\n        \"\"\"\n        opts ={'maxiter':200,'maxfun':200,'disp': False}\n\n        # epsilon, ls, var, noise var\n        #bounds=np.asarray([[9e-3,0.007],[1e-2,self.noise_upperbound]])\n        bounds=np.asarray([[1e-3,1]])\n\n        init_theta = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(10, 1))\n        logllk=[0]*init_theta.shape[0]\n        for ii,val in enumerate(init_theta):           \n            logllk[ii]=self.log_llk(self.X,self.Y,hyper_values=val) #noise_delta=self.noise_delta\n            \n        x0=init_theta[np.argmax(logllk)]\n\n        res = minimize(lambda x: -self.log_llk(self.X,self.Y,hyper_values=x),x0,\n                                   bounds=bounds,method=\"L-BFGS-B\",options=opts)#L-BFGS-B\n        \n        if self.verbose:\n            print(\"estimated lengthscale\",res.x)\n            \n        return res.x  \n   \n    def predict(self,Xtest,isOriScale=False):\n        \"\"\"\n        ----------\n        Xtest: the testing points  [N*d]\n        Returns\n        -------\n        pred mean, pred var, pred mean original scale, pred var original scale\n        \"\"\"    \n        \n        if isOriScale:\n            Xtest=self.Xscaler.transform(Xtest)\n            \n        if len(Xtest.shape)==1: # 1d\n            Xtest=np.reshape(Xtest,(-1,self.X.shape[1]))\n            \n        if Xtest.shape[1] != self.X.shape[1]: # different dimension\n            Xtest=np.reshape(Xtest,(-1,self.X.shape[1]))\n       \n        KK_xTest_xTest=self.mycov(Xtest,Xtest,self.hyper)+np.eye(Xtest.shape[0])*self.noise_delta\n        KK_xTest_x=self.mycov(Xtest,self.X,self.hyper)\n\n        mean=np.dot(KK_xTest_x,self.alpha)\n        v=np.linalg.solve(self.L,KK_xTest_x.T)\n        var=KK_xTest_xTest-np.dot(v.T,v)\n\n        #mean_ori=mean*np.std(self.Y_ori)+np.mean(self.Y_ori)\n        std=np.reshape(np.diag(var),(-1,1))\n        \n        #std_ori=std*np.std(self.Y_ori)#+np.mean(self.Y_ori)\n        \n        #return mean,std,mean_ori,std_ori\n        return  np.reshape(mean,(-1,1)),std  \n\n",
        "experimental_info": "Gaussian Process:\n- Kernel: Radial Basis Function (RBF) with product kernel k([x,t],[x',t']) = k(x,x')*k(t,t').\n- Initial lengthscale_x: 0.02\n- Initial lengthscale_t: 0.2\n- Initial midpoint (logistic function): 0.0\n- Initial growth (logistic function): 1.0\n- Noise delta: 5e-4\n\nHyperparameter Optimization for GP and Logistic function:\n- GP lengthscale_x search space: [0.03, 0.3]\n- GP lengthscale_t search space: [0.3, 0.6] (derived from [10*SearchSpace_l_min,2*SearchSpace_l_max] with SearchSpace_l_min=0.03 and SearchSpace_l_max=0.3)\n- Logistic midpoint search space: [-2, 3]\n- Logistic growth search space: [0.5, 2]\n- Optimization method: L-BFGS-B (scipy.optimize.minimize)\n- Number of random initializations for hyperparameter search: 20\n- L-BFGS-B options: {'maxiter': 30*dim, 'maxfun': 30*dim} where dim is the search space dimension of the hyperparameter vector (4 in this case: lengthscale_x, lengthscale_t, midpoint, growth).\n- Frequency of hyperparameter optimization: Every 2*dim iterations (where dim is the total search space dimension of x and t).\n\nCost Model:\n- Type: Linear Regressor (sklearn.linear_model.LinearRegression)\n- Input: Combined hyperparameters (x) and iteration (t)\n- Output: Predicted cost\n\nAcquisition Function:\n- Name: 'ei_mu_max' (Expected Improvement using the maximum of the GP's predictive mean as the incumbent).\n- Cost-aware formulation: The acquisition function is computed as log(utility) - log(mean_cost). This combined function is then minimized using `acq_min_scipy_kwargs` which effectively maximizes the ratio (or difference of logs).\n- Acquisition function maximization (for next point selection):\n    - Optimization method: L-BFGS-B (scipy.optimize.minimize)\n    - Number of random initializations: 50*dim points generated randomly, then `range(1*dim)` multi-starts for L-BFGS-B in `acq_max_scipy`. For cost-aware acquisition (minimization), `range(3*dim)` multi-starts with `size=(50*dim, dim)` random points.\n    - L-BFGS-B options: {'maxiter': 10*dim, 'maxfun': 20*dim} (for `acq_max_scipy`), and {'maxiter': 10*dim, 'maxfun': 20*dim} (for `acq_min_scipy_kwargs`).\n\nData Augmentation:\n- Technique: Selective data augmentation based on maximizing GP predictive uncertainty.\n- Maximum number of augmentations: `max_n_augmentation=10` (per real observation's curve), scaled by the iteration `t_max` (i.e., `np.int(t_max * self.max_n_augmentation)`).\n- Selection criteria: Maximizes GP predictive uncertainty ('pure_exploration').\n- Stopping condition for augmentation: If the natural log of the covariance matrix's condition number `log_cond > 15` or if the predictive uncertainty `pred_var_value < (gp.noise_delta + 1e-3)` or if a repetition occurs.\n- Virtual observations: Are generated for a fixed hyperparameter `x` at different iteration `t` values along its learning curve. These virtual points are then transformed into utility scores using the current logistic function parameters and an interpolated cost.\n\nInitialization:\n- Number of initial real points: 3 (default).\n- Initial points are randomly sampled.\n- Virtual observations are generated for each initial real point.\n\nTraining Curve Compression:\n- Method: Sigmoid (Logistic) preference function `1.0/(1+np.exp(-growth*(x-midpoint)))`.\n- Parameters: `midpoint` and `growth` are learned dynamically by maximizing the GP's log marginal likelihood.\n- Score calculation: Average of `curve * logistic_value` from a threshold `(midpoint+6-2)*len(curve)/(12)` to the end of the curve.\n"
      }
    },
    {
      "title": "An Analytical Theory of Curriculum Learning in Teacher-Student Networks",
      "abstract": "In humans and animals, curriculum learning -- presenting data in a curated\norder - is critical to rapid learning and effective pedagogy. Yet in machine\nlearning, curricula are not widely used and empirically often yield only\nmoderate benefits. This stark difference in the importance of curriculum raises\na fundamental theoretical question: when and why does curriculum learning help?\n  In this work, we analyse a prototypical neural network model of curriculum\nlearning in the high-dimensional limit, employing statistical physics methods.\nCurricula could in principle change both the learning speed and asymptotic\nperformance of a model. To study the former, we provide an exact description of\nthe online learning setting, confirming the long-standing experimental\nobservation that curricula can modestly speed up learning. To study the latter,\nwe derive performance in a batch learning setting, in which a network trains to\nconvergence in successive phases of learning on dataset slices of varying\ndifficulty. With standard training losses, curriculum does not provide\ngeneralisation benefit, in line with empirical observations. However, we show\nthat by connecting different learning phases through simple Gaussian priors,\ncurriculum can yield a large improvement in test performance. Taken together,\nour reduced analytical descriptions help reconcile apparently conflicting\nempirical results and trace regimes where curriculum learning yields the\nlargest gains. More broadly, our results suggest that fully exploiting a\ncurriculum may require explicit changes to the loss function at curriculum\nboundaries.",
      "full_text": "An Analytical Theory of Curriculum Learning in Teacher-Student Networks Luca Saglietti†,∗, Stefano Sarao Mannelli‡,∗, and Andrew Saxe‡,§ Abstract In animals and humans, curriculum learning—presenting data in a curated order— is critical to rapid learning and effective pedagogy. A long history of experiments has demonstrated the impact of curricula in a variety of animals but, despite its ubiquitous presence, a theoretical understanding of the phenomenon is still lacking. Surprisingly, in contrast to animal learning, curricula strategies are not widely used in machine learning and recent simulation studies reach the conclusion that curric- ula are moderately effective or even ineffective in most cases. This stark difference in the importance of curriculum raises a fundamental theoretical question: when and why does curriculum learning help? In this work, we analyse a prototypical neural network model of curriculum learning in the high-dimensional limit, employ- ing statistical physics methods. We study a task in which a sparse set of informative features are embedded amidst a large set of noisy features. We analytically derive average learning trajectories for simple neural networks on this task, which estab- lish a clear speed beneﬁt for curriculum learning in the online setting. However, when training experiences can be stored and replayed the advantage of curricu- lum in standard neural networks disappears, in line with observations from the deep learning literature. Inspired by synaptic consolidation techniques developed to combat catastrophic forgetting, we propose curriculum-aware algorithms that consolidate synapses at curriculum change points and investigate whether this can boost the beneﬁts of curricula. We derive generalisation performance as a function of consolidation strength (implemented as an L2 regularisation/elastic coupling connecting learning phases), and show that curriculum-aware algorithms can yield a large improvement in test performance. Our reduced analytical descriptions help reconcile apparently conﬂicting empirical results, trace regimes where curriculum learning yields the largest gains, and provide experimentally-accessible predictions for the impact of task parameters on curriculum beneﬁts. More broadly, our results suggest that fully exploiting a curriculum may require explicit adjustments in the loss. 1 Introduction Presenting learning materials in a meaningful order according to a curriculum greatly helps learning in animals and humans [ 1, 2, 3, 4], and is considered an essential aspect of good pedagogy [ 5]. For example, humans have been shown to learn visual discriminations faster when presented with examples that exaggerate the relevant difference between classes, a phenomenon known as “fading” [6, 7, 8]. Beyond humans, curricula in the form of “shaping” or “staircase” procedures are a near- universal feature of task designs in animal studies, without which training often fails entirely. For †Department of Computing Sciences, Bocconi University. ‡Gatsby Computational Neuroscience Unit & Sainsbury Wellcome Centre, University College London. §FAIR, Meta AI ∗Equal contributions. Preprint. Under review. arXiv:2106.08068v2  [cs.LG]  12 Oct 2022instance, the International Brain Laboratory task, a standardised perceptual decision-making training paradigm in mice, involves six stages of increasing difﬁculty before reaching ﬁnal performance [9]. Building from this intuition, a seminal series of papers proposed a similar curriculum learning approach for machine learning (ML) [ 10, 11, 12]. In striking contrast to the clear beneﬁts of curriculum in biological systems, however, curriculum learning has generally yielded equivocal beneﬁts in artiﬁcial systems. Experiments in a variety of domains [13, 14] have found usually modest speed and generalisation improvements from curricula. Recent extensive empirical analyses have found minimal beneﬁts on standard datasets [ 15]. Indeed, a common intuition in deep learning practice holds that training distributions should ideally be as close as possible to testing distributions, a notion which runs counter to curriculum. Perhaps the only areas where curricula are actively used are in large language models [16] and certain reinforcement learning settings [17]. This gap between the effect of curriculum in biological and artiﬁcial learning systems poses a puzzle for theory. When and why is curriculum learning useful? What properties of a task determine the extent of possible beneﬁts? What ordering of learning material is most beneﬁcial? And can new learning algorithms better exploit curricula? Compared to the empirical investigations of curriculum learning, theoretical results on curriculum learning remain sparse. Most notably, [18, 19] show that curriculum can lead to faster learning in a simple setting, but the effects of curriculum on asymptotic generalisation and the dependence on task structure remain unclear. A hint that indeed curriculum learning might lead to statistically different minima comes from a connection between constraint- satisfaction problems and physics results on ﬂow networks [20], but to our knowledge no direct result has been reported in the modern theoretical ML literature. In this work we study the impact of curriculum using the analytically tractable teacher-student framework and the tools of statistical physics [ 21, 22, 23, 24]. High-dimensional teacher-student models are a popular approach for systematically studying learning behaviour in neural networks [25, 26, 22], and have recently been leveraged to analyse a variety of phenomena [27, 28, 29, 30, 31, 32]. Using a simple model to build structured data [12], we examine the impact of ordering examples by increasing difﬁculty (curriculum), decreasing difﬁculty (anti-curriculum), or standard shufﬂed training. We derive exact expressions for the online learning dynamics and the performance of batch learning. However, in the latter, curriculum confers no beneﬁt under standard training in our model setting. Motivated by theories of synaptic consolidation and elastic weight consolidation [33, 34], we introduce elastic penalties (Gaussian priors) that regularise training toward solutions obtained in prior curriculum phases, instantiating a long-term memory effect. With these priors, curriculum yields beneﬁts both in the online 3 and in the batch 4 settings. Further related work. The ﬁrst empirical investigation of curriculum learning appeared in 1927 [35], consisting in a visual discrimination task for dogs under curriculum and no-curriculum paradigms. Later behavioural studies proved curricula to be beneﬁcial independent of the ani- mal (dogs, mice, rats, pigeons, humans) and the data modality (visual, auditory, or tactile stimuli) [36, 1, 2, 37, 38, 6]. However, these experimental observations were not observed in standard artiﬁcial neural networks (ANNs). Several ideas in the connectionist community were proposed in order to show curriculum effects in the learning dynamics of ANNs [39, 40, 10, 11]. While these studies were able to match previous experimental data, they also required substantial changes in the architecture of the ANN and/or in the learning rule. Except for very few instances [ 16, 17], standard ML practice tends to avoid taking curricula into account. An obvious obstacle is the fact that most datasets do not provide meta-data about sample difﬁculties. An interesting line of research pointed out the possible relevance of implicit curricula, based on the observation that neural networks tend to consistently learn the samples in a certain order [41]. Thus, a possible way of addressing the lack of difﬁculty labels would be to use the natural learning order as indicative of the various difﬁculties of the training samples. However, a recent work [15], which compared several heuristics for curriculum learning —including implicit curricula— in a variety of settings, showed limited beneﬁts with this strategy. The picture that emerges from the literature seems contradictory: on the one hand, curricula appear fundamental to biological learning; on the other hand, curricula appear largely irrelevant in many machine learning settings. The core motivation behind our work is to reconcile these views and contribute to a theoretical understanding of curriculum learning. 2... Teacher Student ... Input Curriculum Anti-curriculum No curriculum Easy Hard Example # EasyHard Randomly shuffled RelevantIrrelevant (a) (b) (c) Figure 1: Teacher-student setting for curriculum learning. (a) Illustration of teacher-student setting in which a “student” network is trained fromi.i.d. inputs with labels from a “teacher” network. Since the teacher network is sparse, its output depends only on a subset of relevant input features. (b) We consider curricula which order examples by difﬁculty, here taken to be the variance in the irrelevant feature dimensions. We refer to increasing, decreasing, and random difﬁculty order as curriculum, anti-curriculum, and no curriculum, respectively. (c) Example test error on hard examples for the student over training. The switch-point between easy and hard samples lies at α= 1/2. Solid lines show numerical simulations, while dashed lines show theoretical predictions derived in Section 3. For this particular parameter setting, curriculum speeds learning but only modestly improves ﬁnal performance at α= 1. Parameters: α1 = 1, α2 = 1, ∆1 = 0, ∆2 = 1, γ = 10−5, η= 3. 2 Model deﬁnition and overview of approach In the following, we revisit a prototypical model of curriculum learning from [12] that ﬁnds corre- spondence to the fading literature [6] as highlighted in Sec. 5. Our setting is summarised in Fig. 1. The model entails a simple teacher-student setup, where teacher and student are each shallow 1-layer neural networks of size N (also known as perceptrons). The learning task for the student is a binary classiﬁcation problem, with dataset D= {(yµ,xxxµ)}M µ=1, where the ground-truth labels are produced by the teacher network yµ = sign WWWT ·xxxµ. The student learns via empirical risk minimisation of an L2 regularised convex loss. A key feature of this model is that the teacher network is sparse, with only a fraction ρ <1 of ∼N(0,1) non-zero components. Therefore, in order to achieve a good test accuracy, the student has to guess which components should be set to zero and align the relevant weights in the correct direction. A large range of 0 <ρ< 1 could give rise to the phenomenology we seek to analyse. In the remainder of the paper we will focus on the case ρ= 0.5. We model the variable degree of difﬁculty in the samples by decomposing each input vector as xxxµ = [xxxµ r,xxxµ i] ∈RN, where xxxµ r ∈RρN denotes the relevant components of the input, and xxxµ i ∈ R(1−ρ)N the irrelevant ones. Note that, crucially, the sparse teacher network is completely blind to the irrelevant part of the input: yµ = sign ∑ρN j=1 WT,jxµ r,j. While xµ r,j i.i.d. N(0,1) ,∀µ,1 we consider the variance for the irrelevant components to be sample-dependent xµ i,j ∼N(0,∆µ). A smaller variance in the irrelevant part induces a higher SNR in the student learning problem. The dataset is partitioned according to difﬁculty levels given by the variances of the irrelevant inputs. For simplicity we consider only two partitions in most of our analysis, but generalisations to multiple difﬁculty levels follow straightforwardly. We thus have a dataset with M = (α1 + α2)N = αN samples in total. In the ﬁrst α1N samples the irrelevant inputs have variance ∆1, while for the remaining α2N samples the variance is ∆2 >∆1. In the curriculum learning condition we present the easy examples ﬁrst, while in the anti-curriculum condition we present the hard examples ﬁrst. Standard learning presents examples shufﬂed in random order. 1In [12] the input distribution is uniform between 0 and 1, but this does not qualitatively change the results. 33 Online dynamical solution in the large input limit We start by focusing on the same online learning setting explored in [ 12]. We consider a 1-layer student network with sigmoidal activation function,σ(·) = erf(·/ √ 2), that learns to minimise a mean square error loss with L2 regularisation of intensity γ, using gradient descent. This yields the updates WWWµ+1=WWWµ− η√ N σ′ (WWWµ·xxxµ √ N )( σ (WWWµ·xxxµ √ N ) −yµ ) xxxµ−γWWWµ. (1) The dynamics of the model can be analysed in the high-dimensional limit N,M →∞ with α = M/N = O(1). Generalising the results of [26, 42] on the online stochastic gradient descent dynamics in single-layer regression problems, we obtain a precise description of the performance at all times, as a function of several order parameters: the squared norm of the relevant and irrelevant part of the student weights Qr = 1 NWWWr ·WWWr and Qi = 1 NWWWi ·WWWi, respectively; the overlap of the relevant weights of the student and teacher R = 1 NWWWr ·WWWT; and the squared norm of the teacher vector T = 1 NWWWT ·WWWT. In particular, given Qr, Qi, Rand T, the test loss (i.e. average loss on a new example) on a dataset with variance ∆ in the irrelevant inputs is given by LMSE = 1 2 + 1 πsin−1 Qr + ∆Qi 1 + Qr + ∆Qi −2 πsin−1 R/ √ T√Qr + ∆Qi + 1, the accuracy by A= E [1 2(ysignˆy+ 1) ] = 1 2 + 1 πsin−1 ( R√ T(Qr + ∆Qi) ) . (2) If the dataset contains a random mixture of different difﬁculty levels ∆1,∆2,... , the loss and accuracy can be obtained by taking a weighted average over the partitions. To understand how test performance changes through learning, we study the evolution of the order parameters. Combining their deﬁnition with the deﬁnition of the dynamics (1) and the fact that the random variables concentrate in the high-dimension as N →∞, we obtain an analytic form for the updates: Qr ←fQr ( Qr,Qi,R,T ) ,Qi ←fQi ( Qr,Qi,R,T ) ,R ←fR ( Qr,Qi,R,T ) ; where fQr, fQi and fR are long but explicit expressions that are reported in the supplementary material (SM). Dynamical advantages of curriculum. With these theoretical results in hand, we can now char- acterise the performance of curricula in the online setting. We obtain a description of the learning trajectories for each learning protocol, yielding the evolution of training and test accuracies, and of other observables such as the norm of the student and its overlap with the teacher. Solving the dynamical equations gives two key advantages relative to simulating models in this setting. First, they are free of ﬁnite size effects and stochastic ﬂuctuations. And second, their evaluation is very fast (up to 6 orders of magnitude in simulation time reduction see SM E), enabling systematic exploration of the parameter space of the problem, along with ﬁne-grained optimisation over hyper-parameters such as learning rate, weight decay and scaling in the initialisation. Optimising ﬁnal test accuracy separately for each curriculum strategy, we ﬁnd that curriculum learning is the optimal strategy, followed by baseline (no-curriculum) and lastly anti-curriculum. In Fig. 1c we show typical learning trajectories for a dataset with equal numbers of easy and hard samples. The results of the simulations (solid lines) are well-described by our theoretical equations (dashed lines), and show that the curriculum strategy leads to better performance throughout training. Fig. 1c shows the evolution during training of the test accuracy computed on the whole dataset. Next, we systematically trace the effect of curriculum for a range of total dataset sizes ( α1 + α2) and number of easy examples α1 in the phase diagram in Fig. 2. This diagram shows in panels (a) and (b) the accuracies on hard instances reached at the end of training, by curriculum learning and anti-curriculum learning respectively, normalised by the accuracy reached by the standard strategy. The two heatmaps show that curriculum learning always outperforms standard learning and that, on the other hand, anti-curriculum learning outperforms standard learning only in part of the diagram. Comparing the two strategies, in Fig. 2 (c), we can observe that there is a region for small αand α1 where anti-curriculum learning is the best strategy, while in the majority of the situations curriculum 40.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001 (a) Curriculum learning. 0.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001  (b) Anti-curriculum learning. 0.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001 0.990 0.995 1.000 1.005 1.010 (c) Curriculum vs anti-curriculum. Figure 2: Phase diagram of online learning performance gap with optimal parameters. The colour scale shows the ratio of the accuracy on hard instances reached by curriculum over no- curriculum (a), anti-curriculum over no-curriculum (b), and curriculum over anti-curriculum (c), as a function of the total dataset size (α1 + α2) and easy dataset size (α1). Curriculum broadly beneﬁts performance and anti-curriculum is effective in certain regions, but the size of the improvement is modest. Parameters: ρ= 0.50,∆1 = 0,∆2 = 1. learning is best. Interestingly, there is a sizeable region of the diagram in which both curriculum and anti-curriculum help, possibly explaining why both have been recommended in prior work [12, 14, 43, 44, 45]. A possible intuition behind this counter-intuitive phenomenon highlighted by our analysis is that, in some settings, the large amount of noise contained in the hard data will always be too disruptive for effective learning. Thus, leaving the easy (cleaner) data for last could allow the model to better exploit it. Further, we ﬁnd that our setting, in which a small task-relevant signal is embedded in large task- irrelevant variation, is critical to the beneﬁt of curriculum. Fig. 4 shows performance as a function of sparsity ρ, additional details are deferred in the SM C. Non-sparse tasks do not beneﬁt. Hence curriculum aids tasks with many irrelevant factors of variation. Interestingly, the literature from human psychology shows precisely this: no curriculum beneﬁts for low-dimensional tasks or tasks with no variation in irrelevant dimensions [6]. Our results also highlight the intricate dependence of curriculum on parameters of the learning setup. If not all parameters are correctly optimised, we can observe more complex scenarios. For instance, the initialisation condition for the norm of the weights of the student plays an important role. We explore this dependence by changing the variance of the normal distribution from which the initial weights are sampled from. We observe that anti-curriculum learning becomes the best strategy when the variance is large, as shown in Fig. 3 for weights of order 1. In this case, curriculum learning shows an advantage only in the ﬁrst phase when easy examples are shown, which is consistent with the results of [19]. However, in the next phase when hard examples are shown, the curriculum strategy does not extract enough information and it is outperformed by the other two strategies. The fact that curriculum or anti-curriculum can look better depending on the parameter setting might help explain the confusion in the literature over the best protocol [12, 14, 43, 44, 45]. At least in this model, better performance from anti-curriculum is a signature of a sub-optimal choice of the parameters. To summarise our ﬁndings in this online learning setting, curriculum mainly offers a dynamical advantage: it speeds up learning but has minimal impact on asymptotic performance. 4 Batch learning solution The previous section discussed the online case where each example is used once and then discarded. However, in common machine learning practice, neural networks typically revisit each sample repeat- edly until convergence. Therefore an important question is: can curricula lead to a generalisation improvement when trained on the same dataset until convergence? We investigate this question by considering a student that learns from slices of a dataset in distinct optimisation phases, where in each phase the student optimises aL2-regularised logistic loss. Without 50.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001 (a) Curriculum learning. 0.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001 0.98 0.99 1.00 1.01 1.02 (b) Anti-curriculum learning. 0.0 0.2 0.4 0.6 0.8 1.0 50 55 60 65 70accuracy % curriculum anti-curriculum no curricula (c) Accuracy trajectories. Figure 3: Performance gap starting from high initialisation norm. The ﬁrst two ﬁgures show the accuracy-gap on hard instances between curriculum learning and the baseline (a) and anti-curriculum learning and the baseline (b). Contrary to the phase diagram in Fig. 2, curriculum learning is not always the optimal and anti-curriculum is not always the worst strategy. The right panel shows the accuracy evaluated on the hard samples for α1 = α2 = 0.5. further modiﬁcation, curriculum can have no effect in this setting: due to the convex nature of the teacher-student setup [22], the network is bound to converge to a minimum uniquely determined by the ﬁnal slice of data, with no memory of the progress made at intermediate steps. This simple observation may help explain empirical observations on real data, such as [15], which ﬁnd no beneﬁt of curriculum in standard settings. In fact, in principle curriculum could still inﬂuence non-convex problems [12] but empirical results in the ML ﬁeld are not showing clear signals of memory retention. A possible explanation of this is that relying on dynamical memory effects requires careful tuning of the learning rate and of the number of training epochs, while typical choices for these hyper- parameters could lead to memory loss and performance inconsistencies. These observations raise the theoretical question of how to better implement curriculum learning to induce a non-vanishing effect also in batch learning settings. To instantiate a long-term memory effect in our model, we propose biasing the optimisation landscape via a Gaussian prior, centred around the optimiser of the previous learning phase. The additional term in the loss acts as an elastic coupling between the successive phases, and the associated intensity γ12 is then an additional hyper-parameter of the model. This scheme is similar to regularisation methods proposed against catastrophic interference in continual learning, such as Synaptic Intelligence [46]. Changing the loss according to the curriculum prescription effectively makes the learning algorithm aware of the different levels of difﬁculty in the dataset. Tools from statistical physics can be used to analytically compute test performance under this scheme. In order to simplify the presentation, we ﬁrst consider just two learning phases. It is natural to frame this setting as a 2-level problem, involving two systems with independent copies of the network weights WWW1 and WWW2. In a typical statistical physics approach, we associate a Boltzmann-Gibbs measure to the systems, with an energy function determined by the regularised logistic lossLγ. While the statistical properties of the ﬁrst system can be determined self-consistently, the added elastic interaction creates a dependence of the second measure on the conﬁgurations of the ﬁrst system. In mathematical terms, the coupled system is represented by the following partition function: ⟨Z(WWW2,WWW1; D1,D2)⟩WWW1 = ∫ dWWW1 e−β1Lγ1(WWW1,D1) Z1(WWW1) log ∫ dWWW2 e−β2(Lγ2(WWW2,D2)+ γ12 2 ∥WWW2−WWW1∥2 2) (3) where D1,D2 denote the two dataset slices. This object represents the normalisation of the Boltzmann- Gibbs measure, and allows one to extract relevant information on the asymptotic behaviour of our model. The optimisations entailed in each learning phase can be described in the “low noise” limit of β1,β2 →∞, where the measures focus on the minimisers of the respective losses. In order to study a self-averaging quantity that does not depend on a speciﬁc realisation of the dataset, we aim to compute the associated average free-energy: Φ = lim N→∞ lim β1,β2→∞ 1 β2N ⟨ log ⟨Z(WWW2,WWW1; D1,D2)⟩WWW1 ⟩ D1,D2 . (4) 65  4  3  2  1  0 1 2 3 log 12 0.74 0.76 0.78 0.80 0.82 0.84accuracy hard samples %  curriculum anti-curriculum random splitting no-curriculum (a) Learning with curricula 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 0.94 0.96 0.98 1.00 1.02 1.04 1.06 (b) Curriculum vs no-curriculum Figure 4: Effect of elastic coupling (Gaussian prior) between curriculum phases.(a) comparison between asymptotic performance of curricula (full lines) and single batch learning, at α1 = 1 α2 = 1, with a regularisationγ1 that yields the best generalisation when learning the entire dataset (in principle not optimal for the other strategies). The points represent the results from 10 numerical simulations at size N = 2000. Parameters: ρ= 0.50, ∆1 = 0 and ∆2 = 1. (b) ratio between the accuracy reached by curriculum learning over anti-curriculum as a function of the number of easy samples in a dataset of dimension α1 + α2 = 1, and of the sparsity level of the teacher ρ. Note that ρcan also be seen as the fraction of relevant components in the inputs. ∆1 = 0 and ∆1 = 1. γ1 = γ2 and γ12 where set the values that optimise test performance. This quantity can be seen as a special case of the so-called Franz-Parisi potential computation [47, 48], and the entailed double average can be evaluated through the replica method. Refer to SM for details. Similar to the online case, in high-dimensions the free-entropy concentrates on a deterministic function that depends on several order parameters that capture the geometrical distribution of teacher and student conﬁgurations. In addition to those already introduced in Sec. 3, we also have δQ, which is linked to the variance of the student norm. Moreover, for each order parameter we also need to introduce a conjugate parameter, denoted in the following with the hat symbol. The ﬁnal expression for the free-energy reads: Φ = extr [ − ( ˆRR+ 1 2 (( ˆQδQ−δˆQQ ) r+i )) + gS(γ1,γ2,γ12) + α1 gE(∆1) + α2 gE(∆2) ] (5) where gS and gE are two scalar functions, often called entropic and energetic channels, that encode the dependence of the optimisation problem on the Gaussian prior and the logistic loss respectively. The extremum condition for the free-energy yields a system of ﬁxed-point equations that converge to an asymptotic prediction for the order parameters, comparable with the results of numerical simulations on large instances, Fig. 4. At convergence, the order parameters can be inserted again in Eq. 2 to obtain an estimate of the test accuracy. Note that this formalism is not limited to two phases, but can be extended to the case of a discrete number of sequential stages. The importance of sparsity. Sparsity is a key ingredient in determining the impact of curriculum strategies. It naturally introduces a notion of relevant and irrelevant inputs, and deﬁnes a secondary learning goal: identifying what part of the presented data should be disregarded by the model. Curriculum learning can aid this identiﬁcation process, since the easy samples are more transparent to this structure. This is also observed in human experiments [ 6]. However, the relative difﬁculty of the problem of inferring the support of the teacher and the problem of aligning with its non-zero components depends on the degree of sparsity ρ, so the effectiveness of curriculum can vary with it. In the right panel of Fig. 4, we explore the interplay between the sparsity of the teacher ρand the fraction of easy samples in the dataset α1, comparing curriculum with the no-curriculum baseline. The phase diagram highlights the variability in the impact of the curriculum ordering: • Curriculum is most effective at low values of ρand close to the diagonal, where the fraction of easy examples in the dataset is comparable to the fraction of relevant dimensions. 7• When ρ >0.5, the possible gain from ordering the samples according to difﬁculty is counterbalanced by the instrinsic cost of splitting the information content into two blocks, thus curriculum can become detrimental. • When α1 is too small compared to ρ(above diagonal), the ﬁrst stage in the curriculum strategy can only help in the support identiﬁcation problem, but will not allow a good estimation of the direction of the teacher. Because of the elastic prior, the second stage cannot improve too much over it and the effect of curriculum is small. • When αis larger than the sparsity (below diagonal), the easy examples contain sufﬁcient information for solving both the support and the teacher estimation problems, and this information is also exploited by the baseline. Thus the improvement of curriculum becomes negligible. We refer to the SM for an in-depth comparison with anti-curriculum. 0.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001 (a) Curriculum learning. 0.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001  (b) Anti-curriculum learning. 0.5 1.0 1.5 2.0 1 + 2 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.001 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 (c) Curriculum vs anti-curriculum. Figure 5: Phase diagram for the performance gap in the batch setting. The colour scale shows the ratio of the accuracy on hard instances for curriculum over no-curriculum (a), anti-curriculum over no-curriculum (b), and curriculum over anti-curriculum (c), as a function of the total dataset size (α1 + α2) and easy dataset size (α1). In contrast to the online case, performance beneﬁts are greater and curriculum is strictly better than anti-curriculum. Both γ1 = γ2 and γ12 are optimised point-wise, in order to yield the best test accuracy. Parameters: ρ= 0.50,∆1 = 0,∆2 = 1. Asymptotic advantages of curriculum. Contrary to the case of online SGD, if the fraction of relevant directions is small, batch learning with elastic coupling notably improves test accuracy of both curriculum and anti-curriculum above the baseline. This conﬁrms the utility of curriculum strategies when the signal is partially \"hidden in clutter\" [49]. Fig. 5 shows similar phase diagrams to Fig. 2 but for the batch setting. At each point in the phase diagram the regularisation level γ1 = γ2 and the coupling γ12 are optimised to yield the best accuracy. We ﬁnd that the performance order is nearly always preserved: curriculum followed by anti-curriculum followed by baseline. In the SM we see similar improvements by applying the elastic coupling strategy both in the online setting and on real data. In summary, in the batch setting, splitting the learning process in stages might not be advantageous per se. However, our observations show that if the loss is modiﬁed to reduce memory loss between the learning stages, curriculum learning strategies can offer a measurable asymptotic advantage. 5 Connection with experimental literature Recent work has suggested that curriculum learning could provide an important window into the learning algorithms at work in biology [51]. Our analysis makes several predictions for curriculum effects. In this section we assess these predictions based on connections to extant experiments and propose future experimental tests. First, we ﬁnd that a curriculum strategy yields a speed up in learning in all the tested settings (see Fig. 1c). This acceleration is broadly consistent with the ﬁndings from cognitive science [ 1, 2, 6]. By contrast, our results show that the speed improvement does not necessarily translate into a sizeable 80.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0accuracy ratio 0 10 20 30 40 50 1.01 1.02 1.03 11.0 11.2 11.4 11.6 11.8 difficulty 1.0 1.5 2.0 (a) Generalisation gap per difﬁculty. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0accuracy ratio 0.2 0.4 0.6 0.8 1.0 1.00 1.05 1 = 0.05, 2 = 0.95 1 = 0.10, 2 = 0.90 1 = 0.30, 2 = 0.70 hidden signal clear signal confusion level 1.0 1.2 1.4 1.6 (b) Role of sparsity. EasyHard OldNew (c) Fading experiment [6]. Figure 6: Connection with psychology experiments. (a) Top: Accuracy ratio of different strategies in the model, with curriculum/no-curriculum in green and curriculum/anti-curriculum in orange. The ratio shows non-monotonic behaviour. Bottom: The accuracy ratio obtained by [ 50]. Parameters ρ = 0.5, ∆1 = 0.0, ∆2 = 1.0, α1 = 1, α2 = 1 and optimal learning rate, norm at initialisation and weight decay intensity. (b) Top: Dependence on the sparsity of the generalisation gain of curriculum over no-curriculum, measured as ratio between ﬁnal accuracy, for ﬁxed total dataset size (α1 + α2 = 1). Bottom: The ratio obtained from experiments 3 and 4 of [ 6]. (c) Example cartoon stimuli from the “fading” paradigm used in [6], where participants distinguish daemons of the old world from daemons of the new world. The distinguishing feature (horn length) is diluted among many irrelevant features (colour, eye size, mouth size). Highlighting the relevant feature to participants leads to better and faster learning. generalisation error improvement, and the performance achieved at the end of training can even deteriorate when learning hyperparameters are not fully optimised (c.f. Fig. 3). Deterioration due to curricula has generally not been reported in the psychology literature, though it has been observed in ML [15]. This fact may suggest that animals naturally learn with near-optimal hyperparameters such that curricula generally confer beneﬁts. A more speciﬁc observation concerns the performance on different difﬁculties after learning. As reported in [50], human and rodent subjects trained in an auditory task using curricula showed the greatest improvement for intermediate levels of difﬁculty as depicted in Fig. 6a bottom panel. The same conclusion can be drawn from the experiment of [7, 8], where, surprisingly, subjects trained with curricula to classify medical images showed poor performance in hard tasks compared to the control group. To address this phenomenon, we calculate accuracy as a function of difﬁculty in the model in Fig. 6a top panel. Consistent with these experiments, we ﬁnd regimes where the gap between curriculum learning and the baseline is non-monotonic, with the largest performance gain for intermediate difﬁculties. Contrary to [7, 8], however, we do not observe negative effects of curriculum for high difﬁculties. Further experiments that more systematically manipulate training and transfer difﬁculties could provide a stronger test of these predictions. A key ingredient in our model is the role of sparsity, such that a small signal is embedded amidst many irrelevant features. Experimentally, the importance of having many factors of variation to obtaining a curriculum effect has been documented in the “fading” experiments of [ 6]. Human subjects were trained on classiﬁcation tasks involving stimuli with one task-relevant feature dimension and a variable number of task-irrelevant feature dimensions. Example cartoon “daemon” stimuli are depicted in Fig. 6c, where for instance horn height might be the distinguishing feature while colour, eye size, and mouth size might constitute task-irrelevant features. Without any irrelevant factors of variation (ρ= 1), they report no curriculum beneﬁt. By contrast when 75% of features are irrelevant (ρ= .25), they record a strong curriculum effect, as shown in Fig. 6b bottom. This qualitative trend is also observed in our model (Fig. 6b top). While these experiments tested only two sparsity levels, further experiments could sample this dimension more extensively and test for interactions with the fraction of easy and hard examples. We note that while the connectionist literature has addressed the effect of curriculum in several settings [39, 40, 10, 11], we found that easy-to-hard effects appear even in a simple setup without need for complex networks and/or dynamics. Finally, our results may shed light on self-generated curricula during human development [52, 53]. Children undergo a vocabulary spurt that coincides with their ability to grasp and centre objects in the 9(a) (b) (d)(c) 0 20000 40000 Total dataset size 0 10000 20000 30000 40000 50000Easy dataset size 0.94 0.96 0.98 1.00 1.02 1.04 1.06 Figure 7: Experimental setting on CIFAR10-derived data. (a) Input samples combine a task- relevant image with a distractor image, and become progressively harder from left to right. (b) Ratio between ﬁnal accuracy on hard instances for curriculum learning versus no curriculum. η,γ,γ 12,init, and stopping time are optimised. visual ﬁeld [53]. Quantitative estimates of the amount of clutter (irrelevant objects) in self-generated views decrease due to this grasping ability, yielding a self-generated curriculum [49, 54]. Our model similarly predicts that reducing clutter should improve learning speed and performance. Real-World Demonstration. To verify this prediction in a richer visual setting, we construct a simple cluttered object classiﬁcation task from the CIFAR10 dataset [55] by patching two images together into a 32 ×64 input image (Fig. 7a). The task is to produce the class label of the image on the left. The right image is a distractor that is irrelevant to the classiﬁcation. To vary difﬁculty, we scale the contrast of the irrelevant image (Fig. 7a-d). We train a single-layer network with the cross-entropy loss and the curriculum protocol with Gaussian prior between two curriculum stages, implemented in Pytorch Lightning to ensure that training parameters accord with standard practice. We optimised hyperparameters in each curriculum phase separately. We trained all combinations of ﬁve elastic penalties log spaced between 1e−3 and 1e2, and weight decay parameters {0,.2,.5}. We then compute the best performing model for ﬁve random seeds and take the mean over seeds. Further dataset, model and experimental details are given in Appendix D. As shown in Fig. 7b, curriculum improves performance, particularly when easy examples make up a large proportion of the dataset, conﬁrming that curricula that reduce clutter can beneﬁt learning. 6 Conclusions We analysed a model of curriculum learning introduced by [12] and amenable of analytical treatment. This simple setting sheds light on results observed in the cognitive science and machine learning literature, and the theoretical tractability allows for exploration of a wide range of parameters that would be costly to obtain through experiments. Future work will need to move beyond models with simple loss landscapes to address the impact of curricula in complex tasks like reinforcement learning. Nevertheless, the model recapitulates a variety of observations in the literature [50, 56, 57], revealing that easy-to-hard effects can appear when a sparse signal is embedded in many irrelevant dimensions of variation. We ﬁnd that making the algorithm curriculum-aware by modifying the loss can better exploit curricula, offering a potential route for improved practical algorithms. Other curriculum-aware approaches are possible such as adapting the learning algorithm [ 58] or the architecture [10]. On the psychology side, our predictions can help in designing new experiments, for instance testing the counter-intuitive beneﬁt of anti-curriculum learning for intermediate sparsity. Acknowledgments and Disclosure of Funding We thank Miguel Ruiz-Garcia and Ronald Dekker for important discussions. L.S. acknowledges funding from the ERC European Union Horizon 2020 Research and Innovation Program Grant Agreement 714608-SIiLe. S.S.M. & A.S. were supported by a Wellcome and Royal Society Henry Dale Fellowship (216386/Z/19/Z) and Sainsbury Wellcome Centre Core Grant (219627/Z/19/Z, GAT3755). A.S. is a CIFAR Azrieli Global Scholar in the Learning in Machines & Brains programme. 10References [1] Douglas H Lawrence. The transfer of a discrimination along a continuum. Journal of Compara- tive and Physiological Psychology, 45(6):511, 1952. [2] Robert A Baker and Stanley W Osgood. Discrimination transfer along a pitch continuum. Journal of Experimental Psychology, 48(4):241, 1954. [3] Renee Elio and John Anderson. The effects of information order and learning mode on schema abstraction. Memory & Cognition, 12:20–30, January 1984. [4] Robert C. Wilson, Amitai Shenhav, Mark Straccia, and Jonathan D. Cohen. The Eighty Five Percent Rule for optimal learning. Nature Communications, 10(1):4646, November 2019. [5] Judith Avrahami, Yaakov Kareev, Yonatan Bogot, Ruth Caspi, Salomka Dunaevsky, and Sharon Lerner. Teaching by Examples: Implications for the Process of Category Acquisition. The Quar- terly Journal of Experimental Psychology Section A, 50(3):586–606, August 1997. Publisher: SAGE Publications. [6] Harold Pashler and Michael C. Mozer. When does fading enhance perceptual category learning? Journal of Experimental Psychology: Learning, Memory, and Cognition , 39(4):1162–1173, 2013. [7] Adam N. Hornsby and Bradley C. Love. Improved classiﬁcation of mammograms following idealized training. Journal of Applied Research in Memory and Cognition, 3(2):72–76, June 2014. [8] Brett D. Roads, Buyun Xu, June K. Robinson, and James W. Tanaka. The easy-to-hard training advantage with real-world medical images. Cognitive Research: Principles and Implications, 3, October 2018. [9] The International Brain Laboratory, Valeria Aguillon-Rodriguez, Dora Angelaki, Hannah Bayer, Niccolo Bonacchi, Matteo Carandini, Fanny Cazettes, Gaelle Chapuis, Anne K Churchland, Yang Dan, Eric Dewitt, Mayo Faulkner, Hamish Forrest, Laura Haetzel, Michael Häusser, Sonja B Hofer, Fei Hu, Anup Khanal, Christopher Krasniak, Ines Laranjeira, Zachary F Mainen, Guido Meijer, Nathaniel J Miska, Thomas D Mrsic-Flogel, Masayoshi Murakami, Jean-Paul Noel, Alejandro Pan-Vazquez, Cyrille Rossant, Joshua Sanders, Karolina Socha, Rebecca Terry, Anne E Urai, Hernando Vergara, Miles Wells, Christian J Wilson, Ilana B Witten, Lauren E Wool, and Anthony M Zador. Standardized and reproducible measurement of decision-making in mice. eLife, 10:e63711, May 2021. Publisher: eLife Sciences Publications, Ltd. [10] Jeffrey L. Elman. Learning and development in neural networks: the importance of starting small. Cognition, 48(1):71–99, July 1993. [11] Kai A. Krueger and Peter Dayan. Flexible shaping: how learning in small steps helps.Cognition, 110(3):380–394, March 2009. [12] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48, 2009. [13] Anastasia Pentina, Viktoriia Sharmanska, and Christoph H. Lampert. Curriculum learning of multiple tasks. pages 5492–5500. IEEE Computer Society, June 2015. ISSN: 1063-6919. [14] Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks. In ICML, volume 97, pages 2535–2544. PMLR, 2019. [15] Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? ICLR, 2020. [16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Advances in Neural Information Processing Systems, 33:1877–1901, 2020. [17] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized Level Replay. arXiv:2010.03934 [cs], January 2021. arXiv: 2010.03934. 11[18] Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning: Theory and experiments with deep networks. In International Conference on Machine Learning, pages 5238–5246. PMLR, 2018. [19] Daphna Weinshall and Dan Amir. Theory of curriculum learning, with convex loss functions. Journal of Machine Learning Research, 21(222):1–19, 2020. [20] Miguel Ruiz-García, Andrea J Liu, and Eleni Katifori. Tuning and jamming reduced to their minima. Physical Review E, 100(5):052608, 2019. [21] Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientiﬁc Publishing Company, 1987. [22] Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge University Press, 2001. [23] Lenka Zdeborová and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. Advances in Physics, 65(5):453–552, 2016. [24] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl- Dickstein, and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of Condensed Matter Physics, 2020. [25] Leticia F Cugliandolo and Jorge Kurchan. Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model. Physical Review Letters, 71(1):173, 1993. [26] Michael Biehl and Holm Schwarze. Learning by on-line gradient descent. Journal of Physics A: Mathematical and general, 28(3):643, 1995. [27] M.S. Advani, A.M. Saxe, and H. Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428 – 446, 2020. [28] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [29] Stefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova. Passed & spurious: Descent algorithms and local minima in spiked matrix-tensor models. 97:4333– 4342, 09–15 Jun 2019. [30] Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, and Lenka Zde- borová. Who is afraid of big bad minima? analysis of gradient-ﬂow in spiked matrix-tensor models. In Advances in Neural Information Processing Systems, pages 8679–8689, 2019. [31] Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Complex dynamics in simple neural networks: Understanding gradient ﬂow in phase retrieval. 33:3265–3274, 2020. [32] Hugo Cui, Luca Saglietti, and Lenka Zdeborová. Large deviations for the perceptron model and consequences for active learning. In Mathematical and Scientiﬁc Machine Learning, pages 390–430. PMLR, 2020. [33] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning , pages 3987–3995. PMLR, 2017. [34] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. [35] P Ivan Pavlov. Conditioned reﬂexes: an investigation of the physiological activity of the cerebral cortex. Annals of neurosciences, 17(3):136, 2010. [36] Burrhus Frederic Skinner. The behavior of organisms: An experimental analysis. BF Skinner Foundation, 2019. [37] Merav Ahissar and Shaul Hochstein. Task difﬁculty and the speciﬁcity of perceptual learning. Nature, 387(6631):401–406, 1997. 12[38] C Donald Morris, John D Bransford, and Jeffery J Franks. Levels of processing versus transfer appropriate processing. Journal of verbal learning and verbal behavior, 16(5):519–533, 1977. [39] Kim Plunkett, Virginia Marchman, and Steen Ladegaard Knudsen. From rote learning to system building: acquiring verb morphology in children and connectionist nets. In Connectionist Models, pages 201–219. Elsevier, 1991. [40] Kim Plunkett and Virginia Marchman. U-shaped learning and frequency effects in a multi- layered perception: Implications for child language acquisition. Cognition, 38(1):43–102, 1991. [41] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [42] David Saad and Sara A Solla. Exact solution for on-line learning in multilayer neural networks. Physical Review Letters, 74(21):4337, 1995. [43] Tom Kocmi and Ond ˇrej Bojar. Curriculum Learning and Minibatch Bucketing in Neural Machine Translation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 379–386, Varna, Bulgaria, September 2017. INCOMA Ltd. [44] Nathan Schneider, Dirk Hovy, Anders Johannsen, and Marine Carpuat. Semeval-2016 task 10: Detecting minimal semantic units and their meanings (dimsum). In Steven Bethard, Daniel M. Cer, Marine Carpuat, David Jurgens, Preslav Nakov, and Torsten Zesch, editors,Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, USA, June 16-17, 2016, pages 546–559. The Association for Computer Linguistics, 2016. [45] Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat, and Kevin Duh. Curriculum Learning for Domain Adaptation in Neural Machine Translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1903– 1915, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [46] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3987–3995, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. [47] Silvio Franz and Giorgio Parisi. Phase diagram of coupled glassy systems: A mean-ﬁeld study. Physical review letters, 79(13):2486, 1997. [48] Luca Saglietti and Lenka Zdeborová. Solvable model for inheriting the regularization through knowledge distillation. CoRR, abs/2012.00194, 2020. [49] Elizabeth M. Clerkin, Elizabeth Hart, James M. Rehg, Chen Yu, and Linda B. Smith. Real-world visual statistics and infants’ ﬁrst-learned object names. Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences, 372(1711), January 2017. [50] Estella H Liu, Eduardo Mercado III, Barbara A Church, and Itzel Orduña. The easy-to-hard effect in human (homo sapiens) and rat (rattus norvegicus) auditory identiﬁcation. Journal of Comparative Psychology, 122(2):132, 2008. [51] Daniel R. Kepple, Rainer Engelken, and Rajan Kanaka. Curriculum learning as a tool to uncover learning principles in the brain. ICLR, 2022. [52] Hadar Karmazyn Raz, Drew H. Abney, David Crandall, Chen Yu, and Linda B. Smith. How do infants start learning object names in a sea of clutter? Annual Conference of the Cognitive Science Society, 2019:521–526, July 2019. [53] Linda B. Smith and Lauren K. Slone. A Developmental Approach to Machine Learning? Frontiers in Psychology, 8, 2017. [54] Chen Yu and Linda B. Smith. Embodied attention and word learning by toddlers. Cognition, 125(2):244–262, November 2012. [55] Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009. 13[56] Itzel Orduña, Estella H Liu, Barbara A Church, Ann C Eddins, and Eduardo Mercado III. Evoked-potential changes following discrimination learning involving complex sounds.Clinical neurophysiology, 123(4):711–719, 2012. [57] Barbara A Church, Eduardo Mercado III, Matthew G Wisniewski, and Estella H Liu. Temporal dynamics in auditory perceptual learning: impact of sequencing and incidental learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 39(1):270, 2013. [58] Miguel Ruiz-Garcia, Ge Zhang, Samuel S Schoenholz, and Andrea J Liu. Tilting the playing ﬁeld: Dynamical loss functions for machine learning. In International Conference on Machine Learning, pages 9157–9167. PMLR, 2021. 14Supplemental Material A State evolution of the online dynamics In this section we show how to derive the dynamical equations for the online dynamics. The equations given in an implicit form in the main text, fQr,fQi,fR, are reported explicitly at the end of the next section, Eqs. (A.23-A.25). Finally, in the subsequent section, we comment on how the state evolution is modiﬁed to deal with the Gaussian priors and we derive the new dynamical equations for that case. Derivation We follow the derivation proposed in [26, 42] to derive the averaged high-dimensional dynamical equations. The student is a 1-layer network that minimises sample-wise the square error Lµ = 1 2 (yµ −ˆyµ)2 .= 1 2 (δµ)2 . (A.1) Given φ(·) = sign(·),σ(·) = erf(·/ √ 2), the online stochastic gradient descent updates are WWWµ+1 = WWWµ − η√ N σ′(λµ r + λµ i)δµxxxµ, (A.2) with λµ r = 1√ N WWWr ·xxxµ r, (A.3) λµ i = 1√ N WWWi ·xxxµ i, (A.4) ρµ = 1√ N WWWT ·xxxµ r. (A.5) The evolution of the dynamics can be tracked using 4 order parameters: Qr = 1 NWWWr ·WWWr, (A.6) Qi = 1 NWWWi ·WWWi, (A.7) R= 1 NWWWr ·WWWT, (A.8) T = 1 NWWWT ·WWWT; (A.9) representing the overlaps between the weights of student (relevant and irrelevant parts) and teacher. The evolution of those follow from the deﬁnition of the dynamics Eq. (A.2). In the high-dimensional limit the random variables in the problem concentrates around the mean, therefor to the leading order we have the following equations Qr[k+ 1] = Qr[k] + 1 N [ 2ηE[δσ′(λr + λi)λr] + ρ∆η2E[δ2 σ′(λr + λi)2] ] ; (A.10) Qi[k+ 1] = Qr[k] + 1 N [ 2ηE[δσ′(λr + λi)λi] + (1−ρ)∆η2E[δ2 σ′(λr + λi)2] ] ; (A.11) T[k+ 1] = Qr[k] + 1 N [ηE[δσ′(λr + λi)ρ]] . (A.12) 15(A.13) Where the expectation acts with respect to all the stochastic variables. In order to obtain explicit formulae we need to evaluate those averages. The random variables in the equations – λr, λi and ρ– are Gaussian with zero mean, to characterise them we only need their covariance: Σλr,λi,ρ = (Qr 0 R 0 Qi 0 R 0 T ) . In order to derive analytical expression we must evaluate the expected values: E[φ(ρ)σ′(λ)ρ], E[φ(ρ)σ′(λ)λ], E[σ(λ)σ′(λ)ρ], E[σ(λ)σ′(λ)λ], E[φ(ρ)2σ′(λ)2], E[σ(λ)2σ′(λ)2], and E[φ(ρ)σ(λ)σ′(λ)2]. Where σ is the activation function of the student and φ is the activa- tion function of the teacher (in particular φ(·) = sign(·) for classiﬁcation). E[φ(ρ)σ′(λ)ρ] = 2 π √ T(Qr + Qi + 1) −R2 Qr + Qi + 1 (A.14) E[φ(ρ)σ′(λ)λr] = 2 π R(Qi + 1) Qr + Qi + 1 1√ T(Qr + Qi + 1) +R2 . (A.15) E[φ(ρ)σ′(λ)λi] = −2 π RQi Qr + Qi + 1 1√ T(Qr + Qi + 1) +R2 . (A.16) E[σ(λ)σ′(λ)ρ] = 2 π R Qr + Qi + 1 √ Qi + 1 2Q2 i + 2QrQi + 3Qi + 2Qr + 1. (A.17) E[σ(λ)σ′(λ)λr] = 2 π Qr Qr + Qi + 1 √ Qi + 1 2Q2 i + 2QrQi + 3Qi + 2Qr + 1. (A.18) E[σ(λ)σ′(λ)λi] = 2 π Qi Qr + Qi + 1 √ Qr + 1 2Q2r + 2QrQi + 3Qr + 2Qi + 1. (A.19) E[φ(ρ)2σ′(λ)2] = 2 π 1√2Qr + 2Qi + 1. (A.20) E[σ(λ)2σ′(λ)2] = 4 π2 1√ 1 + 2(Qr + Qi) sin−1 ( Qr + Qi 1 + 3(Qr + Qi) ) . (A.21) E[φ(ρ)σ(λ)σ′(λ)2] = 4 π2 1√ 2(Qr + Qi) + 1 sin−1 ( R√Qr + Qi√ 3(Qr + Qi) + 1 √ (2Qr + 2Qi + 1)[T(Qr + Qi) −R2] + R2 ) . (A.22) Finally, we can substitute those equations into the Eqs. (A.10-A.12) and obtained the state evolution equations used in the main Sec. 3: fQr ( Qr[k],Qi[k],R[k],T ) = (1 −ηγ)2Qr[k] + 4η(1 −ηγ) Nπ(Qr[k] + ∆Qi[k] + 1)× [ R[k](∆Qi[k] + 1)√ T(Qr[k] + ∆Qi[k] + 1) +R[k]2 − Qr[k]√ 2Qr[k] + 2∆Qi[k] + 1 ] + 4 π2 ρη2 N √ 2(Qr[k] + ∆Qi[k]) + 1 [ π 2 + sin−1 ( Qr[k] + ∆Qi[k] 1 + 3(Qr[k] + ∆Qi[k]) ) + −2 sin−1 ( R[k]√ 3(Qr[k] + ∆Qi[k]) + 1 √ T(2Qr[k] + 2∆Qi[k] + 1)−2R[k]2 )] ; (A.23) 16fQi ( Qr[k],Qi[k],R[k],T ) = (1 −ηγ)2Qi[k] − 4η(1 −ηγ)∆Qi[k] Nπ(Qr[k] + ∆Qi[k] + 1)× [ R[k]√ T(Qr[k] + ∆Qi[k] + 1) +R[k]2 + 1√ 2Qr[k] + 2∆Qi[k] + 1 ] + + 4 π2 (1 −ρ)∆η2 N √ 2(Qr[k] + ∆Qi[k]) + 1 [ π 2 + sin−1 ( Qr[k] + ∆Qi[k] 1 + 3(Qr[k] + ∆Qi[k]) ) + −2 sin−1 ( R[k]√ 3(Qr[k] + ∆Qi[k]) + 1 √ T(2Qr[k] + 2∆Qi[k] + 1)−2R[k]2 )] ; (A.24) fR ( Qr[k],Qi[k],R[k],T ) = (1 −ηγ)R[k] + 2η Nπ(Qr[k] + ∆Qi[k] + 1)× [ T(Qr[k] + ∆Qi[k] + 1)−R[k]2 √ T(Qr[k] + ∆Qi[k] + 1)−R[k]2 − R[k]√ 2Qr[k] + 2∆Qi[k] + 1 ] . (A.25) Elastic coupling The introduction of the elastic coupling between stages of learning adds ﬁve new order parameters: three of them are just reminder of the previous stage and do not need to by updated ˜Qr = WWWr 1 ·WWWr 1/N, ˜Qi = WWWi 1 ·WWWi 1/N, and ˜R= WWWi 1 ·WWWT/N; two measure the correlation between the two stages Sr = WWWr 1 ·WWWr 2/N and Si = WWWi 1 ·WWWi 2/N to the equations. These terms have associated their own state evolution equations slightly modiﬁed the updates of the other order parameters. Qr[k+ 1] = (1 −ηγ+ ηγ12)2Qr[k] + 2η N(1 −ηγ+ ηγ12)E[δσ′(λr + λi)λr] + ρ∆η2 NE[δ2 σ′(λr + λi)2] + 2ηγ12(1 −ηγ+ ηγ12)Sr[k] + η2γ2 12 ˜Qr[k] −2η2γ12 N E[δσ′(λr + λi)˜λr]; (A.26) Qi[k+ 1] = (1 −ηγ+ ηγ12)2Qi[k] + 2η N(1 −ηγ+ ηγ12)E[δσ′(λr + λi)λi] + (1 −ρ)∆η2 NE[δ2 σ′(λr + λi)2] + 2ηγ12(1 −ηγ+ ηγ12)Si[k] + η2γ2 12 ˜Qi[k] −2η2γ12 N E[δσ′(λr + λi)˜λi]; (A.27) R[k+ 1] = (1 −ηγ+ ηγ12)R[k] + η NE[δσ′(λr + λi)ρ] −ηγ12 ˜R[k]; (A.28) Sr[k+ 1] = (1 −ηγ+ ηγ12)Sr[k] + η NE[δσ′(λr + λi)˜λr] −ηγ12 ˜Qr[k]; (A.29) Si[k+ 1] = (1 −ηγ+ ηγ12)Si[k] + η NE[δσ′(λr + λi)˜λi] −ηγ12 ˜Qi[k]. (A.30) Introduced ˜λr = 1√ Nxxxr · ˜WWWr and ˜λi = 1√ Nxxxi · ˜WWWi, this two additional random variables need to be averaged together with the others. The joint distribution of λr,λi,˜λr,˜λi,ρ is still Gaussian with zero mean and covariance Σλr,λi,˜λr,˜λi,ρ =   Qr 0 ˜Sr 0 R 0 Qi 0 ˜Si 0 ˜Sr 0 ˜Qr 0 ˜R 0 ˜Si 0 ˜Qi 0 R 0 ˜R 0 T   . Notice that, a part from a slight change of the existing equations, the coupling introduces only two additional integrals E[δ σ′(λr + λi)˜λr] and E[δ σ′(λr + λi)˜λi]. After long, but straightforward, 170.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4 0.5m curriculum anti-curriculum no curricula 0.0 0.1 0.2 0.3 0.4 0.40 0.42 0.44 0.46 0.48 0.50 0.0 0.1 0.2 0.3 0.4 50.0 52.5 55.0 57.5 60.0 62.5 65.0 67.5accuracy % (a) No elastic coupling. 0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4 0.5m curriculum anti-curriculum no curricula 0.0 0.1 0.2 0.3 0.4 0.40 0.42 0.44 0.46 0.48 0.50 0.0 0.1 0.2 0.3 0.4 50.0 52.5 55.0 57.5 60.0 62.5 65.0 67.5 70.0accuracy % (b) Optimal elastic coupling. Figure A.1: Effect of elastic coupling in the curriculum. Figures showing the teacher-student cosine, the validation loss, and the accuracy of the three learning strategies. The two ﬁgures show the performance in presence (above) and absence (below) of elastic coupling. The dashed lines are obtained from the theoretical analysis, the full line come from the average of 500 simulations. The parameters η, γ, initialisation are set to the optimal values for each protocol. Parameters: ρ= 0.5, α1 = 0.2, α2 = 0.2, ∆1 = 0, ∆2 = 1. computations we obtain E[δσ′(λr + λi)˜λr] = 2 π Sr Qr + Qi + 1 Qi + 1 2Q2 i + 2QrQi + 3Qi + 2Qr + 1+ −2 π TSr −R˜R QrT −R2 R(Qi + 1) Qr + Qi + 1 1√ T(Qr + Qi + 1) −R2 + −2 π T ˜R−RSr QrT −R2 1√ T(Qr + Qi + 1) −R2 1 1 T + R2 QrT−R2 ( 1 T − Qi+1 T(Qr+Qi+1)−R2 ), (A.31) E[δσ′(λr + λi)˜λi] = 2 π Si Qr + Qi + 1 Qr + 1 2Q2r + 2QrQi + 3Qr + 2Qi + 1+ −2 π SiR Qr + Qi + 1 1√ T(Qr + Qi + 1) −R2 . (A.32) Finally all the expected values are known and we can obtain the analytic updates Eqs. (A.26-A.30) with the coupling. Fig. A.1a shows an instance of the problem at α1 = 0.2 and α2 = 0.2, a situation that is particularly adversarial for curriculum according the phase diagram Fig. 2. This situation is treated by the introduction of Gaussian priors, Fig. A.1b, consistently with the phase diagram in Fig. 7c. 18B Replica computation for the batch case We here the detailed replica computation employed to obtain the analytic description of curriculum learning in the batch case, in section 4. As mentioned in the main, we aim to study a coupled system, represented by the following partition function: ⟨Z(WWW2,WWW1; D1,D2)⟩WWW1 = ∫ dWWW1 e−β1Lγ1(WWW1,D1) Z1(WWW1) log ∫ dWWW2 e−β2(Lγ2(WWW2,D2)+ γ12 2 ∥WWW2−WWW1∥2 2), (B.1) where the examples is D1,D2 are characterised by a different variances in the irrelevant components. This type of quantity is usually denoted as a “disordered” partition function in statistical physics jargon, meaning that it is still dependent on a given realisation of the datasets – i.e., the source of disorder in this model. We want to characterise a typical realisation of this object, in the high- dimensional limit. However, because of its long-tailed statistics, the partition function turns out not to be a self-averaging quantity, i.e. its expectation over the dataset realisations will not correspond to the typical case scenario we are after. It is instead better to focus on the computation of the associated average free-entropy: Φ = lim N→∞ lim β1,β2→∞ 1 β2N ⟨ log ⟨Z(WWW2,WWW1; D1,D2)⟩WWW1 ⟩ D1,D2 . (B.2) What is immediately apparent is that we have to take the expectation of a logarithm, which is not tractable with rigorous mathematical methods. Moreover, we also have to average over the measure for WWW1, which is also a complicated operation. Fortunately, replica theory offers a method for approaching this calculation [47, 48]. The idea is to exploit two separate replica tricks: • in order to evaluate the disorder average, the logarithm can be removed by replicating the second weight conﬁguration, i.e. introducing n identical replicas {W a 2 }n a=1, and extrapolating the ﬁnal result from the n →0 limit. This is based on the mathematical identity log x= limn→0 ∂nxn. • the average over the teacher can instead be computed by introducing ˜n−1 non-interacting and a single interacting replica of the ﬁrst weight conﬁguration {wc 1}˜n c=1. Thus, only the c= 1 replica will enter the Gaussian prior in the student measure. The sought statistical average is again recovered in the limit ˜n→0. Because of the high-dimensional limit we are considering, all typical realisations of the teacher vector with a given sparsity ρwill yield an identical free-entropy. Thus, we can avoid averaging and instead ﬁx a gauge WWWT,i = 1 for i= 1,...,ρN and WWWT,i = 0 elsewhere. In order to simplify the presentation, in the following we will assume that the datasets contain respectively α1 and α2 patterns, and that a curriculum ordering was employed, ∆1 <∆2. Moreover, to avoid confusion with component and replica indices, we will denote with ˜WWW = WWW1 and WWW = WWW2, so that all quantities with a tilde refer to the optimisation on the ﬁrst dataset. After the described replication procedures, we get the following expression for the average free- entropy: Φ = 1 N lim n,˜n→0 ∂n ⟨ lim ˜β,β→∞ 1 β ∫ ˜n∏ c=1 d ˜Wce− ˜βγ1 2 ∥˜Wc∥2 2 α1N∏ µ=1 ˜n∏ c=1 e −β 2 ℓ ( sign (∑ρN i=1 xµ i√ N ) ,σ (∑N i=1 ˜Wc ixµ i(∆1) √ N )) (B.3) × ∫ n∏ a=1 dWae−βγ2 2 ∥Wa∥2 2e−βγ12 2 ∥Wa−˜W1∥2 2 α2∏ µ=1 ∏ a e −β 2 ℓ ( sign (∑ρN i=1 xµ i√ N ) ,σ (∑N i=1 Wa i xµ i(∆2) √ N ))⟩ {xµ} , where ℓ(y,ˆy) = log(1 + e−yˆy) indicates the standard logistic loss. The next step is to explicitly compute the averages over the dataset realisations. Before doing that, we need to isolate the dependence of our expression on the patterns, and we achieve this by introducing Dirac’sδ-functions for the pre-activations. We will use the integral representation of the δ, with integration variables u for the teacher preactivations λfor the student preactivations: 1 N lim n,˜n→0 ∂n ∫ ˜n∏ c=1 d ˜Wce−βλ 2 ∥˜Wc∥2 2 ∫ ˜n∏ a=1 dWae−βλ 2 ∥Wa∥2 2e−βλ12 2 ∥Wa−˜W1∥2 2 (B.4) 19× ⟨∫ ∏ µ d˜u1µdˆ˜u1µ 2π e iˆ˜u1µ ( ˜u1µ−∑ρN i=1 (˜x1)µ i√ N )∫ ∏ µ,c d˜λc 1µdˆ˜λc 1µ 2π e iˆ˜λc 1µ ( λc 1µ−∑N i=1 ˜Wc i(˜x1)µ i√ N ) × ∫ ∏ µ du2µdˆu2µ 2π e iˆu2µ ( u2µ−∑ρN i=1 (x2)µ i√ N )∫ ∏ µ,a dλa 2µdˆλa 2µ 2π e iˆλa 2µ ( λa 2µ−∑N i=1 Wa i (x2)µ i√ N )⟩ {xµ} × ∏ µ,c e−β 2 ℓ(sign(˜u1µ),σ(˜λc 1µ)) ∏ µ,a e−β 2 ℓ(sign(u2µ),σ(λa 2µ)). Thus, the disorder average is now factorised and only involves exponential terms. Since the two datasets are independent now that we made the teacher explicit, we can take the averages over each one separately. In both cases we get: ⟨.⟩ = ρN∏ i=1 E(xrel)µ i e −i ( ˆu√ N+∑ aˆλµ a Wa i√ N ) (xrel)µ i N∏ i=ρN+1 E(xirr)µ i e −i (∑ aˆλµ a Wa i√ N ) (xirr)µ i = ρN∏ i=1 ( 1 −i ( ˆu√ N + ∑ a ˆλµ a Wa i√ N ) xrel −1 2 ( ˆu√ N + ∑ a ˆλµ a Wa i√ N )2 Var (xrel) ) × N∏ i=ρN+1 ( 1 −i ∑ a ˆλµ a Wa i√ N xirr −1 2 (∑ a ˆλµ a Wa i√ N )2 Var(xirr) ) (B.5) = ρN∏ i=1 ( 1 − 1 2N (ˆuµ)2 − 1 N ∑ a ˆuµˆλµ aWa i − 1 2N ∑ ab ˆλµ aˆλµ bWa i Wb i ) N∏ i=ρN+1 ( 1 −∆µ 2N ∑ ab ˆλµ aˆλµ bWa i Wb i ) = e −1 2 ∑ ab ˆλµ aˆλµ b (∑ρN i=1 Wa i Wb i N +∆ ∑N i=ρN+1 Wa i Wb i N ) −ρ 2 (ˆuµ)2−ˆuµ∑ a ˆλµ a ∑ηN i=1 Wa i N . (B.6) This expression suggests what are the order parameters that capture the interactions of the model, namely: • the teacher-student overlap at the end of the ﬁrst learning phase: ˜Rc = ∑ρN i=1 ˜Wc i N . • the teacher-student overlap at the end of the second learning phase: Ra = ∑ρN i=1 Wa i N • the norm of the student after the ﬁrst stage, decomposed into relevant/irrelevant parts: ˜Qcd r = ∑ρN i=1 ˜Wc i ˜Wd i N , ˜Qcd i = ∑N i=ρN+1 ˜Wc i ˜Wd i N • the norm of the student after the second stage, decomposed into relevant/irrelevant parts: Qab r = ∑ρN i=1 Wa i Wb i N , Qab i = ∑N i=ρN+1 Wa i Wb i N Therefore, after introducing these deﬁnitions by means of Dirac’s δ-functions, we can rewrite our replicated expression as: Ωn = ∫ ∏ c d˜Rcdˆ˜Rc 2π/N ∫ ∏ a dRadˆRa 2π/N ∫ ∏ cd d˜Qcd r dˆ˜Qcd r 2π/N ∫ ∏ cd d˜Qcd i dˆ˜Qcd i 2π/N ∫ ∏ ab dQab r dˆQab r 2π/N ∫ ∏ ab dQab i dˆQab i 2π/N ×GiGS (ˆ˜R, ˆR, ˆ˜Qr, ˆQr )ρN GS ( 0,0, ˜Qi,Qi )(1−ρ)N GE ( ∆1, ˜Qr, ˜Qi, ˜R,˜n )α1N GE(∆2,Qr,Qi,R,n )α2N (B.7) Where we introduced interaction, entropic and energetic potentials: Gi = exp ( −N (∑ c ˆ˜mc˜mc + ∑ a ˆmama + ∑ cd ˆ˜Qcd r ˜Qcd r + ∑ cd ˆ˜Qcd i ˜Qcd i + ∑ ab ˆQab r Qab r + ∑ ab ˆQab i Qab i )) (B.8) GS ( ˜R,R, ˜Q,Q ) = ∫ ∏ c [ d˜Wce−βγ 2 ( ˜Wc)2] e−nβγ12 2 ( ˜W1)2 ∫ ∏ a [ dWae−β(γ+γ12) 2 (Wa)2] (B.9) 20×exp (∑ c ˆ˜Rc ˜Wc + ∑ a ˆRaWa + ∑ cd ˆ˜Qcd ˜Wc ˜Wd + ∑ ab ˆQabWaWb + βγ12Wa ˜W1 ) GE(∆,Qr,Qi,m,n ) = ∫ dudˆu 2π eiuˆue−ρ 2 (ˆu)2 ∫ n∏ a=1 dλadˆλa 2π eiλaˆλa (B.10) ×e−1 2 ∑ ab ˆλaˆλb(Qab r +∆Qab i )−ˆu∑ a ˆλaRa−β 2 ℓ(u,λa) Replica Symmetric Ansatz The replica trick allowed us to express the average free-entropy as a function of the overlap order parameters. However, these objects are n×nmatrices or n-dimensional vectors and in principle we have to average over all their possible realisations. Fortunately, the integrand function is exponential in N and in the thermodynamic limit N →∞ the integrals are dominated by the extremisers of the action, and thus can be approximated with the saddle-point method. Still, we need a guess for how to parametrise these order parameters. The simplest possible ansatz, which turns out to be the correct one in convex problems as the one at hand, is the so-called Replica Symmetric ansatz, given by: • ˜Rc = ˜R • Ra = R • ˜Qcd r/i = ˜qr/i, for c̸= d; ˜Qcd r/i = ˜Qr/i for c= d. • Qab r/n = qr/n for a̸= b; Qab r/n = Qr/n for a= b. We also perform a Wick rotation −iˆQac,bd → ˆQac,bd in order to deal with real valued conjugate parameters and pose a similar ansatz for them. In the next paragraph we will compute the three terms separately, and ﬁnally put them together in the expression for the RS free-entropy. Interaction term We start by evaluating the interaction term, or better its normalised logarithm gi = lim˜n→0 log Gi/(nN): gi = −lim ˜n→0 1 n ( ˜nˆ˜R˜R+ nˆRR+ ˜n (ˆ˜Qr ˜Qr 2 + ˆ˜Qi ˜Qi 2 ) + ˜n(˜n−1) 2 ( ˆ˜qr˜qr + ˆ˜qi˜qi ) +n (ˆQrQr 2 + ˆQiQi 2 ) + n(n−1) 2 (ˆqrqr + ˆqiqi) ) (B.11) = −  ˆRR+ ( ˆQrQr + ˆQiQi ) 2 −1 2 (ˆqrqr + ˆqiqi)   (B.12) In order to recover the optimisation problems entailed in the curriculum procedure, we now have to consider the zero temperature limit of this expression. When β →∞, the order parameters follow non-trivial scaling laws: • ˆQ→β2 ˆQ+ O(β), ˆq→β2 ˆQ • ( ˆQ−ˆq) →−βδˆQ • ˆR→βˆR • Q−q= δQ/β and similarly for the tilde parameters. Intuitively, looking at the last scaling law, we see that as the measure gets focused on the single minimiser of the loss, the overlap between different replicas q rapidly converges to the norm Q. Moreover, the scaling with the inverse temperature of the conjugate 21parameters prevents the interaction term from becoming sub-dominant in the saddle-point. If we substitute the rescaled parameters in the above expression we obtain: gi = −β ( ˆRR+ 1 2 ( ˆQrδQr −δˆQrQr ) + 1 2 ( ˆQiδQi −δˆQiQi )) (B.13) Entropic term We can now compute a similar quantity for the entropic potential,gi = limn→0 n log GS ( ˜R,R, ˜Q,Q ) . The general expression we will obtain can be specialised to the two cases({ ˜R,R, ˜Qr,Qr } , { 0,0, ˜Qi,Qi }) appearing in the free-entropy. After substituting the RS ansatz we ﬁnd: gS = lim ˜n→0 1 nlog ∫ ∏ c [ d˜Wce−βγ 2 ( ˜Wc)2] e−nβγ12 2 ( ˜W1)2 ∫ ∏ a [ dWae−β(γ+γ12) 2 (Wa)2 ] (B.14) ×exp ( ˆ˜R ∑ c ˜Wc + ˆR ∑ a Wa + 1 2 (ˆ˜Q−ˆ˜q )∑ c ( ˜Wc )2 + ˆ˜q 2 (∑ c ˜Wc )2 + +1 2 ( ˆQ−ˆq )∑ a (Wa)2 + ˆq 2 (∑ a Wa )2 + βγ12 ∑ a ˜W1Wa ) = lim ˜n→0 1 nlog ∫ Dz ∫ D˜z ∫ ∏ c [ d˜Wce−βγ 2 ( ˜Wc)2] e−nβγ12 2 ( ˜W1)2 ∫ ∏ a dWae−β(γ+γ12) 2 (Wa)2 ×exp ( 1 2 (ˆ˜Q−ˆ˜q )∑ c ( ˜Wc )2 + 1 2 ( ˆQ−ˆq )∑ a (Wa)2 + + ( ˆ˜R+ √ ˆ˜q˜z )∑ c ˜Wc + ( ˆR+ βγ12W1 + √ ˆqz )∑ a Wa ) = ∫ Dz ∫ D˜z ∫ d˜We−1 2 (β˜γ−( ˆ˜Q−ˆ˜q)) ˜W2+( ˆ˜R+ √ ˆ˜q˜z) ˜W log (∫ dWe−1 2 (β(γ+γ12)−( ˆQ−ˆq))W2+( ˆR+βγ12W1+√ˆqz)W ) ∫ d˜We−1 2 (β˜γ−( ˆ˜Q−ˆ˜q)) ˜W2+( ˆ˜R+ √ ˆ˜q˜z) ˜W (B.15) In the zero-temperature limit, we consider the same rescaling of the order parameters we described above. The integrals over the weights become an extremum operation: gs = lim β→∞ β ∫ Dz ∫ D˜zM⋆ s, (B.16) where: M⋆ s = maxW { −1 2 ( (γ+ γ12) + δˆQ ) W2 + ( ˆR+ γ12 ˜W⋆ + √ ˆQz ) W } (B.17) = 1 2 ( ˆR+ γ12 ˜W⋆ + √ ˆQz )2 (γ+ γ12) + δˆQ (B.18) and where: ˜W⋆ = argmax ˜W { −1 2 (˜γ+ δˆ˜Q) ˜W2 + ( ˆ˜R+ √ ˆ˜Q˜z) ˜W } = ˆ˜R+ √ ˆ˜Q˜z ˜γ+δˆ˜Q . Finally also the ∫ Dz ∫ D˜zintegrations can be carried out, giving: β ∫ Dz ∫ D˜zM⋆ s = β ∫ Dz ∫ D˜z1 2 ( ˆR+ γ12 ˆ˜R+ √ ˆ˜Q˜z ˜γ+ δˆ˜Q + √ ˆQz )2 (γ+ γ12) + δˆQ (B.19) 22= β 2 ( ˆR+ ˆ˜R γ12 ˜γ+δˆ˜Q )2 + ( γ12 √ ˆ˜Q ˜γ+δˆ˜Q )2 + ˆQ (γ+ γ12) + δˆQ (B.20) So, specialising to the the two terms that appear in the free-entropy we get: gS(γ1,γ2,γ12) = ρgs ( ˜R,R, ˜Qr,Qr ) + (1 −ρ) gs ( 0,0, ˜Qi,Qi ) (B.21) = β 2   ρ ( ˆR+ ˆ˜R γ12 γ1+δˆ˜Qr )2 + ( γ12 √ ˆ˜Qr γ1+δˆ˜Qr )2 + ˆQr (γ2 + γ12) + δˆQr + (1 −ρ) ( γ12 √ ˆ˜Qi γ1+δˆ˜Qi )2 + ˆQi (γ2 + γ12) + δˆQi   Energetic term Since one of the two energetic terms appearing in the replicated free-energy depends on the ˜n replicas of the ﬁrst weight conﬁguration, and there is no interaction, we can take the ˜n→0 limit directly. Therefore we only have to evaluate the other contribution (dependent on the nreplicas of the second weight conﬁguration). Deﬁning Q = Qr + ∆Qi, Q = Qr + ∆Qi, we evaluate gE = limn→0 1 n log(GE) in the RS ansatz: gE = lim n→0 1 nlog ∫ dudˆu 2π eiuˆue−ρ 2 (ˆu)2 ∫ ∏ a ( dλadˆλa 2π eiλaˆλa ) (B.22) ×e−1 2 (Q−q) ∑ a(ˆλa) 2 −1 2 q( ∑ a ˆλa) 2 −ˆuR∑ a ˆλa−β∑ aℓ(u,λa) = lim n→0 1 nlog ∫ du√2πρ ∫ ∏ a ( dλadˆλa 2π eiλaˆλa ) (B.23) ×e−1 2 (Q−q) ∑ a(ˆλa) 2 −1 2 q( ∑ a ˆλa) 2 −β∑ aℓ(u,λa)−1 2ρ(u+iR ∑ a ˆλa) 2 = lim n→0 1 nlog ∫ Dz0 ∫ Du {∫ Dλe −βℓ (√ρu,σ (√ (Q−q)λ+ √ q−m2 ρ z0+ m√ρu ))}n (B.24) = ∫ Dz0 ∫ Dulog ∫ Dλe −βℓ (√ρu,σ (√Q−qλ+ √ q−m2 ρ z0+ m√ρu )) So in the β →∞ limit, with the proper rescalings, we get: gE = β ∫ Dz ∫ DuM⋆ E, (B.25) where: M⋆ E = max λ −λ2 2 −ℓ ( sign (√ρu) ,σ ( √ δQr + ∆δQiλ+ √ Qr + ∆Qi −R2 ρ z+ R√ρu )) (B.26) 23RS Free-entropy Finally, assuming the we can write down the RS free-entropy for the curriculum ordering as: Φ/β = −extr ( ˆRR+ 1 2 (( ˆQδQ−δˆQQ ) r + ( ˆQδQ−δˆQQ ) i )) (B.27) + gS(γ1,γ2,γ12) + α2 gE(∆2) , where gS is deﬁned in equation (B.21) and gE is deﬁned in equation (B.25). The order parameters for the teacher system are obtained independently from identical equations, after substituting λ1 → 0,λ2 =→λ1 and λ12 →0, α2 →α1 and ∆2 →∆1, and after adding a tilde to the remaining parameters. The saddle-point equations, yielding at convergence the asymptotic prediction for the order parameters, can be found by posing stationarity conditions for the free-entropy with respect to all overlaps. Note that, if instead of the simple setting just considered, where the data slice in the second stage has homogeneous variance for the irrelevant components, there are multiple subsets with different sizes and variances, the only variation in the free-entropy is in the energetic contribution. In general one will have a sum: ∑ s αsgE(∆s) (B.28) over each of these subsets. Moreover, if instead of two stages we consider multiple learning stages, the free-entropy for each successive step has an identical form, and one only has to substitute the tilde parameters with the order parameters obtained at the previous step. Note that the simplicity of nesting stages in this problem is connected to the convexity of this learning setting. Generally, adding more steps would increase the complexity of the calculation considerably. Generalisation error With the saddle-point values for the order parameters, one can easily evaluate the generalisation error on new datapoints, which is the measure of performance we are employing in the main. This performance can be obtained as: 1 −ϵg = ⟨ Θ ((WWWT ·x√ N )(WWW2 ·x√ N ))⟩ x(∆) (B.29) where ∆ is the variance of the irrelevant components for the new pattern. A shortcut for evaluating this expression is to insert the order parameters in the expression through Dirac’s δs. After a straightforward calculation, along the same lines of the one presented above, one obtains: ϵg = 1 πarccos ( R√ ρ(Qr + ∆Qi) ) . (B.30) Of course, the generalisation accuracy is just the complementary quantity 1 −ϵg. C Additional results on sparsity We complement the discussion on the importance of sparsity, Sec. 4, with the comparison with other learning protocols. Observe that anti-curriculum suffers the same issue of the curriculum method for sufﬁciently large fractions of relevant features ρ. In that regime, the splitting becomes sub-optimal because the solution found in the splitting does not provide enough information to help the other phase of learning. Consequently, the network is forced to set neglect the information in the batch in favour of exploring solutions further away from that one. This is outperform by standard learning, where all the bits of information are used. 240.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 0.94 0.96 0.98 1.00 1.02 1.04 1.06 (a) Curriculum learning. 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 (b) Anti-curriculum learning. 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 0.97 0.98 0.99 1.00 1.01 1.02 1.03 (c) Curriculum vs anti-curriculum. Figure C.1: Effect of sparsity. Phase diagram on the effect of sparsity, Fig. 4b, extended for all learning protocols. D Simulations on CIFAR10 Task design. Because a sparse set of relevant features is crucial to observing curriculum effects in our model, we created a task based on real data that has this property. In particular we create32×64 pixel input examples by concatenating two images side-by-side from the CIFAR10 dataset. The correct output label is given by the label of the image on the left, while the image on the right is an irrelevant distractor. To vary difﬁculty, we scale the contrast of the irrelevant image. This dataset is meant to instantiate a simple example of learning an object classiﬁcation amidst clutter. We emphasise that, as in our synthetic data model, each training sample always contains the same relevant and distractor images (i.e., we are not considering a data augmentation setting where each relevant image appears with many non-relevant images). To ensure no cross-contamination of training and testing samples, the distractor images for the training and test sets are drawn only from the same set. Model architecture and training regime. We train a single layer network with cross entropy loss (i.e. softmax regression), implemented in Pytorch Lightning by modifying the MIT-licensed PyTorch_CIFAR10 repository (https://zenodo.org/record/4431043#.YLmz6zZKhsA) to en- sure that training parameters accord with standard practice. Networks were trained with SGD and Nesterov momentum, under default parameters: a learning rate of 1e−2, momentum parameter 0.9, batch size 256, and 100 epochs. The learning rate was annealed according to the ‘WarmUpCosine’ schedule used in PyTorch_CIFAR10, which linearly reduces the learning rate over the ﬁrst 30% of training steps before switching to a cosine shaped schedule on the remainder. Experiment details and hyperparameter optimisation. For the ﬁrst phase of training, we used dataset sizes in 10 equal steps between 1000 and 50000. For the second phase, we used nine dataset sizes in 9 equal steps between 5333 and 48000. We optimised hyperparameters in each phase separately. In the ﬁrst phase, we evaluated all combinations of initialisation scales of {0,.2,.5,1.}, weight decay parameters of {0,.2,.5,1.,2.}, and curriculum policy, for ﬁve random seeds. In the second phase, for each random seed and curriculum condition, we continued training from the best- performing model obtained in the ﬁrst phase. We trained all combinations of ﬁve elastic penalties log spaced between 1e−3 and 1e2, and weight decay parameters {0,.2,.5}. We then compute the best performing model for each seed and take the mean over seeds. Finally, to evaluate the no-curriculum performance, we train shufﬂed dataset models with initialisation scales {0,.2,.5,1.}and weight decay parameters {0,.2,.5}. For visualisation purposes, we used nearest-neighbors interpolation in the phase portrait to provide values for all points used in the synthetic experiments. Experiments were run on V100 GPUs and required approximately 10000 GPU hours (including debugging and development), or ≈1110 kg CO2 eq according to the MachineLearning Impact calculator of Lacoste et al., 2019. E Speed-up theory vs simulations As remarked in the main text, one of the advantages of the theoretical analysis is a huge speed-up in the time to collect the results, without need of averaging to reduce the ﬂuctuations. In this section, we brieﬂy report a comparison between the time required for the lines from theory and simulations shown in the main text. 25In order to obtain ﬁgure 1c, a single run of the ODE equations takes 2 milliseconds and a run of the simulations takes 500 milliseconds. The ﬁgure is however obtained optimizing over all the hyperparameters (learning rate, initialization, weight decay) totalling 400 milliseconds for the analytical solution; while, due to noise, simulation results for a single set of hyperparameters requires averaging 5000 realizations totalling 41 minutes. We note that we did the hyperparameter optimization only once using the theoretical framework and then used the optima in the simulations in order to save compute time. The best comparison should therefore be done for a ﬁxed set of hyperparameters and gives 2 milliseconds vs 41 minutes. Overall, the analytical solution is between 2 and 6 orders of magnitude faster. 26",
      "meta_data": {
        "arxiv_id": "2106.08068v2",
        "doi": "10.1088/1742-5468/ac9b3c",
        "authors": [
          "Luca Saglietti",
          "Stefano Sarao Mannelli",
          "Andrew Saxe"
        ],
        "published_date": "2021-06-15T11:48:52Z",
        "pdf_url": "https://arxiv.org/pdf/2106.08068v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research analyzes a prototypical neural network model of curriculum learning using statistical physics methods to understand when and why it helps. The key findings include demonstrating a clear speed benefit for curriculum learning in the online setting. However, it also shows that this advantage disappears in standard neural networks when training experiences can be stored and replayed (batch learning), aligning with observations from the deep learning literature. To overcome this, the paper proposes curriculum-aware algorithms that consolidate synapses at curriculum change points through elastic coupling (L2 regularization), which significantly improves test performance in both online and batch settings. The work helps reconcile conflicting empirical results, traces regimes where curriculum learning yields the largest gains (especially with sparse, informative features amidst noise), and suggests that fully exploiting a curriculum may require explicit adjustments in the loss function.",
        "methodology": "The study employs a teacher-student framework with shallow 1-layer neural networks (perceptrons). The learning task is binary classification where the teacher network is sparse, focusing on a subset of relevant input features. Sample difficulty is modeled by varying the variance of irrelevant input components (∆µ), creating 'easy' (smaller ∆µ) and 'hard' (larger ∆µ) examples. The learning protocols examined are curriculum (easy-to-hard), anti-curriculum (hard-to-easy), and no-curriculum (randomly shuffled). Statistical physics methods, particularly high-dimensional limit analysis, are used to derive exact analytical expressions for online learning dynamics (mean square error loss with L2 regularization via gradient descent) and batch learning performance (L2-regularized logistic loss). For batch learning, a Gaussian prior, acting as an elastic coupling (γ12), is introduced between successive learning phases to instantiate a long-term memory effect, and its impact is analyzed using the replica method for free-energy computation.",
        "experimental_setup": "The primary experimental setup involves synthetic data generated from the teacher-student model. The student networks are 1-layer perceptrons trained on datasets with varying total sizes (α1 + α2), proportions of easy (α1) and hard (α2) samples, and teacher sparsity (ρ=0.5 in most cases). Difficulty is controlled by ∆1 (easy) and ∆2 (hard). Hyperparameters like learning rate, weight decay, initialisation norm, and elastic coupling strength (γ12) are systematically optimized for each curriculum strategy. Numerical simulations validate the theoretical predictions, showing typical learning trajectories. A real-world demonstration is conducted using a CIFAR10-derived dataset where 32x64 input images are created by concatenating a task-relevant image with an irrelevant distractor, with difficulty varied by scaling the distractor's contrast. A single-layer network is trained with cross-entropy loss and the proposed curriculum protocol with Gaussian prior, using PyTorch Lightning with optimized hyperparameters (elastic penalties, weight decay) and evaluated over multiple random seeds.",
        "limitations": "The theoretical analysis focuses on a simple teacher-student model with convex loss landscapes, which may not fully capture the complexities of curriculum learning in modern deep learning with highly non-convex problems (e.g., in large language models or reinforcement learning). The model's reliance on a sparse set of informative features amidst noise, while crucial for the observed curriculum benefits, might not generalize to all learning tasks. In the online learning setting, achieving benefits from curriculum learning depends on careful tuning of learning rate and training epochs, as suboptimal choices can lead to performance deterioration or anti-curriculum outperforming curriculum. For the batch setting, when the fraction of relevant features (ρ) is sufficiently high (>0.5), splitting data into curriculum phases can become detrimental due to the intrinsic cost of information partitioning. The batch learning analysis primarily focuses on a two-phase curriculum, with extensions to multiple phases mentioned but not explicitly detailed.",
        "future_research_directions": "Future work should extend this analysis beyond simple loss landscapes to address the impact of curricula in more complex tasks, such as reinforcement learning. The research also suggests exploring other curriculum-aware approaches, including adapting the learning algorithm (e.g., dynamic loss functions) or the network architecture itself. On the psychology side, the analytical predictions can guide the design of new experiments, particularly to test the counter-intuitive benefit of anti-curriculum learning for intermediate sparsity levels. Further experiments are proposed to systematically manipulate training and transfer difficulties and to extensively sample sparsity levels, testing for interactions with the fraction of easy and hard examples, especially in human learning studies (e.g., fading experiments). An interesting biological question is how animals might naturally learn with 'near-optimal hyperparameters' to consistently exploit curriculum benefits without deterioration."
      }
    },
    {
      "title": "Understanding the Complexity Gains of Single-Task RL with a Curriculum",
      "abstract": "Reinforcement learning (RL) problems can be challenging without well-shaped\nrewards. Prior work on provably efficient RL methods generally proposes to\naddress this issue with dedicated exploration strategies. However, another way\nto tackle this challenge is to reformulate it as a multi-task RL problem, where\nthe task space contains not only the challenging task of interest but also\neasier tasks that implicitly function as a curriculum. Such a reformulation\nopens up the possibility of running existing multi-task RL methods as a more\nefficient alternative to solving a single challenging task from scratch. In\nthis work, we provide a theoretical framework that reformulates a single-task\nRL problem as a multi-task RL problem defined by a curriculum. Under mild\nregularity conditions on the curriculum, we show that sequentially solving each\ntask in the multi-task RL problem is more computationally efficient than\nsolving the original single-task problem, without any explicit exploration\nbonuses or other exploration strategies. We also show that our theoretical\ninsights can be translated into an effective practical learning algorithm that\ncan accelerate curriculum learning on simulated robotic tasks.",
      "full_text": "Understanding the Complexity Gains of Single-Task RL with a Curriculum Qiyang Li * 1 Yuexiang Zhai* 1 Yi Ma 1 Sergey Levine 1 Abstract Reinforcement learning (RL) problems can be challenging without well-shaped rewards. Prior work on provably efficient RL methods gener- ally proposes to address this issue with dedi- cated exploration strategies. However, another way to tackle this challenge is to reformulate it as a multi-task RL problem, where the task space contains not only the challenging task of inter- est but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solv- ing a single challenging task from scratch. In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curricu- lum. Under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the origi- nal single-task problem, without any explicit ex- ploration bonuses or other exploration strategies. We also show that our theoretical insights can be translated into an effective practical learning al- gorithm that can accelerate curriculum learning on simulated robotic tasks. 1. Introduction Reinforcement learning (RL) provides an appealing and simple way to formulate control and decision-making prob- lems in terms of reward functions that specify what an agent should do, and then automatically train policies to learn how to do it. However, in practice the specification of the reward function requires great care: if the reward func- tion is well-shaped, then learning can be fast and effective, but if rewards are delayed, sparse, or can only be achieved 1UC Berkeley. Correspondence to: Qiyang Li <qcli@berkeley.edu>, Yuxiang Zhai <simonzhai@berkeley.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). Learning π⋆ ω′ from scratch Learning π⋆ ω′ with ROLLIN Figure 1: Illustration of R OLLIN . The red circle rep- resents the initial state distribution. The dark curve rep- resents the optimal policy of the preceding task ω. The blue diamonds represent the optimal state distributions dπ⋆ ω µ , d π⋆ ω′ µ of the preceding task ω and the current ω′ re- spectively. ROLLIN runs the optimal policy of the preced- ing task π⋆ ω to obtain a better initial state distribution for faster learning of the optimal policy of the current taskπ⋆ ω′. after extensive explorations, RL problems can be excep- tionally difficult (Kakade and Langford, 2002; Andrychow- icz et al., 2017; Agarwal et al., 2019). This challenge is of- ten overcome with either reward shaping (Ng et al., 1999; Andrychowicz et al., 2017; 2020; Gupta et al., 2022) or dedicated exploration methods (Tang et al., 2017; Stadie et al., 2015; Bellemare et al., 2016; Burda et al., 2018), but reward shaping can bias the solution away from opti- mal behavior, while even the best exploration methods, in general, may require covering the entire state space before discovering high-reward regions. On the other hand, a number of recent works have proposed multi-task learning methods in RL that involve learning contextual policies that simultaneously represent solutions to an entire space of tasks, such as policies that reach any 1 arXiv:2212.12809v3  [cs.LG]  18 Jun 2023Understanding the Complexity Gains of Single-Task RL with a Curriculum potential goal (Fu et al., 2018; Eysenbach et al., 2020b; Fu- jita et al., 2020; Zhai et al., 2022), policies conditioned on language commands (Nair et al., 2022), or even policies conditioned on the parameters of parametric reward func- tions (Kulkarni et al., 2016; Siriwardhana et al., 2019; Ey- senbach et al., 2020a; Yu et al., 2020b). While such meth- ods are often not motivated directly from the standpoint of handling challenging exploration scenarios, but rather di- rectly aim to acquire policies that can perform all tasks in the task space, these multi-task formulations often present a more tractable learning problem than acquiring a solu- tion to a single challenging task in the task space (e.g., the hardest goal, or the most complex language command). We pose the following question: When do we expect solving the reformulated multi-task RL problem with task-conditioned policies to be more efficient than solving the original single-task problem directly? In this work, we study this question by analyzing the com- plexity of learning an optimal policy in the stochastic pol- icy gradient (SPG) setting (Agarwal et al., 2021; Mei et al., 2020; Ding et al., 2021) with a curriculum (learning a list of tasks in sequence). As pointed out by Ding et al. (2021), for learning an optimal policy, SPG requires a polynomial sample complexity if the initialization is near-optimal.1 In general, there is no guarantee that the initial policy is near- optimal, which could potentially lead to an unbounded den- sity ratio and thus poor sample complexity bound. While there have been a lot of prior works that utilize exploration bonuses to address the sample complexity (Azar et al., 2017; Jin et al., 2018; Agarwal et al., 2020; Zhang et al., 2020d), we take a different approach without the need for exploration bonuses by making use of a curriculum of tasks where adjacent tasks in the curriculum are close in terms of their optimal state visitation distributions. Our algorithm, ROLLIN , works by (1) using the optimal policy parameters of the previous task as an initialization for the current task, and (2) constructing the initial state distribution as a mix- ture of the optimal state visitation distribution of the previ- ous task and the original initial state distribution of interest. In a nutshell, ROLLIN mixes in the distribution of the opti- mal policy of the preceding task to the initial distribution to make sure that such distribution is close to the optimal state visitation distribution of the current task, reducing the den- sity mismatch ratio and yielding better sample complexity. We illustrate the intuition of ROLLIN in Figure 1. We adopt the contextual MDP formulation, where we assume each MDP, Mω, is uniquely defined by a context ω in the con- text spaceW, and we are given a curriculum{ωk}K k=0, with the last MDP, MωK , being the MDP of interest. Our main results require a Lipschitz continuity assumption on the 1See Definition 4.2 of Section 4.2. context-dependent reward functionrω and a fixed transition dynamics model across all contexts. We show that learning π⋆ K by recursively rolling in with a near-optimal policy for ωk to construct the initial distribution µk+1 for the next context ωk+1, can have a smaller minimum required sam- ple complexity compared with learning π⋆ ωK from scratch directly. In particular, we show that when an appropriate sequence of contexts is selected, we can reduce the mini- mum required iteration and sample complexity bounds of entropy-regularized softmax policy gradient (with an inex- act stochastic estimation of the gradient) from an original exponential dependency on the state space size, as sug- gested by Ding et al. (2021), to a polynomial dependency. We also prescribe a practical implementation of ROLLIN . Our contributions are as follows. We introduce R OLLIN , a simple algorithm that facilitates single-task learning by re- casting it as a multi-task problem. Theoretically, we show that under the entropy-regularized softmax policy gradient (PG) setting, our algorithm reduces the exponential com- plexity bound to a polynomial dependency on S. Empiri- cally, we verify our theory on a tabular MDP and provide a practical implementation of R OLLIN that can accelerate curriculum learning in the tabular environment and a range of simulated robotic tasks. 2. Related Work Convergence of policy gradient methods. Theoreti- cal analysis of policy gradient methods has a long his- tory (Williams, 1992; Sutton et al., 1999; Konda and Tsitsiklis, 1999; Kakade and Langford, 2002; Peters and Schaal, 2008). Motivated by the recent empirical suc- cess (Schulman et al., 2015; 2017) in policy gradient (PG) methods, the theory community has extensively studied the convergence of PG in various settings (Fazel et al., 2018; Agarwal et al., 2021; 2020; Bhandari and Russo, 2019; Mei et al., 2020; Zhang et al., 2020b; Agarwal et al., 2020; Zhang et al., 2020a; Li et al., 2021; Cen et al., 2021; Ding et al., 2021; Yuan et al., 2022; Moskovitz et al., 2022). Agarwal et al. (2021) established the asymptotic global convergence of policy gradient under different policy pa- rameterizations. We extend the result of entropy regu- larized PG with stochastic gradient (Ding et al., 2021) to the contextual MDP setting. In particular, our contextual MDP setting reduces the exponential state space depen- dency w.r.t. the iteration number and per iteration sample complexity suggested by Ding et al. (2021) to a polyno- mial dependency. While there exists convergence analyses on other variants of PG that produce an iteration number that does not suffer from an exponential state space depen- dency (Agarwal et al., 2021; Mei et al., 2020), they assume access to the exact gradient during each update of PG. In contrast, we assume a stochastic estimation of the gradient. 2Understanding the Complexity Gains of Single-Task RL with a Curriculum Exploration bonuses. A number of prior works have shown that one can achieve a polynomial complexity of learning an optimal policy with effective exploration meth- ods (Azar et al., 2017; Jin et al., 2018; Du et al., 2019; Misra et al., 2020; Agarwal et al., 2020; Zhang et al., 2020d). The computational efficiency suggested by our work is different from some of the aforementioned prior methods that rely on adding exploration bonuses (Azar et al., 2017; Jin et al., 2018; Agarwal et al., 2020; Zhang et al., 2020d), as we assume access to a “good” curriculum which ensures the optimal policy of the next context is not too different from the optimal policy of the current context while eschewing exploration bonuses entirely. Contextual MDPs. Contextual MDPs (or MDPs with side information) have been studied extensively in the the- oretical RL literature (Abbasi-Yadkori and Neu, 2014; Hal- lak et al., 2015; Dann et al., 2019; Jiang et al., 2017; Modi et al., 2018; Sun et al., 2019; Dann et al., 2019; Modi et al., 2020). We analyze the iteration complexity and sample complexity of (stochastic) policy gradient methods, which is distinct from these prior works that mainly focus on re- gret bounds (Abbasi-Yadkori and Neu, 2014; Hallak et al., 2015; Dann et al., 2019) and PAC bounds (Jiang et al., 2017; Modi et al., 2018; Sun et al., 2019; Dann et al., 2019; Modi et al., 2020). Several works assumed linear transition kernel and reward model (or generalized linear model (Abbasi-Yadkori and Neu, 2014)) with respect to the context (Abbasi-Yadkori and Neu, 2014; Modi et al., 2018; Dann et al., 2019; Modi et al., 2020; Belogolovsky et al., 2021). These assumptions share similarity to our assump- tions — we have a weaker Lipschitz continuity assumption with respect to the context space (since linear implies Lip- schitz) on the reward function and a stronger shared transi- tion kernel assumption. Curriculum learning in reinforcement learning. Cur- riculum learning is a powerful idea that has been widely used in RL (Florensa et al., 2017; Kim and Choi, 2018; Omidshafiei et al., 2019; Ivanovic et al., 2019; Akkaya et al., 2019; Portelas et al., 2020; Bassich et al., 2020; Fang et al., 2020; Klink et al., 2020; Dennis et al., 2020; Parker- Holder et al., 2022; Liu et al., 2022) (also see (Narvekar et al., 2020) for a detailed survey). Although curric- ula formed by well-designed reward functions (Vinyals et al., 2019; OpenAI, 2018; Berner et al., 2019; Ye et al., 2020; Zhai et al., 2022) are usually sufficient given enough domain knowledge, tackling problems with limited do- main knowledge requires a more general approach where a suitable curriculum is automatically formed from a task space. In the goal-conditioned reinforcement learning lit- erature, this corresponds to automatic goal proposal mech- anisms (Florensa et al., 2018; Warde-Farley et al., 2018; Sukhbaatar et al., 2018; Ren et al., 2019; Ecoffet et al., 2019; Hartikainen et al., 2019; Pitis et al., 2020; Zhang et al., 2020c; OpenAI et al., 2021; Zhang et al., 2021). The practical instantiation of this work is also similar to (Bassich et al., 2020; Liu et al., 2022), where a curricu- lum is adopted for learning a progression of a set of tasks. Klink et al. (2022) also analyzed the theoretical benefits of curriculum learning in RL, but is primarily concerned with the problem of representations for value functions when utilizing approximate value iteration methods for curricu- lum learning. This is accomplished by using boosting to in- crease the effective capacity of the value function estimator. In contrast, our method does not make any prescription in regard to the representation, but is aimed at studying sam- ple complexity and exploration, showing that \"rolling in\" with the previous policy and then collecting data with the new policy leads to good sample complexity. In principle, we could even imagine in future work combining the repre- sentation analysis in Klink et al. (2022) with the discussion of state coverage in our analysis. Learning conditional policies in multi-task RL. Multi- task RL (Tanaka and Yamamura, 2003) approaches usually learn a task-conditioned policy that is shared across differ- ent tasks (Rusu et al., 2015; Rajeswaran et al., 2016; An- dreas et al., 2017; Finn et al., 2017; D’Eramo et al., 2020; Yu et al., 2020a; Ghosh et al., 2021; Kalashnikov et al., 2021; Agarwal et al., 2022). Compared to learning each task independently, joint training enjoys the sample effi- ciency benefits from sharing the learned experience across different tasks as long as the policies generalize well across tasks. To encourage generalization, it is often desirable to condition policies on low dimensional feature representa- tions that are shared across different tasks instead (e.g., us- ing variational auto-encoders (Nair et al., 2018; Pong et al., 2019; Nair et al., 2020) or variational information bottle- neck (Goyal et al., 2019; 2020; Mendonca et al., 2021)). The idea of learning contextual policies has also been dis- cussed in classical adaptive control literature (Sastry et al., 1990; Tao, 2003; Landau et al., 2011; Åström and Witten- mark, 2013; Goodwin and Sin, 2014). Different from these prior works which have been mostly focusing on learning policies that can generalize across different tasks, our work focuses on how the near-optimal policy from a learned task could be used to help the learning of a similar task. 3. Preliminaries We consider the contextual MDP setting, where a con- textual MDP, MW = ( W, S, A, P, rω, γ, ρ), consists of a context space W, a state space S, an action space A, a transition dynamic function P : S × A → P(S) (where P(X) denotes the set of all probability distribu- tions over set X), a context-conditioned reward function r : W×S×A →[0, 1], a discount factorγ ∈ (0, 1], and an 3Understanding the Complexity Gains of Single-Task RL with a Curriculum initial state distribution of interest ρ. For convenience, we use S = |S|, A= |A| to denote the number of states and actions. While some contextual MDP formulations (Hallak et al., 2015) have context-conditioned transition dynamics and reward functions, we consider the setting where only the reward function can change across contexts. We denote rω as the reward function conditioned on a fixed ω ∈ W and Mω = ( S, A, P, rω, γ, ρ) as the MDP induced by such fixed reward function. We use π(a|s) : S → P(A) to denote a policy and we adopt the softmax parameteri- zation: πθ(a|s) = exp[θ(s,a)]P a′ exp[θ(s,a′)] , where θ : S × A 7→R. We use dπ ρ (s) := (1 − γ) P∞ t=0 γtPπ(st = s|s0 ∼ ρ) to denote the discounted state visitation distribution and V π ω := E[P∞ t=0 γtrω(st, at)] + αH(ρ, π) to denote the entropy regularized discounted return on Mω, where H(ρ, π) := Es0∼ρ,ah∼π(·|sh) \u0002P∞ h=0 −γh log π(ah|sh) \u0003 is the discounted entropy term. We use π⋆ ω := arg maxπ V π ω to denote an optimal policy that maximizes the discounted return under Mω. We assume all the contextual re- ward functions are bounded within [0, 1]: rω(s, a) ∈ [0, 1], ∀ω ∈ W, ∀(s, a) ∈ S × A. Similarly to previ- ous analysis (Agarwal et al., 2021; Mei et al., 2020; Ding et al., 2021), we assume the initial distribution ρ for PG or stochastic PG satisfies ρ(s) > 0, ∀s ∈ S. Supposing we are given a curriculum{ωk}K k=0, where the last contextωK defines MωK the MDP of interest, our goal is to show that sequentially solving π⋆ ωk for k = 0, 1, . . . , K, enjoys bet- ter computational complexity and sample complexity than learning π⋆ ωK problem MωK from scratch. 3.1. Assumptions As we will show in Section 4, if there is a curricu- lum {ωk}K k=0 where the optimal policies π⋆ ωk , π⋆ ωk+1 with respect to two consecutive contexts ωk, ωk+1 are close enough to each other in terms of their state visitation distri- butions, using an ε-optimal policy of ωk as an initialization allows us to directly start from the near-optimal regime of ωk+1, hence only requiring polynomial complexity to learn π⋆ ωk+1 . We describe our curriculum assumptions as follows. Assumption 3.1 (Lipschitz reward in the context space) . The reward function is Lipschitz continuous with re- spect to the context: maxs,a |rω(s, a) − rω′(s, a)| ≤ Lr ∥ω − ω′∥2, ∀ω, ω′ ∈ W. Intuitively, Assumption 3.1 defines the similarity between two tasks via a Lipschitz continuity in the context space. Similar Lipschitz assumptions also appears in (Abbasi- Yadkori and Neu, 2014; Modi et al., 2018; Dann et al., 2019; Modi et al., 2020; Belogolovsky et al., 2021). Assumption 3.2 (Similarity of Two Contexts) . The cur- riculum {ωk}K k=0 satisfies max0≤k≤K−1 ∥ωk+1 − ωk∥2 ≤ O \u0000 S−2\u0001 , and we have access to a near-optimal initializa- tion θ(0) 0 for learning π⋆ ω0 (formally defined in Section 4.2). At first glance, the near-optimal initialization θ(0) 0 for the first task ω0 in the curriculum (suggested by Assump- tion 3.2) may seem like a strong assumption, but in many practical settings, it could be quite easy to obtain. For ex- ample, if the tasks correspond to reaching different goals, the curriculum might start with a goal right on top of the starting state, and therefore trivially easy to learn. As an- other example, if the task is a locomotion task and ω con- texts correspond to target velocities, ω0 might correspond to a velocity of zero, corresponding to standing still. Assumption 3.1 and Assumption 3.2 together quantify the maximum difference between two consecutive tasks Mωk−1 , Mωk , in terms of the maximum difference be- tween their reward function, which plays a crucial role in reducing the exponential complexity to a polynomial one. We will briefly discuss intuition in the next section. 3.2. Prior Analysis on PG with stochastic gradient Ding et al. (2021) proposed a two-phased PG convergence analysis framework with a stochastic gradient. In partic- ular, the author demonstrates that with high probability, stochastic PG with arbitrary initialization achieves an ε- optimal policy can be achieved with iteration numbers of T1, T2 and per iteration sample complexities of B1, B2 in two separate phases where T1 = eΩ \u0010 S2S3 \u0011 , T2 = eΩ \u0000 S3/2\u0001 (eΩ(·) suppresses the log S and terms that do not contain S) and B1 = eΩ \u0010 S2S3 \u0011 , B2 = eΩ \u0000 S5\u0001 , respec- tively, and PG enters phase 2 only when the updating pol- icy becomes ε0-optimal, where ε0 is a term depending onS (formally defined by (19) in Appendix A.3). For complete- ness, we restate the main theorem of Ding et al. (2021) in Theorem A.2, provide the details of such dependencies on S in Corollary A.3, and describe the two-phase procedure in Algorithm 4. The main implication of the two-phase re- sults is that, when applying SPG to learn an optimal policy from an arbitrary initialization, we suffer from exponen- tial complexities, unless the initialization isε0-optimal. We will now discuss how Assumption 3.1 and Assumption 3.2 enable an ε0-optimal initialization for every ωk, reducing the exponential complexities to polynomial complexities. 4. Theoretical Analysis In this section, we first introduce R OLLIN , a simple algo- rithm that accelerates policy learning under the contextual MDP setup by bootstrapping new context learning with a better initial distribution (Algorithm 1). Then, we pro- vide the total complexity analysis of applying R OLLIN to stochastic PG for achieving an ε-optimal policy. Finally, we validate our theoretical results on a tabular MDP. 4Understanding the Complexity Gains of Single-Task RL with a Curriculum 4.1. ROLLIN The theoretical version of R OLLIN is provided in Algo- rithm 1. The intuition behind R OLLIN is that when two consecutive contexts in the curriculum {ωk}K k=1 are close, their optimal parameters θ⋆ ωk−1 , θ⋆ ωk should be close to each other. Let θ(k) t denote the parameters at the tth iteration of stochastic PG for learning θ⋆ ωk . If we initialize θ(k) 0 as the optimal parameter of the previous context θ⋆ ωk−1 (line 5 in Algorithm 1), and set the initial distribution µk as a mix- ture of the optimal state visitation distribution of the previ- ous context d π⋆ ωk−1 µk−1 and the original distribution of interest ρ with β ∈ (0, 1) (line 6 in Algorithm 1), µk = βd π⋆ ωk−1 µk−1 + (1 − β)ρ, (1) then we can show that stochastic PG enjoys a faster conver- gence rate. This is because setting θ(k) 0 = θ⋆ k−1 ensures a near-optimal initialization for learning ωk, and setting µk as the mixture further improves the rate of convergence by decreasing the density mismatch ratio \r\r\rd π⋆ ωk µk /µk \r\r\r ∞ (a term with that influences the convergence rate). 4.2. Main Results We now discuss how to use a sequence of contexts to learn the target contextωK with provable efficiency given a near- optimal policy πθ(0) 0 of the initial context ω0, without in- curring an exponential dependency on S (as mentioned in Section 3.2). Our polynomial complexity comes as a result of enforcing an ε0-optimal initialization (ε0 is the same as Section 3.2 and (19)) for running stochastic PG (line 6 of Algorithm 1). Hence, stochastic PG directly enters phase 2, with a polynomial dependency on S. Our main results consist of two parts. We first show that when two consecutive contexts ωk−1, ωk are close enough to each other, using R OLLIN for learning θ⋆ k with initial- ization θ(k) 0 = θ⋆ ωk−1 and applying an initial distribution µk = βd π⋆ ωk−1 µk−1 + (1 − β)ρ improves the convergence rate. Specifically, the iteration number and complexity for learn- ing θ⋆ ωk from θ⋆ ωk−1 are stated as follows: Theorem 4.1 (Complexity of Learning the Next Context) . Consider the context-based stochastic softmax policy gra- dient (line 7 of Algorithm 1), and suppose Assumption 3.1 and Assumption 3.2 hold, then the iteration number of ob- taining an ε-optimal policy for ωk from θ⋆ ωk−1 is eΩ (S) and the per iteration sample complexity is eΩ \u0010 Lr α(1−β) S3 \u0011 . Theorem 4.1 shows that when ωk−1, ωk are close enough, ROLLIN reduces the minimum required iteration and sample complexity from an exponential dependency of eΩ(S2S3 ) to an iteration number of eΩ(S) and per iteration Algorithm 1 Provably Efficient Learning via ROLLIN 1: Input: ρ, {ωk}K k=0, MW, β ∈ (0, 1), θ(0) 0 . 2: Initialize µ0 = ρ. 3: Run stochastic PG (Algorithm 4) with initialization θ(0) 0 , µ0, Mω0 and obtain θ⋆ ω0 . 4: for k = 1, . . . , Kdo 5: Set θ(k) 1 = θ⋆ ωk−1 . ▷ πθ⋆ωk−1 is optimal for ωk−1. 6: Set µk = βd π⋆ ωk−1 µk−1 + (1 − β)ρ. 7: Run stochastic PG (Algorithm 4) with initialization θ(k) 0 , µk, Mωk and obtain θ⋆ ωk . 8: end for 9: Output: θ⋆ ωK sample complexity of eΩ(S3). It is worth noting that the the- orem above only addresses the iteration number and sample complexity for learning θ⋆ ωk from θ⋆ ωk−1 . Theorem 4.3 pro- vides the total complexity for learning θ⋆ ωK from θ(0) 0 via recursively applying the results in Theorem 4.1. Before in- troducing Theorem 4.3, we first provide a criterion for the desired initialization of θ(0) 0 . Definition 4.2 (Near-optimal Initialization). We say θ0 is a near-optimal initialization for learning θ⋆ ω if θ0 satisfies V π⋆ ω ω (ρ) −V πθ0 ω (ρ) < ε0 and \r\r\rρ − d π⋆ ω0 ρ \r\r\r 1 ≤ ∥ω1 − ω0∥2. Note that in the above definition, π⋆ ωk represents the op- timal policy of ωk, and V π ωk represents value function of context ωk under policy π. We now introduce the results for the overall complexity: Theorem 4.3 (Main Results: Total Complexity of ROLLIN ). Suppose Assumption 3.1 and Assumption 3.2 hold, and θ(0) 0 is a near-optimal initialization, then the total number of iterations of learning π⋆ ωK using Algorithm 1 is Ω(KS) and the per iteration sample complexity is eΩ \u0000 S3\u0001 , with high probability. A direct implication of Theorem 4.3 is that, with a cur- riculum {ωk}K k=0 satisfying Assumption 3.1 and Assump- tion 3.2, one can reduce the daunting exponential depen- dency on S caused by poor initialization to a polynomial dependency. Admittedly the state spaceS itself is still large in practice, but reducing the state space S itself requires extra assumptions on S, which is beyond the scope of this work. We now provide a sketch proof of Theorem 4.1 and Theorem 4.3 in the next subsection and leave all the details to Appendix A.4 and Appendix A.5 respectively. 4.3. Proof Sketch Sketch proof of Theorem 4.1. The key insight for prov- ing Theorem 4.1 is to show that in MDP Mωk , the value 5Understanding the Complexity Gains of Single-Task RL with a Curriculum Entropy Coeff. Baseline R OLLIN Hard α = 0.01 0.500 ± 0.000 0 .562 ± 0.000 α = 0.001 0.856 ± 0.006 1 .000 ± 0.000 Easy α = 0.01 0.944 ± 0.003 1 .000 ± 0.000 α = 0.001 1.000 ± 0.000 1 .000 ± 0.000 Entropy Coeff. Baseline R OLLIN Hard α = 0.01 0.000 ± 0.000 0 .000 ± 0.000 α = 0.001 0.424 ± 0.023 1 .067 ± 0.000 Easy α = 0.01 4.093 ± 0.224 7 .374 ± 0.216 α = 0.001 10.536 ± 0.002 10 .620 ± 0.002 Table 1: Curriculum progress κ (left) and final return V π (right) on the four-room navigation with stochastic PG. Both metrics are reported at the 50, 000th gradient step. We use a mixing ratio of β = 0.75. Across two entropy coefficients and two reward settings (easy and hard), stochastic PG with ROLLIN consistently achieves better curriculum progress and final return. The standard error is computed over 10 random seeds. function with respect to π⋆ ωk , π⋆ ωk−1 can be bounded by the ℓ2-norm between ωk and ωk−1. In particular, we prove such a relation in Lemma A.5: V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) ≤ 2Lr ∥ωk − ωk−1∥2 (1 − γ)2 . (2) By setting θ(k) 0 = θ⋆ ωk−1 , Equation (2) directly implies V π⋆ ωk ωk (ρ) − V θ(k) 0 (ρ) ≤ 2Lr∥ωk−ωk−1∥2 (1−γ)2 . As suggested by Ding et al. (2021) stochastic PG can directly start from stage 2 with polynomial complexity of T2 = eΩ(S), B2 = eΩ(S5), if V π⋆ ωk ωk (ρ) − V θ(k) 0 (ρ) ≤ ε0, where ε0 (formally defined in Equation (19) in Appendix A.3) is a constant sat- isfying ε0 = O(S−2). Hence, by enforcing two consecu- tive contexts to be close enough∥ωk − ωk−1∥2 ≤ O(S−2), we can directly start from a near-optimal initialization with polynomial complexity with respect to S. It is worth high- lighting that the per iteration sample complexity B2 shown by Ding et al. (2021) scales as eΩ(S5), while our result in Theorem 4.1 only requires a smaller sample complexity of eΩ(S3). Such an improvement in the sample complexity comes from line 6 of ROLLIN : µk = βd π⋆ ωk−1 µk−1 + (1 − β)ρ. Intuitively, setting µk as βd π⋆ ωk−1 µk−1 + (1 − β)ρ allows us to provide an upper bound on the density mismatch ratio: \r\r\r\rd π⋆ µk µk /µk \r\r\r\r ∞ ≤ eO \u0012 Lr α(1 − β)∆k ωS \u0013 , (3) where ∆k ω = max1≤i≤k ∥ωi − ωi−1∥2. Since the sample complexity B2 (provided in Corollary A.3) contains one multiplier of \r\r\r\rd π⋆ µk µk /µk \r\r\r\r ∞ , setting ∆k ω = O(S−2) imme- diately reduces the complexity by an order ofS2. The proof of the upper bound of the density mismatch ratio (Equa- tion (3)) is provided in Lemma A.1. Sketch proof of Theorem 4.3. We obtain Theorem 4.3 by recursively applying Theorem 4.1. More precisely, we use induction to show that, if we initialize the parameters of the policy as θ(k) 0 = θ⋆ ωk−1 , when t = eΩ(S), ∀k ∈ [K], we have V π⋆ ωk ωk (ρ)−V π θ(k−1) t ωk (ρ) < ε0. Hence, for any con- text ωk, k∈ [K], initializing θ(k) 0 = θ(k−1) t from learning Figure 2: Visualization of the two reward functions for the four-room navigation environment. Left: easy; Right: hard. Darker color indicates a higher reward when the agent reaches the state (with the goal state the darkest). The agent receives 0 reward when it is too far from the goal location (5 steps for the easy reward function and 4 steps for the hard reward function). The black line indicates walls in the environment where the agent cannot pass through. The reward function visualization in this fig- ure is for the last context. The reward function for other contexts (other goals) is the same but with the reward function shifted ac- cording to the goal state. π⋆ ωk−1 via stochastic PG after t = Ω(S) iteration, θ(k) 0 will directly start from the efficient phase 2 with polynomial complexity. Hence, the total iteration number for learning the θ⋆ K is Ω(KS), and the per iteration sample complexity remains the same as Theorem 4.1 (eΩ \u0000 S3\u0001 ). 4.4. Numerical Experiments: Four-room Navigation To provide empirical support for the validity of our theoret- ical results, we follow the exact setting that is used in our theoretical analysis and implement R OLLIN with stochas- tic PG and the softmax policy parameterization on a tabular contextual MDP. It is worth highlighting that this is distinct from the practical implementation of R OLLIN in Section 5 – the focus there is to apply the idea of R OLLIN to design a sample-efficient deep RL algorithm for a more practical setting (e.g., continuous state and action space), whereas the focus here is solely to validate our theory in the theoret- ical setting. The contextual MDP is a grid world consisting of 12 × 12 grid cells where each cell corresponds to a state in the MDP. The agent always starts from the bottom-left corner of the grid world and navigates around to collect re- wards. In particular, the agent receives a positive reward 6Understanding the Complexity Gains of Single-Task RL with a Curriculum when it is close to a pre-defined goal cell/state which is de- fined by the context. We use a curriculum consisting of 17 contexts/goals in sequence, {ωk}16 k=0, which form a path from the initial state of the agent to a far away goal loca- tion, and we switch from the current context to the next one in the curriculum whenever the current goal location is reached with more than 50% probability. We experiment with two different reward functions (visualized in Figure 2). Table 1 summarizes the results of our experiments. ROLLIN is able to consistently improve upon the vanilla stochastic PG baseline (across two different entropy coeffi- cients and two reward functions with varying difficulties) in terms of the curriculum progress and the final return. This verifies that ROLLIN can indeed improve the sample com- plexity of stochastic PG in a curriculum learning setting, validating our theory. See more implementation details of the numerical experiments in Appendix F. 5. Practical Implementation of ROLLIN We have shown empirical evidence through numerical ex- periments that R OLLIN can lead to sample complexity re- duction under our theoretical setting (tabular MDP with discrete action space and state space, softmax parameter- ization and entropy regularized objective). Now, we in- troduce a practical implementation of R OLLIN using Soft- Actor-Critic (Haarnoja et al., 2018) such that R OLLIN can be applied to more practical problems with continuous ac- tion space and state space. SAC can be seen as a variant of entropy-regularized stochastic PG with the addition of the critics to reduce gradient variance. Recall that in the theo- retical analysis, we learn a separate policy for each context that can start from the near-optimal state distribution of the previous context to achieve a good return under the current context. However, in practice, we usually would want to have a policy that can directly start from the initial distri- bution ρ to obtain a good return for the final context ωK. In order to learn such a policy, we propose to have two context-conditioned RL agents training in parallel, where the first agent πmain is the main agent that eventually will learn to achieve a good return fromρ, and the second agent πexp is an exploration agent that learns to achieve a good return under the current context from the near-optimal state density of the previous context. Another purpose of the ex- ploration agent (as the name suggests) is to provide a better exploration experience for the main agent to learn the cur- rent context better. This is made convenient by using an off-policy RL agent where the main agent can learn from the data collected by the exploration agent. Specifically, for each episode, there is a probability of β where we run the main agent conditioned on the previous context for a random number of steps until we switch to the exploration agent to collect experience for the current con- text until the episode ends. Otherwise, we directly run the main agent for the entire episode. Both agents are trained to maximize the return under the current context. Whenever the average return of the last 10 episodes exceeds a per- formance threshold R, we immediately switch to the next context and re-initialize the exploration agent and its replay buffer. A high-level description is available in Algorithm 2 (a more detailed version in Algorithm 8). Algorithm 2 Practical Implementation of ROLLIN 1: Input: {ωk}K k=0: input curriculum, R: near-optimal threshold, β: roll-in ratio, H: horizon, γ: discount fac- tor. 2: Initialize D ← ∅, Dexp ← ∅, k← 0, and two SAC agents πmain and πexp. 3: for each episode do 4: if average return of the last 10 episodes under con- text ωk is greater than R then 5: k ← k + 1, Dexp ← ∅, and re-initialize the exploration agent πexp 6: end if 7: if k >0 and with probability of β then 8: h ∼ Geom(1 − γ) (truncated at H) 9: run πmain(a|s, ωk−1) from the initial state for h steps and switch to πexp(a|s, ωk) until the episode ends to obtain trajectory τ0:H = {s0, a0, r0, s1, a1, ··· , sH}. 10: record τ0:H in D, and τh:H in Dexp. 11: else 12: run πmain(a|s, ωk) to obtain trajectory τ0:H and record τ0:H in D. 13: end if 14: at each environment step in the episode, update πmain(·|·, ωk) using D and πexp(·|·, ωk) using Dexp. 15: end for 16: Output: πmain 6. Experimental Results While the focus of our work is on developing a provably ef- ficient approach to curriculum learning, we also conduct an experimental evaluation of our practical implementation of ROLLIN with soft actor-critic (SAC) (Haarnoja et al., 2018) as the RL algorithm on several continuous control tasks in- cluding a goal reaching task and four non-goal reaching tasks with oracle curricula. 6.1. Goal Reaching with an Oracle Curriculum We adopt the antmaze-umaze environment (Fu et al., 2020) for evaluating the performance of R OLLIN in goal- reaching tasks. We use a hand-crafted path of contexts, where each specifies a goal location (as shown in Ap- 7Understanding the Complexity Gains of Single-Task RL with a Curriculum w/o Geometric Sampling w/ Geometric Sampling Setting Method ∆ = 1/24 ∆ = 1 /12 ∆ = 1 /24 ∆ = 1 /12 Vanilla Baseline 0.40 ± 0.02 0 .36 ± 0.00 0 .82 ± 0.08 0 .38 ± 0.03 ROLLIN 0.49 ± 0.04 0 .44 ± 0.01 0 .92 ± 0.02 0 .55 ± 0.04 Relabeling Baseline 0.89 ± 0.03 0 .66 ± 0.04 0 .76 ± 0.02 0 .72 ± 0.03 ROLLIN 0.91 ± 0.03 0 .74 ± 0.01 0 .78 ± 0.01 0 .73 ± 0.00 Go-Explore Baseline 0.37 ± 0.02 0 .38 ± 0.01 0 .82 ± 0.07 0 .42 ± 0.03 Noise = 0.1 R OLLIN 0.52 ± 0.07 0 .38 ± 0.01 0 .95 ± 0.02 0 .43 ± 0.02 Table 2: Learning progress κ at 3 million environment steps with varying curriculum step size ∆ of different settings of goal reaching in antmaze-umaze. We pick β = 0.1 for all experiments using ROLLIN , the results of using other βs, ∆s, and exploration noises can be found in Table 8, Table 9, and Table 10 in Appendix G.1. The standard error is computed over 8 random seeds. pendix E.1, Figure 3). We consider a path of contexts ω(κ) parameterized by κ ∈ [0, 1] where ω(0) = ω0 and ω(1) = ωK, and step through the contexts along the path with a fixed step size ∆. See Appendix E.1 for more im- plementation details. We combine R OLLIN with a variety of prior methods, and we evaluate the following conditions: (1) stan- dard goal reaching; (2) goal reaching with goal relabel- ing (Andrychowicz et al., 2017); (3) goal reaching with an exploration phase that is similar to Go-Explore (Ecof- fet et al., 2019). For goal relabeling, we adopt a similar relabeling technique as Pitis et al. (2020), where each mini- batch contains 1/3 original transitions, 1/3 transitions with future state relabeling, and 1/3 transitions with next state relabeling. We implemented the Go-Explore method by adding an additional standard Gaussian exploration noise in the action to the agent for learning the next goalω(k+1), once it reaches the current goal ω(k). We empirically ob- served that sampling the replay buffer from a geometric dis- tribution with p = 10−5 (more recent transitions are sam- pled more frequently) improves the overall performance. Hence, in all future experiments, we compare the perfor- mance of R OLLIN with classic uniform sampling and the new geometric sampling. We compare the learning speed of ROLLIN with parameter β = 0.1 on three different step sizes ∆ = 1 24 , 1 18 , 1 12 in Table 2. Main comparisons. We first provide an overview exper- iments that compares R OLLIN with a fixed β = 0 .1 on different step sizes ∆ in different settings. In each case, we compare the prior method (vanilla, relabeled, or Go- Explore) with and without the addition of R OLLIN . As shown in Table 2, ROLLIN improves the largest value of κ reached by the agent in most presented settings (except Go- Explore with ∆ = 1/12). This result suggests that ROLLIN facilitates goal-conditioned RL with a curriculum, as we only update the learning progress κ to κ + ∆ when the re- turn of the current policy reaches a certain thresholdR (See detailed update of κ in Algorithm 2). Note that β = 0 .1 does not always produce the best result, we will provide more results comparing different βs in different settings later in this section, and we leave all the learning curves and detailed tables to Appendix G.1. Note that we do not include the results of directly learning the last context in the antmaze-umaze environment because the agent cannot reach the goal without the aid of a curriculum, which is corroborated by (Pitis et al., 2020). 6.2. Non-Goal Reaching Tasks For the non-goal tasks, we consider the tasks of gradually increasing the x-velocity of a locomotion agent in the fol- lowing environments: walker2d, hopper, humanoid, and ant in OpenAI gym (Brockman et al., 2016). More specifically, we set the desired speed range to be[λκ, λ(κ+ 0.1)), where λ is a parameter depending on the physics of the agent in different environments and we choose a fixed contextual space with ten discrete contexts: κ ∈ {0.1, 0.2, . . . ,1}. The agent receives a higher reward when the x-velocity is within the desired speed range and a lower reward otherwise. In each environment, we increase the task difficulty with later curriculum steps (larger κ), by in- creasing the near-optimal threshold R(κ). Detailed param- eters of the desired speed range λ, near optimal-threshold R(κ), and the reward functions are in Appendix E.2. Main comparisons. We first compare ROLLIN with a fixed β = 0.1 at different environment steps: 0.5 ×106, 1 ×106. In each case, we compare the learning progressκ, averaged x-velocity, and averaged return, with and without the addi- tion of ROLLIN . Note that for the case without ROLLIN , we still provide the curriculum to the agent for training. The results in Table 3 show that ROLLIN improves most bench- marks (See detailed update of κ in Algorithm 2). Note that β = 0 .1 does not always produce the best result, and we provide more results comparing different βs in different settings later in this section, with learning curves and more 8Understanding the Complexity Gains of Single-Task RL with a Curriculum Step = 0.5 × 106 Step = 1.0 × 106 Env. Method κ x -velocity return κ x -velocity return walker Scratch n/a 3.07 ± 0.26 3373 .1 ± 170.5 n/a 3.30 ± 0.36 4212 .3 ± 151.4 Baseline 0.83 ± 0.03 3 .09 ± 0.31 3450 .1 ± 307.4 0 .92 ± 0.03 3 .69 ± 0.27 4032 .3 ± 224.3 ROLLIN 0.79 ± 0.04 2 .83 ± 0.31 3350 .4 ± 184.6 0 .94 ± 0.03 3 .62 ± 0.26 4128 .8 ± 159.6 hopper Scratch n/a 2.50 ± 0.13 2943 .6 ± 80.3 n/a 2.55 ± 0.12 3073 .2 ± 137.7 Baseline 0.85 ± 0.02 2 .42 ± 0.18 3192 .5 ± 80.4 0 .88 ± 0.01 2 .58 ± 0.16 3386 .2 ± 124.7 ROLLIN 0.82 ± 0.03 2 .26 ± 0.22 3148 .6 ± 160.7 0 .89 ± 0.00 2 .65 ± 0.15 3421 .9 ± 109.8 humanoid Scratch n/a 0.24 ± 0.05 2417 .1 ± 188.2 n/a 0.37 ± 0.05 2763 .8 ± 96.5 Baseline 0.32 ± 0.05 0 .26 ± 0.05 2910 .1 ± 262.9 0 .67 ± 0.03 0 .39 ± 0.05 3017 .2 ± 169.0 ROLLIN 0.36 ± 0.04 0 .32 ± 0.07 2939 .7 ± 392.0 0 .69 ± 0.06 0 .46 ± 0.09 3173 .6 ± 238.3 ant Scratch n/a 3.60 ± 0.49 2910 .7 ± 354.3 n/a 4.55 ± 0.36 4277 .9 ± 120.0 Baseline 0.72 ± 0.02 3 .38 ± 0.43 2976 .2 ± 252.4 1 .00 ± 0.00 4 .29 ± 0.51 4248 .5 ± 88.6 ROLLIN 0.82 ± 0.06 3 .85 ± 0.41 3593 .1 ± 237.8 1 .00 ± 0.00 4 .66 ± 0.30 4473 .0 ± 102.2 Table 3: Learning progress κ, average x-velocity, and average return at 0.5 and 1.0 million environment steps inwalker, hopper, humanoid, and ant. The average x-velocity and return are estimated using the last 50k environment steps. “Scratch” shows the results of directly training the agent with the last context ω(1). “Baseline” indicates β = 0, where we provide the curriculum ω(κ) to the agent without using ROLLIN . We pick β = 0.1 for all experiments using ROLLIN , the results of using other βs can be found in Table 11, Table 12, and Table 13 in Appendix G.2. The standard error is computed over 8 random seeds. detailed tables in Appendix G.2. 6.3. Experimental Summary We empirically showed that R OLLIN improves the perfor- mance of one goal-reaching task and four non-goal tasks in different settings. Although ROLLIN introduces an extra parameter β, our experiments show reasonable improve- ment by simply choosing β = 0 .1 or 0.2. More careful selection of β might lead to further improvements. 7. Discussion and Future Work We presented ROLLIN , a simple algorithm that accelerates curriculum learning under the contextual MDP setup by rolling in a near-optimal policy to bootstrap the learning of new nearby contexts with provable learning efficiency benefits. Theoretically, we show that ROLLIN attains poly- nomial sample complexity by utilizing adjacent contexts to initialize each policy. Since the key theoretical insight of ROLLIN suggests that one can reduce the density mismatch ratio by constructing a new initial distribution, it would be interesting to see how R OLLIN can affect other variants of convergence analysis of PG (e.g., NPG (Kakade, 2001; Cen et al., 2021) or PG in a feature space (Agarwal et al., 2021; 2020)). On the empirical side, our experiments demon- strate that ROLLIN improves the empirical performance of various tasks beyond our theoretical assumptions, which reveals the potential of ROLLIN in other practical RL tasks with a curriculum. Our initial practical instantiation of the ROLLIN algorithm has a lot of room for future research. First of all, our implementation requires domain-specific knowledge of a “good” return value as it currently rely on a fixed return threshold R to determine when we are go- ing to switch from the current context to the next context. Another promising direction is to combine our algorithm with context-based meta-RL methods such as learning to generate sub-goal/context to accelerate the learning of the current sub-goal/context. Finally, our method is not spe- cific to the goal-conditioned settings, which opens up the possibility of applying our algorithm to more challenging domains. 8. Acknowledgements We are thankful to Laura Smith, Dibya Ghosh, Chuer Pan, and other members of the RAIL lab for feedback and sug- gestions on earlier drafts. QL would like acknowledge the support of the Berkeley Fellowship. YZ would like to thank Jincheng Mei from Google and Yuhao Ding from UC Berkeley for insightful discussions on the related proof. YM would like to acknowledge the support of ONR grants N00014-20-1-2002, N00014-22-1-2102, the joint Simons Foundation-NSF DMS grant # 2031899, and Tsinghua- Berkeley Shenzhen Institute (TBSI) Research Fund. SL would like to acknowledge Air Force Office of Scien- tific Research AFOSR FA9550-22-1-0273. The research is supported by Savio computational cluster provided by the Berkeley Research Compute program. 9Understanding the Complexity Gains of Single-Task RL with a Curriculum References Yasin Abbasi-Yadkori and Gergely Neu. Online learn- ing in MDPs with side information. arXiv preprint arXiv:1406.6812, 2014. Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019. Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. PC-PG: Policy cover directed exploration for prov- able policy gradient learning. Advances in Neural Infor- mation Processing Systems, 33:13399–13412, 2020. Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Op- timality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1–76, 2021. Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Provable bene- fits of representational transfer in reinforcement learn- ing. arXiv preprint arXiv:2205.14571, 2022. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Ma- teusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019. Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning , pages 166–175. PMLR, 2017. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mc- Grew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.Advances in neu- ral information processing systems, 30, 2017. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa- chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipula- tion. The International Journal of Robotics Research, 39 (1):3–20, 2020. Karl J Åström and Björn Wittenmark. Adaptive control. Courier Corporation, 2013. Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement learn- ing. In International Conference on Machine Learning, pages 263–272. PMLR, 2017. Andrea Bassich, Francesco Foglino, Matteo Leonetti, and Daniel Kudenko. Curriculum learning with a progres- sion function. arXiv preprint arXiv:2008.00511, 2020. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Ad- vances in Neural Information Processing Systems, pages 1471–1479, 2016. Stav Belogolovsky, Philip Korsunsky, Shie Mannor, Chen Tessler, and Tom Zahavy. Inverse reinforcement learning in contextual MDPs. Machine Learning, 110(9):2295– 2334, 2021. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Jalaj Bhandari and Daniel Russo. Global optimality guar- antees for policy gradient methods. arXiv preprint arXiv:1906.01786, 2019. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym, 2016. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. Opera- tions Research, 2021. Christoph Dann, Lihong Li, Wei Wei, and Emma Brun- skill. Policy certificates: Towards accountable reinforce- ment learning. In International Conference on Machine Learning, pages 1507–1516. PMLR, 2019. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexan- dre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. Advances in Neu- ral Information Processing Systems , 33:13049–13061, 2020. Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowledge in multi-task deep reinforcement learning. In International Confer- ence on Learning Representations, 2020. URL https: //openreview.net/forum?id=rkgpv2VFvr. 10Understanding the Complexity Gains of Single-Task RL with a Curriculum Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond ex- act gradients: Convergence of stochastic soft-max pol- icy gradient methods with entropy regularization. arXiv preprint arXiv:2110.10117, 2021. Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient RL with rich observations via latent state decod- ing. In International Conference on Machine Learning, pages 1665–1674. PMLR, 2019. Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-Explore: a new ap- proach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. Ben Eysenbach, Xinyang Geng, Sergey Levine, and Russ R Salakhutdinov. Rewriting history with inverse rl: Hind- sight inference for policy improvement. Advances in neural information processing systems , 33:14783– 14795, 2020a. Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-learning: Learning to achieve goals via re- cursive classification. arXiv preprint arXiv:2011.08909, 2020b. Kuan Fang, Yuke Zhu, Silvio Savarese, and Li Fei- Fei. Adaptive procedural task generation for hard- exploration problems. arXiv preprint arXiv:2007.00350, 2020. Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mes- bahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International Confer- ence on Machine Learning , pages 1467–1476. PMLR, 2018. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep net- works. In International conference on machine learning, pages 1126–1135. PMLR, 2017. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum genera- tion for reinforcement learning. In Conference on robot learning, pages 482–495. PMLR, 2017. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pages 1515–1528. PMLR, 2018. Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse control with events: A gen- eral framework for data-driven reward definition. arXiv preprint arXiv:1805.11686, 2018. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven re- inforcement learning. arXiv preprint arXiv:2004.07219, 2020. Yasuhiro Fujita, Kota Uenishi, Avinash Ummadisingu, Prabhat Nagarajan, Shimpei Masuda, and Mario Yno- cente Castro. Distributed reinforcement learning of tar- geted grasping with active vision for mobile manipula- tors. In 2020 IEEE/RSJ International Conference on In- telligent Robots and Systems (IROS), pages 9712–9719. IEEE, 2020. Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Manon Devin, Benjamin Eysenbach, and Sergey Levine. Learning to reach goals via iterated supervised learning. In International Conference on Learning Rep- resentations, 2021. URL https://openreview. net/forum?id=rALA0Xo6yNJ. Graham C Goodwin and Kwai Sang Sin. Adaptive filtering prediction and control. Courier Corporation, 2014. Anirudh Goyal, Riashat Islam, DJ Strouse, Zafarali Ahmed, Hugo Larochelle, Matthew Botvinick, Sergey Levine, and Yoshua Bengio. Transfer and exploration via the information bottleneck. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id=rJg8yhAqKm. Anirudh Goyal, Yoshua Bengio, Matthew Botvinick, and Sergey Levine. The variational bandwidth bottleneck: Stochastic evaluation on an information budget. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum? id=Hye1kTVFDS. Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham M Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. arXiv preprint arXiv:2210.09579 , 2022. Tuomas Haarnoja. Acquiring diverse robot skills via maxi- mum entropy deep reinforcement learning. University of California, Berkeley, 2018. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. Assaf Hallak, Dotan Di Castro, and Shie Mannor. Con- textual markov decision processes. arXiv preprint arXiv:1502.02259, 2015. 11Understanding the Complexity Gains of Single-Task RL with a Curriculum Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance learning for semi-supervised and unsupervised skill discovery. arXiv preprint arXiv:1907.08225, 2019. Boris Ivanovic, James Harrison, Apoorva Sharma, Mo Chen, and Marco Pavone. BARC: Backward reach- ability curriculum for robotic reinforcement learning. In 2019 International Conference on Robotics and Automa- tion (ICRA), pages 15–21. IEEE, 2019. Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning , pages 1704–1713. PMLR, 2017. Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? Ad- vances in neural information processing systems , 31, 2018. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In Proc. 19th In- ternational Conference on Machine Learning . Citeseer, 2002. Sham M Kakade. A natural policy gradient. Advances in neural information processing systems, 14, 2001. Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Ben- jamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. MT-opt: Contin- uous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021. Tae-Hoon Kim and Jonghyun Choi. Screenernet: Learning self-paced curriculum for deep neural networks. arXiv preprint arXiv:1801.00904, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. Pascal Klink, Carlo D’Eramo, Jan R Peters, and Joni Pa- jarinen. Self-paced deep reinforcement learning. Ad- vances in Neural Information Processing Systems , 33: 9216–9227, 2020. Pascal Klink, Carlo D’Eramo, Jan Peters, and Joni Pajari- nen. Boosted curriculum reinforcement learning. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum? id=anbBFlX1tJ1. Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999. Ilya Kostrikov. JAXRL: Implementations of Reinforce- ment Learning algorithms in JAX, 10 2021. URL https://github.com/ikostrikov/jaxrl. Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep successor reinforcement learning. arXiv preprint arXiv:1606.02396, 2016. Ioan Doré Landau, Rogelio Lozano, Mohammed M’Saad, and Alireza Karimi. Adaptive control: algorithms, anal- ysis and applications. Springer Science & Business Me- dia, 2011. Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take ex- ponential time to converge. In Conference on Learning Theory, pages 3107–3110. PMLR, 2021. Xingyu Liu, Deepak Pathak, and Kris M Kitani. Revolver: Continuous evolutionary models for robot-to-robot pol- icy transfer. arXiv preprint arXiv:2202.05244, 2022. Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In International Conference on Machine Learning, pages 6820–6829. PMLR, 2020. Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Dani- jar Hafner, and Deepak Pathak. Discovering and achiev- ing goals via world models. Advances in Neural Infor- mation Processing Systems, 34, 2021. Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learn- ing. In International conference on machine learning , pages 6961–6971. PMLR, 2020. Aditya Modi, Nan Jiang, Satinder Singh, and Ambuj Tewari. Markov decision processes with continuous side information. In Algorithmic Learning Theory , pages 597–618. PMLR, 2018. Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning using linearly combined model ensembles. In Interna- tional Conference on Artificial Intelligence and Statis- tics, pages 2010–2020. PMLR, 2020. Ted Moskovitz, Michael Arbel, Jack Parker-Holder, and Aldo Pacchiano. Towards an understanding of default policies in multitask policy optimization. In Interna- tional Conference on Artificial Intelligence and Statis- tics, pages 10661–10686. PMLR, 2022. Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. Advances in neural infor- mation processing systems, 30, 2017. 12Understanding the Complexity Gains of Single-Task RL with a Curriculum Ashvin Nair, Shikhar Bahl, Alexander Khazatsky, Vitchyr Pong, Glen Berseth, and Sergey Levine. Contextual imagined goals for self-supervised robotic learning. In Conference on Robot Learning, pages 530–539. PMLR, 2020. Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in neural infor- mation processing systems, 31, 2018. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced an- notation. In Conference on Robot Learning, pages 1303– 1315. PMLR, 2022. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A frame- work and survey. arXiv preprint arXiv:2003.04960 , 2020. Andrew Y Ng, Daishi Harada, and Stuart Russell. Pol- icy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pages 278–287, 1999. Shayegan Omidshafiei, Dong-Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher Amato, Murray Campbell, and Jonathan P How. Learning to teach in cooperative multiagent reinforcement learning. In Pro- ceedings of the AAAI conference on artificial intelli- gence, volume 33 issue 01, pages 6128–6136, 2019. OpenAI. OpenAI Five. https://blog.openai. com/openai-five/, 2018. OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D’Sa, Arthur Petron, Henrique P d O Pinto, et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021. Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, and Tim Rocktäschel. Evolving curricula with regret-based environment design. arXiv preprint arXiv:2203.01302, 2022. Jan Peters and Stefan Schaal. Natural actor-critic. Neuro- computing, 71(7-9):1180–1190, 2008. Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In Interna- tional Conference on Machine Learning , pages 7750– 7761. PMLR, 2020. Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-fit: State- covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019. Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre- Yves Oudeyer. Teacher algorithms for curriculum learn- ing of deep rl in continuously parameterized environ- ments. In Conference on Robot Learning , pages 835– 853. PMLR, 2020. Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravin- dran, and Sergey Levine. Epopt: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016. Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng. Exploration via hindsight goal generation. Ad- vances in Neural Information Processing Systems , 32, 2019. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gul- cehre, Guillaume Desjardins, James Kirkpatrick, Raz- van Pascanu, V olodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015. Shankar Sastry, Marc Bodson, and James F Bartram. Adaptive control: stability, convergence, and robustness, 1990. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy opti- mization. In International conference on machine learn- ing, pages 1889–1897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad- ford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shamane Siriwardhana, Rivindu Weerasakera, Denys JC Matthies, and Suranga Nanayakkara. VUSFA: Varia- tional universal successor features approximator to im- prove transfer DRL for target driven visual navigation. arXiv preprint arXiv:1908.06376, 2019. Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incen- tivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814 , 2015. Sainbayar Sukhbaatar, Emily Denton, Arthur Szlam, and Rob Fergus. Learning goal embeddings via self-play for hierarchical reinforcement learning. arXiv preprint arXiv:1811.09083, 2018. Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agar- wal, and John Langford. Model-based RL in contextual 13Understanding the Complexity Gains of Single-Task RL with a Curriculum decision processes: PAC bounds and exponential im- provements over model-free approaches. In Conference on learning theory, pages 2898–2933. PMLR, 2019. Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforce- ment learning with function approximation. Advances in neural information processing systems, 12, 1999. Fumihide Tanaka and Masayuki Yamamura. Multitask re- inforcement learning on the distribution of MDPs. In Proceedings 2003 IEEE International Symposium on Computational Intelligence in Robotics and Automation. Computational Intelligence in Robotics and Automation for the New Millennium (Cat. No. 03EX694) , volume 3, pages 1108–1113. IEEE, 2003. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip De- Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learn- ing. In Advances in neural information processing sys- tems, pages 2753–2762, 2017. Gang Tao. Adaptive control design and analysis , vol- ume 37. John Wiley & Sons, 2003. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782): 350–354, 2019. David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and V olodymyr Mnih. Unsupervised control through non-parametric discrimi- native rewards. arXiv preprint arXiv:1811.11359, 2018. Ronald J Williams. Simple statistical gradient-following al- gorithms for connectionist reinforcement learning. Ma- chine learning, 8(3-4):229–256, 1992. Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, Yinyuting Yin, Bei Shi, Liang Wang, Tengfei Shi, Qiang Fu, Wei Yang, Lanxiao Huang, and Wei Liu. Towards playing full moba games with deep reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 621–632. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 06d5ae105ea1bea4d800bc96491876e9-Paper. pdf. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural In- formation Processing Systems, 33:5824–5836, 2020a. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Ju- lian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi- task and meta reinforcement learning. In Conference on Robot Learning, pages 1094–1100. PMLR, 2020b. Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gra- dient. In International Conference on Artificial Intelli- gence and Statistics, pages 3332–3380. PMLR, 2022. Yuexiang Zhai, Christina Baek, Zhengyuan Zhou, Jiantao Jiao, and Yi Ma. Computational benefits of intermediate rewards for goal-reaching policy learning. Journal of Artificial Intelligence Research, 73:847–896, 2022. Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efficient reinforcement learn- ing with reinforce. arXiv preprint arXiv:2010.11364 , page 97, 2020a. Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to (al- most) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):3586–3612, 2020b. Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhut- dinov, Sergey Levine, and Joseph E Gonzalez. C- planning: An automatic curriculum for learning goal- reaching tasks. arXiv preprint arXiv:2110.12080, 2021. Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value disagreement. Ad- vances in Neural Information Processing Systems , 33: 7648–7659, 2020c. Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost op- timal model-free reinforcement learningvia reference- advantage decomposition. Advances in Neural Informa- tion Processing Systems, 33:15198–15207, 2020d. 14Understanding the Complexity Gains of Single-Task RL with a Curriculum A. Generalization Between Different Tasks in the Context Space A.1. Summaries of Notations and Assumptions 1. The maximum entropy RL (MaxEnt RL) objective with initial state distributionρ in reinforcement aims at maximizing (Equation 15 & 16 of (Mei et al., 2020)) V π(ρ) := ∞X h=0 γhEs0∼ρ,ah∼π(ah|sh) [r(sh, ah)] + αH(ρ, π) (4) and H(π(ah|sh)) is the discounted entropy term H(ρ, π) := E s0∼ρ,ah∼π(·|sh) \" ∞X h=0 −γh log π(ah|sh) # , (5) and α is the penalty term. For simplicity, we denote the optimization objective function in (4) as α-MaxEnt RL. Similar to Equation 18 & 19 of (Mei et al., 2020), we also define the advantage and Q-functions and for MaxEnt RL as Aπ(s, a) := Qπ(s, a) − α log π(s, a) − V π(s), Qπ(s, a) := r(s, a) + γ X s′ P(s′|s, a)V π(s). (6) 2. We let dπ s0 (s) = (1 − γ) ∞X t=0 γtPπ(st = s|s0), (7) to denote the discounted state visitation of policy π starting at state s0, and let dπ ρ (s) = Es∼ρ[dπ s (s)] (8) denote the initial state visitation distribution under initial state distribution ρ. 3. We assume the reward functions under all context are bounded within [0, 1]: rω(s, a) ∈ [0, 1], ∀ω ∈ Ω, ∀(s, a) ∈ S × A. (9) 4. Similar to previous analysis in (Agarwal et al., 2021; Mei et al., 2020; Ding et al., 2021), we assume the initial distribution µ for PG/stochastic PG satisfies ρ(s) > 0, ∀s ∈ S. A.2. Main Results: Mismatch Coefficient Upper Bound Lemma A.1 (Density Mismatch Ratio via R OLLIN ). Assuming ρ = Unif(S), and µk = βd π⋆ ωk−1 µk−1 + (1 − β)ρ (using (1) from ROLLIN ), the density mismatch ratio \r\r\r\rd π⋆ ωk µk /µk \r\r\r\r ∞ satisfies \r\r\r\r\r\r d π⋆ ωk µk µk \r\r\r\r\r\r ∞ ≤ eO \u0012 Lr α(1 − β)∆k ωS \u0013 , (10) where ∆k ω = max1≤i≤k ∥ωi − ωi−1∥2. 15Understanding the Complexity Gains of Single-Task RL with a Curriculum Proof. By (1) from ROLLIN , we have \r\r\r\r\r\r d π⋆ ωk µk µk \r\r\r\r\r\r ∞ = \r\r\r\r\r\r d π⋆ ωk µk − d π⋆ ωk−1 µk−1 + d π⋆ ωk−1 µk−1 µk \r\r\r\r\r\r ∞ (i) ≤ \r\r\r\rd π⋆ ωk µk − d π⋆ ωk−1 µk−1 \r\r\r\r 1 min µk + \r\r\r\r\r\r d π⋆ ωk−1 µk−1 βd π⋆ωk−1 µk−1 + (1 − β)ρ \r\r\r\r\r\r ∞ (ii) ≤ \r\r\r\rd π⋆ ωk µk − d π⋆ ωk−1 µk−1 \r\r\r\r 1 min µk + 1 β (11) where inequality (i) holds because of (1), and inequality (ii) holds because ρ(s) ≥ 0, ∀s ∈ S. Now it remains to bound\r\r\r\rd π⋆ ωk+1 µk+1 − d π⋆ ωk µk \r\r\r\r 1 using the difference ∥ωk+1 − ωk∥2. Let Pk h = P π⋆ ωk h (s′|s0 ∼ µk) denote the state visitation distribution resulting from π⋆ ωk probability starting at µk, then we have Pk h(s′) − Pk−1 h (s′) = X s,a \u0010 Pk h−1(s)π⋆ ωk (a|s) − Pk−1 h−1(s)π⋆ ωk−1 (a|s) \u0011 P(s′|s, a) = X s,a \u0010 Pk h−1(s)π⋆ ωk (a|s) − Pk h−1(s)π⋆ ωk−1 (a|s) + Pk−1 h−1(s)π⋆ ωk−1 (a|s) − Pk−1 h−1(s)π⋆ ωk−1 (a|s) \u0011 P(s′|s, a) = X s Pk h−1(s) \"X a \u0010 π⋆ ωk (a|s) − π⋆ ωk−1 (a|s) \u0011 P(s′|s, a) # + X s \u0000 Pk h−1(s) − Pk−1 h−1(s) \u0001 \"X a π⋆ ωk−1 (a|s)P(s′|s, a) # . (12) Taking absolute value on both side, yields \r\rPk h − Pk−1 h \r\r 1 = X s′ \f\fPk h(s′) − Pk−1 h (s′) \f\f ≤ X s Pk h−1(s) X a \f\f\fπ⋆ ωk (a|s) − π⋆ ωk−1 (a|s) \f\f\f | {z } ≤c1∥ωk−ωk−1∥2 X s′ P(s′|s, a) + X s \f\fPk h−1(s) − Pk−1 h−1(s) \f\f \"X s′ X a π⋆ ωk−1 (a|s)P(s′|s, a) # (i) ≤c1 ∥ωk − ωk−1∥2 + \r\rPk h−1 − Pk−1 h−1 \r\r 1 ≤ ··· ≤c1h ∥ωk − ωk−1∥2 + \r\rPk 0 − Pk−1 0 \r\r 1 (ii) = c1h ∥ωk − ωk−1∥2 + ∥µk − µk−1∥1 , (13) where inequality (i) holds by applying Lemma B.2 with c1 = Lr/α(1 − γ) and equality (ii) holds because the initial distribution of Pk h is µk. By the definition of dπ µ, we have d π⋆ ωk µk (s) − d π⋆ ωk−1 µk−1 (s) (i) = dk(s) − dk−1(s) = (1 − γ) ∞X h=0 γh \u0000 Pk h(s) − Pk−1 h (s) \u0001 , ∀s ∈ S. (14) where in equality (i), we use dk to denote d π⋆ ωk µk . Adding ℓ1 norm on both sides of (14) and applying (13), yields ∥dk − dk−1∥1 ≤ (1 − γ) ∞X h=0 γh (c1h ∥ωk − ωk−1∥2 + ∥µk − µk−1∥1) (i) = γc1 1 − γ ∥ωk − ωk−1∥2 + ∥µk − µk−1∥1 (ii) = γc1 1 − γ ∥ωk − ωk−1∥2 + β ∥dk−1 − dk−2∥1 , (15) 16Understanding the Complexity Gains of Single-Task RL with a Curriculum where equality (i) holds because P∞ h=0 γhh = γ/(1 − γ)2 and equality (ii) holds because of (1). Hence, we know that ∥dk − dk−1∥1 ≤ γc1 1 − γ ∥ωk − ωk−1∥2 + β ∥dk−1 − dk−2∥1 ≤ γc1 1 − γ k−1X i=0 \u0002 ∥ωi+1 − ωi∥2 βk−i\u0003 + βk−1 ∥d1 − d0∥1 ≤ γc1 1 − γ · 1 1 − β ∆k ω + βk−1 ∥d1 − d0∥1 ≤ \u0012 γc1 (1 − γ)(1 − β) + 1 \u0013 ∆k ω (16) where ∆k ω = max1≤i≤k ∥ωi − ωi−1∥2 and the last inequality holds due to the near optimality definition (Definition 4.2). Therefore, applying (16) back to (11), we know that \r\r\r\r\r\r d π⋆ ωk µk µk \r\r\r\r\r\r ∞ ≤ \r\r\r\rd π⋆ ωk µk − d π⋆ ωk−1 µk−1 \r\r\r\r 1 min µk + 1 β (i) ≤ 1 min µk \u0012 γc1 (1 − γ)(1 − β) + 1 \u0013 ∆k ω + 1 β = eO \u0012 Lr α(1 − β)∆k ωS \u0013 , (17) where inequality (i) holds since (1) Lemma B.2 implies c1 = Lr/α(1 − γ), and we omit the 1/(1 − γ)6 and log in the eO; (2) 1/ min µk ≤ S/(1 − β) according to µk = βd π⋆ ωk−1 µk−1 + (1 − β)ρ. Note that we can only achieve the final bound eO \u0010 Lr α(1−β) ∆k ωS \u0011 by setting β as a constant. If we pick an arbitrarily small β, then the 1/β term will dominate the complexity and we will not have the final bound of eO \u0010 Lr α(1−β) ∆k ωS \u0011 . A.3. Complexity of Vanilla Stochastic PG Theorem A.2 (Complexity of Stochastic PG (Theorem 5.1 of (Ding et al., 2021))). Consider an arbitrary tolerance level δ >0 and a small enough tolerance levelε >0. For every initial pointθ0, if θT+1 is generated by SPG (Algorithm 4) with T1 ≥ \u00126D(θ0) δε0 \u0013 8L C0 δ ln 2 , T2 ≥ \u0010 ε0 6δε − 1 \u0011 t0, T = T1 + T2, B1 ≥ max \u001a 30σ2 C0 δ ε0δ , 6σT1 log T1 ¯∆L \u001b , B2 ≥ σ2 ln(T2 + t0) 6Cζδε , ηt = η ≤ min \u001alog T1 T1L , 8 C0 δ , 1 2L \u001b ∀1 ≤ t ≤ T1, ηt = 1 t − T1 + t0 ∀t > T1, (18) where D(θt) = V π⋆ (ρ) − V πθt (ρ), ε0 = min (\u0012α mins∈S ρ(s) 6 ln 2 \u00132 \u0014 ζ exp \u0012 − 1 (1 − γ)α \u0013\u00154 , 1 ) , t0 ≥ s 3σ2 2δε0 , C0 δ = 2α S \r\r\r\r\r dπ⋆ ρ ρ \r\r\r\r\r −1 ∞ min s∈S ρ(s) min θ∈G0 δ min s,a πθ(a|s)2, Cζ = 2α S \r\r\r\r\r dπ⋆ ρ ρ \r\r\r\r\r −1 ∞ min s∈S ρ(s)(1 − ζ)2 min s,a π⋆(a|s)2, G0 δ := \u001a θ ∈ RS×A : min θ⋆∈Θ⋆ ∥θ − θ⋆∥2 ≤ (1 + 1/δ) ¯∆ \u001b , ¯∆ = \r\rlog c¯θ1,η − log π⋆\r\r 2 , c¯θ1,η = inf t≥1 min s,a πθt(a|s), σ2 = 8 (1 − γ)2 \u00121 + (α log A)2 (1 − γ1/2)2 \u0013 , L= 8 + α(4 + 8 logA) (1 − γ)3 , (19) then we have P(D(θT+1) ≤ ε) ≥ 1 − δ.2 2Note that the ζ here is an optimization constant that appears in ε0 and Cζ. 17Understanding the Complexity Gains of Single-Task RL with a Curriculum Corollary A.3 (Iteration Complexity and Sample Complexity for ε-Optimal Policies). Suppose we set the tolerance level ε, δ= O(S−1), the iteration complexity and sample complexity of obtaining an ε-optimal policy using stochastic softmax policy gradient (Algorithm 4) in phase 1 and phase 2 satisfies: • Phase 1: T1 = eΩ \u0010 S2S3 \u0011 , B1 = eΩ \u0010 S2S3 \u0011 , • Phase 2: T2 = eΩ (S), B2 = eΩ \u0000 S5\u0001 , with probability at least 1 − δ. Proof. We first check the dependency of (19) on S. Notice that • ε0: 1 ε0 = max (\u0012 6 ln 2 α mins∈S ρ(s) \u00132 \u0014 ζ exp \u0012 − 1 (1 − γ)α \u0013\u0015−4 , 1 ) = eΩ(S2); (20) • t0: t0 ≥ s 3σ2 2δε0 = eΩ(S); (21) • C0 δ : 1 C0 δ = S 2α \r\r\r\r\r dπ⋆ ρ ρ \r\r\r\r\r ∞ max s∈S ρ(s)−1 1 minθ∈G0 δ mins,a πθ(a|s)2 = eΩ(S3); (22) • Cζ: 1 Cζ = S 2α \r\r\r\r\r dπ⋆ ρ ρ \r\r\r\r\r ∞ max s∈S ρ(s)−1(1 − ζ)−2 max s,a π⋆(a|s)−2 = eΩ(S3). (23) Hence, the complexities in phase 1 scales at T1 ≥ \u00126D(θ0) δε0 \u0013 8L C0 δ ln 2 = eΩ \u0010 S2S3 \u0011 , B1 ≥ max \u001a 30σ2 C0 δ ε0δ , 6σT1 log T1 ¯∆L \u001b = eΩ \u0010 S2S3 \u0011 . (24) To enforce a positive T2, the tolerance level ε, δshould satisfy ε0 6δε ≥ 1, which implies 1 δε = Ω(S2). Hence, assuming ε0 δε = o(S), ε, δ= O(S−1), then the complexities in phase 2 scales at T2 ≥ \u0010 ε0 6δε − 1 \u0011 t0 = eΩ (S) , B2 ≥ σ2 ln(T2 + t0) 6Cζδε = eΩ \u0000 S5\u0001 . (25) A.4. Complexity of Learning the Next Context Theorem A.4 (Theorem 4.1: Complexity of Learning the Next Context) . Consider the context-based stochastic softmax policy gradient (line 7 of Algorithm 1), suppose Assumption 3.1 and Assumption 3.2 hold, then the iteration number of obtaining an ε-optimal policy for ωk from θ⋆ ωk−1 is eΩ (S) and the per iteration sample complexity is eΩ \u0010 Lr α(1−β) S3 \u0011 . We first introduce the following lemma to aid the proof of Theorem A.4. Lemma A.5 (Bounded Optimal Values Between two Adjacent Contexts). Under the same conditions as Theorem A.4, we have V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) ≤ 2Lr ∥ωk − ωk−1∥2 (1 − γ)2 . (26) 18Understanding the Complexity Gains of Single-Task RL with a Curriculum Proof. Let V π ω denote the value function of policy π with reward function rω. From (65) of Lemma B.3, we know that for any initial distribution ρ, we have V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) = 1 1 − γ X s \u0014 d π⋆ ωk−1 ρ (s) · α · DKL \u0010 π⋆ ωk−1 (·|s)||π⋆ ωk (·|s) \u0011\u0015 . (27) From (47) of Lemma B.1, we know that π⋆ ωk−1 (a|s) = h softmax(Q π⋆ ωk−1 (·, s)/α) i a := exp h Q π⋆ ωk−1 (s, a)/α i P a′ exp h Q π⋆ωk−1 (s, a′)/α i π⋆ ωk (a|s) = h softmax(Qπ⋆ ω (·, s)/α) i a := exp h Qπ⋆ ωk (s, a)/α i P a′ exp \u0002 Qπ⋆ωk (s, a′)/α \u0003, (28) hence, we have DKL \u0010 π⋆ ωk−1 (·|s)||π⋆ ωk (·|s) \u0011 = X a π⋆ ωk−1 (a|s) n log \u0010h softmax(Q π⋆ ωk−1 (a, s)/α) i a \u0011 − log \u0010h softmax(Qπ⋆ ωk (a, s)/α) i a \u0011o . (29) Let f(x) denote the log soft max function for an input vector x = [x1, x2, . . . , xA]⊤ such that xi ≥ 0, then for a small perturbation ∆ ∈ RA, the intermediate value theorem implies |[f(x + ∆)]i − [f(x)]i| = \f\f\f∆⊤∇z [f(z)]i \f\f\f, (30) for some vector z on the segment [x, x + ∆]. Now consider the Jacobian of the log softmax function ∂[∇zf(z)]i/∂zj: ∂[∇zf(z)]i ∂zj = ( 1 − pi(z) ∈ (0, 1) if i = j, −pj(z) ∈ (−1, 0) otherwise, (31) where pi(z) = exp(zi)/ PA k=1 exp(zk). hence, we know that |[f(x + ∆)]i − [f(x)]i| = \f\f\f∆⊤∇z [f(z)]i \f\f\f ≤ ∥∆∥∞ AX k=1 \f\f\f\f ∂[f(z)]i ∂zk \f\f\f\f = ∥∆∥∞  1 − pi(z) + X j̸=i pj(z)   ≤ 2 ∥∆∥∞ . (32) Now let x = 1 α[Q π⋆ ωk−1 (s, a1), Q π⋆ ωk−1 (s, a2), . . . , Q π⋆ ωk−1 (s, aA)], x + ∆ = 1 α[Qπ⋆ ωk (s, a1), Qπ⋆ ωk (s, a2), . . . , Qπ⋆ ωk (s, aA)], (33) (57) from Lemma B.2 implies that 1 α \r\r\rQπ⋆ ωk − Q π⋆ ωk−1 \r\r\r ∞ ≤ Lr ∥ωk − ωk−1∥2 α(1 − γ) , (34) substituting (34) and (32) into (29), yields DKL \u0010 π⋆ ωk−1 (·|s)||π⋆ ωk (·|s) \u0011 ≤ X a 2π⋆ ωk−1 (a|s) ∥∆∥∞ ≤ 2 ∥∆∥∞ ≤ 2Lr ∥ωk − ωk−1∥2 α(1 − γ) . (35) 19Understanding the Complexity Gains of Single-Task RL with a Curriculum Combine (35) with (27), we have V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) = 1 1 − γ X s \u0014 d π⋆ ωk−1 ρ (s) · α · DKL \u0010 π⋆ ωk−1 (·|s)||π⋆ ωk (·|s) \u0011\u0015 ≤2Lr ∥ωk − ωk−1∥2 (1 − γ)2 , (36) which completes the proof. Now we are ready to proceed to the proof of Theorem A.4. Proof. From (19) we know that ε0 = min (\u0012α mins∈S ρ(s) 6 ln 2 \u00132 \u0014 ζ exp \u0012 − 1 (1 − γ)α \u0013\u00154 , 1 ) = O \u0012 1 S2 \u0013 . (37) And from Section 6.2 of (Ding et al., 2021), we can directly enter phase 2 of the stochastic PG when V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) ≤ ε0. (38) Hence, when ∆k ω = max1≤i≤k ∥ωi − ωi−1∥2 = O(1/S2), we have V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) ≤ 2Lr∆ω (1 − γ)2 ≤ ε0 2 , (39) which implies we can directly enter phase 2 and enjoys the faster iteration complexity of T2 = Ω ( S) (by choosing δ = O(S−1)) and the smaller batch size of B2 ≥ σ2 ln(T2 + t0) 6Cζδε (i) = eΩ \u0012 Lr α(1 − β)∆k ωS5 \u0013 (ii) = eΩ \u0012 Lr α(1 − β)S3 \u0013 , (40) where equation (i) holds by applying Lemma A.1 to (23): σ2 ln(T2 + t0) 6Cζδε = eΩ \u0012 S4 · \r\r\r\rd π⋆ ωk µk /µk \r\r\r\r ∞ \u0013 = eΩ \u0012 Lr α(1 − β)∆k ωS5 \u0013 , and equality (ii) holds by the assumption that ∆k ω = O(S−2) and we omit the log term and components not related to S in eΩ. A.5. Total Complexity of ROLLIN Theorem A.6 (Theorem 4.3: Total Complexity of Learning the Target Context) . Suppose Assumption 3.1 and Assump- tion 3.2 hold, andθ(0) 0 is an near-optimal initialization, then the total number of iteration of learningπ⋆ ωK using Algorithm 1 is Ω(KS) and the per iteration is eΩ \u0000 S3\u0001 , with high probability. Proof. From Lemma A.5, we know that V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) ≤ 2Lr ∥ωk − ωk−1∥2 (1 − γ)2 . (41) Suppose for each context ωk, we initialize the parameters of the policy as θ(k) 0 = θ⋆ ωk−1 , and let θ(k) t denote the parameters at the tth iteration of SPG. We will use induction to show that when t = eΩ(S), ∀k ∈ [K], we have V π⋆ ωk ωk (ρ) − V π θ(k−1) t ωk (ρ) < ε0, (42) 20Understanding the Complexity Gains of Single-Task RL with a Curriculum this implies that for any context ωk, k∈ [K], we can always find a good initialization by setting θ(k) 0 = θ(k−1) t from learning π⋆ ωk−1 using SPG after t = Ω(S) iteration. This result guarantees that every initialization θ(k) 0 for learning the optimal contextual policy π⋆ ωk will directly start from the efficient phase 2. Induction: k = 0. When k = 0, Assumption 3.2 and the near-optimal initialization (Definition 4.2) of θ(0) 0 implies that V π⋆ ω0 ω0 (ρ) − V π θ(0) 0 ω0 (ρ) < ε0. (43) This result implies that a near-optimal initialization allows the initialization to directly start from phase 2 of SPG. Induction: from k − 1 to k. Suppose the result in (42) holds for k − 1, then we know that V π⋆ ωk−1 ωk−1 (ρ) − V π θ(k−1) 0 ωk−1 (ρ) = V π⋆ ωk−1 ωk−1 (ρ) − V π θ(k−2) t ωk−1 (ρ) < ε0. (44) Select ε such that ε ≤ ε0/2. Theorem A.4 suggests that when t′ = ˜Ω(S), with high probability, we have V π⋆ ωk ωk (ρ) − V π θ(k−1) t′ ωk (ρ) < ε≤ ε0 2 . (45) Hence, if we initialize θ(k) 0 = θ(k−1) t , with high probability when t′ = eΩ(S), we have V π⋆ ωk ωk (ρ) − V π θ(k−1) t′ ωk (ρ) = V π⋆ ωk ωk (ρ) − V π⋆ ωk−1 ωk (ρ) + V π⋆ ωk−1 ωk (ρ) − V π θ(k−1) t′ ωk (ρ) (i) ≤ ε0 2 + V π⋆ ωk ωk (ρ) − V π θ(k−1) t′ ωk (ρ) (ii) < ε0, (46) where inequality (i) holds by equation (39) in Theorem A.4, inequality (ii) holds because of the induction assumption in (45). Therefore, we have shown (42) holds for t = eΩ(S), ∀k ∈ [K]. Since we have K contexts in total, we know that Algorithm 1 can enforce a good initialization θ(k) 0 that directly starts from phase 2 for learning all π⋆ ωk , and for each k ∈ [K], the iteration complexity is eΩ(S). Hence the total iteration complexity of obtaining an ε-optimal policy for the final context ωK is eΩ (KS), with per iteration sample complexity of eΩ \u0000 S3\u0001 . B. Key Lemmas B.1. Optimal Policy of Maximum Entropy RL (Nachum et al., 2017) Lemma B.1. The optimal policy π⋆ that maximizes the α-MaxEnt RL objective (4) with penalty term α satisfies: π⋆(a|s) = exp h\u0010 Qπ⋆ (s, a) − V π⋆ (s) \u0011 /α i = exp \u0000 Qπ⋆ (s, a)/α \u0001 P a exp (Qπ⋆ (s, a)/α) (47) for all h ∈ N, where Qπ⋆ (s, a) := r(s, a) + γEs′∼P(s′|s,a)V (s′) V π⋆ (s) := α log  X a exp \u0010 Qπ⋆ (s, a)/α \u0011! . (48) Proof. Similar proof appears in (Nachum et al., 2017), we provide the proof for completeness. At the optimal policy πθ = π⋆, take the gradient of (4) w.r.t. p ∈ ∆(A) and set it to 0, we have ∂ ∂p(a) \"X a∈A p(a) \u0010 Qπ⋆ (s, a) − α ln p(a) \u0011# = Qπ⋆ (s, a) − α ln p(a) − α = 0, (49) which implies p(a) = exp \u0012Qπ⋆ (s, a) α − 1 \u0013 ∝ exp \u0012Qπ⋆ (s, a) α \u0013 . (50) Hence, we conclude that π⋆(a|s) ∝ exp(Q⋆(s, a)/α). 21Understanding the Complexity Gains of Single-Task RL with a Curriculum B.2. Bounding the Difference between Optimal Policies Lemma B.2. Suppose Assumption 3.1 holds, let π⋆ ω(a|s), π⋆ ω′(a|s) denote the optimal policy for α-MaxEnt RL (47), then ∀(s, a) ∈ S × A, the optimal policies of α-MaxEnt RL under context ω, ω′ satisfy: |π⋆ ω(a|s) − π⋆ ω′(a|s)| ≤Lr ∥ω − ω′∥2 α(1 − γ) . (51) Proof. From Lemma C.1, we know that the soft value iteration T Q(s, a) = r(s, a) + γαEs′ \" log X a′ exp Q(s′, a′)/α # (52) is a contraction. Let Qt ω, Qt ω′ denote the Q functions at the tth value iteration under context ω, ω′ respectively, we know Q∞ ω = Qπ⋆ ω and Q∞ ω′ = Qπ⋆ ω′ . Let εt = ∥Qt ω − Qt ω′∥∞, then we have εt+1 = \r\rQt+1 ω − Qt+1 ω′ \r\r ∞ = \r\r\r\r\rrω(s, a) − rω′(s, a) + γαEs′ \" log X a′ exp Qt ω′(s′, a′) α # − γαEs′ \" log X a′ exp Qt ω′(s′, a′) α #\r\r\r\r\r ∞ ≤∥rω − rω′∥∞ + γα \r\r\r\r\rEs′ log X s′ exp Qt ω(s′, a′)/α − Es′ log X s′ exp Qt ω′(s′, a′)/α \r\r\r\r\r ∞ ≤∥rω − rω′∥∞ + γ \r\rQt ω − Qt ω′ \r\r ∞ = ∥rω − rω′∥∞ + γεt, (53) where the last inequality holds because f(x) = log Pn i=1 exp(xi) is a contraction. From (53), we have εt+1 ≤ ∥rω − rω′∥∞ + γεt ≤ (1 + γ) ∥rω − rω′∥∞ + γ2εt−1 ≤ ··· ≤ ∥rω − rω′∥∞ tX i=0 γi + γtε1, (54) which implies \r\r\rQπ⋆ ω − Qπ⋆ ω′ \r\r\r ∞ = ε∞ ≤ ∥rω − rω′∥∞ 1 − γ ≤ Lr ∥ω − ω′∥2 1 − γ , (55) where the last inequality holds by Assumption 3.1. Hence, we have 1 α \f\f\fQπ⋆ ω (s, a) − Qπ⋆ ω′ (s, a) \f\f\f ≤ Lr ∥ω − ω′∥2 α(1 − γ) , ∀s, a∈ S × A (56) which implies 1 α \r\r\rQπ⋆ ω − Qπ⋆ ω′ \r\r\r ∞ ≤ Lr ∥ω − ω′∥2 α(1 − γ) . (57) Next, let π⋆ ω, π⋆ ω′ denote the maximum entropy policy RL under context ω, ω′ respectively. Then for a fixed state action pair (s, a) ∈ S × A, we have π⋆ ω(a|s) = h softmax(Qπ⋆ ω (·, s)/α) i a := exp \u0002 Qπ⋆ ω (s, a)/α \u0003 P a′ exp [Qπ⋆ω (s, a′)/α], π⋆ ω′(a|s) = h softmax(Qπ⋆ ω′ (·, s)/α) i a := exp \u0002 Qπ⋆ ω′ (s, a)/α \u0003 P a′ exp \u0002 Qπ⋆ ω′ (s, a′)/α \u0003, (58) where Qπ⋆ ω (·, s), Qπ⋆ ω′ (·, s) ∈ RA, and we want to bound |π⋆ ω(a|s) − π⋆ ω′(a|s)|. Next we will use (57) to bound |π⋆ ω(a|s) − π⋆ ω′(a|s)|, where the last inequality holds by (56). Let f(x) denote the softmax function for an input vec- tor x = [ x1, x2, . . . , xA]⊤ such that xi ≥ 0, then for a small perturbation ∆ ∈ RA, the intermediate value theorem implies |[f(x + ∆)]i − [f(x)]i| = \f\f\f∆⊤∇x [f(z)]i \f\f\f, (59) 22Understanding the Complexity Gains of Single-Task RL with a Curriculum for some vector z on the segment [x, x + ∆]. Hence |[f(x + ∆)]i − [f(x)]i| = \f\f\f∆⊤ [∇xf(z)]i \f\f\f ≤ ∥∆∥∞ AX k=1 \f\f\f\f ∂[f(z)]i ∂zk \f\f\f\f ≤∥∆∥∞  pi(z)(1 − pi(z)) + X j̸=i pi(z)pj(z)   < ∥∆∥∞  pi(z) + X j̸=i pj(z)   = ∥∆∥∞ , (60) where the Jacobian of the softmax function ∂ [∇xf(z)]i /∂zj satisfies: ∂ [∇xf(z)]i ∂zj = ( pi(z)(1 − pi(z)) if i = j, pi(z)pj(z) otherwise, (61) and pi(z) = exp(zi)/ PA k=1 exp(zk). Now let x = 1 α[Qπ⋆ ω (s, a1), Qπ⋆ ω (s, a2), . . . , Qπ⋆ ω (s, aA)], x + ∆ = 1 α[Qπ⋆ ω′ (s, a1), Qπ⋆ ω′ (s, a2), . . . , Qπ⋆ ω′ (s, aA)]. (62) We know that f(x) = π⋆ ω(a|s) and f(x + ∆) = π⋆ ω′(a|s). Then (57) implies that ∥∆∥∞ ≤ Lr ∥ω − ω′∥2 α(1 − γ) , (63) substituting this bound on ∥∆∥∞ into (60), we have |π⋆ ω(a|s) − π⋆ ω′(a|s)| = |f(x) − f(x + ∆)| ≤ ∥∆∥∞ ≤ Lr ∥ω − ω′∥2 α(1 − γ) , (64) which completes the proof. B.3. Soft Sub-Optimality lemma (Lemma 25 & 26 of (Mei et al., 2020)) Lemma B.3. For any policy π and any initial distribution ρ, the value function V π(ρ) of the α-MaxEnt RL (48) satisfies: V π⋆ (ρ) − V π(ρ) = 1 1 − γ X s \u0002 dπ ρ (s) · α · DKL (π(·|s)||π⋆(·|s)) \u0003 , (65) where π⋆ is the optimal policy of the α-MaxEnt RL (4). Proof. Similar proof appears in Lemma 25 & 26 of (Mei et al., 2020), we provide the proof here for completeness. 23Understanding the Complexity Gains of Single-Task RL with a Curriculum Soft performance difference. We first show a soft performance difference result for the MaxEnt value function (Lemma 25 of (Mei et al., 2020)). By the definition of MaxEnt value function and Q-function (4), (6), ∀π, π′, we have V π′ (s) − V π(s) = X a π′(a|s) · h Qπ′ (s, a) − α log π′(a|s) i − X a π(a|s) · [Qπ(s, a) − α log π(a|s)] = X a (π′(a|s) − π(a|s)) · h Qπ′ (a|s) − α log π′(a|s) i + X a π(a|s) · h Qπ′ (s, a) − α log π′(a|s) − Qπ(s, a) + α log π(a|s) i = X a (π′(a|s) − π(a|s)) · h Qπ′ (a|s) − α log π′(a|s) i + αDKL (π(·|s)||π′(·|s)) + γ X a π(a|s) X s′ P(s′|s, a) · h V π′ (s′) − V π(s′) i = 1 1 − γ X s′ dπ s (s′) \"X a′ (π′(a′|s′) − π(a′|s′)) h Qπ′ (s′, a′) − α log π′(a′|s′) i + αDKL (π(·|s′)||π′(·|s′)) # , (66) where the last equality holds because by the definition of state visitation distribution dπ s0 (s) = (1 − γ) ∞X t=0 γtPπ(st = s|s0), (67) taking expectation of s with respect to s ∼ ρ, yields V π′ (ρ) − V π(ρ) = 1 1 − γ X s′ dπ ρ (s′) \"X a′ (π′(a′|s′) − π(a′|s′)) · h Qπ′ (s′, a′) − α log π′(a′|s′) i + αDKL (π(·|s′)||π′(·|s′)) # , (68) and (68) is known as the soft performance difference lemma (Lemma 25 in (Mei et al., 2020)). Soft sub-optimality. Next we will show the soft sub-optimality result. By the definition of the optimal policy of α- MaxEnt RL (47), we have α log π⋆(a|s) = Qπ⋆ (s, a) − V π⋆ (s). (69) 24Understanding the Complexity Gains of Single-Task RL with a Curriculum Substituting π⋆ into the performance difference lemma (68), we have V π⋆ (s) − V π(s) = 1 1 − γ X s′ dπ s (s′) · \"X a′ (π⋆(a′|s′) − π(a′|s′)) · h Qπ⋆ (s′, a′) − α log π⋆(a′|s′) i | {z } =V π⋆(s′) + αDKL (π(·|s′)||π⋆(·|s′)) # = 1 1 − γ X s′ dπ s (s′) · \"X a′ (π⋆(a′|s′) − π(a′|s′)) | {z } =0 ·V π⋆ (s′) + αDKL (π(·|s′)||π⋆(·|s′)) # = 1 1 − γ X s′ [dπ s (s′) · αDKL (π(·|s′)||π⋆(·|s′))] , (70) taking expectation s ∼ ρ yields V π⋆ (ρ) − V π(ρ) = 1 1 − γ X s \u0002 dπ ρ (s) · α · DKL (π(·|s)||π⋆(·|s)) \u0003 , (71) which completes the proof. C. Supporting Lemmas C.1. Bellman Consistency Equation of MaxEnt RL Lemma C.1 (Contraction of Soft Value Iteration). From (48) and (6), the soft value iteration operator T defined as T Q(s, a) := r(s, a) + γαEs′ \" log X a′ exp (Q(s′, a′)/α) # (72) is a contraction. Proof. A similar proof appears in (Haarnoja, 2018), we provide the proof for completeness. To see (72) is a contraction, for each (s, a) ∈ S × A, we have T Q1(s, a) = r(s, a) + γα log X a′ exp \u0012Q1(s, a) α \u0013 ≤r(s, a) + γα log X a′ exp \u0012Q2(s, a) + ∥Q1 − Q2∥∞ α \u0013 ≤r(s, a) + γα log ( exp \u0012∥Q1 − Q2∥∞ α \u0013X a′ exp \u0012Q2(s, a) α \u0013) =γ ∥Q1 − Q2∥∞ + r(s, a) + γα log X a′ exp \u0012Q2(s, a) α \u0013 = γ ∥Q1 − Q2∥∞ + T Q2(s, a), (73) which implies T Q1(s, a) − TQ2(s, a) ≤ γ ∥Q1 − Q2∥∞. Similarly, we also have T Q2(s, a) − TQ1(s, a) ≤ γ ∥Q1 − Q2∥∞, hence we conclude that |Q1(s, a) − Q2(s, a)| ≤γ ∥T Q1 − TQ2∥∞ , ∀(s, a) ∈ S × A, (74) which implies ∥Q1 − Q2∥∞ ≤ γ ∥T Q1 − TQ2∥∞. Hence T is a γ-contraction and the optimal policy π⋆ of it is unique. 25Understanding the Complexity Gains of Single-Task RL with a Curriculum C.2. Constant Minimum Policy Probability Lemma C.2 (Lemma 16 of (Mei et al., 2020)). Using the policy gradient method (Algorithm 3) with an initial distribution ρ such that ρ(s) > 0, ∀S, we have c := inf t≥1 min s,a πθt(a|s) > 0 (75) is a constant that does not depend on t. Remark C.3 (State Space Dependency of constant c, C0 δ ). For the exact PG case, c in Lemma C.2 could also depend on S, similarly for the constant C0 δ in the stochastic PG case. As pointed out by Li et al. (2021) (Table 1), the constant c (or C0 δ in Theorem A.2 of the SPG case) may depend on the structure of the MDP . The ROLLIN technique only improves the mismatch coefficient \r\rdπ⋆ µ /µ \r\r ∞, instead of the constant c (or C0 δ ). Still, in the exact PG case, if one replaces the constant c with other S dependent function f(S), one still can apply a similar proof technique for Theorem 4.1 to show thatROLLIN reduces the iteration complexity, and the final iteration complexity bound in Theorem 4.1 will include an additional f(S). In addition, omitting the factorC0 δ , ROLLIN can improve the exponential complexity dependency incurred by the stochastic optimization to a polynomial dependency. D. Supporting Algorithms Algorithm 3 PG for α-MaxEnt RL (Algorithm 1 in (Mei et al., 2020)) 1: Input: ρ, θ0, η >0. 2: for t = 0, . . . , Tdo 3: θt+1 ← θt + η · ∂V πθt (ρ) ∂θt 4: end for Algorithm 4 Two-Phase SPG for α-MaxEnt RL (Algorithm 5.1 in (Ding et al., 2021)) 1: Input: ρ, θ0, α, B1, B2, T1, T,{ηt}T t=0 2: for t = 0, 1, . . . , Tdo 3: if t ≤ T1 then 4: B = B1 ▷ Phase 1 5: else 6: B = B2 ▷ Phase 2 7: end if 8: Run random horizon SPG with ρ, α, θt, B, t, ηt ▷ Algorithm 5 9: end for Algorithm 5 Random-horizon SPG for α-MaxEnt RL Update (Algorithm 3.2 in (Ding et al., 2021)) 1: Input: ρ, α, θ0, B, t, ηt 2: for i = 1, 2, ..., Bdo 3: si Ht, ai Ht ← SamSA(ρ, θt, γ) ▷ Algorithm 6 4: ˆQπθt,i ← EstEntQ(si Ht, ai Ht, θt, γ, α) ▷ Algorithm 7 5: end for 6: θt+1 ← θt + ηt (1−γ)B PB i=1 h ∇θ log πθt(ai Ht|si Ht) \u0010 ˆQπθt,i − α log πθt \u0011 (ai Ht|si Ht) i Remark D.1. Lemma 3.4 in (Ding et al., 2021) implies that the estimator 1 (1 − γ) h ∇θ log πθt(ai Ht|si Ht) \u0010 ˆQπθt,i − α log πθt \u0011 (ai Ht|si Ht) i (76) in line 6 of Algorithm 6 is an unbiased estimator of the gradient ∇θV πθ (ρ). 26Understanding the Complexity Gains of Single-Task RL with a Curriculum Algorithm 6 SamSA: Sample s, afor SPG (Algorithm 8.1 in (Ding et al., 2021)) 1: Input: ρ, θ, γ 2: Draw H ∼ Geom(1 − γ) ▷ Geom(1 − γ) geometric distribution with parameter 1 − γ 3: Draw s0 ∼ ρ, a0 ∼ πθ(·|s0) 4: for h = 1, 2, . . . , H− 1 do 5: Draw sh+1 ∼ P(·|sh, ah), ah+1 ∼ πθt(·|sh+1) 6: end for 7: Output: sH, aH Algorithm 7 EstEntQ: Unbiased Estimation of MaxEnt Q (Algorithm 8.2 in (Ding et al., 2021)) 1: Input: s, a, θ, γ, α 2: Initialize s0 ← s, a0 ← a, ˆQ ← r(s0, a0) 3: Draw H ∼ Geom(1 − γ) 4: for h = 0, 1, . . . , H− 1 do 5: sh+1 ∼ P(·|sh, ah), ah+1 ∼ πθ(·|sh+1) 6: ˆQ ← ˆQ + γh+1/2 [r(sh+1, ah+1) − α log πθ(ah+1|sh+1)] 7: end for 8: Output: ˆQ 27Understanding the Complexity Gains of Single-Task RL with a Curriculum E. Experimental Details We use the SAC implementation from https://github.com/ikostrikov/jaxrl (Kostrikov, 2021) for all our experiments in the paper. E.1. Goal Reaching with an Oracle Curriculum For our antmaze-umaze experiments with oracle curriculum, we use a sparse reward function where the reward is 0 when the distance D between the ant and the goal is greater than 0.5 and r = exp(−5D) when the distance is smaller than or equal to 0.5. The performance threshold is set to be R = 200 . Exceeding such threshold means that the ant stays on top of the desired location for at least 200 out of 500 steps, where 500 is the maximum episode length of the antmaze-umaze environment. We use the average return of the last 10 episodes and compare it to the performance threshold R. For both of the SAC agents, we use the same set of hyperparameters shown in Table 4. See Algorithm 8, for a more detailed pseudocode. Algorithm 8 Practical Implementation of ROLLIN 1: Input: {ωk}K k=0: input curriculum, ρ: initial state distribution, R: near-optimal threshold, β: roll-in ratio, discount factor γ. 2: Initialize D ← ∅, Dexp ← ∅, k← 0, and two off-policy RL agents πmain and πexp. 3: for each environment step do 4: if episode terminating or beginning of training then 5: if average return of the last 10 episodes under context ωk is greater than R then 6: k ← k + 1, Dexp ← ∅ 7: Re-initialize the exploration agent πexp 8: end if 9: Start a new episode under context ωk with s0 ∼ ρ, t ← 0 10: if k >0 and with probability of β then 11: enable Rollin for the current episode. 12: else 13: disable Rollin for the current episode. 14: end if 15: end if 16: if Rollin is enabled for the current episode then 17: if Rollin is stopped for the current episode then 18: at ∼ πexp(at|st, ωk) 19: else 20: at ∼ πmain(at|st, ωk−1) 21: with probability of 1 − γ, stop Rollin for the current episode 22: end if 23: else 24: at ∼ πmain(at|st, ωk) 25: end if 26: take action at in the environment and receives st+1 and rt = rωk (st, at) 27: add (st, at, st+1, rt) in replay buffer D 28: if Rollin is disabled for the current episode then 29: update πmain using D. 30: end if 31: if πexp was used to produce at then 32: add (st, at, st+1, rt) in replay buffer Dexp 33: update πexp using Dexp. 34: end if 35: t ← t + 1 36: end for 37: Output: πmain 28Understanding the Complexity Gains of Single-Task RL with a Curriculum Figure 3: Oracle curriculum of desired goals on antmaze-umaze. The ant starts from the right top corner and the farthest goal is located at the bottom right corner. Initial Temperature 1.0 Target Update Rate update rate of target networks 0.005 Learning Rate learning rate for the Adam optimizer 0.0003 Discount Factor 0.99 Batch Size 256 Warmup Period number of steps of initial random exploration (random actions) 10000 Network Size (256, 256) Table 4: Hyperparameters used for the SAC algorithm (Haarnoja et al., 2018) E.2. Non-Goal Reaching For the non goal reaching tasks in walker2d, hopper, humanoid, and ant experiments, the desired x-velocity range [λκ, λ(κ + 0.1)), the near-optimal threshold R(κ), and the healthy_reward all depend on the environments. The maximum episode length 1000. Details are provided in Table 5. healthy_reward Env. λ R (κ) original high low walker 5 500 + 4500 κ 1.0 1 .5 0 .5 hopper 3 500 + 4500 κ 1.0 1 .5 0 .5 humanoid 1 2500 + 2500 κ 5.0 7 .5 2 .5 ant 6 500 + 4500 κ 1.0 1 .5 0 .25 Table 5: Learning progress κ, average x-velocity, and average return at the 0.75 and 1.0 million environment steps in walker, hopper, humanoid, and ant. The average x-velocity and return are estimated using the last 50k time steps. We pick β = 0.1 for all experiments using ROLLIN , the results of using other βs can be found in Table 11, Table 12, and Table 13 in Appendix G.2. The standard error is computed over 8 random seeds. 29Understanding the Complexity Gains of Single-Task RL with a Curriculum F. Numerical Experiments: The Four Room Navigation F.1. MDP Setup The grid world consists of 12 × 12 grid cells where each cell corresponds to a state in the MDP. The agent can take four different actions to move itself in four directions to a different cell or take a fifth action to receive reward (positive if close to the goal, 0 otherwise). Each context in the context space represents a distinct goal state in the grid-world. The agent (when taking the fifth action) receives higher reward the closer it is to the goal state and receives 0 reward when it is too far (4 steps away for easy, and 5 steps away for hard). We also include 100 additional dummy actions in the action space (taking these actions do not result in reward nor state changes) to make the exploration problem challenging. See Figure 2 for a visualization of the environment and the two reward functions we use. More concretely, letD(s, g) be the number of action it takes to go from state s to state g (the current goal) if the walls did not exist (the Manhattan distance), the reward received when taking the fifth action at state s is rfour_room(s) = ( γD(s,g) reward, D (s, g) ≤ Dthreshold 0, D (s, g) > Dthreshold For the easy reward function, γreward = 0.9, Dthreshold = 5. For the hard reward function, γreward = 0.5, Dthreshold = 4. F.2. Pre-defined curriculum. Our curriculum contains 16 contexts in sequence, {ωk}16 k=0, which form a continuous path from the start location of the agent (0, 0) to the goal location of the agent at (8, 8). We use a fixed success rate threshold (an episode is considered to be successful if the agent reaches the goal state and perform the fifth action at that goal state) to determine convergence of the stochastic PG algorithm. We switch to the next context/curriculum step whenever the success rate exceeds 50%. We use κ ∈ [0, 1] to denote a normalized curriculum progress which is computed as the current curriculum step index divided by the total number of curriculum steps. F.3. Stochastic PG description We follow Algorithm 1 closely for our implementation. In particular, we adopt the softmax parameterization of π that is parameterized by θ ∈ RS×A as πθ(a = j|s = i) = exp(θij)P j′ exp(θij′) with i ∈ [S] and j ∈ [A] (in this MDP, S = 144 as there are 12 × 12 = 144 cells in the grid world and A = 105 due to the dummy actions). To sample from d π⋆ ωk−1 µk−1 (Line 6), we rollout the policy from the previous context πθk−1 in the MDP for h steps (where h being sampled from Geom(1 − γ), the geometric distribution with a rate of 1 − γ) and take the resulting state as a sample from d π⋆ ωk−1 µk−1 . We implement the stochastic PG using Adam optimizer (Kingma and Ba, 2015) on θ with a constant learning rate of 0.001. Every gradient step is computed over 2000 trajectories with the trajectory length capped at 50 for each. The empirical policy gradient for πθ is computed as over B = 2000 trajectories ({(sb 0, ab 0, sb 1, ··· , sb T }B b=1) and T = 50 time steps collected by rolling out the current policy πθ: 1 BT P b P t ∇θ log πθ(ab t|sb t)Rb t with Rb t = −PT t′=t γt′−trb ent,t′, rb ent,t = r(sb t, ab t) −α log π(ab t|sb t) where Rb t is the Monte-Carlo estimate of the discounted, entropy-regularized cumulative reward. F.4. Results Setting Entropy Coefficient β = 0.0 (Baseline) β = 0.1 β = 0.2 β = 0.3 β = 0.5 β = 0.75 β = 0.9 Hard α = 0.01 0.500 ± 0.000 0 .506 ± 0.001 0 .512 ± 0.001 0 .525 ± 0.000 0 .562 ± 0.000 0 .562 ± 0.000 0 .562 ± 0.000 α = 0.001 0.856 ± 0.006 0 .981 ± 0.003 0 .981 ± 0.001 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 Easy α = 0.01 0.944 ± 0.003 0 .994 ± 0.001 0 .994 ± 0.001 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 α = 0.001 1.000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 1 .000 ± 0.000 Table 6: Curriculum progress κ on the four-room navigation with stochastic PG at step 50,000. We tested with two different entropy coefficients and seven different β’s. The standard error is computed over 10 random seeds. 30Understanding the Complexity Gains of Single-Task RL with a Curriculum Setting Entropy Coefficient β = 0.0 (Baseline) β = 0.1 β = 0.2 β = 0.3 β = 0.5 β = 0.75 β = 0.9 Hard α = 0.01 0.000 ± 0.000 0 .000 ± 0.000 0 .000 ± 0.000 0 .000 ± 0.000 0 .000 ± 0.000 0 .000 ± 0.000 0 .000 ± 0.000 α = 0.001 0.424 ± 0.023 0 .939 ± 0.0014 0 .710 ± 0.021 1 .010 ± 0.005 1 .062 ± 0.000 1 .067 ± 0.000 1 .060 ± 0.000 Easy α = 0.01 4.093 ± 0.224 5 .136 ± 0.230 4 .156 ± 0.228 1 .040 ± 0.140 3 .913 ± 0.218 7 .374 ± 0.216 4 .227 ± 0.232 α = 0.001 10.536 ± 0.002 10 .566 ± 0.003 10 .602 ± 0.003 10 .593 ± 0.002 10 .611 ± 0.002 10 .620 ± 0.002 10 .575 ± 0.002 Table 7: Final return V π on the four-room navigation with stochastic PG at step 50,000. We tested with two different entropy coefficients and seven different β’s. The standard error is computed over 10 random seeds. 0 20000 40000 # of Gradient Steps 0 2 4 6 8 10Return (w/ entropy) 0 20000 40000 # of Gradient Steps 0 2 4 6 8 10Return (w/o entropy) 0 20000 40000 # of Gradient Steps 0.0 0.2 0.4 0.6 0.8 1.0 Curriculum Progress β = 0.0 β = 0.1 β = 0.2 β = 0.3 β = 0.5 β = 0.75 β = 0.9 0 20000 40000 # of Gradient Steps 2 4 6 8Return (w/ entropy) 0 20000 40000 # of Gradient Steps 0 2 4 6Return (w/o entropy) 0 20000 40000 # of Gradient Steps 0.0 0.2 0.4 0.6 0.8 1.0 Curriculum Progress β = 0.0 β = 0.1 β = 0.2 β = 0.3 β = 0.5 β = 0.75 β = 0.9 Figure 4: Learning curves for the numerical experiments on the easy curriculum. 31Understanding the Complexity Gains of Single-Task RL with a Curriculum 0 20000 40000 # of Gradient Steps 0.2 0.4 0.6 0.8 1.0 Return (w/ entropy) 0 20000 40000 # of Gradient Steps 0.0 0.2 0.4 0.6 0.8 1.0 Return (w/o entropy) 0 20000 40000 # of Gradient Steps 0.0 0.2 0.4 0.6 0.8 1.0 Curriculum Progress β = 0.0 β = 0.1 β = 0.2 β = 0.3 β = 0.5 β = 0.75 β = 0.9 0 20000 40000 # of Gradient Steps 1.32 1.34 1.36 1.38 1.40 Return (w/ entropy) 0 20000 40000 # of Gradient Steps −0.04 −0.02 0.00 0.02 0.04 Return (w/o entropy) 0 20000 40000 # of Gradient Steps 0.0 0.1 0.2 0.3 0.4 0.5 Curriculum Progress β = 0.0 β = 0.1 β = 0.2 β = 0.3 β = 0.5 β = 0.75 β = 0.9 Figure 5: Learning curves for the numerical experiments on the hard curriculum. 32Understanding the Complexity Gains of Single-Task RL with a Curriculum G. Additional Learning Curves and Tables G.1. Goal Reaching 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.00 0.25 0.50 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.00 0.25 0.50 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.00 0.25 0.50 Learning Progress (κ) ∆=1/12 Baseline β = 0.9 β = 0.75 β = 0.5 β = 0.2 β = 0.1 Vanilla ROLLIN without geometric sampling 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/12 Vanilla ROLLIN with geometric sampling Figure 6: Vanilla Goal reaching. Accelerating learning on antmaze-umaze with ROLLIN on an oracle curriculum in Figure 3. The confidence interval represents the standard error computed over 8 random seeds. 33Understanding the Complexity Gains of Single-Task RL with a Curriculum 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/12 Baseline β = 0.9 β = 0.75 β = 0.5 β = 0.2 β = 0.1 ROLLIN + relabeling without geometric sampling 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/12 ROLLIN + relabeling with geometric sampling Figure 7: Goal relabeling. Accelerating learning on antmaze-umaze with ROLLIN on an oracle curriculum in Figure 3. The confidence interval represents the standard error computed over 8 random seeds. 34Understanding the Complexity Gains of Single-Task RL with a Curriculum 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.00 0.25 0.50 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/12 Baseline β = 0.9 β = 0.75 β = 0.5 β = 0.2 β = 0.1 ROLLIN + Go-Explore without geometric sampling 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/12 ROLLIN + Go-Explore with geometric sampling Figure 8: Go-Explore (exploration noise = 0.1). Accelerating learning on antmaze-umaze with ROLLIN on an oracle curriculum in Figure 3. The confidence interval represents the standard error computed over 8 random seeds. 35Understanding the Complexity Gains of Single-Task RL with a Curriculum 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.00 0.25 0.50 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 Learning Progress (κ) ∆=1/12 Baseline β = 0.9 β = 0.75 β = 0.5 β = 0.2 β = 0.1 ROLLIN + Go-Explore without geometric sampling 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/12 ROLLIN + Go-Explore with geometric sampling Figure 9: Go-Explore (exploration noise = 0.25). Accelerating learning on antmaze-umaze with ROLLIN on an oracle curriculum in Figure 3. The confidence interval represents the standard error computed over 8 random seeds. 36Understanding the Complexity Gains of Single-Task RL with a Curriculum 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/12 Baseline β = 0.9 β = 0.75 β = 0.5 β = 0.2 β = 0.1 ROLLIN + Go-Explore without geometric sampling 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ∆=1/24 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.5 Learning Progress (κ) ∆=1/18 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Env. Steps ( ×106) 0.0 0.2 0.4 Learning Progress (κ) ∆=1/12 ROLLIN + Go-Explore with geometric sampling Figure 10: Go-Explore (exploration noise = 0.5). Accelerating learning on antmaze-umaze with ROLLIN on an oracle curriculum in Figure 3. The confidence interval represents the standard error computed over 8 random seeds. 37Understanding the Complexity Gains of Single-Task RL with a Curriculum Geo ∆ Baseline β = 0.1 β = 0.2 β = 0.5 β = 0.75 β = 0.9 ✗ 1/24 0 .40 ± 0.02 0 .49 ± 0.04 0 .51 ± 0.05 0.57 ± 0.04 0.55 ± 0.02 0 .49 ± 0.02 ✗ 1/18 0 .36 ± 0.01 0 .39 ± 0.01 0 .46 ± 0.01 0.54 ± 0.03 0.46 ± 0.03 0 .50 ± 0.02 ✗ 1/12 0 .36 ± 0.00 0 .44 ± 0.01 0 .41 ± 0.02 0 .47 ± 0.02 0.56 ± 0.05 0 .56 ± 0.02 ✓ 1/24 0 .82 ± 0.08 0 .92 ± 0.02 0.95 ± 0.02 0.88 ± 0.01 0 .81 ± 0.01 0 .70 ± 0.02 ✓ 1/18 0 .68 ± 0.07 0 .74 ± 0.07 0 .76 ± 0.06 0.78 ± 0.03 0.75 ± 0.02 0 .72 ± 0.02 ✓ 1/12 0 .38 ± 0.03 0 .55 ± 0.04 0 .55 ± 0.04 0 .64 ± 0.06 0.69 ± 0.06 0.67 ± 0.03 Table 8: Vanilla Goal reaching. Learning progress κ at 3 million environment steps with varying β and curriculum step size ∆ of vanilla goal reaching task. Geo indicates the usage of geometric sampling. Baseline corresponds to β = 0 , where no R OLLIN is used. The standard error is computed over 8 random seeds. We highlight the values that are larger than the baseline (β = 0) in purple, and the largest value in bold font. Geo ∆ β = 0 β = 0.1 β = 0.2 β = 0.5 β = 0.75 β = 0.9 ✗ 1/24 0 .89 ± 0.03 0.91 ± 0.03 0.85 ± 0.04 0 .86 ± 0.02 0 .86 ± 0.02 0 .80 ± 0.02 ✗ 1/18 0 .76 ± 0.03 0 .81 ± 0.01 0.85 ± 0.04 0.79 ± 0.01 0 .84 ± 0.03 0 .68 ± 0.04 ✗ 1/12 0 .66 ± 0.04 0.74 ± 0.01 0.73 ± 0.03 0 .66 ± 0.06 0 .67 ± 0.05 0 .69 ± 0.03 ✓ 1/24 0 .76 ± 0.02 0 .78 ± 0.01 0 .78 ± 0.03 0 .82 ± 0.02 0.87 ± 0.02 0.83 ± 0.02 ✓ 1/18 0 .72 ± 0.02 0 .76 ± 0.01 0 .76 ± 0.02 0 .78 ± 0.04 0.85 ± 0.03 0.81 ± 0.01 ✓ 1/12 0 .72 ± 0.03 0.73 ± 0.00 0 .73 ± 0.00 0 .73 ± 0.03 0.69 ± 0.04 0 .66 ± 0.04 Table 9: Goal relabeling. All other settings are the same as Table 8. EN Geo ∆ β = 0 β = 0.1 β = 0.2 β = 0.5 β = 0.75 β = 0.9 0.1 ✗ 1/24 0 .37 ± 0.02 0 .52 ± 0.07 0.54 ± 0.06 0.51 ± 0.06 0 .39 ± 0.02 0 .37 ± 0.01 0.1 ✗ 1/18 0 .38 ± 0.01 0 .37 ± 0.01 0 .36 ± 0.01 0 .38 ± 0.01 0 .38 ± 0.01 0 .35 ± 0.00 0.1 ✗ 1/12 0 .38 ± 0.01 0 .38 ± 0.01 0 .36 ± 0.00 0.39 ± 0.01 0.36 ± 0.00 0 .36 ± 0.00 0.1 ✓ 1/24 0 .82 ± 0.07 0.95 ± 0.02 0.91 ± 0.02 0 .92 ± 0.02 0 .71 ± 0.02 0 .45 ± 0.01 0.1 ✓ 1/18 0 .57 ± 0.09 0.71 ± 0.08 0.65 ± 0.07 0 .63 ± 0.07 0 .62 ± 0.02 0 .43 ± 0.01 0.1 ✓ 1/12 0 .42 ± 0.03 0 .43 ± 0.02 0 .38 ± 0.04 0.49 ± 0.04 0.45 ± 0.02 0 .39 ± 0.01 0.25 ✗ 1/24 0 .38 ± 0.02 0 .49 ± 0.06 0 .48 ± 0.05 0.55 ± 0.07 0.43 ± 0.04 0 .40 ± 0.02 0.25 ✗ 1/18 0 .35 ± 0.00 0.39 ± 0.03 0 .39 ± 0.02 0.36 ± 0.01 0 .36 ± 0.01 0 .35 ± 0.00 0.25 ✗ 1/12 0 .36 ± 0.00 0 .36 ± 0.00 0 .36 ± 0.00 0 .36 ± 0.00 0 .36 ± 0.00 0 .36 ± 0.00 0.25 ✓ 1/24 0 .82 ± 0.10 0 .97 ± 0.02 1.00 ± 0.00 0.94 ± 0.02 0 .77 ± 0.02 0 .49 ± 0.02 0.25 ✓ 1/18 0 .64 ± 0.10 0 .70 ± 0.07 0.79 ± 0.07 0.64 ± 0.06 0 .63 ± 0.03 0 .44 ± 0.01 0.25 ✓ 1/12 0 .39 ± 0.01 0.47 ± 0.03 0.45 ± 0.02 0.47 ± 0.03 0.36 ± 0.04 0 .40 ± 0.02 0.5 ✗ 1/24 0 .49 ± 0.06 0 .60 ± 0.08 0.66 ± 0.08 0.61 ± 0.08 0 .65 ± 0.06 0 .46 ± 0.04 0.5 ✗ 1/18 0 .36 ± 0.01 0.41 ± 0.02 0.39 ± 0.01 0 .38 ± 0.01 0 .37 ± 0.01 0 .35 ± 0.00 0.5 ✗ 1/12 0 .36 ± 0.00 0.38 ± 0.01 0 .38 ± 0.01 0.36 ± 0.00 0.38 ± 0.01 0.36 ± 0.00 0.5 ✓ 1/24 0 .92 ± 0.08 1.00 ± 0.00 1 .00 ± 0.00 1 .00 ± 0.00 0.87 ± 0.03 0 .55 ± 0.03 0.5 ✓ 1/18 0 .66 ± 0.09 0 .71 ± 0.08 0.80 ± 0.08 0.68 ± 0.08 0 .67 ± 0.04 0 .44 ± 0.02 0.5 ✓ 1/12 0 .41 ± 0.02 0 .44 ± 0.04 0 .43 ± 0.03 0.50 ± 0.04 0.47 ± 0.03 0 .39 ± 0.04 Table 10: Go-Explore with different exploration noise. EN represents the multiplier for the Gaussian exploration noise. All other settings are the same as Table 8. 38Understanding the Complexity Gains of Single-Task RL with a Curriculum G.2. Non Goal Reaching Tasks 0.0 0.5 1.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) walker 0.0 0.5 1.0 Env. Steps ( ×106) 0 2 4x velocity walker 0.0 0.5 1.0 Env. Steps ( ×106) 0 2000 4000return walker Baseline β = 0.1 0.0 0.5 1.0 Env. Steps ( ×106) 0.0 0.5 Learning Progress (κ) hopper 0.0 0.5 1.0 Env. Steps ( ×106) 0 2x velocity hopper 0.0 0.5 1.0 Env. Steps ( ×106) 0 2000return hopper 0.0 0.5 1.0 Env. Steps ( ×106) 0.0 0.5 Learning Progress (κ) humanoid 0.0 0.5 1.0 Env. Steps ( ×106) 0.0 0.5 x velocity humanoid 0.0 0.5 1.0 Env. Steps ( ×106) 0 2000 4000return humanoid 0.0 0.5 1.0 Env. Steps ( ×106) 0.0 0.5 1.0 Learning Progress (κ) ant 0.0 0.5 1.0 Env. Steps ( ×106) 0.0 2.5 5.0 x velocity ant 0.0 0.5 1.0 Env. Steps ( ×106) 0 2000 4000return ant Figure 11: Accelerating learning on several non goal-reaching tasks. The confidence interval represents the standard error computed over 8 random seeds, for β = 0.1. 39Understanding the Complexity Gains of Single-Task RL with a Curriculum Env. Step β = 0 β = 0.1 β = 0.2 β = 0.5 β = 0.75 walker 0.5m 0.83 ± 0.03 0 .79 ± 0.04 0 .75 ± 0.04 0 .78 ± 0.05 0 .76 ± 0.05 1m 0.92 ± 0.03 0.94 ± 0.03 0.90 ± 0.01 0 .92 ± 0.04 0 .92 ± 0.03 hopper 0.5m 0.85 ± 0.02 0 .82 ± 0.03 0 .83 ± 0.02 0 .78 ± 0.02 0 .75 ± 0.02 1m 0.88 ± 0.01 0.89 ± 0.00 0 .89 ± 0.03 0.82 ± 0.02 0 .81 ± 0.02 humanoid 0.5m 0.32 ± 0.05 0.36 ± 0.04 0.21 ± 0.07 0 .33 ± 0.04 0 .14 ± 0.06 1m 0.67 ± 0.03 0 .69 ± 0.06 0 .62 ± 0.02 0.76 ± 0.03 0.71 ± 0.06 ant 0.5m 0.72 ± 0.02 0.82 ± 0.06 0.68 ± 0.08 0 .64 ± 0.05 0 .47 ± 0.05 1m 1.00 ± 0.00 1 .00 ± 0.00 0 .83 ± 0.08 0 .86 ± 0.06 0 .71 ± 0.07 Table 11: Learning progress κ at 0.5 and 1.0 million environment steps with varying β of non goal reaching tasks. Baseline corresponds to β = 0 , where no R OLLIN is used. The standard error is computed over 8 random seeds. We highlight the values that are larger than the baseline (β = 0) in purple, and the largest value in bold font. Env. Step β = 0 β = 0.1 β = 0.2 β = 0.5 β = 0.75 walker 0.5m 3.09 ± 0.31 2 .83 ± 0.31 2 .41 ± 0.33 2 .77 ± 0.31 2 .88 ± 0.32 1m 3.69 ± 0.27 3 .62 ± 0.26 3 .09 ± 0.28 3 .48 ± 0.27 3 .14 ± 0.34 hopper 0.5m 2.42 ± 0.18 2 .26 ± 0.22 2.45 ± 0.14 2.34 ± 0.16 2 .34 ± 0.16 1m 2.58 ± 0.16 2.65 ± 0.15 2 .65 ± 0.17 2.39 ± 0.18 2 .52 ± 0.19 humanoid 0.5m 0.26 ± 0.05 0 .32 ± 0.07 0 .27 ± 0.05 0 .34 ± 0.05 0.38 ± 0.07 1m 0.39 ± 0.05 0 .46 ± 0.09 0 .41 ± 0.05 0 .41 ± 0.06 0.49 ± 0.10 ant 0.5m 3.38 ± 0.43 3.85 ± 0.41 3.43 ± 0.53 3 .15 ± 0.45 2 .38 ± 0.46 1m 4.29 ± 0.51 4.66 ± 0.30 3.93 ± 0.45 3 .99 ± 0.48 3 .50 ± 0.49 Table 12: Average x-direction velocity of the last 50k time steps, at 0.5 and 1.0 million environment steps with varying β of non goal reaching tasks. Baseline corresponds to β = 0, where no R OLLIN is used. The standard error is computed over 8 random seeds. We highlight the values that are larger than the baseline (β = 0) in purple, and the largest value in bold font. Env. Step β = 0 β = 0.1 β = 0.2 β = 0.5 β = 0.75 walker 0.5m 3450.1 ± 307.4 3350 .4 ± 184.6 2897 .4 ± 276.5 3255 .9 ± 203.8 3185 .8 ± 341.5 1m 4032.3 ± 224.3 4128.8 ± 159.6 3685.5 ± 135.6 4028 .8 ± 164.2 3895 .4 ± 265.4 hopper 0.5m 3192.5 ± 80.4 3148 .6 ± 160.7 3241.5 ± 130.8 3116.5 ± 141.8 3059 .6 ± 153.8 1m 3386.2 ± 124.7 3421.9 ± 109.8 3262.3 ± 98.1 3170 .7 ± 180.6 3394 .5 ± 126.5 humanoid 0.5m 2910.1 ± 262.9 2939 .7 ± 392.0 2598 .9 ± 309.8 3137.3 ± 305.6 2259.6 ± 245.4 1m 3017.2 ± 169.0 3173 .6 ± 238.3 2935 .8 ± 181.1 2905 .5 ± 125.9 3290.7 ± 275.9 ant 0.5m 2976.2 ± 252.4 3593.1 ± 237.8 3071.8 ± 340.0 2818 .3 ± 265.2 2188 .3 ± 256.2 1m 4248.5 ± 88.6 4473.0 ± 102.2 3683.1 ± 345.0 3708 .7 ± 290.5 3250 .1 ± 316.2 Table 13: Average return of the last 50k time steps, at the 0.5 and 1.0 million environment steps with varying β of non goal reaching tasks. Baseline corresponds to β = 0, where no ROLLIN is used. The standard error is computed over 8 random seeds. We highlight the values that are larger than the baseline ( β = 0) in purple, and the largest value in bold font. 40",
      "meta_data": {
        "arxiv_id": "2212.12809v3",
        "authors": [
          "Qiyang Li",
          "Yuexiang Zhai",
          "Yi Ma",
          "Sergey Levine"
        ],
        "published_date": "2022-12-24T19:46:47Z",
        "pdf_url": "https://arxiv.org/pdf/2212.12809v3.pdf",
        "github_url": "https://github.com/ikostrikov/jaxrl"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of learning in Reinforcement Learning (RL) with sparse or delayed rewards by reformulating single-task problems as multi-task problems with an implicit curriculum. The main contribution is the theoretical framework and algorithm, ROLLIN, which shows that sequentially solving tasks in a curriculum is more computationally efficient than solving a single challenging task from scratch. ROLLIN reduces the exponential iteration and sample complexity bounds of entropy-regularized softmax policy gradient to a polynomial dependency on the state space size (S) by initializing the current task's policy with the previous task's optimal policy and constructing a mixed initial state distribution. This approach achieves computational efficiency without explicit exploration bonuses. Empirical validation is provided on tabular MDPs and simulated robotic tasks.",
        "methodology": "The core methodology revolves around the **ROLLIN algorithm** operating within a contextual MDP framework. It utilizes entropy-regularized softmax policy gradient (PG) with stochastic gradient estimation. ROLLIN enhances learning by: 1) initializing the current task's policy parameters with the optimal parameters of the preceding task, and 2) constructing the initial state distribution as a mixture of the previous task's optimal state visitation distribution and the original initial state distribution. This 'rolling in' strategy aims to start learning from a near-optimal regime, reducing density mismatch. The theoretical analysis relies on Lipschitz continuity of the reward function and a similarity assumption between consecutive contexts in the curriculum. For practical implementation, ROLLIN is adapted to **Soft-Actor-Critic (SAC)**, using two parallel agents (a main agent `πmain` and an exploration agent `πexp`) to collect and learn from experience, with a probability `β` for the 'roll-in' exploration strategy. Curriculum progression is managed by a performance threshold `R` for switching tasks.",
        "experimental_setup": "The paper validates its theory and practical implementation across two settings. For **theoretical validation**, experiments were conducted on a tabular 12x12 grid-world MDP with 144 states and 105 actions (including dummy actions). A curriculum of 17 goal-reaching tasks was used, with two reward function types (easy and hard). Stochastic PG with Adam optimizer was employed, and performance was measured by curriculum progress and final return over 50,000 gradient steps. For **practical implementation validation**, Soft-Actor-Critic (SAC) was used on: 1) a **goal-reaching task** (`antmaze-umaze` environment with an oracle curriculum), comparing ROLLIN with vanilla SAC, goal relabeling, and Go-Explore, under varying curriculum step sizes and replay buffer sampling strategies (uniform vs. geometric); and 2) **four non-goal-reaching tasks** (`walker2d`, `hopper`, `humanoid`, `ant` in OpenAI Gym), where the objective was to gradually increase the x-velocity, with increasing task difficulty. Performance was evaluated at 0.5 and 1.0 million environment steps based on curriculum progress, average x-velocity, and average return.",
        "limitations": "The theoretical analysis acknowledges that reducing the state space size itself requires additional assumptions beyond the scope of this work, implying that while exponential dependency on 'S' is reduced to polynomial, 'S' can still be large. A practical limitation is the requirement for domain-specific knowledge to define a 'good' return value (performance threshold `R`) for switching between curriculum contexts. Additionally, the ROLLIN algorithm introduces an extra hyperparameter `β` which may require careful tuning, although experiments suggest that simple choices (e.g., 0.1 or 0.2) can yield reasonable improvements. The theoretical results also assume access to a 'good' curriculum where adjacent tasks are sufficiently similar.",
        "future_research_directions": "Future work could explore how ROLLIN's insights (reducing density mismatch by constructing new initial distributions) can be applied to other policy gradient variants, such as Natural Policy Gradient (NPG) or PG in feature spaces. Another promising direction is to integrate ROLLIN with context-based meta-RL methods, specifically for learning to generate sub-goals or contexts to accelerate the learning of current sub-goals. The authors also suggest combining their state coverage analysis with representation analysis from prior work (e.g., Klink et al., 2022). Lastly, the method's applicability is not restricted to goal-conditioned settings, opening up possibilities for its use in more challenging and diverse RL domains.",
        "experimental_code": "@functools.partial(jax.jit,static_argnames=('backup_entropy', 'update_target'))def _update_jit(rng: PRNGKey, actor: Model, critic: Model, target_critic: Model,temp: Model, batch: Batch, discount: float, tau: float,target_entropy: float, backup_entropy: bool, update_target: bool) -> Tuple[PRNGKey, Model, Model, Model, Model, InfoDict]:rng, key = jax.random.split(rng)new_critic, critic_info = update_critic(key,actor,critic,target_critic,temp,batch,discount,backup_entropy=backup_entropy)if update_target:new_target_critic = target_update(new_critic, target_critic, tau)else:new_target_critic = target_criticrng, key = jax.random.split(rng)new_actor, actor_info = update_actor(key, actor, new_critic, temp, batch)new_temp, alpha_info = temperature.update(temp, actor_info['entropy'],target_entropy)return rng, new_actor, new_critic, new_target_critic, new_temp, {**critic_info,**actor_info,**alpha_info}class SACLearner(object):def __init__(self,seed: int,observations: jnp.ndarray,actions: jnp.ndarray,actor_lr: float = 3e-4,critic_lr: float = 3e-4,temp_lr: float = 3e-4,hidden_dims: Sequence[int] = (256, 256),discount: float = 0.99,tau: float = 0.005,target_update_period: int = 1,target_entropy: Optional[float] = None,backup_entropy: bool = True,init_temperature: float = 1.0,init_mean: Optional[np.ndarray] = None,policy_final_fc_init_scale: float = 1.0):\"\"\"An implementation of the version of Soft-Actor-Critic described in https://arxiv.org/abs/1812.05905\"\"\"action_dim = actions.shape[-1]if target_entropy is None:self.target_entropy = -action_dim / 2else:self.target_entropy = target_entropyself.backup_entropy = backup_entropyself.tau = tauself.target_update_period = target_update_periodself.discount = discountrng = jax.random.PRNGKey(seed)rng, actor_key, critic_key, temp_key = jax.random.split(rng, 4)actor_def = policies.NormalTanhPolicy(hidden_dims,action_dim,init_mean=init_mean,final_fc_init_scale=policy_final_fc_init_scale)actor = Model.create(actor_def,inputs=[actor_key, observations],tx=optax.adam(learning_rate=actor_lr))critic_def = critic_net.DoubleCritic(hidden_dims)critic = Model.create(critic_def,inputs=[critic_key, observations, actions],tx=optax.adam(learning_rate=critic_lr))target_critic = Model.create(critic_def, inputs=[critic_key, observations, actions])temp = Model.create(temperature.Temperature(init_temperature),inputs=[temp_key],tx=optax.adam(learning_rate=temp_lr))self.actor = actorself.critic = criticself.target_critic = target_criticself.temp = tempself.rng = rngself.step = 1def sample_actions(self,observations: np.ndarray,temperature: float = 1.0) -> jnp.ndarray:rng, actions = policies.sample_actions(self.rng, self.actor.apply_fn,self.actor.params, observations,temperature)self.rng = rngactions = np.asarray(actions)return np.clip(actions, -1, 1)def update(self, batch: Batch) -> InfoDict:self.step += 1new_rng, new_actor, new_critic, new_target_critic, new_temp, info = _update_jit(self.rng, self.actor, self.critic, self.target_critic, self.temp,batch, self.discount, self.tau, self.target_entropy,self.backup_entropy, self.step % self.target_update_period == 0)self.rng = new_rngself.actor = new_actorself.critic = new_criticself.target_critic = new_target_criticself.temp = new_tempreturn infodefine update(key: PRNGKey, actor: Model, critic: Model, temp: Model,batch: Batch) -> Tuple[Model, InfoDict]:def actor_loss_fn(actor_params: Params) -> Tuple[jnp.ndarray, InfoDict]:dist = actor.apply_fn({'params': actor_params}, batch.observations)actions = dist.sample(seed=key)log_probs = dist.log_prob(actions)q1, q2 = critic(batch.observations, actions)q = jnp.minimum(q1, q2)actor_loss = (log_probs * temp() - q).mean()return actor_loss, {'actor_loss': actor_loss,'entropy': -log_probs.mean()}new_actor, info = actor.apply_gradient(actor_loss_fn)return new_actor, infodefine target_update(critic: Model, target_critic: Model, tau: float) -> Model:new_target_params = jax.tree_multimap(lambda p, tp: p * tau + tp * (1 - tau), critic.params,target_critic.params)return target_critic.replace(params=new_target_params)def update(key: PRNGKey, actor: Model, critic: Model, target_critic: Model,temp: Model, batch: Batch, discount: float,backup_entropy: bool) -> Tuple[Model, InfoDict]:dist = actor(batch.next_observations)next_actions = dist.sample(seed=key)next_log_probs = dist.log_prob(next_actions)next_q1, next_q2 = target_critic(batch.next_observations, next_actions)next_q = jnp.minimum(next_q1, next_q2)target_q = batch.rewards + discount * batch.masks * next_qif backup_entropy:target_q -= discount * batch.masks * temp() * next_log_probsdef critic_loss_fn(critic_params: Params) -> Tuple[jnp.ndarray, InfoDict]:q1, q2 = critic.apply_fn({'params': critic_params}, batch.observations,batch.actions)critic_loss = ((q1 - target_q)**2 + (q2 - target_q)**2).mean()return critic_loss, {'critic_loss': critic_loss,'q1': q1.mean(),'q2': q2.mean()}new_critic, info = critic.apply_gradient(critic_loss_fn)return new_critic, infodefine update(temp: Model, entropy: float,target_entropy: float) -> Tuple[Model, InfoDict]:def temperature_loss_fn(temp_params):temperature = temp.apply_fn({'params': temp_params})temp_loss = temperature * (entropy - target_entropy).mean()return temp_loss, {'temperature': temperature, 'temp_loss': temp_loss}new_temp, info = temp.apply_gradient(temperature_loss_fn)return new_temp, infoimport osimport numpy as npimport tqdmfrom absl import app, flagsfrom ml_collections import config_flagsfrom tensorboardX import SummaryWriterfrom jaxrl.agents import AWACLearner, SACLearnerfrom jaxrl.datasets import ReplayBufferfrom jaxrl.datasets.dataset_utils import make_env_and_datasetfrom jaxrl.evaluation import evaluatefrom jaxrl.utils import make_envFLAGS = flags.FLAGSflags.DEFINE_string('env_name', 'HalfCheetah-v2', 'Environment name.')flags.DEFINE_enum('dataset_name', 'awac', ['d4rl', 'awac'], 'Dataset name.')flags.DEFINE_string('save_dir', './tmp/', 'Tensorboard logging dir.')flags.DEFINE_integer('seed', 42, 'Random seed.')flags.DEFINE_integer('eval_episodes', 10,'Number of episodes used for evaluation.')flags.DEFINE_integer('log_interval', 1000, 'Logging interval.')flags.DEFINE_integer('eval_interval', 10000, 'Eval interval.')flags.DEFINE_integer('batch_size', 1024, 'Mini batch size.')flags.DEFINE_integer('max_steps', int(1e6), 'Number of training steps.')flags.DEFINE_integer('init_dataset_size', None,'Number of samples from the dataset to initialize the replay buffer.')flags.DEFINE_integer('num_pretraining_steps', int(5e4),'Number of pretraining steps.')flags.DEFINE_boolean('tqdm', True, 'Use tqdm progress bar.')flags.DEFINE_boolean('save_video', False, 'Save videos during evaluation.')config_flags.DEFINE_config_file('config','configs/awac_default.py','File path to the training hyperparameter configuration.',lock_config=False)def main(_):summary_writer = SummaryWriter(os.path.join(FLAGS.save_dir, 'tb', str(FLAGS.seed)))if FLAGS.save_video:video_train_folder = os.path.join(FLAGS.save_dir, 'video', 'train')video_eval_folder = os.path.join(FLAGS.save_dir, 'video', 'eval')else:video_train_folder = Nonevideo_eval_folder = Noneenv, dataset = make_env_and_dataset(FLAGS.env_name, FLAGS.seed,FLAGS.dataset_name, video_train_folder)eval_env = make_env(FLAGS.env_name, FLAGS.seed + 42, video_eval_folder)np.random.seed(FLAGS.seed)kwargs = dict(FLAGS.config)algo = kwargs.pop('algo')replay_buffer_size = kwargs.pop('replay_buffer_size')if algo == 'sac':agent = SACLearner(FLAGS.seed,env.observation_space.sample()[np.newaxis],env.action_space.sample()[np.newaxis], **kwargs)elif algo == 'awac':agent = AWACLearner(FLAGS.seed,env.observation_space.sample()[np.newaxis],env.action_space.sample()[np.newaxis], **kwargs)else:raise NotImplementedError()replay_buffer = ReplayBuffer(env.observation_space, env.action_space,replay_buffer_size or FLAGS.max_steps)replay_buffer.initialize_with_dataset(dataset, FLAGS.init_dataset_size)eval_returns = []observation, done = env.reset(), Falsefor i in tqdm.tqdm(range(1 - FLAGS.num_pretraining_steps,FLAGS.max_steps + 1),smoothing=0.1,disable=not FLAGS.tqdm):if i >= 1:action = agent.sample_actions(observation)next_observation, reward, done, info = env.step(action)if not done or 'TimeLimit.truncated' in info:mask = 1.0else:mask = 0.0replay_buffer.insert(observation, action, reward, mask,float(done), next_observation)observation = next_observationif done:observation, done = env.reset(), Falsefor k, v in info['episode'].items():summary_writer.add_scalar(f'training/{k}', v,info['total']['timesteps'])else:info = {}info['total'] = {'timesteps': i}batch = replay_buffer.sample(FLAGS.batch_size)update_info = agent.update(batch)if i % FLAGS.log_interval == 0:for k, v in update_info.items():summary_writer.add_scalar(f'training/{k}', v, i)summary_writer.flush()if i % FLAGS.eval_interval == 0:eval_stats = evaluate(agent, eval_env, FLAGS.eval_episodes)for k, v in eval_stats.items():summary_writer.add_scalar(f'evaluation/average_{k}s', v,info['total']['timesteps'])summary_writer.flush()eval_returns.append((info['total']['timesteps'], eval_stats['return']))np.savetxt(os.path.join(FLAGS.save_dir, f'{FLAGS.seed}.txt'),eval_returns,fmt=['%d', '%.1f'])if __name__ == '__main__':app.run(main)",
        "experimental_info": "The core method is an adaptation of Soft-Actor-Critic (SAC). The default hyperparameters for SAC are as follows:\n`config.algo = 'sac'`\n`config.actor_lr = 3e-4`\n`config.critic_lr = 3e-4`\n`config.temp_lr = 3e-4`\n`config.hidden_dims = (256, 256)`\n`config.discount = 0.99`\n`config.tau = 0.005`\n`config.target_update_period = 1`\n`config.init_temperature = 1.0`\n`config.target_entropy = None`\n`config.backup_entropy = True`\n`config.replay_buffer_size = None`\n\nFor aspects related to sequential learning and initializing from prior knowledge, the `examples/train_finetuning.py` script is relevant. Its key experimental settings include:\n- `env_name`: 'HalfCheetah-v2'\n- `dataset_name`: 'awac' (used to create an initial dataset)\n- `seed`: 42\n- `batch_size`: 1024\n- `max_steps`: 1e6 (total training steps)\n- `init_dataset_size`: None (defaults to using the full initial dataset for replay buffer initialization)\n- `num_pretraining_steps`: 5e4 (number of steps to train solely on the initial dataset before interacting with the environment)\n- `log_interval`: 1000\n- `eval_interval`: 10000\n\nThe `replay_buffer` is initialized with data using `replay_buffer.initialize_with_dataset(dataset, FLAGS.init_dataset_size)`. The training loop then proceeds, allowing for pretraining (`i < 1`) where the agent updates only from the pre-filled replay buffer, followed by online data collection and updates (`i >= 1`).\n\n**Note on ROLLIN specific features:**\nThe provided repository content implements a Soft-Actor-Critic (SAC) learner as a base. The `examples/train_finetuning.py` script provides a mechanism for initializing the replay buffer with an existing dataset and performing `num_pretraining_steps` before online interaction. This serves as a basic form of policy initialization and sequential learning, which is conceptually related to 'initializing the current task's policy parameters with the optimal parameters of the preceding task' and using a 'previous task's optimal state visitation distribution' for initial states, although it does not explicitly implement the 'optimal parameters' transfer or 'mixture' components described in the ROLLIN method. Explicit implementations for 'two parallel agents (πmain and πexp)', a specific 'β for 'roll-in' exploration strategy' (distinct from AWAC's advantage weighting beta), or 'curriculum progression managed by a performance threshold R for switching tasks' are not found within the provided code."
      }
    },
    {
      "title": "Growing Action Spaces",
      "abstract": "In complex tasks, such as those with large combinatorial action spaces,\nrandom exploration may be too inefficient to achieve meaningful learning\nprogress. In this work, we use a curriculum of progressively growing action\nspaces to accelerate learning. We assume the environment is out of our control,\nbut that the agent may set an internal curriculum by initially restricting its\naction space. Our approach uses off-policy reinforcement learning to estimate\noptimal value functions for multiple action spaces simultaneously and\nefficiently transfers data, value estimates, and state representations from\nrestricted action spaces to the full task. We show the efficacy of our approach\nin proof-of-concept control tasks and on challenging large-scale StarCraft\nmicromanagement tasks with large, multi-agent action spaces.",
      "full_text": "Growing Action Spaces Gregory Farquhar∗ University of Oxford Laura Gustafson Facebook AI Research Zeming Lin Facebook AI Research Shimon Whiteson University of Oxford Nicolas Usunier Facebook AI Research Gabriel Synnaeve Facebook AI Research Abstract In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefﬁcient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to accelerate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efﬁciently transfers data, value estimates, and state representations from restricted action spaces to the full task. We show the efﬁcacy of our approach in proof-of-concept control tasks and on challenging large- scale StarCraft micromanagement tasks with large, multi-agent action spaces. 1 Introduction The value of curricula has been well established in machine learning, reinforcement learning, and in biological systems. When a desired behaviour is sufﬁciently complex, or the environment too unforgiving, it can be intractable to learn the behaviour from scratch through random exploration. Instead, by “starting small” [Elman, 1993], an agent can build skills, representations, and a dataset of meaningful experiences that allow it to accelerate its learning. Such curricula can drastically improve sample efﬁciency [Bengio et al., 2009]. Typically, curriculum learning uses a progression of tasks or environments. Simple tasks that provide meaningful feedback to random agents are used ﬁrst, and some schedule is used to introduce more challenging tasks later during training [Graves et al., 2017]. However, in many contexts neither the agent nor experimenter has such unimpeded control over the environment. In this work, we instead make use of curricula that are internal to the agent, simplifying the exploration problem without changing the environment. In particular, we grow the size of the action space of reinforcement learning agents over the course of training. At the beginning of training, our agents use a severely restricted action space. This helps guide exploration, and provides low variance updates during learning. The action space is then grown progressively. Eventually, using the most unrestricted action space, the agents are able to ﬁnd superior policies. Each action space is a strict superset of the more restricted ones. This paradigm requires some domain knowledge to identify a suitable hierarchy of action spaces. However, such a hierarchy is often easy to ﬁnd. For example, continuous spaces can be discretised with increasing resolution. Similarly, curricula for coping with the large combinatorial action spaces induced by many agents can be obtained from the prior that nearby agents are more likely to need to coordinate. We propose an approach that uses off-policy reinforcement learning to improve sample efﬁciency in this type of curriculum learning. Since data from exploration using a restricted action space is still ∗Work done while at Facebook AI Research. Correspondence to gregory.farquhar@cs.ox.ac.uk Preprint. Under review. arXiv:1906.12266v1  [cs.LG]  28 Jun 2019valid in the Markov Decision Processes (MDPs) corresponding to the less restricted action spaces, we can learn value functions in the less restricted action space with ‘off-action-space’ data collected by exploring in the restricted action space. In our approach, we learn value functions corresponding to each level of restriction simultaneously. We can use the relationships of these value functions to each other to accelerate learning further, by using value estimates themselves as initialisations or as bootstrap targets for the less restricted action spaces, as well as sharing learned state representations. In this way, we transfer data, value estimates, and representations for value functions with restricted action spaces to those with less restricted actions spaces. Empirically, we ﬁrst demonstrate the efﬁcacy of our approach in two simple control tasks, in which the resolution of discretised actions is progressively increased. We then tackle a more challenging set of problems with combinatorial action spaces, in the context of StarCraft micromanagement with large numbers of agents (50-100). Given the heuristic prior that nearby agents in multiagent setting are likely to need to coordinate, we use hierarchical clustering to impose a restricted action space on the agents. Agents in a cluster are restricted to take the same action, but we progressively increase the number of groups that can act independently of one another over the course of training. Our method substantially improves sample efﬁciency on a number of tasks, outperforming learning any particular action space from scratch, a number of ablations, and an actor-critic baseline that learns a single value function for the behaviour policy, as in the work of Czarnecki et al. [2018]. Code is available at https://github.com/TorchCraft/TorchCraftAI/tree/gas-micro. 2 Related work Curriculum learning has a long history, appearing at least as early as the work of Selfridge et al. [1985] in reinforcement learning, and for the training of neural networks since Elman [1993]. In supervised learning, one typically has control of the order in which data is presented to the learning algorithm. For learning with deep neural networks, Bengio et al. [2009] explored the use of curricula in computer vision and natural language processing. Many approaches use handcrafted schedules for task curricula, but others [Zaremba and Sutskever, 2014, Pentina et al., 2015, Graves et al., 2017] study diagnostics that can be used to automate the choice of task mixtures throughout training. In a self-supervised control setting, Murali et al. [2018] use sensitivity analysis to automatically deﬁne a curriculum over action dimensions and prioritise their search space. In some reinforcement learning settings, it may also be possible to control the environment so as to induce a curriculum. With a resettable simulator, it is possible to use a sequence of progressively more challenging initial states [Asada et al., 1996, Florensa et al., 2017]. With a procedurally generated task, it is often possible to automatically tune the difﬁculty of the environments [Tamar et al., 2016]. Similar curricula also appear often in hierarchical reinforcement learning, where skills can be learned in comparatively easy settings and then composed in more complex ways later [Singh, 1992]. Taylor et al. [2007] use more general inter-task mappings to transfer Q-values between tasks that do not share state and action spaces. In adversarial settings, one may also induce a curriculum through self-play [Tesauro, 1995, Sukhbaatar et al., 2017, Silver et al., 2017]. In this case, the learning agents themselves deﬁne the changing part of the environment. A less invasive manipulation of the environment involves altering the reward function. Such reward shaping allows learning policies in an easier MDP, which can then be transferred to the more difﬁcult sparse-reward task [Colombetti and Dorigo, 1992, Ng et al., 1999]. It is also possible to learn reward shaping on simple tasks and transfer it to harder tasks in a curriculum [Konidaris and Barto, 2006]. In contrast, learning with increasingly complex function approximators does not require any control of the environment. In reinforcement learning, this has often taken the form of adaptively growing the resolution of the state space considered by a piecewise constant discretised approximation [Moore, 1994, Munos and Moore, 2002, Whiteson et al., 2007]. Stanley and Miikkulainen [2004] study continual complexiﬁcation in the context of coevolution, growing the complexity of neural network architectures through the course of training. These works progressively increase the capabilities of the agent, but not with respect to its available actions. In the context of planning on-line with a model, there are a number of approaches that use progressive widening to consider increasing large action spaces over the course of search [Chaslot et al., 2008], 2including in planning for continuous action spaces [Couëtoux et al., 2011]. However, these methods cannot directly be applied to grow the action space in the model-free setting. A recent related work tackling our domain is that of Czarnecki et al. [2018], who train mixtures of two policies with an actor-critic approach, learning a single value function for the current mixture of policies. The mixture contains a policy that may be harder to learn but has a higher performance ceiling, such as a policy with a larger action space as we consider in this work. The mixing coefﬁcient is initialised to only support the simpler policy, and adapted via population based training [Jaderberg et al., 2017]. In contrast, we simultaneously learn a different value function for each policy, and exploit the properties of the optimal value functions to induce additional structure on our models. We further use these properties to construct a scheme for off-action-space learning which means our approach may be used in an off-policy setting. Empirically, in our settings, we ﬁnd our approach to perform better and more consistently than an actor-critic algorithm modeled after Czarnecki et al. [2018], although we do not take on the signiﬁcant additional computational requirements of population based training in any of our experiments. 3 Background We formalise our problem as a MDP, speciﬁed by a tuple < S,A,P,r,γ >. The set of possible states and actions are given by Sand A, P is the transition function that speciﬁes the environment dynamics, and γ is a discount factor used to specify the discounted return R = ∑T t=0 γtrt for an episode of length T. We wish our agent to maximise this return in expectation by learning a policy π that maps states to actions. The state-action value function (Q-function) is given by Qπ = Eπ[R|s,a]. The optimal Q-function Q∗satisﬁes the Bellman optimality equation: Q∗(s,a) = TQ∗(s,a) = E[r(s,a) + γmax a′ Q∗(s′,a′)]. (1) Q-learning [Watkins and Dayan, 1992] uses a sample-based approximation of the Bellman optimality operator Tto iteratively improve an estimate ofQ∗. Q-learning is an off-policy method, meaning that samples from any policy may be used to improve the value function estimate. We use this property to engage Q-learning for off-action-space learning, as described in the next section. We also introduce some notation for restricted action spaces. In particular, for an MDP with unrestricted action space Awe deﬁne a set of N action spaces Aℓ,ℓ ∈{0,...,N −1}. Each action space is a subset of the next: A0 ⊂A1 ⊂... ⊂AN−1 ⊆A. A policy restricted to actions Aℓ is denoted πℓ(a|s). The optimal policy in this restricted policy class is π∗ ℓ(a|s), and its corresponding value and action-value functions are V∗ ℓ (s) = maxaQ∗ ℓ(s,a) and Q∗ ℓ(s,a). Additionally, there is often structure in the problem that creates a natural hierarchy of actions, such that we can identify for every action a∈Aℓ,ℓ> 0 a parent action parentℓ(a) in the space of Aℓ−1. For example, a discretised continuous action can identify its nearest neighbour in Aℓ−1 as a parent. Since action spaces are subsets of larger action spaces, for all a∈Aℓ−1,parentℓ(a) = a, i.e., one child of each action is itself. 4 Curriculum learning with growing action spaces We build our approach to growing action spaces (GAS) on off-policy value-based reinforcement learning. Q-learning and its deep-learning adaptations have shown strong performance [Hessel et al., 2018], and admit a simple framework for off-policy learning. In our setting, off-policy learning and reasoning about optimal value functions becomes particularly useful as it enables strategies for efﬁcient transfer of data, value estimates, and representations from restricted to unrestricted action spaces. 4.1 Off-action-space learning A value function for an action space Aℓ may be updated with transitions using actions drawn from its own action space, or any more restricted action spaces, as long as we use an off-policy learning algorithm. The restricted transitions simply form a subset of the data required to learn the value 3functions of the less restricted action spaces. As a result, we may use samples drawn from a behaviour policy based on a value function for low ℓto directly train a higher ℓvalue function. To exploit this, we simultaneously learn an estimated optimal value function ˆQ∗ ℓ(s,a) for each action space Aℓ. At the beginning of each episode, we sample ℓaccording to some distribution. The experiences generated in that episode are used to update all of the ˆQ∗ ≥ℓ(s,a). This off-action-space learning is a type of off-policy learning that enables efﬁcient exploration by restricting it to the low-ℓregime. We sample at the beginning of the episode rather than at each timestep because, if the agent uses a high-ℓ action, it may enter a state that is inaccessible for a lower- ℓpolicy, and we do not wish to force a low-ℓvalue function to generalise to states that are only accessible at higher ℓ. Since data from a restricted action space only supports a subset of the state-action space relevant for the value functions of less restricted action spaces, we hope that a suitable function approximator still allows some generalisation to the unexplored parts of the less restricted state-action space. 4.2 Value estimates Note that: V∗ i (s) ≤V∗ j (s)∀sif i<j. (2) This is because each action space is a strict subset of the larger ones, so the agent can always in the worst case fall back to a policy using a more restricted action space. This monotonicity intuitively recommends an iterative decomposition of the value estimates, in which ˆQ∗ ℓ+1(s,a) is estimated as a sum of ˆQ∗ ℓ(s,a) and some positive ∆ℓ(s,a). This is not immediately possible due to the mismatch in the support of each function. However, we can leverage a hierarchical structure in the action spaces when present, as described in Section 3. In this case we can use: ˆQ∗ ℓ+1(s,a) = ˆQ∗ ℓ(s,parentℓ(a)) + ∆ℓ(s,a). (3) This is a task-speciﬁc upsampling of the lower-ℓvalue function to intialise the next value function. Both ˆQ∗ ℓ(s,a) and ∆ℓ(s,a) are learned components. We could further regularise or restrict the functional form of ∆ℓ to ensure its positivity. However, we did not ﬁnd this to be necessary in our experiments, and simply initialised ∆ℓ to be small. The property (2) also implies a modiﬁed Bellman optimality equation: Q∗ ℓ(s,a) = E[r(s,a) + γmax i≤ℓ max a′ Q∗ i(s′,a′)] (4) The maxi<ℓ are redundant in their role as conditions on the optimal value function Q∗ ℓ. However, the Bellman optimality equation also gives us the form of a Q-learning update, where the term in the expectation on the RHS is used as an operator that iteratively improves an estimate of Q∗. When these estimates are inaccurate, the modiﬁed form of the Bellman equation may lead to different updates, allowing the solutions to higher ℓto be bootstrapped from those at lower ℓ. We expect that policies with low ℓare easier to learn, and that therefore the corresponding ˆQ∗ ℓ is more accurate earlier in training. When values are positive, these values could be picked up by the modiﬁed bootstrap, and rapidly learned by the higher-ℓvalue functions. Empirically however, we ﬁnd that using this form for the target in our loss function performs no better than just maximising over ˆQ∗ ℓ(s′,a′). We discuss the choice of target and these results in more detail in Section 6.2. 4.3 Representation By sharing parameters between the function approximators of each Qℓ, we can learn a joint state representation, which can then be iteratively decoded into estimates of the value for each action space. This shared embedding can be iteratively reﬁned by, e.g., additional network layers for eachQℓ to maintain ﬂexibility along with transfer of useful representations. This simple approach has had great success in improving the efﬁciency of many multi-task solutions using deep learning [Ruder, 2017]. 4.4 Curriculum scheduling It may be difﬁcult to choose a good distribution from which to sample ℓfor the behaviour policy, and an appropriate schedule with which to increase its mean during training. Czarnecki et al. [2018] use 4population based training [Jaderberg et al., 2017] to choose a mixing parameter on the ﬂy. However, this comes at signiﬁcant computational cost, and optimises greedily for immediate performance gains rather than for optimal learning efﬁciency. Many other strategies exist for tuning a curriculum automatically, such as those explored by Graves et al. [2017]. We use a simple linear schedule on a mixing parameter α∈[0,N], picking ℓ= ⌊α⌋with probability ⌈α⌉−αand ℓ= ⌈α⌉with probability α−⌊α⌋. This worked well empirically with little effort to tune. A more sophisticated or automated schedule could be beneﬁcial, at the cost of additional overhead and algorithmic complexity. 5 Growing action spaces for multi-agent control In cooperative multi-agent control, the full unrestricted action space allows each of N agents to take actions from a set Aagent, resulting in an exponentially large action space of size |Aagent|N. Random exploration in this action space is highly unlikely to produce sensical behaviours, so growing the action space as we propose is particularly valuable in this setting. One approach would be to limit the actions available to each agent, as done in our discretised continuous control experiments (Section 6.1) and those of Czarnecki et al. [2018]. However, the joint action space would still be exponential in N. We propose instead to use hierarchical clustering, and to assign the same action to nearby agents. At the ﬁrst level of the hierarchy, we treat the whole team as a single group, and all agents are constrained to take the same action. At the next level of the hierarchy, we split the agents into k groups using an unsupervised clustering algorithm, allowing each group to act independently. At each further level, every group is split once again into ksmaller groups. In practice, we simply use k-means clustering based on the agent’s spatial position, but this can be easily extended to more complex hierarchies using other clustering approaches. To estimate the value function, we compute a state-value score ˆV(s), and a group-action delta ∆ℓ(s,ag,g) for each group gat each level ℓ. Then, we compute an estimated group-action value for each group, at each level, using a per-group form of (3): ˆQ∗ ℓ+1(s,ag) = ˆQ∗ ℓ(s,parentk(ag)) + ∆ℓ(s,ag,g). We use ˆQ∗ −1(s,·) = ˆV(s) to initialise the iterative computation, similarly to the dueling architecture of Wang et al. [2015]. The estimated value of the parent action is the estimated value of the entire parent group all taking the same action as the child group. At each level ℓwe now have a set of group-action values. In effect, a multi-agent value-learning problem still remains at each levelℓ, but with a greatly reduced number of agents at low ℓ. We could simply use independent Q-learning [Tan, 1993], but instead choose to estimate the joint-action value at each level as the mean of the group-action values for the groups at that ℓ, as in the work of Sunehag et al. [2017]. A less restrictive representation, such as that proposed by Rashid et al. [2018], could help, but we leave this direction to future work. A potential problem is that the clustering changes for every state, which may interfere with gener- alisation across state-action pairs as group-actions will not have consistent semantics. We address this in two ways. First, we include the clustering as part of the state, and the cluster centroids are re-initialised from the previous timestep for t> 0 to keep the cluster semantics approximately consis- tent. Second, we use a functional representation that produces group-action values that are broadly agnostic to the identiﬁer of the group. In particular, we compute a spatially resolved embedding, and pool over the locations occupied by each group. See Figure 2 and Section 6.2 for more details. 6 Experiments We investigate two classes of problems that have a natural hierarchy in the action space. First, simple control problems where a coarse action discretisation can help accelerate exploration, and ﬁne action discretisation allows for a more optimal policy. Second, the cooperative multi-agent setting, discussed in Section 5, using large-scale StarCraft micromanagement scenarios. 6.1 Discretised continuous control As a proof-of-concept, we look at two simple examples: versions of the classic Acrobot and Mountain Car environments with discretised action spaces. Both tasks have a sparse reward of +1 when the goal is reached, and we make the exploration problem more challenging by terminating episodes 5(a) Acrobot  (b) Mountain Car Figure 1: Discretised continuous control with growing action spaces. We report the mean and standard error (over 10 random seeds) of the returns during training, with a moving average over the past 20 episodes. A2 (slow ϵ) is an ablation of A2 that decays ϵat a quarter the rate. with a penalty of -1 if the goal is not reached within 500 timesteps. The normalised remaining time is concatenated to the state so it remains Markovian despite the time limit. There is a further actuation cost of 0.05∥a∥2. At A0, the actions apply a force of +1 and −1. At each subsequent Aℓ>0, each action is split into two children, one that is the same as the parent action, and the other applying half the force. Thus, there are 2ℓ actions in Aℓ. The results of our experiments are shown in Figure 1. Training with the lower resolutions A0 and A1 from scratch converges to ﬁnding the goal, but incurs signiﬁcant actuation costs. Training with A2 from scratch almost never ﬁnds the goal with ϵ-greedy exploration. We also tried decaying the ϵat a quarter of the rate (A2 slow ϵ) without success. In these cases, the policy converges to the one that minimises actuation costs, never ﬁnding the goal. Training with a growing action space explores to ﬁnd the goal early, and then uses this experience to transition smoothly into a solution that ﬁnds the goal but takes a slower route that minimises actuation costs while achieving the objective. 6.2 Combinatorial action spaces: StarCraft battles 6.2.1 Large-scale StarCraft Micromanagement The real-time strategy game StarCraft and its sequel StarCraft II have emerged as popular platforms for benchmarking reinforcement learning algorithms [Synnaeve et al., 2016, Vinyals et al., 2017]. Full game-play has been tackled by e.g. [Lee et al., 2018, Vinyals et al., 2019], while other works focus on sub-problems such as micromanagement, the low-level control of units engaged in a battle between two armies (e.g. [Usunier et al., 2016]). Efforts to approach the former problem have required some subset of human demonstrations, hierarchical methods, and massive compute scale, and so we focus on the latter as a more tractable benchmark for a comparative evaluation of our methods. Most previous work on RL benchmarking with StarCraft micromanagement is restricted to maximally 20-30 units [Samvelyan et al., 2019, Usunier et al., 2016]. In our experiments we focus on much larger-scale micromanagement scenarios with 50-100 units on each side of the battle. To further increase the difﬁculty of these micromanagement scenarios, in our setting the starting locations of the armies are randomised, and the opponent is controlled by scripted logic that holds its position until any agent-controlled unit is in range, and then focus-ﬁres on the closest enemy. This increases the exploration challenge, as our agents need to learn to ﬁnd the enemy ﬁrst, while they hold a strong defensive position. The action space for each unit permits an attack-move or move action in eight cardinal directions, as well as a stop action that causes the unit to passively hold its position. In our experiments, we use k = 2 for k-means clustering and split down to at most four or eight groups. The maximum number of groups in an experiment with Aℓ is 2ℓ. Although our approach is designed for off-policy learning, we follow the common practice of using n-step Q-learning to accelerate the propagation of values [Hessel et al., 2018]. Our base algorithm uses the objective of n-step Q-learning from the work of Mnih et al. [2016], and collects data from multiple workers into a short queue similarly to Espeholt et al. [2018]. Full details can be found in the Appendix. 6Figure 2: Architecture for GAS with hierarchical clustering. For clarity, only two levels of hierarchy are shown. The dark shaded regions identify the locations that are pooled over before state-value or group-action scores are computed. 6.2.2 Model architecture We propose an architecture to efﬁciently represent the value functions of the action-space hierarchy. The overall structure is shown in Figure 2. We start with the state of the scenario (1). Ally units are blue and split into two groups. From the state, features are extracted from the units and map (see Appendix for full details). These features are concatenated with a one-hot representation of the unit’s group (for allied agents), and are embedded with a small MLP. A 2-D grid of embeddings is constructed by adding up the unit embeddings for all units in each cell of the grid (2). The embeddings are passed through a residual CNN to produce a ﬁnal embedding (3), which is copied several times and decoded as follows. First, a state-value branch computes a scalar value by taking a global mean pooling (4) and passing the result through a 2-layer MLP (6). Then, for each ℓ, a masked mean-pooling is used to produce an embedding for each group at that Aℓ by masking out the positions in the spatial embedding where there are no units of that group (5a, 5b, 5c). A single evaluation MLP for each ℓis used to decode this embedding into a group action-score (7a, 7b, 7c). This architecture allows a shared state representation to be efﬁciently decoded into value-function contributions for groups of any size, at any level of restriction in the action space. We consider two approaches for combining these outputs. In our default approach, described in Section 5, each group’s action-value is given by the sum of the state-value and group-action-scores for the group and its parents (8a, 8b). In ‘SEP-Q’, each group’s action-value is simply given by the state-value added to the group-action score, i.e., ˆQ∗ ℓ(s,ag) = ˆV(s)+∆ ℓ(s,ag,g). This is an ablation in which the action-value estimates for restricted action spaces do not initialise the action-value estimates of their child actions. 6.2.3 Results and discussion Figure 3 presents the results of our method, as well as a number of baselines and ablations, on a variety of micromanagement tasks. Our method is labeled Growing Action Spaces GAS(ℓ), such that GAS(2) will grow from A0 to A2. Our primary baselines are policies trained with action spaces A0 or A2 from scratch. GAS(2) consistently outperforms both of these variants. Policies trained from scratch on A2 struggle with exploration, in particular in the harder scenarios where the opponent has a numbers advantage. Policies trained from scratch on A0 learn quickly, but plateau comparatively low, due to the limited ability of a single group to position effectively. GAS(2) beneﬁts from the efﬁcient exploration enabled by an intialisation at A0, and uses the data gathered under this policy to efﬁciently transfer to A2; enabling a higher asymptotic performance. We also compare against a Mix&Match (MM) baseline following the actor-critic approach of Czarnecki et al. [2018], but adapted for our new multi-agent setting and supporting a third level in 7Figure 3: StarCraft micromanagement with growing action spaces. We report the mean and standard error (over 5 random seeds) of the evaluation winrate during training, with a moving average over the past 500 episodes. the mixture of policies (A0, A1, A2). We tuned hyperparameters for all algorithms on the easiest, fastest-training scenario (80 marines vs. 80 marines). On this scenario, MM learns faster but plateaus at the same level as GAS(2). MM underperforms on all other scenarios to varying degrees. Learning separate value functions for each Aℓ, as in our approach, appears to accelerate the transfer learning in the majority of settings. Another possible explanation is that MM may be more sensitive to hyperparameters. We do not use population based training to tune hyperparameters on the ﬂy, which could otherwise help MM adapt to each scenario. However, GAS would presumably also beneﬁt from population based training, at the cost of further computation and sample efﬁciency. The policies learned by GAS exhibit good tactics. Control of separate groups is used to position our army so as to maximise the number of attacking units by forming a wall or a concave that surrounds the enemy, and by coordinating a simultaneous assault. Figure 4 in the Appendix shows some example learned policies. In scenarios where MM fails to learn well, it typically falls into a local minimum of attacking head-on. In each scenario, we test an ablation GAS (2): ON-AC that does not use our off-action-space update, instead training each level of the Q-function only with data sampled at that level. This ablation performs slightly, or considerably, worse in each scenario. As expected, it is beneﬁcial to accelerate learning for ﬁner action spaces using data drawn from the off-action-space policy. We present a number of further ablations on two scenarios. The most striking failure is of the ‘SEP-Q’ variant which does not compose the value function as a sum of scores in the hierarchy. Evidently, it is critical to compute the value function iteratively, so that values are well-initialised as we move to less restricted action spaces. Optimising each value head separately also means that the gradients they propagate to the encoder can compete for the learned representation, with potentially damaging effects. The choice of target is less important: performing a max over coarser action spaces to construct the target as described in Section 4.2 does not improve learning speed as intended. One potential reason is that maximising over more potential targets increases the maximisation bias already present in Q-learning [Hasselt, 2010]. Additionally, we use an n-step objective which combines a partial 8on-policy return with the bootstrap target, which could reduce the relative impact of the choice of target. Surprisingly, the max-targets objective does lead to a slight increase in the ﬁnal performance, by which time the behaviour policy only uses the most unrestricted action space. Finally, we experiment with a higher ℓ. Unfortunately, asymptotic performance is degraded slightly once we use A3 or higher. One potential reason is that it decreases the average group size, pushing against the limits of the spatial resolution that may be captured by our CNN architecture. Higher ℓ also considerably increase the amount of time that there are fewer units than groups, leaving certain groups empty. This renders our masked pooling operation degenerate, and may hurt the optimisation process. We do not see a fundamental limitation that should restrict the further growth of the action space, although we note that most hierarchical approaches in the literature avoid too many levels of depth. For example, Czarnecki et al. [2018] only mix between two sizes of action spaces rather than the three we progress through in the majority of our GAS experiments. 7 Conclusion In this work, we presented an algorithm for growing action spaces with off-policy reinforcement learning to efﬁciently shape exploration. We learn value functions for all levels of a hierarchy of restricted action spaces simultaneously, and transfer data, value estimates, and representations from more restricted to less restricted action spaces. We also present a strategy for using this approach in cooperative multi-agent control. In discretised continuous control tasks and challenging multi-agent StarCraft micromanagement scenarios, we demonstrate empirically the effectiveness of our approach and the value of off-action-space learning. An interesting avenue for future work is to automatically identify how to restrict action spaces for efﬁcient exploration, potentially through meta-optimisation. We also look to explore more complex and deeper hierarchies of action spaces. Acknowledgments We thank Danielle Rothermel, Daniel Gant, Jonas Gehring, Nicolas Carion, Daniel Haziza, Dexter Ju, and Vegard Mella for useful discussions and their work on the StarCraft codebase. This work was supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement number 637713). References Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, and Koh Hosoda. Purposive behavior acquisition for a real robot by vision-based reinforcement learning. Machine learning, 23(2-3):279–303, 1996. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM, 2009. Guillaume M JB Chaslot, Mark HM Winands, H JAAP V AN DEN HERIK, Jos WHM Uiterwijk, and Bruno Bouzy. Progressive strategies for monte-carlo tree search. New Mathematics and Natural Computation, 4(03): 343–357, 2008. Marco Colombetti and Marco Dorigo. Robot shaping: developing situated agents through learning. International Computer Science Institute, 1992. Adrien Couëtoux, Jean-Baptiste Hoock, Nataliya Sokolovska, Olivier Teytaud, and Nicolas Bonnard. Continuous upper conﬁdence trees. In International Conference on Learning and Intelligent Optimization, pages 433–445. Springer, 2011. Wojciech Marian Czarnecki, Siddhant M Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Simon Osindero, Nicolas Heess, and Razvan Pascanu. Mix&match-agent curricula for reinforcement learning. arXiv preprint arXiv:1806.01780, 2018. Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99, 1993. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, V olodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018. 9Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generation for reinforcement learning. arXiv preprint arXiv:1707.05300, 2017. Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pages 1311–1320. JMLR. org, 2017. Hado V Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, pages 2613–2621, 2010. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846, 2017. George Konidaris and Andrew Barto. Autonomous shaping: Knowledge transfer in reinforcement learning. In Proceedings of the 23rd international conference on Machine learning, pages 489–496. ACM, 2006. Dennis Lee, Haoran Tang, Jeffrey O Zhang, Huazhe Xu, Trevor Darrell, and Pieter Abbeel. Modular architecture for starcraft ii with deep reinforcement learning. In Fourteenth Artiﬁcial Intelligence and Interactive Digital Entertainment Conference, 2018. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015. V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928–1937, 2016. Andrew W Moore. The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. In Advances in neural information processing systems, pages 711–718, 1994. Rémi Munos and Andrew Moore. Variable resolution discretization in optimal control. Machine learning, 49 (2-3):291–323, 2002. Adithyavairavan Murali, Lerrel Pinto, Dhiraj Gandhi, and Abhinav Gupta. Cassl: Curriculum accelerated self-supervised learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 6453–6460. IEEE, 2018. Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pages 278–287, 1999. Anastasia Pentina, Viktoriia Sharmanska, and Christoph H Lampert. Curriculum learning of multiple tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5492–5500, 2015. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. arXiv preprint arXiv:1803.11485, 2018. Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. 2019. Oliver G Selfridge, Richard S Sutton, and Andrew G Barto. Training and tracking in robotics. In IJCAI, pages 670–672, 1985. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks.Machine Learning, 8(3-4):323–339, 1992. 10Kenneth O Stanley and Risto Miikkulainen. Competitive coevolution through evolutionary complexiﬁcation. Journal of artiﬁcial intelligence research, 21:63–100, 2004. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017. Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017. Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on real-time strategy games. arXiv preprint arXiv:1611.00625, 2016. Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. InAdvances in Neural Information Processing Systems, pages 2154–2162, 2016. Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330–337, 1993. Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for temporal difference learning. Journal of Machine Learning Research, 8(Sep):2125–2167, 2007. Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58–68, 1995. Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic exploration for deep deter- ministic policies: An application to starcraft micromanagement tasks. arXiv preprint arXiv:1609.02993, 2016. Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017. Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M. Czar- necki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Yuhuai Wu, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Sil- ver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/ alphastar-mastering-real-time-strategy-game-starcraft-ii/ , 2019. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and Nando De Freitas. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015. Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992. Shimon Whiteson, Matthew E Taylor, Peter Stone, et al. Adaptive tile coding for value function approximation. Computer Science Department, University of Texas at Austin, 2007. Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. 118 Appendix 8.1 Discretised continuous control For our experiments in discretised continous control, we use a standard DQN trainer [Mnih et al., 2015] with the following parameters. Parameter Value batch size 128 replay buffer size 10000 target update interval 200 ϵinitial 1.0 ϵﬁnal 0.1 ϵdecay 25000 env steps ℓlead-in 25000 env steps ℓgrowth 25000 env steps env steps per model udpate 4 Adam learning rate 5e-4 Adam ϵ 1e-4 For GAS experiments, we keep the mixing coefﬁcient α= 0for 25000 environment steps, and then increase it linearly by 1 every 25000 steps until reaching the maximum value. We use γ = 0.998 for our Acrobot experiments, but reduce it to γ = 0.99 for Mountain Car to prevent diverging Q-values. Our model consists of fully-connected ReLU layers, with 128 hidden units for the ﬁrst and 64 hidden units for all subsequent layers. Two layers are applied as an encoder. Then, for each ℓone layer is applied on the current embedding to produce a new embedding, and an evaluation layer on that embedding produces the Q-values for that level. 8.2 StarCraft micromanagement scenarios 8.2.1 Scenarios and learned strategies We explore ﬁve Starcraft micromanagement scenarios: 50 hydralisks vs 50 hydralisks, 80 marines vs 80 marines, 80 marines vs 85 marines, 60 marines vs 65 marines, 95 zerglings vs 50 marines. In these scenarios, our model controls the ﬁrst set of units, and the opponent controls the second set. The opponent is a scripted opponent that holds its location until an opposing unit is within range to attack. Then, the opponent will engage in an \"attack-closest\" behavior, as described in Usunier et al. [2016], where each unit individually targets the closest unit to it. Having the opponent remain stationary until engaged makes this a more difﬁcult problem – the agent must ﬁnd its opponent, and attack into a defensive position, which requires good positions prior to engagement. As mentioned in section 6.2, all of our scenarios require control of a much larger number of units than previous work. The 50 hydralisks and 80v80 marines scenarios are both imbalanced as a result of attacking into a defensive position. The optimal strategy for 80 marines vs 85 marines and 60 vs 65 marines requires slightly more sophisticated unit positioning, and the 95 zerglings vs 50 marines scenario requires the most precise positioning. The agent can use the enemy’s initial stationary positioning to its advantage by slightly surrounding the opponent in a concave, ensuring that the outermost units are in its attack range, but far enough away to be out of range of the center-most enemy units. Ideally, the timing of the groups in all scenarios should be coordinated such that all units get in range of the opponent at roughly the same point in time. Figure 4 shows how our model is able to exhibit this level of unit control. 8.2.2 Features We use a standard features for the units and map, given by TorchcraftAI2 For each of the units, the following features are extracted: • Current x, y positions. • Current x, y velocities. 2https://github.com/TorchCraft/TorchCraftAI 12Figure 4: Final learned policies of StarCraft micromanagement unit control with growing action spaces. Scenarios shown from left to right at time 0, 3, 5, 10, 15 seconds. Top to bottom the scenarios are: 60 marines vs 65 marines, 50 hydralisks vs 50 hydralisks, 95 zerglings vs 50 marines. In these examples, the opponent is always on the right, and the agent controlled by model trained with GAS is on the left. • Current hitpoints • Armor and damage values • Armor and damage types • Range versus both ground and air units • Current weapon cooldown • A few boolean ﬂags on some miscellaneous unit attributes Approximate normalization for each feature keep its value approximately between 0-1. For the map, the following features are extracted for each tile in the map: • a one-hot encoding of tile’s the ground height (4 channels) • boolean representing or not the given tile is walkable • boolean representing or not the given tile is buildable • and boolean representing or not the given tile is covered by fog of war. The features form a HxWx7 tensor, where our map has height Hand width W. 8.2.3 Environment details We use a frame-skip of 25, approximately 1 second of real time, allowing for reasonably ﬁne-grained control but without making the exploration and credit assignment problems too challenging. We calculate at every timestep the difference in total health points (HP) and number of units for the enemy from the last step, normalised by the total starting HP and unit count. As a reward function, we use the normalised damage dealt, plus 4 times the normalised units killed, plus an additional reward of 8 for winning the scenario by killing all enemy units. This reward function is designed such that the agent gets some reward for doing damage and killing units, but the reward from doing damage will never be greater than from winning the scenario. Ties and timeouts are considered losses. 138.3 Experimental details 8.3.1 Model As described in Section 6.2.2 a custom model architecture is used for Starcraft micromanagement. Each unit’s feature vector is embedded to size 128 in step 2 of Figure 2. The grid where the unit features and map features are scattered onto is the size of the Starcraft map of the scenario in walktiles downsampled by a factor of 8. After being embedded, the unit features for ally and enemy units are concatenated with the downsampled map features and sent into a ResNet encoder with four residual blocks (stride 7 padding 3). The output is an embedding of size 64. The decoder uses a mean pooling over the embedding cells as described in Section 6.2.2. Each evaluator is a 2-layer MLP with 64 hidden units and 17 outputs, one for each action. All layers are separated with ReLU nonlinearities. 8.3.2 Training hyperparameters We use 64 parallel actors to collect data in a short queue from which batches are removed when they are consumed by the learner. We use batches of 32 6-step segments for each update. For the Q-learning experiments, we used the Adam optimizer with a learning rate of2.5×10−4 and ϵ= 1×10−4. For the MM baseline experiments, we use a learning rate of 1 ×10−4, entropy loss coefﬁcient of 8 ×10−3 and value loss coefﬁcient 0.5. The learning rates and entropy loss coefﬁcient were tuned by random search, training with A0 from scratch on the 80 marines vs 80 marines scenario with 10 conﬁgurations sampled from log_uniform(−5,−3) for the learning rate and log_uniform(−3,−1) for the entropy loss coefﬁcient. For Q-learning, we use an ϵ-greedy exploration strategy , decaying ϵlinearly from 1.0 to 0.1 over the ﬁrst 10000 model updates. We also use a target network that copies the behaviour model’s parameters every 200 model updates. We also use a linear schedule to grow the action-space. There is a lead in of 5000 model updates, during which the action-space is held constant at A0, to prevent the action space from growing when ϵor the policy entropy is too high. The action-space is then grown linearly at a rate of 10000 model updates per level of restriction, so that after 10000 updates, we act entirely at A1 and after 20000, entirely at A2. 14",
      "meta_data": {
        "arxiv_id": "1906.12266v1",
        "authors": [
          "Gregory Farquhar",
          "Laura Gustafson",
          "Zeming Lin",
          "Shimon Whiteson",
          "Nicolas Usunier",
          "Gabriel Synnaeve"
        ],
        "published_date": "2019-06-28T15:35:11Z",
        "pdf_url": "https://arxiv.org/pdf/1906.12266v1.pdf",
        "github_url": "https://github.com/TorchCraft/TorchCraftAI"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Growing Action Spaces (GAS), an off-policy reinforcement learning algorithm that uses a curriculum of progressively growing action spaces to accelerate learning in complex tasks with large combinatorial action spaces. It proposes simultaneously estimating optimal value functions for multiple action spaces and efficiently transferring data, value estimates, and state representations from restricted action spaces to the full task. The approach is demonstrated to be effective in proof-of-concept control tasks and challenging large-scale StarCraft micromanagement scenarios, significantly improving sample efficiency and asymptotic performance compared to learning from scratch or existing baselines.",
        "methodology": "The core methodology is built on off-policy value-based reinforcement learning (Q-learning). It introduces 'off-action-space learning' where estimated optimal value functions (Q*l) are learned simultaneously for a hierarchy of action spaces (A0 subset of A1 ... subset of AN-1). Experiences generated by a behavior policy from a restricted action space (Al) are used to update all Q* values for less restricted action spaces (A>=l). Value estimates are transferred by leveraging the monotonicity property (V*i(s) <= V*j(s) for i<j) and initializing Q*l+1(s,a) as Q*l(s,parentl(a)) + Delta_l(s,a) when a hierarchical structure exists. State representations are transferred by sharing parameters between function approximators of each Ql, allowing for a shared embedding. A simple linear schedule is used to grow the action space over training. For multi-agent control, hierarchical k-means clustering (k=2) assigns the same action to nearby agents, progressively increasing the number of independent groups. The joint-action value is estimated as the mean of group-action values, and a specific architecture handles dynamic clustering and produces spatially resolved embeddings.",
        "experimental_setup": "Experiments were conducted on two classes of problems: discretised continuous control and combinatorial action spaces. Discretised continuous control involved versions of Acrobot and Mountain Car with sparse rewards (+1 for goal, -1 for timeout, 0.05 actuation cost), using action spaces (Al) where each action in Al-1 splits into two children in Al, doubling the action count. Combinatorial action spaces involved large-scale StarCraft micromanagement with 50-100 units per side in scenarios like 50 hydralisks vs 50 hydralisks and 80 marines vs 80/85 marines. The opponent uses scripted logic (holds position, then attack-closest). Performance was evaluated using mean returns (continuous control) and mean evaluation winrate (StarCraft), with moving averages over episodes. Baselines included training from scratch on different action spaces (A0, A2), an ablation with slower epsilon decay, and a Mix&Match (MM) actor-critic baseline. Ablations included 'GAS (2): ON-AC' (without off-action-space update) and 'SEP-Q' (without iterative value function composition). The StarCraft model used a custom architecture with MLP unit embeddings, a residual CNN encoder for spatial features, and separate MLP decoders for state-value and group-action scores. Training used n-step Q-learning, 64 parallel actors, Adam optimizer, epsilon-greedy exploration, and a linear schedule for action space growth.",
        "limitations": "The approach requires domain knowledge to identify a suitable hierarchy of action spaces. While a simple linear schedule for curriculum progression worked well, a more sophisticated or automated schedule could be beneficial but adds overhead. The Mix&Match baseline's underperformance in some scenarios might be due to hyperparameter sensitivity, and while population-based training could help, it incurs significant computational cost. Asymptotic performance degraded slightly with very high action space levels (A3 or higher), possibly due to decreased average group size pushing against the spatial resolution limits of the CNN architecture or leading to degenerate masked pooling when units are fewer than groups. The 'SEP-Q' ablation's failure highlights the criticality of iteratively computing the value function for proper initialization and preventing competing gradients. Maximizing over coarser action spaces in the Bellman target did not improve learning speed, possibly due to increased maximization bias or the n-step objective reducing its impact. A less restrictive representation for multi-agent value learning (e.g., QMIX) could further improve results but was not explored.",
        "future_research_directions": "Future work includes automatically identifying how to restrict action spaces for efficient exploration, potentially through meta-optimization. Exploring more complex and deeper hierarchies of action spaces is another avenue. Additionally, investigating less restrictive multi-agent representations, such as those proposed by QMIX, for the multi-agent value learning problem at each hierarchical level could be beneficial.",
        "experimental_code": "The provided repository content does not contain the direct implementation code for the core reinforcement learning methodology described (off-policy Q-learning, off-action-space learning, value/state transfer, action space growth, or multi-agent clustering). The `submit-collect-samples.py` script is a job submission utility that calls an external binary (`bos-train`), and the `cherryvis` directory contains code for a replay visualization web server, neither of which directly implement the machine learning method.",
        "experimental_info": "The `submit-collect-samples.py` script configures and submits SLURM jobs to run an executable named `/workspace/build/scripts/bo-switch/bos-train`. The experimental settings passed to this binary and the SLURM environment are as follows: Command-line arguments for `bos-train`: `-mode online`, `-v -1`, `-vmodule bos-train-supervised-online=1`, `-bos_model_type idle`, `-playoutput /workspace/playoutput`, `-num_game_threads 12`, `-bandit ucb1rolling`, `-strategy training`. SLURM job settings include: `partition`: 'scavenge' (default, configurable), `time`: '24:00:00', `nodes`: '1', `ntasks-per-node`: 1, `cpus-per-task`: 20, `mem-per-cpu`: '6G', `num_jobs`: 1000 (configurable), `num_parallel_jobs`: 1000 (configurable)."
      }
    },
    {
      "title": "Curriculum Reinforcement Learning via Constrained Optimal Transport",
      "abstract": "Curriculum reinforcement learning (CRL) allows solving complex tasks by\ngenerating a tailored sequence of learning tasks, starting from easy ones and\nsubsequently increasing their difficulty. Although the potential of curricula\nin RL has been clearly shown in various works, it is less clear how to generate\nthem for a given learning environment, resulting in various methods aiming to\nautomate this task. In this work, we focus on framing curricula as\ninterpolations between task distributions, which has previously been shown to\nbe a viable approach to CRL. Identifying key issues of existing methods, we\nframe the generation of a curriculum as a constrained optimal transport problem\nbetween task distributions. Benchmarks show that this way of curriculum\ngeneration can improve upon existing CRL methods, yielding high performance in\nvarious tasks with different characteristics.",
      "full_text": "This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 1 On the Benefit of Optimal Transport for Curriculum Reinforcement Learning Pascal Klink, Carlo D’Eramo, Jan Peters, Joni Pajarinen Abstract—Curriculum reinforcement learning (CRL) allows solving complex tasks by generating a tailored sequence of learning tasks, starting from easy ones and subsequently increasing their difficulty. Although the potential of curricula in RL has been clearly shown in various works, it is less clear how to generate them for a given learning environment, resulting in various methods aiming to automate this task. In this work, we focus on framing curricula as interpolations between task distributions, which has previously been shown to be a viable approach to CRL. Identifying key issues of existing methods, we frame the generation of a curriculum as a constrained optimal transport problem between task distributions. Benchmarks show that this way of curriculum generation can improve upon existing CRL methods, yielding high performance in various tasks with different characteristics. Index Terms—Reinforcement Learning, Curriculum Learning, Optimal Transport ✦ 1 I NTRODUCTION R EINFORCEMENT LEARNING (RL) [1] has celebrated great successes as a framework for the autonomous acquisition of desired behavior. With ever-increasing com- putational power, this framework and the algorithms devel- oped under it have resulted in learning agents capable of solving non-trivial long-horizon planning [2, 3] and control tasks [4]. However, these successes have highlighted the need for certain forms of regularization, such as leagues in the context of board games [3], gradual diversification of simulated training environments for robotic manipulation [4] and -locomotion [5], or a tailored training pipeline in the context of humanoid control for soccer [6]. These regu- larizations can help overcome the shortcomings of modern RL agents, such as poor exploratory behavior – an active research topic [7, 8, 9]. One can view the regularizations mentioned above under the umbrella term of curriculum reinforcement learning [10], which aims to avoid the shortcomings of modern (deep) RL agents by learning on a tailored sequence of tasks. Such task sequences can materialize in various ways, and they are motivated by different perspectives in the literature, such as intrinsic motivation or regret minimization, to name some of them [11, 12, 13, 14, 15, 16]. A perspective of particular interest for this article is to interpret a curriculum as a sequence of task distributions that interpolate between an auxiliary task distribution – with the sole purpose of facilitating learning – and a dis- tribution of target tasks [17]. We refer to these approaches • P . Klink and J. Peters are with the Technical University of Darm- stadt, Germany, FG Intelligent Autonomous Systems. Correspondence to: pascal@robot-learning.de. • J. Peters is also with the German Research Center for AI (DFKI), Research Department: Systems AI for Robot Learning, hessian.AI (Germany), and the Centre for Cognitive Science at Technical University of Darmstadt. • C. D’Eramo is with the Center for Artificial Intelligence and Data Science at University of W¨ urzburg (Germany), the Technical University of Darmstadt (Germany), and hessian.AI (Germany). • J. Pajarinen is with the Department of Electrical Engineering and Automation, Aalto University, Finland. J. Pajarinen was supported by Academy of Finland (345521). Task Parameter c1 sprl Task Parameter c1 Task Parameter c2 Target Tasks Initial Tasks currot low highAgent Performance Fig. 1: Our approach ( CURROT ) addresses problems of ex- isting curriculum RL methods, such as SPRL , which create curricula between a distribution of initial tasks (blue) and a distribution of target tasks (green). In this example, the cur- riculum can change the task via two parameters c1 and c2, leading to more or less challenging learning environments for an agent. Looking at the different stages of the curricula (colored points), we see that existing methods can lead to distributions that encode hard- and easy tasks, but ignore tasks of intermediate difficulty. Our method avoids such a splitting behavior, resulting in interpolations that grad- ually increase the task difficulty throughout the curriculum. Please see Sections 4 and 5 for a detailed description. as interpolation-based curricula. While algorithmic realiza- tions of such curricula have been successfully evaluated in the literature [18, 19, 20], some evaluations indicated a relatively poor learning performance of these methods [21]. Furthermore, applications of interpolation-based curricula have been limited to scenarios with somewhat restricted dis- tributions, such as Gaussian- or uniform ones. The observed performance gaps and lack of flexibility w.r.t. distribution parameterization call for a better understanding of these methods’ inner workings to improve their performance and extend their applicability. arXiv:2309.14091v2  [cs.LG]  4 May 2024This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 2 This article investigates the shortcomings of methods that realize curricula as a scheduled interpolation between task distributions based on the KL divergence and an expected performance constraint. We show how both these concepts can fail to produce meaningful curricula in simple examples. The demonstrated failure cases a) illustrate the importance of explicitly reasoning about the similarity of tasks when building a curriculum and b) show how parametric assump- tions on the generated task distributions can masquerade failures of the underlying framework used to generate cur- ricula. To resolve the observed issues, we explicitly spec- ify the similarity of learning tasks via a distance function and use the framework of optimal transport to generate interpolating distributions that, independent of their param- eterization, result in gradual task changes. Based on this explicit notion of task similarity, we propose our approach to curriculum RL ( CURROT ), which replaces the expected performance constraint with a more strict condition to ob- tain the behavior visualized in Figure 1. Furthermore, we contrast our approach with an alternative method, GRADI - ENT , recently proposed by Huang et al. [22]. We outline how both approaches use optimal transport to generate curricula but differ in their use of the agent performance to constrain the curriculum while avoiding the demonstrated pitfalls of expected performance constraints. In experiments, we a) validate the correct behavior of both CURROT and GRADIENT free from approximations and para- metric assumptions in a small discrete MDP and b) compare approximate implementations on a variety of tasks featuring discrete- and continuous task spaces, as well as Euclidean- and non-Euclidean measures of distance between learning tasks. In these experiments, both approaches show convinc- ing performance with CURROT consistently matching and surpassing the performance of all other algorithms. 2 R ELATED WORK This work generates training curricula for reinforcement learning (RL) agents. Unlike supervised learning, where there is an ongoing discussion about the mechanics and effects of curricula in different learning situations [23, 24], the mechanics seem to be more agreed upon in RL. Curriculum Reinforcement Learning:In RL, curricula im- prove the learning performance of an agent by adapting the training environments to its proficiency. This adaptation of task complexity can reduce the sample complexity of RL, e.g., by bypassing poor exploratory behavior of non- proficient agents [25]. Using curricula can avoid the need for extensively engineered reward functions, which come with risks, such as failing to encode the intended behavior [26]. Applications of curricula to RL are widespread, and different terms have been established. Adaptive Domain Randomization [4] uses curricula to gradually diversify the training parameters of a simulator to facilitate sim-to-real transfer. Similarly, unsupervised environment discovery [16, 27, 28] aims to efficiently train an agent robust to variations in the environment and can be seen as a more general view of domain randomization. Automatic curriculum learning methods [12, 14, 17, 29, 30, 31, 32, 33] mainly focus on improving an agent’s learning speed or performance on a set of desired tasks. Curricula are often generated as distri- butions that maximize a specific surrogate objective, such as learning progress [14, 34], intermediate task difficulty [30], regret [28], or disagreement between Q-functions [31]. Curriculum generation can also be interpreted as a two- player game [29]. The work by Jiang et al. [16] hints at a link between surrogate objectives and two-player games. Similar to the variety of objectives that the above algorithms optimize to build a curriculum, their implementations use drastically different approaches to approximate the training distribution for the agent, which is often defined over a continuous space of training tasks. For example, Florensa et al. [30] use a combination of GANs and a replay buffer to represent the task distribution. Portelas et al. [14] use a Gaussian mixture model to approximate the distribution of tasks that promise high learning progress. Jiang et al. [16] use a fixed-size replay buffer to realize an approximate dis- tribution of high-regret tasks, simultaneously encouraging frequent replay of buffered tasks to keep a more accurate estimate of regret. Interpolation-based curriculum RL algorithms formulate the generation of a curriculum as an explicit interpolation be- tween an auxiliary task distribution and a distribution of target tasks [17, 18, 20]. This interpolation is subject to a constraint on the expected agent performance that paces its progress toward the target tasks. As highlighted by Klink et al. [17], such interpolations can be formally linked to successful curricula in supervised learning [35], the concept of annealing in statistics [36], and homotopic continuation methods in optimization [37]. As for the algorithms based on surrogate objectives, realizations of these interpolation- based curricula inevitably need to rely on approxima- tions such as the restriction to Gaussian distributions in [17, 18, 19] or approximate update rules enabled by uniform target task distributions [20]. This article reveals shortcomings of the aforementioned interpolation-based curriculum RL methods, highlighting how approximations can masquerade issues in the concep- tual algorithm formulations. One ingredient to overcome these shortcomings is an explicit notion of task similarity that we formulate as a distance function between tasks. We can then lift this distance function into the space of probability measures using optimal transport. Optimal Transport:Dating back to work by Monge in the 18th century, optimal transport has been understood as a fundamental concept touching upon many fields in both theory and application [38, 39]. In probability theory, opti- mal transport translates to the so-called Wasserstein metric [40] that compares two distributions under a given metric, allowing, e.g., for the analysis of probabilistic inference algorithms as approximate gradient flows [41] and provid- ing well-defined ways of comparing feature distributions or even graphs in computer vision and machine learning [42, 43, 44]. Gromov-Wasserstein distances [45, 46] even allow comparing distributions across metric spaces, which has been of use, e.g., in computational biology [47] or imitation learning [48]. In Reinforcement learning, optimal transport has not found widespread application, albeit some interesting works exist. Zhang et al. [49] provide a natural extension of the work by Liu et al. [41] and interpret policy optimization as Wasserstein gradient flows. Metelli et al.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 3 [50] use Wasserstein barycenters to propagate uncertainty about value function estimates in a Q-learning approach. In more applied scenarios, optimal transport has been used to regularize RL in sequence generation- [51] or combi- natorial optimization problems [52]. In goal-conditioned RL, Wasserstein distances have been previously applied to improve goal generation in the hindsight experience replay framework [53] and to realize well-performing data-driven reward functions by combining them with so-called time- step metrics [54]. Recently, Cho et al. [55] combined the data-driven reward function proposed by Durugkar et al. [54] with a curriculum that, similarly to the work by [53], improves the selection of training goals from a buffer of achieved ones. When it comes to building RL curricula over arbitrary MDPs using Optimal Transport, we are only aware of our work [56] at ICML 2022 and the work by [22] at NeurIPS 2022, which we present from a unified perspective and compare in this journal article. In addition to the aforementioned methods in goal-conditioned RL, this article emphasizes curriculum reinforcement learning as another promising application domain for optimal trans- port. An important issue of applied optimal transport is its computational complexity. In Appendix A, we discuss the computational aspects of optimal transport in more detail. 3 P RELIMINARIES This section introduces the necessary background on (con- textual) RL, curriculum RL, and optimal transport. 3.1 Contextual Reinforcement Learning Contextual reinforcement learning [57] can be seen as a con- ceptual extension to the (single task) reinforcement learning (RL) problem max π J(π) = max π Ep(τ|π) \" ∞X t=0 γtr(st, at) # (1) τ = {(st, at)|t = 1, . . .} p(τ|π) = p0(s0) ∞Y t=1 p(st|st−1, at−1)π(at−1|st−1), which aims to maximize the above expected discounted re- ward objective by finding an optimal policy π:S×A 7→R≥0 for a given MDP M=⟨S, A, p, r, p0⟩ with initial state distri- bution p0 and transition dynamics p. Contextual RL extends this objective to a space of MDPs M(c)=⟨S, A, pc, rc, p0,c⟩ equipped with a distribution µ:C7→R over contextual vari- ables c ∈ C max π J(π, µ) = max π Eµ(c) [J(π, c)] . (2) The policy π : S×C×A 7→R is conditioned on the contex- tual parameter c. The distribution µ(c) encodes the tasks M(c) to be solved by the agent. Objective J(π, c) in Eq. (2) corresponds to objective J(π) in Eq. (1) with the initial state distribution p0, the transition dynamics p as well as the reward function r of M replaced by their counterparts in M(c). This contextual model of optimal decision-making is well-suited for learning in multiple related tasks, as is the case in multi-task- [58], goal-conditioned- [59], or cur- riculum RL [10]. At this point, we want to emphasize that the context c could be readily embedded in the state space S, resulting in a regular MDP in which the context – as part of the state – remains constant throughout an episode. The context distribution µ(c) would then be subsumed into the initial state distribution without losing expressiveness. We nonetheless prefer the contextual RL framework, coined by Hallak et al. [57], as it emphasizes the distribution µ(c), which is at the heart of curriculum RL methods, as we will see now. 3.2 Curriculum Reinforcement Learning On an abstract level, curriculum RL methods can be un- derstood as generating a sequence of task distributions (pi:C7→R)i under which to train an RL agent by maximizing J(π, pi) w.r.t. π. When chosen appropriately, solving this sequence of optimization problems can yield a policy that performs better on the target distribution µ(c) than a policy found by maximizing J(π, µ) directly. The benefit of such mediating distributions is particularly obvious in settings where initially random agent behavior is unlikely to observe any meaningful learning signals, such as in sparse-reward learning tasks. CRL methods differ in the specification of pi. Often, the dis- tribution is defined to prioritize tasks that maximize certain surrogate quantities, such as absolute learning progress [14], regret [28], or tasks of intermediate success probability [30]. This article focuses on CRL methods that model pi as the solution to an optimization problem that aims to minimize a distance or divergence between pi and µ. One of these approaches [17, 18, 19] defines pi as the distribution with minimum KL divergence to µ that fulfills a constraint on the expected agent performance min p DKL (p(c)∥µ(c)) (3) s.t. J(π, p) ≥ δ D KL (p(c)∥q(c)) ≤ ϵ, where δ is the desired level of performance to be achieved by the agent π under p(c) and ϵ limits the maximum KL di- vergence to the previous context distribution q(c)=pi−1(c). The optimizer of (3) balances between tasks likely under the (target) distribution µ(c) and tasks in which the agent currently obtains large rewards. The KL divergence con- straint w.r.t. the previous context distribution q(c) prevents large changes in p(c) during subsequent iterations, avoiding the exploitation of faulty estimates of the agent perfor- mance J(π, p) from a limited amount of samples. Objec- tive (3) performs an interpolation between the distributions pη(c)∝µ(c) exp(ηJ(π, c)) and q(c), given by pα,η(c) ∝ (µ(c) exp(J(π, c))η)α q(c)1−α. (4) The two parameters α and η that control the interpolation are the Lagrangian multipliers of the two constraints in objective (3). We will later investigate the behavior of this interpolating distribution. 3.3 Optimal Transport The problem of optimally transporting density between two distributions has been initially investigated by Monge [60]. As of today, generalizations established by Kantorovich [40] have led to so-called Wasserstein distancesas metricsThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 4 between probability distributions defined on a metric space M=(d, C) with metric d : C × C 7→R≥0 Wp(p1, p2)= \u0012 inf ϕ∈Φ(p1,p2) Eϕ [d(c1, c2)p] \u00131/p , p ≥ 1 Φ(p1, p2)= {ϕ : C×C7→R≥0|pi=Pi#ϕ, i∈{1, 2}}, where Pi# are the push-forwards of the mapsP1(c1, c2)=c1 and P2(c1, c2)=c2. We refer to [38, Chapter 2] for an ex- cellent and intuitive introduction to these concepts. The distance between p1 and p2 is obtained via the solution to an optimization problem that finds a so-called plan, or cou- pling, ϕ. This coupling encodes how to equalize p1 and p2, considering the cost of moving density between parts of the space C. The metric d encodes this cost. In the following, we will always assume to work with 2-Wasserstein distances, i.e., p=2, due to their suitedness for interpolating measures [see 38, Chapter 6 and Remark 2.24]. Similar to how (weighted) means can be defined as solu- tions to optimization problems on a metric space M=(d, C), Wasserstein distances allow us to define what is referred to as Wasserstein barycenters [61] B2(W, P) = arg min p KX k=1 wkW2(p, pk), (5) which represent the (weighted) mean of the distributions P={pk|k∈[1, K]} with weights W={wk|k∈[1, K]}. 4 C URRICULUM REINFORCEMENT LEARNING AS CONSTRAINED OPTIMAL TRANSPORT At this point, we can motivate our approach to curriculum RL by looking at the limitations of Objective 3 caused by a) measuring similarity between context distributions via the KL divergence and b) the expected performance constraint used to control the progression towards µ(c). 4.1 Limitations of the KL Divergence Given the complexity of computing DKL (p(c)∥µ(c)) for arbitrary distributions, previous work restricts µ(c) either to a Gaussian distribution [17, 18, 19] or to be uniform over C to ease computation and optimization of a weighted KL divergence objective [20]. While empirically successful, these design choices masquerade the pitfalls of the KL divergence to measure distributional similarity in a CRL setting, particularly when dealing with a target distribution 0.0 0.2 0.4 0.6 0.8 1.0 α c p1(c)αp2(c)1−α Fig. 2: Interpolations generated by optimizing Objective (6) for different values of ϵ (and with that α). In the top row, p1(c) and p2(c) are Gaussian, while in the bottom row, they assign uniform density over different parts of C. 0.0 0.2 0.4 0.6 0.8 1.0 α c B([α, 1− α], [p1, p2]) Fig. 3: Wasserstein barycenters B([α, 1−α], [p1, p2]) between the distributions shown in Figure 2. In the top row, p1(c) and p2(c) are Gaussian while in the bottom row, they assign uniform density over different parts of C. that does not assign uniform density over all of C. To demonstrate this issue, we focus on an interpolation task between two distributions p1(c)α(ϵ)p2(c)1−α(ϵ) = arg min p∈{q|DKL(q∥p2)≤ϵ} DKL (p∥p1) , (6) corresponding to a version of Objective (3) with no con- straint on the expected agent performance. Figure 2 demon- strates the sensibility of this interpolation to the para- metric representation of the distributions µ(c) and q(c). While for Gaussian distributions, interpolations of the form p1(c)αp2(c)1−α gradually shift density in a metric sense, this behavior is all but guaranteed for non-Gaussian dis- tributions. The interpolation between two uniform distri- butions with quasi-limited support 1 in the bottom row of Figure 2 displaces density from contexts c to contexts c′ with large Euclidean distance ∥c−c′∥2. In settings in which the Euclidean distance between contexts c1 and c2 is a good indicator for the similarity between M(c1) and M(c2), the observed ignorance of the KL divergence w.r.t. the under- lying geometry of the context space leads to curricula with “jumps” in task similarity. We can easily convince ourselves that such jumps are not a hypothetical problem by recalling that neural network-based policiesπ(a|s, c)=fθ(s, c) tend to gradually change their behavior with increasing Euclidean distance to c. At this point, we can leverage the notion of optimal trans- port to explicitly encode the similarity of two tasks, M(c) and M(c′), via a metric d(c, c′) and realize the interpola- tion between distributions on the resulting metric space as Wasserstein barycenters (Eq. 5). As we see in Figure 3, this explicit notion of task similarity allows to generate interpo- lations that are stable across changes in the parameterization of context distributions and interpolate between arbitrary distributions that are not absolutely continuous w.r.t. each other. Consequently, the optimization problem min p W2(p, µ) s.t. J(π, p) ≥ δ (7) is a promising approach to leverage optimal transport in curriculum RL. We iterate on this candidate in the next section by investigating the role of the expected perfor- mance constraint when generating curricula for reinforce- ment learning agents. 1. We ensure a negligible positive probability density across all of C to allow for the computation of KL divergences.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 5 0.0 0.2 0.4 0.6 0.8 1.0 δ arg minp DKL(p(c)∥µ(c)) s.t. Ep [J(π, c)] ≥ δ c arg minp W2(p(c), µ(c)) s.t. Ep [J(π, c)] ≥ δ Fig. 4: Interpolations using KL divergence (top) and Wasser- stein distance (bottom) subject to an expected performance constraint with different threshold values δ. The perfor- mance J(π, c) is visualized in green. 4.2 Challenges of Expected Performance Constraints The SPRL objective (3) controls the interpolation speed between the initial- and target task distribution by the expected performance of the current agent under the cho- sen context distribution J(π, p). As detailed in [17], this expected performance constraint allows for establishing a connection to self-paced learning for supervised learning tasks [35, 62]. While this formal connection is interesting in its own right, we show in Figure 4 that the expected per- formance constraint in SPRL can lead to encoding both too simple and too complex tasks, given the current agent capa- bilities. Furthermore, using Wasserstein distances in Objec- tive (7) does not resolve this issue. In Figure 4, both methods encode tasks with very high and very low agent returns to fulfill the expected performance constraint, sidestepping the goal of encoding tasks of intermediate difficulty. At this point, we can propose our algorithm CURROT and introduce a recent algorithm proposed by Huang et al. [22] – called GRADIENT – as two ways of resolving the observed interpolation issue: 1) CURROT restricts the support of p(c) to those contexts c ∈ Cthat fulfill the performance constraint J(π, c)≥δ. We refer to this set as V(π, δ) = {c|c∈C, J(π, c) ≥ δ}. With this notation in place, we frame the restricted optimization as min p W2(p, µ) s.t. p(V(π, δ))=1. (8) Putting the constraint in words, we require that the cur- riculum assigns all probability density of p to contexts that satisfy the performance constraint. 2) GRADIENT restricts the interpolation to the barycentric interpolation (5) between the initial- and target context distribution, i.e. pα(c) = B2 ([1−α, α], [p0(c), µ(c)]). This restriction prevents the problematic behavior shown in Figure 4 while still allowing to adjust α using an expected performance constraint max α∈[0,1] α s.t. J(π, pα)≥δ. (9) As shown in Figure 5, both of these methods avoid the behavior generated by Objective (7), resulting in an inter- polation that gradually deforms the distribution in a metric sense with changing agent competence. In the remainder of this article, we will investigate exact and approximate 0.0 0.2 0.4 0.6 0.8 1.0 δ gradient c currot Fig. 5: Interpolations generated by GRADIENT (Eq. 9, top) and CURROT (Eq. 8, bottom) for different threshold values δ. The performance J(π, c) is visualized in green. versions of these algorithms to understand their behavior better. The first observation in this regard is that the cur- riculum of GRADIENT is entirely predetermined by the given metric d(c1, c2) as well as the target- and initial distribution µ(c) and p0(c). The agent performance only influences how fast the curriculum proceeds towards µ(c). On the other hand, CURROT reshapes the curriculum based on the current agent performance to avoid sampling contexts with a performance lower than the threshold δ. Figure 5 shows that this reshaping results in a tendency of CURROT to place all probability density on the border of the desired agent performance δ until reaching regions of non-zero probability density under µ(c). At this point, the curriculum matches the target density in those parts of C, in which the perfor- mance constraint is fulfilled, and continues to concentrate all remaining density on the boundaries of agent capability. This behavior is similar to those CRL methods that combine task-prioritization with a replay buffer of, e.g., previously solved tasks to prevent catastrophic forgetting, such as GOALGAN or PLR [28, 30]. To the best of our knowledge, such behavior has not yet been motivated by a first-principle optimization objective in the context of curriculum RL. 5 A PPROXIMATE ALGORITHMS FOR DISCRETE - AND CONTINUOUS CONTEXT SPACES Objectives (8) and (9) face challenges in more realistic ap- plication scenarios with either large discrete- or continuous context spaces due to two reasons: 1) We do not have access to the expected performance J(π, c) of an agent π in context c but can only estimate it from observed training episodes. 2) Computing Wasserstein barycenters for arbitrary continuous- or discrete distributions in non-Euclidean spaces can quickly become intractably expensive. The following sections address the above problems to benchmark CURROT and GRADIENT in non-trivial experi- mental settings. 5.1 Approximate Wasserstein Barycenters Before branching into the description of the two algorithms, we first describe a particle-based approximation to the computation of Wasserstein Barycenters, which allows us to cheaply approximate Barycenters for the GRADIENT algo- rithm in large discrete state-spaces and is essential for theThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 6 approximate implementation of the CURROT algorithm. For approximating a Barycenter pα=B([1−α, α], [p0, µ]), we first sample a set of N particles from µ(c) and p0(c) to form the empirical distributions ˆµ(c) = 1 N NX n=1 δcµ,n(c), cµ,n ∼ µ(c) (10) ˆp0(c) = 1 N NX n=1 δcp0,n(c), cp0,n ∼ p0(c), where δcref (c) represents a Dirac distribution centered at cref. Due to the discrete nature of ˆµ(c) and ˆp0(c), the coupling ϕ(c1, c2) reduces to a permutation ϕ∈Perm(N), which assigns the particles between ˆp0 and ˆµ [38, Section 2.3]. With that, the computation of W2(ˆp0, ˆµ) reduces to min ϕ∈Perm(N)   1 N NX n=1 d(cp0,n, cµ,ϕ(n))2 !1 2 . (11) Since a permutation is a particular case of a coupling [38, Section 2.3], we overload the meaning of ϕ to be either a permutation or coupling, depending on the number of arguments. With today’s computing hardware, assignment problems like (11) can be solved on a single CPU core in less than a second for N in the hundreds, which is typically enough to represent the context distributions 2. Given this optimal assignment, we then compute the Fr ´echet mean for each particle pair cα,n= arg min c∈C (1−α)d(c, cp0,n)2 + αd(c, cµ,ϕ(n))2 (12) to form the barycenter ˆpα(c)= 1 N PN n=1 δcα,n(c). While cer- tainly less efficient than specialized routines for Barycenter computations in Euclidean Spaces, such as e.g., the Geom- Loss library [63], the presented approach is useful when dealing with large discrete spaces. In this case, faithful Barycenter computations must work with the full distance matrix. Assuming a discrete context space of size S and ne- glecting the cost of computing the optimal assignment, the approximate barycenter computation requiresO(N2+2NS ) evaluations of the distance function. Hence forS ≫ N, even computing the S(S+1) 2 entries of the entire distance matrix required for a single step in the Sinkhorn algorithm becomes more expensive than the presented approximate method. Additionally, reducing the Barycenter computation to an optimization problem over individual particles easily allows to incorporate additional constraints that are required by the CURROT optimization objective (8). 5.2 Approximate GRADIENT Huang et al. [22] propose to compute barycenters between p0(c) and µ(c) for discrete steps of size ϵ. Starting from α=0, the agent trains for M episodes on tasks sampled from the current distribution. If the average episodic return 1 M PM m=1 Rm is greater or equal to δ, α is increased by ϵ and the distribution is set to be the Wasserstein barycenter for the updated value of α. This step-wise increase of α avoids the explicit optimization 2. In our experiments, we use less than a thousand particles in all experiments Algorithm 1Approximate GRADIENT Input: Initial context dist. p0(c), target context dist. µ(c), metric d(c1, c2), performance bound δ, step size ϵ Initialize: α = 0 while True do Compute ˆpα(c) = 1 N PN n=1 δcα,n(c) (Eq. (11) and (12)) Agent Improvement: Sample contexts cm ∼ ˆpα(c), m∈ [1, M] Train policy π under cm and observe episodic rewards Rm = P∞ t=0 γtrcm(st, at), m∈ [1, M] Context Distribution Update: if 1 M PM m=1 Rm≥δ then Advance interpolation α = min(α + ϵ, 1) end if end while over α and, with that, the need to estimate the performance of the current policy π for a given context c. Having laid out a way of computing approximate Barycenters in the previous section, we can summarize our implementation of GRADIENT in Algorithm 1. 5.3 Approximate CURROT As for the GRADIENT algorithm, we make use of an empir- ical distribution ˆp(c) to represent the context distribution p(c) (see Eq. 10). Unlike for GRADIENT , there is no pos- sibility to side-step the estimation of J(π, c) for CURROT , and any estimator of J(π, c) will inevitably make mistakes. The mistakes will be particularly big for contexts c with a considerable distance to those sampled under the current training distribution p(c). To avoid exploiting such erro- neous performance predictions, we introduce a trust region constraint similar to the seminal SPRL objective (3) into CURROT min p W2(p, µ) (13) s.t. p(V(π, δ))=1 W2(p, q) ≤ ϵ, which limits the Wasserstein distance between the current- and next context distribution q(c) and p(c). Please note that we overload the meaning of the symbol ϵ with step size for GRADIENT and the trust region for CURROT , as both concepts limit the change in sampling distribution be- tween updates. We realize the performance estimator using Nadaraya-Watson kernel regression [64, 65] with a squared exponential kernel ˆJ(π, c)= PL l=1 Kh(c, cl)Rl PL l=1 Kh(c, cl) , Kh(c, cl)= exp \u0012 −d(c, cl)2 2h2 \u0013 . This estimator does not rely on gradient-based updates and requires no architectural choices except for the lengthscaleh, consequently not complicating the application of the overall algorithm. We postpone the discussion of this lengthscale parameter h until after we have discussed the approximate optimization of Objective (13) and first focus on the choice of dataset D={(cl, Rl)|l ∈ [1, L]} used to build the kernel regressor. We create the dataset from two buffers, D+ and D−, of size N. We update the buffers with the resultsThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 7 Algorithm 2Approximate CURROT Input: Initial context dist. p0(c), target context dist. µ(c), metric d(c1,c2), performance bound δ, distance bound ϵ Initialize: ˆp(c) = 1 N PN n=1 δcp0,n(c), cp0,n ∼ p0(c) while True do Agent Improvement: Sample contexts cm ∼ ˆp(c), m∈ [1, M] Train policy π under cm and observe episodic rewards Rm = P∞ t=0 γtrcm(st, at), m∈ [1, M] Context Distribution Update: Update buffers D+ and D− with {(cm, Rm)|m∈[1, M]} Estimate ˆJ(π, c) ≈ J(π, c) from D+ and D− Update ˆp(c) via Eq. (14) and ˆJ(π, c), ˆp(c), ˆµ(c) end while of policy rollouts (c, Rc) during agent training, where Rc= P∞ t=0 γtrc(st, at). While D− is simply a circular buffer that keeps the most recent N rollouts with Rc below the performance threshold δ, D+ contains contexts c for which Rc ≥ δ. However, D+ is updated differently if full. Once full, we interpret the samples in D+ as an empirical distri- bution ˆp+(c) and select rollouts from the union of D+ and the set of new rollouts above the performance threshold δ to minimize W2(ˆp+, ˆµ). This optimal selection can be com- puted with a generalized version of the optimal assignment problem (11), where ˆp+ is represented by N+ particles and ˆµ is represented by N particles with N+ ≥ N. The generalized problem then produces a selection of N particles to repre- sent ˆp+, which minimizes the resulting distance W(ˆp+, ˆµ). We can hence interpret ˆp+(c) as a conservative solution to the CURROT objective (8). The solution is conservative since the particles are obtained from past iterations and may exceed the performance threshold δ by some margin, hence not targeting the exact border of the performance threshold. To more precisely target this border of agent competence, we proceed as follows: First, we solve an assignment problem between ˆp(c) and ˆp+(c) to obtain pairs (cp,n, cp+,ϕ(n)). We then reset cp,n=cp+,ϕ(n) for those contexts cp,n with ˆJ(π, cp,n)<δ. Next, we again sample an empirical target distribution ˆµ(c) and solve an assignment problem between the updated empirical distribution ˆp(c) and ˆµ(c) to obtain context pairs (cp,n, cµ,ϕ(n)). We then solve an optimization problem for each pair to obtain the particles for the new empirical context distribution arg min c∈C d(c, cµ,ϕ(n)) (14) s.t. ˆJ(π, c) ≥ δ d (c, cp,n) ≤ ϵ. Note that the restriction d(c, cp,n)≤ϵ ensures that W2(ˆp, ˆq)≤ϵ, while de-coupling the optimization for the indi- vidual particles. We use a simple approximate optimization scheme that samples a set of candidate contexts around cp,n and selects the candidate that minimizes the distance to cµ,ϕ(n) while fulfilling the performance constraint. In the continuous Euclidean settings, we uniformly sample candi- dates in the half ball of contexts that make an angle of less than 90 degrees with the descent direction cp,n − cµ,ϕ(n). In discrete context spaces, we evaluate all contexts in the trust dS(c1, c2)  dP∗(c1, c2)  dE(c1, c2) Fig. 6: E-Maze environment and visualizations of barycen- ters between initial- and target task distribution for the shortest-path distance dS, performance pseudo-distance dP∗ and Euclidean distance dE. Brighter colors correspond to distributions generated at later stages of the interpolation. The states covered by initial- and target task distributions are highlighted by the blue and red lines. region. If even after resetting cp,n=cp+,ϕ(n), no candidate satisfies the performance threshold, and hence Objective (14) is infeasible, we set cp,n to the candidate with maximum performance in the ϵ-ball. Having defined Objective (14), we can discuss the length- scale parameter h of the Nadaraya-Watson estimator. Given that the purpose of the estimator is to capture the trend in the ϵ-ball around a particle cp,n, we simply set the length- scale to 0.3ϵ. This choice ensures that the two-times standard deviation interval of the squared-exponential kernel Kh centered on cp,n covers the trust region. Like for GRADIENT , we train on p0(c) until reaching an average performance of at least δ, at which point we update the distribution according to Algorithm 2. 6 E XPERIMENTS To demonstrate the behavior of the introduced algorithms CURROT and GRADIENT , we benchmark the algorithms in different environments that feature discrete- and continuous context spaces with Euclidean- and non-Euclidean distance metrics. We furthermore evaluate both the exact approaches as well as their approximate implementations. To highlight the benefits of the proposed approach over currently popu- lar CRL methods, we compare against a range of baselines. More precisely, we evaluate ALP -GMM [14], GOALGAN [30], PLR [28], VDS [31] and ACL [66] in addition to a random curriculum and training directly on µ(c) (referred to as De- fault). Details of the experiments, such as hyperparameters and employed RL algorithms, can be found in Appendix C. The code for running the experiments will be made publicly available upon acceptance. 6.1 E-Maze Environment To investigate CURROT and GRADIENT without relying on approximations and highlight the effect of the chosen dis- tance metric, we start the experiments with the environment shown in Figure 6. In this sparse-reward environment that is represented by a 20 × 20 grid, an agent is tasked to reach a goal position by moving around an elongated wall (black tiles in Figure 6). The curricula for this task control the goal position to be reached via the context c. We investigate three different distance functions of C in this environment:This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 8 • A Euclidean distance dE(c1, c2)=∥r(c1) −r(c2)∥2 based on representations r(c) ∈ R3 of the discrete contexts which encode the two-dimensional goal position as well as the height (walls have a height of 200 and regular tiles a height of zero). • A shortest-path distance dS(c1, c2) computed using the Dijkstra algorithm. The search graph for the Dijkstra algorithm is built by connecting neighboring contexts using the previously defined Euclidean distance. • A pseudo-metric investigated by Huang et al. [22] that is based on the optimal policy’s absolute difference in expected return dP∗(c1, c2)= |Jπ∗(c1) − Jπ∗(c2)|. Op- posed to the metrics dE and dS, this pseudo-metric can assign dP∗(c1, c2) = 0 for c1 ̸= c2. While the definition of Wasserstein barycenters is not en- tirely rigorous for the pseudo-metric dP∗, the introduced approximate algorithms can still operate on it without problems. Huang et al. [22] also investigated this pseudo- metric for the current policy π, leading to a different metric in each algorithm iteration. We investigate this interesting concept in Appendix C.2 to keep the main article short and consistent with the previous sections that assumed a fixed distance. Figure 6 visualizes the barycentric interpolations generated by dE, dS, and dP∗. Looking at Figure 6, we can already anticipate a detrimental effect of the Euclidean met- ric dE on the generation of the curriculum. The visualization of dP∗ indicates a weakness of purely performance-based metrics since a similar expected return forc1 and c2 does not guarantee similar outcomes of actions in the two contexts. We visualize the expected return for different curricula in Figure 7. As we can see, CURROT and GRADIENT can signifi- cantly improve performance over both a purely random- as well as no curriculum. However, the performance gains are highly dependent on an appropriate choice of metric. While both CURROT and GRADIENT show strong performance for dS, CURROT ’s performance diminishes for dP∗, and none of the two methods can make the agent proficient on µ(c) when using dE. Figure 8 shows interpolations generated by CURROT for the investigated metrics. We see that the interpolating distri- 0 50K 100K 150K 200K Step 0.0 0.2 0.4 0.6 Episodic Return Default Random currot(dS) currot(dP∗) currot(dE) gradient(dS) gradient(dP∗) gradient(dE) Fig. 7: Expected return on the target task distribution µ(c) in the E-Maze environment achieved by CURROT and GRA - DIENT under varying distance metrics. The shaded area corresponds to two times the standard error (computed from 20 seeds). The red dotted line represents the maximum possible reward achievable on µ(c). dS(c1, c2)  dP∗(c1, c2)  dE (c1, c2) Fig. 8: CURROT sampling distribution without entropy reg- ularization for varying distance measures. Brighter colors correspond to later training iterations. TABLE 1: Final agent performance of CURROT and GRA - DIENT on µ(c) in the E-Maze environment for varying amounts of entropy regularization ( λ and HLB). Mean and standard error are computed from 20 seeds. CURROT HLB 0. 0.5 1.0 2.0 dS 0.62±0 0.61±0 0.53±0.04 0.58±0.03 dP∗ 0±0 0.45±0.06 0.38±0.06 0.42±0.06 dE 0±0 0±0 0±0 0±0 GRADIENT λ 0. 10−8 10−4 10−2 dS 0.60±0.01 0.56±0.04 0.62±0.00 0.60±0.01 dP∗ 0.55±0.03 0.48±0.05 0.45±0.05 0.30±0.06 dE 0.01±0.01 0.03±0.03 0.03±0.03 0.01±0.01 butions of CURROT can collapse to a Dirac distribution for dS and dP∗. As discussed in Section 5, Huang et al. [22] proposed using an entropy-regularized version of optimal transport due to its computational speed. Given that we solve Objectives (8) and (9) analytically, we can investigate the effect of entropy-regularization not with respect to com- putational speed but to performance. In Table 1, we show the final agent performance when using entropy-regularized transport plans for GRADIENT as well as a lower bound HLB on the entropy of the generated task distributions for CURROT . The detailed formulations of these variants are provided in Appendix C.2. As the results show, entropy regularization can benefit CURROT . The visualizations in Figure 9 indicate that this benefit arises from avoiding the aggressive targeting of contexts right at the edge of the performance constraint that we can see in Figures 1, 5, and 8. In the case of the pseudo distance dP∗, the more diverse tasks sampled from p(c) sometimes allowed the agent to generalize enough to solve tasks sampled from µ(c). For dS(c1, c2)  dP∗(c1, c2)  dE (c1, c2) Fig. 9: CURROT sampling distribution for HLB=2 and vary- ing distance measures. Brighter colors correspond to later training iterations.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 9 GRADIENT , we cannot see significant performance gains but can observe that a too-high entropy regularization in combination with dP∗ diminished performance. Given that for an adequate metric (i.e., dS), the observed performance is stable across different amounts of entropy regularization, we do not further explore this avenue in the following experiments. 6.2 Unlock-Pickup Environment In the following environment, we aim to benchmark approx- imate implementations of CURROT and GRADIENT for large discrete context spaces and demonstrate that appropriate distances for non-trivial context spaces can be designed by hand. In Figure 10, we visualize the unlock-pickup environ- ment from the Minigrid environment collection [67] that we chose for this investigation. To master this environment, the agent must pick up a key, unlock a door and eventually pick up a box in the room that has just been unlocked. We define a curriculum by controlling the starting state of an episode via the context c, i.e., controlling the position of the box, key, agent, and door, as well as the state of the door (whether closed or open). As detailed in Appendix C.3, this task parameterization results in 81.920 tasks to compile a curriculum from. The initial context distribution is defined to encode states in which the agent is directly in front of the box, similar to the bottom-right image in Figure 10. Starting from this initial distribution, the learning algorithm needs to generate a curriculum that ultimately allows the agent to reach and pick up the box from a random position in the left room with a closed door. As we show in Appendix C.3, it is possible to define a so-called highway distance function [68] between contexts that properly takes the role of the door and its interaction with the key into account, without relying on a planning algorithm like in the previous environment. We use this distance function in the following evaluations. In addition to the approximate versions of CURROT and GRADIENT , we evaluate PLR , VDS , and ACL on this task. We do not evaluate SPRL , ALP -GMM , and GOALGAN since Fig. 10: The Unlock-Pickup environment, in which an agent needs to pick up the box in the right room by unlocking the door. After reset, the agent is randomly placed in the left room not carrying the key (top left image). After picking up the key (top right), the door can be unlocked (bottom left) to move to the box (bottom right). The door-, box- and key po- sitions as well as their colors vary across environment resets. The agent receives a partial view of the world (highlighted rectangle) that is blocked by walls and closed doors. 0 0.5M 1M 1.5M 2M Step 0.00 0.25 0.50 0.75 Cum. Disc. Ret. Default Random currot gradient acl plr vds Fig. 11: Episodic return on the target task distributionµ(c) in the Unlock-Pickup environment for different curricula. The shaded area corresponds to two times the standard error computed from 20 seeds. those algorithms have been designed for continuous and Euclidean context spaces by, e.g., leveraging Gaussian dis- tributions, kd-trees, or Gaussian sampling noise. The evalu- ation results in Figure 11 show that CURROT and GRADIENT consistently allow mastering the target tasks (a cumulative discounted return of 0.75 ≈ 0.9928 is obtained by solving a task in 28 steps). For both CURROT and GRADIENT , each of the 20 runs led to a well-performing policy, and we can barely see any difference in learning speed between the approaches. Learning directly on the target task distribu- tion allows mastering the environment in some runs while failing to do so in others due to the high dependence on collecting enough positive reward signals at the beginning of learning. These two outcomes lead, on average, to a lower performance compared to CURROT and GRADIENT . Finally, we see that all baseline curriculum methods learn slower than directly learning on the target task distribution µ(c), with ACL not producing policies that collect any reward on the target tasks. Given the successful application of PLR in the Procgen benchmark, which features a diverse set of Arcade game levels with highly distinct visual observations, we wish to discuss the observed low performance of PLR here in more detail. As we show in Appendix C.3, PLR indeed samples contexts occurring under µ(c) with at least 7% in each run. Furthermore, in about half of the runs, the agent also learns to solve those target tasks that are replayed by PLR at some point in the curriculum. However, these replayed target tasks only make up a small fraction of the total number of target tasks, resulting in low performance on all of µ(c). The absence of a notion of target distribution for PLR seems to lead to ineffective use of samples w.r.t improving performance on the target. This lack of target distribution causing problems will be a re-occurring theme for the subsequent experiments. 6.3 Point-Mass Environment In this environment, in which a point-mass agent must pass through a narrow gate to reach a goal position opposite a wall (Figure 12), we benchmark our approximate implemen- tations of CURROT and GRADIENT in continuous settings. The context c∈R2 controls the position and width of the gate that the agent needs to pass. This environment has been introduced with the SPRL algorithm by Klink et al. [18] with a Gaussian target distribution that essentially encodes one narrow gate requiring the agent to detour before reaching the target position. Combined with a dense reward basedThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 10 on the Euclidean distance to the goal, the target task is subject to a prominent local minimum that simply moves the agent close to the wall without passing through. We extend this task with a bi-modal target distribution that challenges SPRL ’s Gaussian restriction that – as we discussed – is required for it to work properly. As seen in Figure 12, CURROT and GRADIENT generate curricula that target both modes of the distribution and allow learning a proficient policy on all of µ(c). As we show in Appendix C.4, the Gaussian restriction of SPRL ’s context distribution leads to p(c) matching only one of the modes of µ(c), resulting in a lower average reward on µ(c) compared to CURROT and GRADIENT . We additionally visualize summary statistics for the other CRL methods in Figure 12, showing that they result in a less targeted sampling of contexts likely under µ(c). This observation, in combination with the lower per- formance compared to CURROT and GRADIENT , once more emphasizes the importance of embedding a notion of target distribution in CRL algorithms. We additionally benchmark CURROT and GRADIENT in ver- sions of the point-mass environment with increasing con- text spaces dimensions. The results in Appendix D show that both approaches can scale to higher dimensions (we investigated up to 30-dimensional context spaces) for this environment. However, they also emphasize the importance of certain algorithmic choices such as the choice of initial context distribution p0(c) for both methods and the choice of the trust region as well as the sampling schemes of candidates for Objective (14) for CURROT . To keep the main article short, we refer the interested reader to Appendix D. 6.4 Sparse Goal-Reaching Environment We next turn to a sparse-reward, goal-reaching environ- ment in which an agent needs to reach a desired position with high precision (Figure 13). Such environments have, e.g., been investigated by Florensa et al. [30]. The context c∈C ⊆R3 of this environment encodes the 2D goal position as well as the allowed tolerance for reaching the goal. This parameterization results in both infeasible tasks being part of C (unreachable regions) as well as tasks that are solely meant to be stepping stones to more complicated ones (low- precision tasks). Given that the agent is ultimately tasked to reach as many goals as possible with the highest precision, i.e., the lowest tolerance, the target distribution µ(c) is a uniform distribution on a 2D slice of C with minimal task tolerance. The walls in the environment (Figure 13) render many target tasks infeasible, requiring the curriculum to identify the feasible subspace of tasks to achieve a good learning performance. Figure 13 shows that CURROT results in the best learning performance across all evaluated CRL methods. Only an oracle, which trains the learning agent only on the feasible subspace of high-precision tasks, can reach higher performance. The evolution of the task tol- erances shown in Figure 13 highlights that CURROT and GRADIENT continuously reduce the task tolerance. The base- line CRL methods lack focus on the tasks encoded by µ(c), sampling tasks with comparatively high tolerance even towards the end of training. Interestingly, SPRL samples high-tolerance tasks throughout all training epochs since its Gaussian context distribution converges to a quasi-uniform distribution over C. Otherwise, SPRL would not be able to cover the non-Gaussian distribution of feasible high- precision target tasks without encoding many infeasible tasks. Figure 13 shows the particle evolution for runs of CURROT and GRADIENT . CURROT gradually decreases the goal tolerance over epochs, starting from contexts that are close to the initial position of the agent. Interestingly, it retains higher tolerance contexts located in the walls of the environment even in later epochs due to the trade-off between sampling high-precision tasks and covering all goal positions. The pre-determined interpolation of GRADIENT cannot adjust to infeasible parts of the context space and reduces to a curriculum that shrinks the upper-bound tub of the tolerance interval [0.05, tub]. Consequently, a decrease in tub increases the number of infeasible tasks on which the agent is trained, slowing down learning and resulting in a significant performance gap between CURROT and GRADIENT in this environment. We additionally evaluate CURROT and GRADIENT with Hindsight Experience Replay (HER ) [11] in Appendix C.5, showing that HER can serve as a drop-in replacement for SAC in this task. −3 3 4 C ⊆R2 pg wg (a) CURROT −3 3 pg 4 wg (b) GRADIENT 0 200K 400K 600K 800K Step 3 6Cum. Disc. Ret.  0 200K 400K 600K 800K Step 0 1 2Gate Position 0 200K 400K 600K 800K Step 0 2 4Gate Width sprl Random Default currot gradient goalgan alp-gmm acl plr vds (c) Performance Comparison Fig. 12: a + b) The point-mass environment with its two-dimensional context space. The target distribution µ(c) encodes the two gates with width wg=0.5, in which the agent (black dot) is required to navigate through a narrow gate at different positions to reach the goal (red cross). The colored dots visualize a curriculum generated by CURROT and GRADIENT for this environment. c) Left: Discounted cumulative return over learning epochs obtained in the point mass environment under different curricula as well as baselines that sample tasks uniformly from all of C (Random) or µ(c) (Default). Middle and Right: Median minimum distance to the target contexts of µ(c) for the two dimensions of the context space (i.e., gate position and -width). Mean and two-times standard error intervals are computed from 20 seeds.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 11 (a) CURROT 0.05 0.70 1.35 2.00  (b) GRADIENT 0 1M 2M 3M 4M Step 0 0.25 0.5 0.75 1 Success Rate 0 1M 2M 3M 4M Step 10−1 100 101 Tolerance sprl Random Oracle currot gradient goalgan alp-gmm acl plr vds (c) Performance Comparison Fig. 13: a + b) Curricula generated by CURROT and GRADIENT in the spare goal-reaching (SGR) environment at different epochs. The starting area of the agent is highlighted in red. The walls are shown in black. The position of the samples encodes the goal to be reached while the color encodes the goal tolerance. c) Success rate on the feasible subspace of C (left) and median goal tolerance (right) for different CRL methods in the SGR environment. We also include an oracle baseline that only samples the feasible tasks in the context space C. For both plots, mean and two-times standard error intervals are computed from 20 runs. 6.5 Teach My Agent In this final evaluation environment, a bipedal agent must learn to maneuver over a track of evenly spaced obstacles of a specified height (see Figure 14). The environment is a modified bipedal walker environment introduced by Porte- las et al. [14] and extended by Romac et al. [21] in which the spacing and height of obstacles is controlled by the context c ∈ R2. The evaluations by Romac et al. [21] demonstrated poor performance of SPRL , often performing statistically sig- nificantly worse than a random curriculum. Given that both CURROT and GRADIENT can be seen as improved versions of SPRL that – among other improvements – explicitly take the geometry of the context space into account, we are interested in whether they can improve upon SPRL . We hence revisit two learning scenarios investigated by Romac et al. [21], in which CRL methods demonstrated a substantial benefit over random sampling: a setting in which most tasks of the context space are infeasible due to large obstacles and a setting in which most tasks of the context space are trivially solvable. Both scenarios lead to slow learning progress when choosing tasks randomly due to frequently encountering too complex or too simple learning tasks. Given that the uniform initial- and target distribution over the context space lead to poor learning performance, we extend the CURROT and GRADIENT method with a simple randomized search to find areas of C where the agent achieves returns above δ, similar in spirit to SPRL . We describe this method in Appendix B. Figure 15 visualizes the performance of CURROT and GRA - DIENT in comparison to other CRL methods that were already evaluated by Romac et al. [21]. We see that CURROT achieves the best performance in all environments, in one case performing statistically significantly better than ALP - GMM , the best method evaluated in [21]. We also see that the extended version of GRADIENT can improve upon a ran- dom curriculum in the “mostly infeasible” scenario while performing insignificantly worse than a random curriculum in the “mostly trivial” scenario. Figure 14 can help shed some light on the observed performance difference between CURROT and GRADIENT . For the “mostly trivial” scenario, GRADIENT consistently arrives at sampling from the uni- form µ(c), whereas CURROT focuses on the contexts at the 2 4 6 8 hobs1 3 5 sobs −1.5 1.5 hobs1 3 5 sobs (a) GRADIENT Curriculum 2 4 6 8 hobs1 3 5 sobs −1.5 1.5 hobs1 3 5 sobs (b) CURROT Curriculum Fig. 14: Sampling distribution of GRADIENT and CURROT on the teach my agent benchmark in the no expert knowledge setting in task spaces with mostly infeasible- (left) and mostly trivial (right) tasks. The small images visualize the obstacles encoded by the corresponding contexts. For environment details, please see [21]. Brighter colors indicate tasks at later epochs of training. The yellow dots represent the samples from the last generated distribution. border of agent competence. For the “mostly infeasible” scenario, the pre-determined interpolation of GRADIENT can fail to encode feasible learning tasks, ultimately leading to a lower overall performance than CURROT . Summarizing, the experimental results underline that em- pirically successful curricula can be generated by framing CRL as an interpolation between context distributions. The leap in performance between GRADIENT and CURROT com- pared to SPRL and the performance differences between GRADIENT and CURROT underline the tremendous impact of design choices, such as the distributional measure of simi- larity and the way of incorporating performance constraints,This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 12 currot alp-gmm cov-gmm gradient riac Random sprl 0 25 50 75 100Performance Mostly infeasible task space currot alp-gmm cov-gmm riac Random gradient sprl Mostly trivial task space Fig. 15: Performance (in percentage of solved tasks) in the Teach My Agent benchmark in the no expert knowledge setting. The baseline results are taken from [21], and only CURROT and GRADIENT are evaluated by us. Statistics have been computed from 32 seeds. Horizontal lines between connecting two methods indicate statistically significant different performances according to Welch’s t-test with p <0.05. on the final algorithm performance. However, when chosen correctly, these curricula exhibit strong performance and allow for guiding training towards tasks of interest specified via µ(c). Especially this last aspect can allow for more flexi- bility in the curriculum design, as it is possible to define aux- iliary task parameterizations without jeopardizing learning progress toward tasks of interest. We saw an example of this trade-off in the sparse goal-reaching environment, where the additional precision parameter boosted the performance of CURROT while diminishing the performance of other CRL methods. 7 C ONCLUSION In this article, we framed curriculum reinforcement learning as an interpolation between distributions of initial- and target tasks. We demonstrated that the lack of an explicit notion of task similarity in combination with an expected performance constraint makes existing approaches highly dependent on the parameterization of the interpolating task distribution. We avoided these pitfalls by explicitly encoding task similarity via an optimal transport formu- lation, and by restricting the generated task distributions to only encode tasks that satisfy a specified performance threshold. The resulting method called CURROT led to good performance in experiments due to its focus on tasks at the performance threshold and the adaptive nature of the cur- riculum. Contrasting our approach to a recently proposed method that generates curricula via Wasserstein barycenters between initial- and target task distributions [22], we saw that the more adaptive nature of our formulation resulted in better performance when facing learning settings with infeasible target tasks. In tasks, in which infeasibility is not a concern, both methods performed similar. In Appendix D, we saw that both methods can scale to higher dimensional tasks although the conceptually more simple GRADIENT algorithm requires less adaptations of its approximations to do so. Together, both methods demonstrate the bene- fit of using optimal transport for curriculum RL and we believe that this benefit can be maximized by developing algorithms that combine the adaptivity of CURROT with the simpler algorithmic realization of GRADIENT . Additionally, we believe that the precise notion of task similarity via the distance d(c1, c2) can prove beneficial in advancing the understanding of curriculum RL. We already saw that an appropriate definition of task similarity is key to successful curriculum learning. We believe that distances learned from experience, which encode a form of intrinsic motivation, will significantly advance these methods by merging the strong empirical results of intrinsic motivation in open- ended learning scenarios [13] with the targeted learning achieved by CURROT and GRADIENT . ACKNOWLEDGMENTS This project has received funding from the DFG project PA3179/1-1 (ROBOLEAP) and the German Federal Ministry of Education and Research (BMBF) (Project: 01IS22078). This work was also funded by Hessian.ai through the project ’The Third Wave of Artificial Intelligence – 3AI’ by the Ministry for Science and Arts of the state of Hessen. Finally, the authors gratefully acknowledge the computing time pro- vided to them on the high-performance computer Lichten- berg at the NHR Centers NHR4CES at TU Darmstadt. This is funded by the Federal Ministry of Education and Research and the state governments participating on the basis of the resolutions of the GWK for national high-performance computing at universities. Joni Pajarinen was supported by Research Council of Finland (formerly Academy of Finland) (decision 345521). REFERENCES [1] R. S. Sutton and A. G. Barto, Introduction to Reinforce- ment Learning. MIT Press, 1998. [2] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve- ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al. , “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, p. 529, 2015. [3] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al. , “Mastering the game of go without human knowledge,” Nature, vol. 550, no. 7676, p. 354, 2017. [4] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas et al., “Solving rubik’s cube with a robot hand,” arXiv preprint arXiv:1910.07113, 2019. [5] N. Rudin, D. Hoeller, P . Reist, and M. Hutter, “Learning to walk in minutes using massively parallel deep re- inforcement learning,” in Conference on Robot Learning , 2022.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 13 [6] S. Liu, G. Lever, Z. Wang, J. Merel, S. Eslami, D. Hennes, W. M. Czarnecki, Y. Tassa, S. Omidshafiei, A. Abdolmaleki et al. , “From motor control to team play in simulated humanoid football,” arXiv preprint arXiv:2105.12196, 2021. [7] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, “Unifying count-based explo- ration and intrinsic motivation,” in Neural Information Processing Systems (NeurIPS), 2016. [8] M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar, “Bayesian reinforcement learning: A survey,” Founda- tions and Trends® in Machine Learning , vol. 8, no. 5-6, pp. 359–483, 2015. [9] M. C. Machado, M. G. Bellemare, and M. Bowling, “Count-based exploration with the successor repre- sentation,” in AAAI Conference on Artificial Intelligence (AAAI), 2020. [10] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Tay- lor, and P . Stone, “Curriculum learning for reinforce- ment learning domains: A framework and survey,” Journal of Machine Learning Research (JMLR), vol. 21, no. 181, pp. 1–50, 2020. [11] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P . Welinder, B. McGrew, J. Tobin, P . Abbeel, and W. Zaremba, “Hindsight experience replay,” in Neural Information Processing Systems (NeurIPS), 2017. [12] C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P . Abbeel, “Reverse curriculum generation for rein- forcement learning,” in Conference on Robot Learning (CoRL), 2017. [13] R. Wang, J. Lehman, J. Clune, and K. O. Stanley, “Poet: open-ended coevolution of environments and their optimized solutions,” in Genetic and Evolutionary Computation Conference (GECCO), 2019, pp. 142–151. [14] R. Portelas, C. Colas, K. Hofmann, and P .-Y. Oudeyer, “Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments,” in Conference on Robot Learning (CoRL), 2019. [15] J. W ¨ohlke, F. Schmitt, and H. van Hoof, “A performance-based start state curriculum framework for reinforcement learning,” in International Conference on Autonomous Agents and Multiagent Systems (AA- MAS), 2020, pp. 1503–1511. [16] M. Jiang, M. Dennis, J. Parker-Holder, J. Foerster, E. Grefenstette, and T. Rockt ¨aschel, “Replay-guided adversarial environment design,” in Neural Information Processing Systems (NeurIPS), 2021. [17] P . Klink, H. Abdulsamad, B. Belousov, C. D’Eramo, J. Peters, and J. Pajarinen, “A probabilistic interpre- tation of self-paced learning with applications to re- inforcement learning,” Journal of Machine Learning Re- search (JMLR), vol. 22, no. 182, pp. 1–52, 2021. [18] P . Klink, H. Abdulsamad, B. Belousov, and J. Peters, “Self-paced contextual reinforcement learning,” in Con- ference on Robot Learning (CoRL), 2020. [19] P . Klink, C. D’ Eramo, J. R. Peters, and J. Pajarinen, “Self-paced deep reinforcement learning,” in Neural Information Processing Systems (NeurIPS), H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., 2020. [20] J. Chen, Y. Zhang, Y. Xu, H. Ma, H. Yang, J. Song, Y. Wang, and Y. Wu, “Variational automatic curricu- lum learning for sparse-reward cooperative multi- agent problems,” Neural Information Processing Systems (NeurIPS), 2021. [21] C. Romac, R. Portelas, K. Hofmann, and P .-Y. Oudeyer, “Teachmyagent: a benchmark for automatic curriculum learning in deep rl,” International Conference on Machine Learning (ICML), 2021. [22] P . Huang, M. Xu, J. Zhu, L. Shi, F. Fang, and D. Zhao, “Curriculum reinforcement learning using optimal transport via gradual domain adaptation,” in Neural Information Processing Systems (NeurIPS), 2022. [23] D. Weinshall and D. Amir, “Theory of curriculum learning, with convex loss functions,” Journal of Ma- chine Learning Research (JMLR) , vol. 21, no. 222, pp. 1– 19, 2020. [24] X. Wu, E. Dyer, and B. Neyshabur, “When do curricula work?” in International Conference on Learning Represen- tations (ICLR), 2021. [25] Q. Li, Y. Zhai, Y. Ma, and S. Levine, “Understanding the complexity gains of single-task rl with a curriculum,” in International Conference on Machine Learning (ICML) , 2023. [26] A. Allievi, P . Stone, S. Niekum, S. Booth, and W. B. Knox, “The perils of trial-and-error reward design: Misdesign through overfitting and invalid task spec- ifications,” in AAAI Conference on Artificial Intelligence (AAAI), 2023. [27] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Rus- sell, A. Critch, and S. Levine, “Emergent complex- ity and zero-shot transfer via unsupervised environ- ment design,” in Neural Information Processing Systems (NeurIPS), 2020. [28] M. Jiang, E. Grefenstette, and T. Rockt ¨aschel, “Prior- itized level replay,” in International Conference on Ma- chine Learning (ICML), 2021. [29] S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus, “Intrinsic motivation and automatic curricula via asymmetric self-play,” in Inter- national Conference on Learning Representations (ICLR) , 2018. [30] C. Florensa, D. Held, X. Geng, and P . Abbeel, “Au- tomatic goal generation for reinforcement learning agents,” in International Conference on Machine Learning (ICML), 2018. [31] Y. Zhang, P . Abbeel, and L. Pinto, “Automatic curricu- lum learning through value disagreement,” in Neural Information Processing Systems (NeurIPS), 2020. [32] S. Racaniere, A. K. Lampinen, A. Santoro, D. P . Re- ichert, V . Firoiu, and T. P . Lillicrap, “Automated curric- ula through setter-solver interactions,” in International Conference on Learning Representations (ICLR), 2020. [33] T. Eimer, A. Biedenkapp, F. Hutter, and M. Lindauer, “Self-paced context evaluation for contextual reinforce- ment learning,” in International Conference on Machine Learning (ICML), 2021. [34] A. Baranes and P .-Y. Oudeyer, “Intrinsically motivated goal exploration for active motor learning in robots: A case study,” in International Conference on Intelligent Robots and Systems (IROS), 2010. [35] M. P . Kumar, B. Packer, and D. Koller, “Self-pacedThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 14 learning for latent variable models,” in Neural Informa- tion Processing Systems (NeurIPS), 2010. [36] R. M. Neal, “Annealed importance sampling,” Statistics and Computing, vol. 11, no. 2, pp. 125–139, 2001. [37] E. L. Allgower and K. Georg, Introduction to numerical continuation methods. SIAM, 2003. [38] G. Peyr ´e, M. Cuturi et al. , “Computational optimal transport: With applications to data science,” Founda- tions and Trends® in Machine Learning , vol. 11, no. 5-6, pp. 355–607, 2019. [39] Y. Chen, T. T. Georgiou, and M. Pavon, “Stochastic con- trol liaisons: Richard sinkhorn meets gaspard monge on a schr ¨odinger bridge,” SIAM Review (SIREV) , vol. 63, no. 2, pp. 249–313, 2021. [40] L. Kantorovich, “On the transfer of masses (in rus- sian),” Doklady Akademii Nauk , vol. 37, no. 2, pp. 227– 229, 1942. [41] C. Liu, J. Zhuo, P . Cheng, R. Zhang, J. Zhu, and L. Carin, “Understanding and accelerating particle- based variational inference,” in International Conference on Machine Learning (ICML), 2019. [42] S. Kolouri, S. R. Park, M. Thorpe, D. Slepcev, and G. K. Rohde, “Optimal mass transport: Signal processing and machine-learning applications,” IEEE Signal Processing Magazine, vol. 34, no. 4, pp. 43–59, 2017. [43] K. Kandasamy, W. Neiswanger, J. Schneider, B. Poc- zos, and E. P . Xing, “Neural architecture search with bayesian optimisation and optimal transport,” in Neu- ral information processing systems (NeurIPS), 2018. [44] M. Togninalli, E. Ghisu, F. Llinares-L ´opez, B. Rieck, and K. Borgwardt, “Wasserstein weisfeiler-lehman graph kernels,” in Neural Information Processing Systems (NeurIPS), 2019. [45] F. M ´emoli, “Gromov–wasserstein distances and the metric approach to object matching,” Foundations of computational mathematics , vol. 11, no. 4, pp. 417–487, 2011. [46] C. Vincent-Cuaz, R. Flamary, M. Corneli, T. Vayer, and N. Courty, “Semi-relaxed gromov-wasserstein di- vergence and applications on graphs,” in International Conference on Learning Representations (ICLR), 2022. [47] P . Demetci, R. Santorella, B. Sandstede, W. S. Noble, and R. Singh, “Gromov-wasserstein optimal transport to align single-cell multi-omics data,” in ICML 2020 Workshop on Computational Biology, 2020. [48] A. Fickinger, S. Cohen, S. Russell, and B. Amos, “Cross- domain imitation learning via optimal transport,” inIn- ternational Conference on Learning Representations (ICLR), 2022. [49] R. Zhang, C. Chen, C. Li, and L. Carin, “Policy op- timization as wasserstein gradient flows,” in Interna- tional Conference on Machine Learning (ICML) , 2018. [50] A. M. Metelli, A. Likmeta, and M. Restelli, “Propagat- ing uncertainty in reinforcement learning via wasser- stein barycenters,” Advances in Neural Information Pro- cessing Systems, vol. 32, 2019. [51] L. Chen, K. Bai, C. Tao, Y. Zhang, G. Wang, W. Wang, R. Henao, and L. Carin, “Sequence generation with optimal-transport-enhanced reinforcement learning,” in AAAI Conference on Artificial Intelligence (AAAI), 2020. [52] Y. L. Goh, W. S. Lee, X. Bresson, T. Laurent, and N. Lim, “Combining reinforcement learning and op- timal transport for the traveling salesman problem,” in 1st International Workshop on Optimal Transport and Structured Data Modeling, 2022. [53] Z. Ren, K. Dong, Y. Zhou, Q. Liu, and J. Peng, “Explo- ration via hindsight goal generation,” Neural Informa- tion Processing Systems (NeurIPS), 2019. [54] I. Durugkar, M. Tec, S. Niekum, and P . Stone, “Adver- sarial intrinsic motivation for reinforcement learning,” 2021. [55] D. Cho, S. Lee, and H. J. Kim, “Outcome-directed reinforcement learning by uncertainty & temporal distance-aware curriculum goal generation,” Interna- tional Conference on Learning Representations (ICLR) , 2023. [56] P . Klink, H. Yang, C. D’Eramo, J. Peters, and J. Pa- jarinen, “Curriculum reinforcement learning via con- strained optimal transport,” in International Conference on Machine Learning. PMLR, 2022, pp. 11 341–11 358. [57] A. Hallak, D. Di Castro, and S. Mannor, “Con- textual markov decision processes,” arXiv preprint arXiv:1502.02259, 2015. [58] A. Wilson, A. Fern, S. Ray, and P . Tadepalli, “Multi- task reinforcement learning: a hierarchical bayesian ap- proach,” in International Conference on Machine Learning (ICML), 2007. [59] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Uni- versal value function approximators,” in International Conference on Machine Learning (ICML), 2015. [60] G. Monge, “M ´emoire sur la th ´eorie des d ´eblais et des remblais,” De l’Imprimerie Royale, 1781. [61] M. Agueh and G. Carlier, “Barycenters in the wasser- stein space,” SIAM Journal on Mathematical Analysis , vol. 43, no. 2, pp. 904–924, 2011. [62] D. Meng, Q. Zhao, and L. Jiang, “A theoretical under- standing of self-paced learning,” Information Sciences , vol. 414, pp. 319–328, 2017. [63] J. Feydy, T. S ´ejourn´e, F.-X. Vialard, S.-i. Amari, A. Trouve, and G. Peyr ´e, “Interpolating between opti- mal transport and mmd using sinkhorn divergences,” in International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. [64] E. A. Nadaraya, “On estimating regression,” Theory of Probability & Its Applications , vol. 9, no. 1, pp. 141–142, 1964. [65] G. S. Watson, “Smooth regression analysis,” Sankhy¯ a: The Indian Journal of Statistics, Series A , pp. 359–372, 1964. [66] A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu, “Automated curriculum learning for neural networks,” in International Conference on Machine Learning (ICML), 2017. [67] M. Chevalier-Boisvert, L. Willems, and S. Pal, “Minimalistic gridworld environment for gymna- sium,” 2018. [Online]. Available: https://github.com/ Farama-Foundation/Minigrid [68] E. Baikousi, G. Rogkakos, and P . Vassiliadis, “Similarity measures for multidimensional data,” in International Conference on Data Engineering (ICDE), 2011. [69] R. Jonker and A. Volgenant, “A shortest augmenting path algorithm for dense and sparse linear assignmentThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 15 problems,” Computing, vol. 38, no. 4, pp. 325–340, 1987. [70] P . Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P . Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perk- told, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P . van Mulbregt, and SciPy 1.0 Contributors, “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,” Nature Methods, vol. 17, pp. 261–272, 2020. [71] J. Feydy and P . Roussillon, “Geomloss,” 2019. [Online]. Available: https://www.kernel-operations. io/geomloss/index.html [72] N. Bonneel, J. Rabin, G. Peyr ´e, and H. Pfister, “Sliced and radon wasserstein barycenters of measures,” Jour- nal of Mathematical Imaging and Vision, vol. 51, no. 1, pp. 22–45, 2015. [73] S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde, “Generalized sliced wasserstein distances,” Neural Information Processing Systems (NeurIPS), 2019. [74] N. Courty, R. Flamary, and M. Ducoffe, “Learning wasserstein embeddings,” in International Conference on Learning Representations (ICLR), 2018. [75] L. Li, A. Genevay, M. Yurochkin, and J. M. Solomon, “Continuous regularized wasserstein barycenters,” Neural Information Processing Systems (NeurIPS), 2020. [76] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann, “Stable-baselines3: Reliable reinforcement learning implementations,” Journal of Machine Learning Research , vol. 22, no. 268, pp. 1–8, 2021. [Online]. Available: http://jmlr.org/ papers/v22/20-1364.html [77] J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyr ´e, “Iterative bregman projections for regular- ized transportation problems,” SIAM Journal on Scien- tific Computing, vol. 37, no. 2, pp. A1111–A1138, 2015. [78] M. Cuturi, “Sinkhorn distances: Lightspeed computa- tion of optimal transport,” Neural Information Processing Systems (NeurIPS), 2013. [79] S. Diamond and S. Boyd, “CVXPY: A Python- embedded modeling language for convex optimiza- tion,” Journal of Machine Learning Research , vol. 17, no. 83, pp. 1–5, 2016. Pascal Klinkis a Ph.D. student with Jan Peters and Joni Pajarinen at the Institute for Intelligent Autonomous Systems (IAS) at the Techincal Uni- versity of Darmstadt since May 2019. In his Ph.D., Pascal focuses on improving the learning performance of reinforcement learning agents by leveraging experience across learning tasks via curricula. He completed a research internship at Amazon Robotics and received the AI newcomer award (2021) from the German computer sci- ence foundation. Before this, Pascal received his M.Sc. from the Technical University of Darmstadt, where he also worked as a student assistant. Carlo D’Eramo is an Associate Professor for Reinforcement Learning and Computational Decision-Making at the Center for Artificial Intel- ligence and Data Science of Julius-Maximilians- Universit¨at W ¨urzburg. He is also an indepen- dent group leader of the Technical University of Darmstadt and hessian.AI. The research of his LiteRL group revolves around the problem of how agents can efficiently acquire expert skills that account for the complexity of the real world. To answer this question, his group investigates lightweight methods to obtain adaptive autonomous agents, focusing on several RL topics, including multi-task, curriculum, adversarial, options, and multi-agent RL. Jan Peters is a full professor (W3) for Intelli- gent Autonomous Systems at the Computer Sci- ence Department of the Technical University of Darmstadt since 2011 and, at the same time, he is the dept head of the research department on Systems AI for Robot Learning (SAIROL) at the German Research Center for Artificial Intelligence (Deutsches Forschungszentrum f ¨ur K¨unstliche Intelligenz, DFKI) since 2022. He is also a founding research faculty member of The Hessian Center for Artificial Intelligence. He has received the Dick Volz Best 2007 US Ph.D. Thesis Runner-Up Award, Robotics: Science & Systems - Early Career Spotlight, INNS Y oung Investigator Award, and IEEE Robotics & Automation Society’s Early Career Award, as well as numerous best paper awards. He received an ERC Starting Grant and was appointed an IEEE fellow, AIAA fellow and ELLIS fellow. Joni Pajarinen is an Assistant Professor at Aalto University, where he leads the Aalto Robot Learning research group. The research group focuses on making robots capable of operat- ing autonomously alongside humans by helping them understand what they need to learn in or- der to perform their assigned tasks. To this end, the group focuses on developing novel decision- making methods in reinforcement learning, plan- ning under uncertainty, and decision-making in multi-agent systems.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 16 APPENDIX A COMPUTATIONAL COST OF OPTIMAL TRANSPORT The benefits of optimal transport (OT), such as explicitly incorporating a ground distance on the sample space, come at the price of a relatively high computational burden caused by the need to solve an optimization problem to compute the Wasserstein distance between two distribu- tions. In practice, OT problems in continuous spaces (such as some of the context spaces investigated in this article) are often reduced to linear assignment problems between sets of particles. Such assignment problems can be exactly solved with variations of the Hungarian algorithm with a time complexity of O(n3) [69]. While this polynomial complexity ultimately leads to prohibitive runtimes for large n, we can typically avoid this problem for curriculum RL. Given the often moderate dimensionality of the chosen context spaces, a few hundred particles are typically sufficient to represent the context distributions. In our experiments, we used less than 500 particles in the continuous environments and 640 particles for the discrete unlock-pickup environment, lead- ing to observed solving times of less than 200ms with the linear_sum_assignment function of the SciPy library [70] on an AMD Ryzen 9 3900X. Since the CURROT and GRADIENT algorithms solve, at most, three OT problems per context distribution update, the computational costs of OT are relatively small for the investigated environments. Furthermore, approximations have emerged to tackle prob- lems that require a large number of particles. For example, the GeomLoss library [71], which we use in the GRADIENT implementations for continuous Euclidean spaces, imple- ments a variant of entropy-regularized OT that has brought down the computation time of OT for sets of hundreds of thousands of samples to seconds on high-end GPUs [63]. So-called sliced Wasserstein distances [72, 73] approx- imately solve the given OT problem by solving M OT problems in 1-D subspaces, reducing the time complexity to O(Mn log(n)), where typically M ≪ n. Finally, neural function approximators have been employed e.g. to speed up the computation of Wasserstein distances by learning a metric embedding from data [74] or enable to compu- tation of regularized free-support Wasserstein barycenters by approximating the dual potentials [75]. Consequently, we see opportunities to significantly increase the number of particles via such approximate approaches, even though our experiments did not indicate a need for that so far. APPENDIX B SEARCH FOR FEASIBLE CONTEXTS As detailed in Section 6.5, the initial context distribution p0(c) may be uninformed and consequently lead to sam- pling many learning tasks for which the agent performance is below δ. In such scenarios, we can initiate a search procedure for tasks in which the current agent achieves a performance at least δ of as long as ¯R = 1 M PM m=1 Rm < δ. We terminate this search procedure as soon as ¯R ≥ δ. Dur- ing this search, D+ contains the best-encountered samples, and D− is empty. When a batch of M new episodes arrives, we add those episodes whose return is at least as large as the median return in D+ to the buffer – and for each new episode added, remove the worst performing episode. The search distribution is a (truncated) Gaussian mixture model psearch(c) = NDX i=1 wiN \u0000 c \f\fci, σ2 i I \u0001 with weights wi and variances σ2 i defined via the minimum return observed over all episodes Rmin and the median performance of the buffered episodes Rmed wi ∝ max(0, Rci − Rmed), σ i = max \u0012 10−3, 2 δ − Rci δ − Rmin \u0013 . For simplicity of exposition, we assume that C = [0 , 1]d, i.e., that the context space is a d-dimensional hyper-cube of edge-length one. Consequently, a context c with a return of Rmin will have a standard deviation of two in each dimension, which, in combination with the Gaussian being truncated, leads to spread-out sampling across the hyper- cube. If the dimensions of C are scaled differently, a simple re-scaling is sufficient to use the above sampling procedure. As detailed in the main article, we only required the search procedure in the teach my agent environments, as in the other environments p0(c) provided enough successful initial episodes. For discrete context spaces, the search distribution would need to be adapted, e.g., by defining a uniform distribution over all contexts c with a distance d(c, ci) less that or equal to a threshold that is similarly scaled as the variance σ2 i . APPENDIX C EXPERIMENTAL DETAILS This section discusses hyperparameters and additional de- tails of the conducted experiments that could not be pro- vided in the main text due to space limitations. For all ex- periments except the teach my agent benchmark, we used RL algorithms from the Stable Baselines 3 library [76]. For teach my agent, we use the SAC implementation provided with the benchmark. C.1 Algorithm Hyperparameters The main parameters of SPRL , CURROT , and GRADIENT all factor into one parameter δ corresponding to the per- formance constraint and one parameter ϵ controlling the interpolation speed. We did not perform an extensive hy- perparameter search for these parameters but used their interpretability to select appropriate parameter regions to search in. The performance parameter δ was chosen by eval- uating values around 50% of the maximum reward. This approach resulted in a search overδ ∈ {3, 4, 5} for the point- mass environment and δ ∈ {0.4, 0.6, 0.8} for the sparse goal-reaching and unlock-pickup environment. For the teach my agent experiments, we evaluated δ ∈ {140, 160, 180} for CURROT and GRADIENT . We did not evaluate SPRL in the teach my agent experiment since we took the results from Romac et al. [21]. We evaluated GRADIENT for ϵ ∈ [0.05, 0.1, 0.2]. For SPRL , we initialized ϵ with a value of 0.05 used in the initial experiments by Klink et al. However, we realized that larger values slightly improved performance. For CURROT , the value of ϵ depends on the magnitude of the distances d and hence changes per experiment. In theThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 17 SPRL CURROT GRADIENT ENV. δ ϵ σLB DKLLB δ ϵ δ ϵ SPARSE GOAL -REACHING 0.6 .25 - - 0.8 1 .2 0 .6 0 .05 POINT MASS 4 .25 [ .2 .1875] 8000 4 0 .7 3 .0 0 .2 UNLOCK -PICKUP - - - - 0.6 3 0 .6 0 .05 TEACH MY AGENT - - - - 180 0 .5|0.4 180 0 .05 TABLE 2: Hyperparameters of SPRL , CURROT , and GRADIENT in the different learning environments. The ϵ parameter of CURROT is computed according to the procedure described in appendix C. We do not provide teach my agent parameters for SPRL as we rely on the results reported by [21]. We also do not evaluate SPRL in the unlock-pickup environment since SPRL is designed for continuous context spaces. conducted experiments, we set the parameter ϵ to around 5% of the maximum distance between any two points in the context space, also evaluating a slightly larger and smaller value. However, we refer to Appendix D for a detailed discussion of how to chose ϵ particularly when dealing with higher dimensional context spaces. When targeting narrow target distributions, Klink et al. introduce a lower bound on the standard deviation σlb of the context distribution of SPRL . This lower bound needs to be respected until the KL divergence w.r.t. µ(c) falls below a threshold DKL, as otherwise, the variance of the context distribution may collapse too early, causing the KL divergence constraint on subsequent distributions to only allow for minimal changes to the context distribution. This detail again highlights the benefit of Wasserstein distances, as they are not subject to such subtleties due to their reliance on a chosen metric. Table 2 shows the parameters of CURROT , GRADIENT , and SPRL for the different environments. For ALP -GMM , the relevant hyperparameters are the per- centage of random samples drawn from the context space prand, the number of completed learning episodes between the update of the context distribution nrollout, and the max- imum buffer size of past trajectories to keep sbuffer. Simi- lar to Klink et al. [17], we chose them by a grid-search over (prand, nrollout, sbuffer) ∈ {0.1, 0.2, 0.3}×{50, 100, 200}× {500, 1000, 2000}. For GOALGAN , we tuned the amount of random noise that is added on top of each sampleδnoise, the number of policy roll- outs between the update of the context distribution nrollout as well as the percentage of samples drawn from the success buffer psuccess via a grid search over (δnoise, nrollout, psuccess) ∈ {0.025, 0.05, 0.1} × {50, 100, 200} × {0.1, 0.2, 0.3}. For ACL , the continuous context spaces of the environments need to be discretized, as the algorithm is formulated as a bandit problem. The Exp3.S bandit algorithm that ultimately realizes the curriculum requires two hyperparameters to be chosen: the scale factor for updating the arm probabilities η and the ϵ parameter of the ϵ-greedy exploration strategy. We combine ACL with the absolute learning progress (ALP) metric also used in ALP -GMM and conducted a hyperparam- eter search over (η, ϵ) ∈ {0.05, 0.1, 0.2}×{0.01, 0.025, 0.05}. Hence, contrasting ACL and ALP -GMM sheds light on the importance of exploiting the continuity of the context space. For ACL , the absolute learning progress in a context c can be estimated by keeping track of the last reward obtained in the bin of c (note that we discretize the context space) and then computing the absolute difference between the return obtained from the current policy execution and the stored last reward. We had numerical issues when implementing the ACL algorithm by Graves et al. [66] due to the normaliza- tion of the ALPs via quantiles. Consequently, we normalized via the maximum and minimum ALP seen over the entire history of tasks. For PLR , the staleness coefficient ρ, the score temperature β, and the replay probabilityp need to be chosen. We did a grid search over (ρ, β, p) ∈ {0.15, 0.3, 0.45} × {0.15, 0.3, 0.45} × {0.55, 0.7, 0.85} and chose the best configuration for each environment. For VDS , the parameters for the training of the Q-function ensemble, i.e., the learning rate lr, the number of epochs ALP -GMM GOALGAN ACL ENV. pRAND nROLLOUT sBUFFER δNOISE nROLLOUT pSUCCESS η ϵ SPARSE GOAL -REACHING .2 200 500 .1 200 .2 0 .05 0 .2 POINT MASS .1 100 500 .1 200 .2 0 .025 0 .2 UNLOCK -PICKUP - - - - - - 0.025 0 .1 PLR VDS ENV. ρ β p LR nEP nBATCH SPARSE GOAL -REACHING .45 .15 .55 5 ×10−4 10 80 POINT MASS .15 .45 .85 10 −3 3 20 UNLOCK -PICKUP .45 .45 .55 10 −3 5 20 TABLE 3: Hyperparameters of the investigated baseline algorithms in the different learning environments, as described in Appendix C.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 18 nep and the number of mini-batches nbatch, need to be chosen. Just as for PLR , we conducted a grid search over (lr, nep, nbatch) ∈ {10−4, 5×10−4, 10−3} × {3, 5, 10} × {20, 40, 80}. The parameters of all employed baselines are given in Table 3. We now continue with the description of experimental details for each environment. C.2 E-Maze Environment The xy-coordinates of the representatives r(c)=[x, y, z]T ∈ R3 of a context c form a grid on [−1, 1] × [−1, 1] and, as mentioned in the main article, z=200 for walls and z=0 for all other cells. The four actions {up, down, left, right} lead to a transition to the corresponding neighboring cell with a probability of 0.9, if the neighboring cell has the same height, and 0 if not. Upon reaching the desired state (controlled by the context c), the agent observes a reward of value one, and the episode terminates. In this environment, we use PPO with λ = 0.99 and all other parameters left to the implementation defaults of the Stable Baselines 3 library. For solving objectives (8) and (9), we make use of the linprog function from the SciPy library [70]. Current Agent Performance as a Distance:In the main text, we have investigated the pseudo-distance dP∗(c1, c2) = |J(π∗, c1) − J(π∗, c2)| (15) that defines the similarity of contexts based on the absolute performance difference of the optimal policy in the contexts c1 and c2. While dP∗ only performed slightly worse than the more informed distance dS for GRADIENT , it could only provide meaningful performance for CURROT if combined with entropy regularization. However, Huang et al. [22] also investigated a pseudo-distance function that computes the similarity of two contexts based on the current policy π dP(c1, c2) = |J(π, c1) − J(π, c2)|, (16) leading to a distance function that changes in each iteration. As we show in Figure 16, this distance, while still leading to slower learning for CURROT compared to dS, leads to stable learning across different levels of entropy regularization without any prior environment knowledge. Figure 17 shows multiple curricula that have been generated by CURROT and GRADIENT . Particularly for CURROT , we can see fairly diverse curricula, which sometimes target all three corridors at once (top middle) and sometimes even back track out of the right-most corridor into the remaining two (top right). We see the good performance and the diverse behavior as indicators for the potential of general purpose distance metrics that encode some form of implicit exploration, calling for future investigations to better understand their mechanics. Furthermore, computational aspects arise with the use of such metrics, since for the case of dP, robust and efficient versions for estimating J(π, c) need to be devised. Entropy-Regularized CURROT and GRADIENT : As dis- cussed in Section 6.1, we benchmark versions of GRADI - ENT and CURROT in which we introduce different forms of entropy regularization. For GRADIENT , we recreate the implementation by Huang et al. [22] by using optimal 0 50K 100K 150K 200K Step 0 0.2 0.4 0.6 Episodic Return currot(dP, HLB=0) currot(dP, HLB=0.5) gradient(dP, λ=0) gradient(dP, λ=10−8) Fig. 16: Expected return on the target task distribution µ(c) in the E-Maze environment achieved by CURROT and GRADIENT under varying entropy regularizations for the current performance-based distance dP. The shaded area corresponds to two times the standard error (computed from 20 seeds). The red dotted line represents the maximum possible reward achievable on µ(c). transport formulations that regularize the entropy of the transport plan ϕ [77, 78] Wp,λ(p1, p2)= \u0012 inf ϕ∈Φ(p1,p2) Eϕ [d(c1, c2)p] − λH(ϕ) \u00131/p , (17) with the constraint set Φ(p1, p2) defined as in Section 3.3 and the entropy H(p) of a distribution p over a sample space X defined as H(p) = − R x∈X p(x) log(p(x)). Note that Huang et al. [22] chose these formulations for com- putational speed rather than curriculum performance. This formulation allows for a straightforward adaptation of the GRADIENT objective to incorporate entropy-regularization max α∈[0,1] α s.t. J(π, pα,λ) ≥ δ (18) pα,λ(c) = arg min p αW2,λ(p, µ) + (1− α)W2,λ(p, p0). (19) For the CURROT algorithm, we choose a more direct form of regularization and directly constrain the entropy of the currot (dP)  currot (dP)  currot (dP) gradient (dP)  gradient (dP)  gradient (dP) Fig. 17: Interpolations generated by CURROT and GRADIENT in different runs for the current performance-based distance dP(c1, c2). Brighter colors indicate later iterations.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 19 interpolating distribution p min p W2(p, µ) (20) s.t. p(V(π, δ)) = 1 H(p) ≥ HLB. The above entropy regularized objectives are not linear pro- grams anymore, and we hence solve the (convex) objectives with the CVXPY library [79]. C.3 Unlock-Pickup Environment We use the Unlock-Pickup environment from the Mini- grid library [67]. We do not change the behavior of the environment and only remove the additional discounting that occurs within the environment, as the environment does not reveal the current timestep to the agent, which, combined with an internally discounted reward, leads to non-Markovian behavior. As stated in the main article, the context c controls the initial state of the environment by specifying the position of the agent, key, and box as well as the position and state of the door (i.e., open or closed). We use the DQN algorithm since the extremely sparse nature of the environment favors RL algorithms with a replay buffer. Compared to the default parameters of the DQN algorithm, we only increase the exploration rate from 0.05 to 0.1 and also increase the batch size to 256. We train the Q-network every fourth step, updating the target network with a Polyak update with τ = 0.005 in each step. The Q-network is realized by encoding the image obser- vation with a convolutional neural network with three convolutions of kernel size (2, 2), ReLU activations after each convolution, and a max-pool operation with kernel size (2, 2) after the first convolution and ReLU operation. We do not use information about the agent orientation or the textual task description, as both are not strictly necessary for our environment. The convolutional network has 32- dimensional hidden layers. The output of the convolutional encoder is 64-dimensional, which is then further processed by two fully connected layers with 64 dimensions and ReLU activations before being reduced to the Q-values for the seven actions available in the environment. As briefly mentioned in the main article, the target distribu- tion µ(c) is a uniform distribution over all those contexts in which the agent is in the left room with a closed door and does not hold the key. The initial state distribution contains one context for each box position in the right room in which the agent is positioned directly next to the box. Distance Function As discussed in Section 6.2, a context c controls the starting state of the environment, which is defined by • the agent position ap : C 7→[1, 9] × [1, 4] • the key position kp : C 7→[1, 9] × [1, 4] • the box position bp : C 7→[6, 9] × [1, 4] • the position of the door in the wall dp : C 7→[1, 4] • the state of the door ds : C 7→ {open, closed}. The images of the individual functions that access the state information of a context are motivated by the two rooms R1 = [1, 4] × [1, 4] and R2 = [6, 9] × [1, 4] that make up the environment. Consequently, the agent and the key can be placed in both rooms, whereas the box can only be placed in R2. The wall that separates the rooms occupies tiles in W(c) = {(5, y) | y∈[1, 4], y̸=dp(c)}. Due to this wall, we restrict the context space C such that it does not contain contexts in which the agent or key is located in the wall, i.e., ap (c) /∈ W(c) and kp (c) /∈ W(c). Additionally, we only allow placing the agent and key in R2 if the door is open. Formally, this requires ap (c)≥4 ⇒ ds(c)=open and kp(c)≥4 ⇒ ds(c)=open. Finally, neither key nor agent can be at the same position as the box, i.e., ap (c) ̸= bp(c) and kp(c) ̸= bp(c). With these restrictions, we arrive at the 81.920 individual contexts mentioned in Section 6.2. Note that the distance function between contexts reasons both about state changes that can be achieved in an episode, such as moving between agent positions, and ones that can’t, such as moving the box. Moving boxes is impossible since the episode terminates successfully when the agent picks up the box. Hence, a distance function that is purely based on state transitions would neglect certain similarities between contexts in this environment. We define the distance function dbase(c1, c2) function via representatives r(c), i.e. d(c1, c2)=    dbase(c1, r(c1))+dbase(r(c1), r(c2)) +dbase(r(c2), c2), if ds(c1)̸=ds(c2) dbase(c1, c2), else. (21) Such distances are also known as highway distances [68]. The mapping r : C 7→ Cfrom a context c to its rep- resentative r(c) ensures that the agent is standing right in front of the open door with the key in its hand, i.e., ds(r(c))=open, and ap (r(c))=kp(r(c))=[4, dp(c)], while ensuring that dp(r(c))=dp(c) and bp(r(c))=bp(c). The base distance dbase(c1, c2) encodes the cost of moving both key and agent from their positions in c1 to those in c2 (via dka) as well as the cost of equalizing the box positions between the contexts (via the L1 distance) dbase(c1, c2) =    dka(c1, c2) + ∥bp(c1) − bp(c2)∥1, if dp(c1)=dp(c2) ∞, else. (22) We see that we render contexts with different door positions incomparable to ease the definition of the distance function. The key-agent distance is defined on top of an object dis- tance dobj,dp that is conditioned on a door position dp dka(c1, c2) =    dobj,dp(c1)(ap(c1), ap(c2)), if kp(c1)=kp(c2) dobj,dp(c1)(ap(c1), kp(c1)) +dobj,dp(c1)(kp(c1), kp(c2)) +dobj,dp(c1)(ap(c2), kp(c2)), else. (23) Note that we can simply take dp (c1) since we know that dp(c1)=dp(c2). The object distance is defined as the L1 dis- tance between the two objects if they are in the same room and incorporates the detour caused by passing through the door in the wall if not dobj,dp(p1, p2) = ( ∥p1 − p2∥1, if p1,0≤4 ⇔ p2,0≤4 ∥p1 − [5, dp]∥1 + ∥[5, dp] − p2∥1, else. (24) We ensured that the resulting distance d(c1, c2) fulfills all axioms of a valid distance function, i.e. d(c1, c2) ≥ 0,This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 20 0.00 0.05 0.10 0.15 0.20 0.25 0.30 Fraction µ(c)>0 0 5 10 15 # Visited Contexts ×103 Training Progress 0.0 0.2 0.4 0.6 0.8 1.0 Ep. Ret. on p(c) Training Progress 0.0 0.2 0.4 0.6 0.8 1.0 Ep. Ret. on µ(c)>0 Fig. 18: Statistics of the PLR curricula in the unlock-pickup environment over training progress. The top left plot shows the fraction of contexts sampled by PLR that are also sam- pled by the target context distribution µ(c). The red dashed line indicates the fraction of target samples generated by a random curriculum. The top right plot shows the number of unique contexts (solid lines), unique target contexts (dashed lines), and unique solved target contexts (dotted lines) sam- pled by PLR at least once. The bottom left plot indicates the performance on the PLR curriculum. The performance in those contexts of the curriculum, which are also sampled by the target context distribution µ(c) (i.e., on the fraction indicated in the top left), are shown in the bottom right. d(c1, c2)=0 ⇔ c1=c2, d(c1, c2) = d(c2, c1), and d(c1, c3) ≤ d(c1, c2) + d(c2, c3) via brute-force computations. Note that the in-comparability of contexts with different door posi- tions effectively splits the context space into four disjoint sets (for the four different door positions) that cannot be compared. Hence, we must only ensure these axioms within the four disjoint sets separately. PLR Performance: As mentioned in Section 6.2, Figure 18 shows statistics of the PLR curricula. We can see that throughout most PLR curricula, the chance of sampling a target context stays relatively constant, even though the number of distinct sampled contexts and the number of distinct sampled target contexts continuously grows. We also see that the agent receives a positive learning signal on p(c) in all runs of PLR . Additionally, we see that the prioritization by PLR suppresses contexts from µ(c) since a purely random curriculum would sample a target context 18.75% of the time. In about half of the runs (orange lines), the agent learned to solve some of the target tasks, although this fraction is rather low (there are 15.360 target tasks). Interestingly, this increase in proficiency on tasks from µ(c) does not go hand-in-hand with a consistently increased sampling rate of target tasks. However, as we see in Figure 19 there seems to be a tendency of PLR runs that are more successful on µ(c) to sample more contexts in which the agent is located in the left room at the beginning Training Progress 0.35 0.40 0.45 0.50 0.55 0.60 Fraction Left Room Training Progress 0.10 0.15 0.20 0.25 0.30 Fraction Door Closed Fig. 19: Fraction of contexts in the PLR curricula in which the agent is placed in the left room (left) and in which the door is closed (right) at the start of the episode. A closed door implies that the agent is located in the left room, hence a more strict condition. Note that the color coding corresponds to the one in Figure 18, indicating runs with high- (orange) and low performance (blue) on p(c). of the episode. Generally speaking, Figures 18 and 19 show that PLR prioritized specific contexts over others. However, either due to the missing notion of a target distribution or the dependence of PLR on the agent’s internal value function (which may be biased and incorrect), the generated curricula did not consistently progress to the most challenging, long- sequence tasks encoded by µ(c). C.4 Point-Mass Environment The environment setup is the same as the one investigated by Klink et al. [17, 19] with the only difference in the target context distributions, which is now defined as a Gaussian mixture µ(c) = 1 2N \u0000 c1, 10−4I \u0001 + 1 2N \u0000 c2, 10−4I \u0001 c1 = [−3 0.5]T , c2 = [3 0.5]T . In this environment, we use PPO with 4.096 steps per policy update, a batch size of 128, and λ=0.99. All other parameters are left to the implementation defaults of the Stable Baselines 3 implementation. Figure 20 shows trajectories generated by agents trained with different curricula in the point-mass environment. We see that directly learning on the two target tasks (Default) prevents the agent from finding the gates in the wall to pass through. Consequently, the agent minimizes the dis- tance to the goal by moving right in front of the wall (but not crashing into it) to accumulate reward over time. We see that random learning indeed generates meaningful behavior. This behavior is, however, not precise enough to pass reliably through the wall. As mentioned in the main article, SPRL only learns to pass through one of the gates, as its uni-modal Gaussian distribution can only encode one of the modes of µ(c) (see Figure 21 for a visualization). CURROT and GRADIENT learn policies that can pass through both gates reliably, showing that the gradual interpolation towards both target tasks allowed the agent to learn both. ALP -GMM and PLR also learn good policies. The generated trajectories are, however, not as precise as the ones learned with CURROT and GRADIENT and sometimes only solve one of the two tasks reliably. ACL , GOALGAN , and VDS partlyThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 21 (a) Default  (b) Random  (c) SPRL  (d) CURROT  (e) GRADIENT (f) ALP -GMM  (g) GOALGAN  (h) ACL  (i) PLR  (j) VDS Fig. 20: Final trajectories generated by the different investigated curricula in the point mass environment. The color encodes the context: Blue represents gates positioned at the left and red at the right. create meaningful behavior. However, this behavior is un- reliable, leading to low returns due to the agent frequently crashing into the wall. C.5 Sparse Goal-Reaching Environment For the sparse goal-reaching task, the goal can be chosen within [−9, 9] × [−9, 9], and the allowed tolerance can be chosen from [0.05, 18]. Hence, the context space is a three- dimensional cube C = [ −9, 9] × [−9, 9] × [0.05, 18]. The actually reachable space of positions (and with that goals) is a subset of [−7, 7] × [−7, 7] due to the “hole” caused by the inner walls of the environment. The target context distribution is a uniform distribution over tasks with a tolerance of 0.05 µ(c) ∝ ( 1, if c3 = 0.05, 0, else. The state s of the environment is given by the agent’s x- and y-position. The reward is sparse, only rewarding the agent if the goal is reached. A goal is considered reached if the Euclidean distance between the goal and position of the point mass falls below the tolerance ∥s − [c1 c2]T ∥2 ≤ c3. The two-dimensional action of the agent corresponds to its displacement in the x− and y− direction. The action is clipped such that the Euclidean displacement per step is no larger than 0.3. Given the sparse reward of the task, we again use an RL algorithm that utilizes a replay buffer. Since the actions are continuous in this environment, we use SAC instead of DQN . Compared to the default algorithm parameters of Stable Baselines 3, we only changed the policy up- date frequency to 5 environment steps, increased the batch size to 512, and reduced the buffer size to 200.000 steps. Figure 21 visualizes the behavior of SPRL in the sparse goal- reaching (SGR). We see that for the SGR environment, SPRL increases the variance of the Gaussian context distribution to assign probability density to the target contexts while fulfilling the expected performance constraint by encoding trivial tasks with high tolerance (Figures 21a and 21b). The inferior performance of an agent trained with SPRL 0.05 2 4 6 8 ≥10 (a) SPRL Curriculum (SGR) 0 1M 2M 3M 4M Step 101 102 Standard Deviation x-Position y-Position Tolerance (b) SPRL Sampling Distribution Stds. (SGR) −3 3 wg 4 pg (c) SPRL Curriculum (Point Mass) Fig. 21: a) Visualization of the sampling distribution of SPRL in the sparse goal-reaching (SGR) task. The color of the dots encodes the tolerance of the corresponding contexts, and the position represents the goal to be reached under that tolerance. The walls are shown in black, and the red area visualizes the starting area of the agent. b) 10-, 50- and 90-percentile of the standard deviation of SPRL ’s sampling distribution on the sparse goal-reaching task. The statistics have been computed from 20 seeds. c) Sampling distribution of SPRL in the point mass environment for a given seed. The color indicates the iteration, where brighter colors correspond to later iterations.This article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 22 0 1M 2M 3M 4M Step 0.0 0.2 0.4 0.6 0.8 1.0 Success Rate Oracle Random Random-LT currot gradient her+sac sac Fig. 22: Comparison of Hindsight Experience Replay ( HER , solid lines) and SAC (dotted lines). Across all curricula, pairing HER and SAC achieves similar or better final success rate compared to SAC alone. The final success rate improves the most when training on random tasks with the target tolerance of 0.05 (Random-LT). When training on random tasks with randomized tolerance ( Random), performance improvements are less pronounced. Mean and two-times standard error intervals are computed from 20 seeds. compared to one trained with a random curriculum shows that the Gaussian approximation to a uniform distribution is a poor choice for this environment. While it may be possible to find other parametric distributions that are better suited to the particular problem, CURROT flexibly adapts the shape of the distribution without requiring any prior choices. Hindsight Experience Replay (HER ): Given the success of HER for sparse-reward goal-reaching tasks, we evaluated its performance in our sparse goal-reaching environment. A difference to the environments evaluated by Andrychowicz et al. [11] is the varying tolerance encoded by the contexts c∈C ⊆ R3. Andrychowicz et al. [11] assumed a fixed tolerance for their investigations of HER . We consequently train HER by uniformly sampling C, corresponding to the Random strategy in Figure 13, and sampling from µ(c), i.e., only sampling high-precision tasks. We refer to the latter sampling strategy as Random-LT, where LT is short for low tolerance. HER only influences experience replay and can be easily combined with arbitrary task sampling strategies. Figure 22 shows the results of training HER with the aforementioned task-sampling strategies and in com- bination with GRADIENT and CURROT . We used the HER implementation in the Stable Baselines 3 library [76] with the future strategy. We tuned the number of additional goals to maximize HER ’s performance, finding that k=2 additional goals for each real goal delivered the best results. Looking at Figure 22, we see that HER is well-compatible with all curricula, either matching or improving upon the success rate of SAC alone. HER drastically improves the performance when directly sampling high-precision tasks of µ(c). Training on random tasks of C and with GRADIENT benefit from HER , whereas the performance of CURROT does not improve with the replay of hindsight goals. Finally, when training only on the feasible tasks of µ(c) (Ora- cle), HER significantly improves learning speed. The results indicate that for this task, HER ’s implicit curriculum has a somewhat orthogonal effect than the explicit curricula realized by the different investigated sampling strategies. C.6 Teach My Agent As mentioned in the main text, we used the environment and SAC learning agent implementation provided by Romac et al. [21]. We only interfaced CURROT and GRADIENT to the setup they provided, allowing us to reuse the baseline evaluations provided by Romac et al. [21]. The two settings (mostly infeasible and mostly trivial) differ in the boundaries of their respective context spaces. The mostly infeasible setting encodes tasks with a stump height in [0, 9] and -spacing in [0, 6]. The mostly trivial setting keeps the same boundaries for the stump spacing while encoding stumps with a height in [−3, 3]. Since a stump with negative height is considered not present, half of the context space of the mostly trivial setting does not encode any obstacles for the bipedal walker to master. The initial- and target context distribution µ(c) is uniform over the respective context space C for both settings. APPENDIX D HIGHER DIMENSIONAL CONTEXT EXPERIMENTS In addition to the low-dimensional context parametriza- tions of the tasks in the main article, we create a higher- dimensional version of the point-mass environment in which we essentially over-parameterize the environment. We do this by keeping the position of the gate pg ∈ [−4, 4] as a parameter but splitting the gate width into a left- and right width parameter wg,l ∈ [0.25, 4] and wg,r ∈ [0.25, 4]. Note that we multiplied the range of two width parameters by a factor of 0.5 compared to the regular point mass environment from the main article. The actual context for this environment consists of multiple instances of these three parameters, i.e. c= [pg1 . . . pgN wg1,l . . . wgN,l wg1,r . . . wgN,r] ∈ C⊆R3N . We instantiate the point-mass environment from this over- parameterized context using two different reductions cmin = \u0014 pgn∗ min n∈[1,N] wgn,l min n∈[1,N] wgn,r \u0015 (25) cmax = \u0014 pgn∗ max n∈[1,N] wgn,l max n∈[1,N] wgn,r \u0015 n∗ = arg max n∈[1,N] |pgn|. The only difference between the environment in Section 6.3 and the one investigated in this section is that we separately parameterize the width of the left- and right gate half. We chose these two reductions to highlight that not only the dimensionality of the context space C is important but also its underlying structure. When using cmin, the chance of sampling tasks with a narrow gate far away from the center increases with N. For cmax, the chance of sampling wide gates increases. Most importantly, the learning task does not get more complex with an increasing value of N since the agent always faces the same learning task and observation space. We can hence be sure that observed performance drops are not due to an inherently more complex learning- or approximation task on the level of the RL agent butThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 23 10 20 30 Context Dimension 2 4 6Cum. Disc. Ret. Using cmax, µl 10 20 30 Context Dimension 2 4 6 Using cmin, µl 10 20 30 Context Dimension 2 4 6 Using cmax, µh 10 20 30 Context Dimension 2 4 6 Using cmin, µh currot gradient alp-gmm currot∗ gradient∗ Default Fig. 23: Performance of CURROT and GRADIENT in high-dimensional context space versions of the point mass environment. The two left plots show the final agent performance when training for the low-entropy target distribution µl(c) (Eq. 26) for different reductions cmax and cmin. The two right plots shows the same results when training for the high-entropy target distribution µh(c). Note that the performance of ALP -GMM is not affected by a change in target distribution since it generates the curriculum without this information. The green line indicates the average final performance of regular training on µ(c). Means (thick lines) and two-times standard errors (shaded areas) are computed from 20 seeds. CURROT ∗ and GRADIENT ∗ refer to versions of CURROT and GRADIENT that use the less adversarial initial task distribution for the cmin reduction (please see Appendix D for a description). are due to the curriculum generation. We first investigate a narrow Gaussian mixture model as the target distribution µl(c) = 1 2N \u0000 c1, 10−4I \u0001 + 1 2N \u0000 c2, 10−4I \u0001 (26) c1 = [−3 . . .−3| {z } N−times 0.25 . . .0.25| {z } N−times 0.25 . . .0.25| {z } N−times ] (27) c2 = [ 3. . .3| {z } N−times 0.25 . . .0.25| {z } N−times 0.25 . . .0.25| {z } N−times ]. We benchmarked CURROT , GRADIENT , and ALP -GMM in this task, keeping all algorithm parameters the same as in the point-mass environment from the main article and only adjusting the trust region parameter ϵ of CURROT according to the rule described in Appendix C since the effective distances between points in C increase with N. Importantly, we always represent the curricula for CURROT and GRA - DIENT using 100 particles. Figure 23 shows the obtained results. As we see, CURROT and GRADIENT generate good curricula even for high-dimensional context spaces when using the cmax reduction but fail for higher-dimensional scenarios when using cmin. However, this failure does not arise from a failing interpolation but due to the increasing likeliness of sampling complex tasks under the initial uni- form distribution over C, leading to CURROT and GRADIENT not reaching the performance threshold δ on the initial distribution p0(c). We first tested the feasible context search from the TeachMyAgent benchmark to remedy this issue. However, this search also failed since, just like for uniform noise, uninformed Gaussian noise increases the chance of sampling small gates for cmin in high dimensions. To bench- mark the algorithms for the cmin reduction, we consequently generate an initial distribution with the same distribution of gate positions and -widths as for N=1, regardless of the choice of N. We do this by sampling contexts for N=1 and then projecting them to the required dimension by sampling appropriate random values for the remaining entries in c. Starting from this initial distribution makes the agent proficient on µ(c) across all dimensions, as shown in Figure 23 (we denote the resulting approaches as CURROT ∗ and GRADIENT ∗). We additionally investigate a setting where µh(c) en- codes all high-dimensional contexts c that result in the same reduced target contexts c1=[−3 0.25 0.25] and c2=[3 0.25 0.25]. When evaluating the CURROT method in this scenario, we saw that our approximate optimization of Objective (14) via uniform samples in a half-sphere did not lead to good progression to the target samples. Adding samples along the direction cµ,ϕ(n)−cp,n was enough to solve this issue in our approximate optimization and ensure good progression. While performing these experiments, we saw that our rule of choosing the ϵ parameter for CURROT , i.e., setting it to 0.05 of the maximum distance dmax between any two contexts in C, can prevent Objective (14) from sampling high-dimensional high-entropy distributions. This problem occurs if the Wasserstein distance between two particle-based representations ˆµ1(c) and ˆµ2(c) of the target distribution µ(c) is larger than ϵ=0.05dmax. Consequently, we adapted our rule of choosing the trust region size for CURROT to ϵ= max(0.05dmax, 1.2W2(ˆµ1, ˆµ2)). Figure 23 shows the results for the high-entropy target distributions, where we again see that both CURROT and GRADIENT can solve these tasks. Choosing Particles in Higher Dimensions:The findings in this section provided a better understanding of the role of the number of particles to represent ˆp(c) that we would like to summarize here. For CURROT and GRADIENT , the particles serve two objec- tives: Approximating the sampling- and target distribution and estimating the agent performance. By restricting the curriculum to the barycentric interpolation, GRADIENT can provide unbiased samples from the interpolation and the target distribution even when using a few particles in high dimensions. Consequently, the need for more particles in higher dimensions only arises from counteracting a poten- tially higher variance of the expected performance estimate. However, the effect of more noisy expected performance estimates can also be counteracted by smaller step sizes ϵ with which to advance the Barycentric interpolation. For CURROT , we saw that a small number of particles in combination with a too-small trust region ϵ can lead to biased sampling of the target distribution. However, we also saw that setting ϵ= max(0.05dmax, 1.2W2(ˆµ1, ˆµ2)) forThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is a pre-print version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3390051. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. 24 a given number of particles N ensures good sampling of the target distribution. With this automated choice of ϵ, the appropriate number of N should, as for the GRADIENT algorithm, be guided by the complexity of the performance estimate. If the performance estimate is not of sufficient quality, increasing the number of particles will decrease the minimum required trust region to sample unbiasedly from µ(c) and yield more samples for the kernel regression. It is also possible to only increase the number of particles in the buffers for the kernel regression while keeping the number of particles representing the context distribution fixed. Finally, a more specific feature of CURROT is the optimiza- tion of Objective (14), which may become more delicate in higher dimensions and require more sophisticated ap- proaches than the simple sampling scheme used in this arti- cle. One option could be to use parallelized gradient-based optimization schemes, which should be easy to implement given the rather simple nature of the constraints. Initial Distribution in Higher Dimensions:We also saw that using a uniform initial distribution p0(c) for GRADIENT and CURROT can be problematic if easy tasks are unlikely under this distribution. In this case, CURROT and GRADI - ENT will not achieve the expected performance threshold to progress the curriculum. Furthermore, simple search approaches for feasible contexts like the one detailed in Appendix B may fail. At this point, it may either be required to implement a more problem-specific search for feasible contexts or provide a more informed initial distribution p0(c) that does not require a search for feasible contexts. Both of these approaches can be used for GRADIENT and CURROT .",
      "meta_data": {
        "arxiv_id": "2309.14091v2",
        "doi": "10.1109/TPAMI.2024.3390051",
        "authors": [
          "Pascal Klink",
          "Carlo D'Eramo",
          "Jan Peters",
          "Joni Pajarinen"
        ],
        "published_date": "2023-09-25T12:31:37Z",
        "pdf_url": "https://arxiv.org/pdf/2309.14091v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of automating curriculum generation in Reinforcement Learning (RL) by framing it as an interpolation between task distributions. It identifies shortcomings in existing interpolation-based methods, such as issues with KL divergence for measuring task similarity and expected performance constraints that can lead to non-gradual task changes or sampling overly complex/simple tasks. The main contribution is the proposal of CURROT, a novel approach that generates curricula by formulating it as a constrained optimal transport problem. CURROT explicitly encodes task similarity via a distance function and restricts the generated task distributions to only encode tasks that satisfy a specified performance threshold. The paper also contrasts CURROT with GRADIENT, a recently proposed optimal transport-based method. Experimental results demonstrate that both CURROT and GRADIENT improve upon existing CRL methods, achieving high performance across various tasks with discrete and continuous context spaces and different distance metrics. Specifically, CURROT's adaptive nature proves more effective in scenarios with infeasible target tasks.",
        "methodology": "The methodology centers on framing curriculum generation as a constrained optimal transport problem. To overcome limitations of KL divergence, the authors propose explicitly defining task similarity using a distance function d(c, c') and leveraging optimal transport to generate interpolating distributions (Wasserstein barycenters), ensuring gradual task changes regardless of distribution parameterization. To address issues with expected performance constraints: 1) CURROT restricts the support of the task distribution p(c) to only contexts c that satisfy a performance constraint J(π, c) ≥ δ, formulated as 'min_p W2(p, µ) s.t. p(V(π, δ))=1', and includes a trust region constraint W2(p, q) ≤ ε. 2) GRADIENT uses a barycentric interpolation pα(c) = B2([1−α, α], [p0(c), µ(c)]) between initial and target distributions, where α is adjusted based on an expected performance constraint. For practical application, approximate algorithms are introduced: particle-based approximations for Wasserstein barycenters using empirical distributions and solving linear assignment problems. Approximate GRADIENT updates α step-wise. Approximate CURROT uses Nadaraya-Watson kernel regression to estimate J(π, c) from task buffers and updates p(c) by minimizing distance to µ(c) under performance and trust region constraints via candidate sampling. The paper also investigates entropy regularization for both methods and employs a search procedure for feasible contexts when initial distributions are uninformative.",
        "experimental_setup": "The algorithms were benchmarked across diverse environments: 1) E-Maze: a 20x20 grid sparse-reward task, using PPO, with goal position as context. Evaluated using Euclidean (dE), Shortest-Path (dS), and performance-based pseudo-distances (dP*, dP). 2) Unlock-Pickup: a Minigrid environment with a large discrete context space (81,920 tasks), using DQN. Context defines agent, key, box, door positions, and door state, employing a custom 'highway distance function'. 3) Point-Mass: a continuous environment (c ∈ R^2) where context controls gate position and width, using PPO. Also tested higher-dimensional versions (up to R^30) with over-parameterized contexts. 4) Sparse Goal-Reaching: a continuous, sparse-reward environment (c ∈ R^3) with goal position and tolerance as context, using SAC. 5) Teach My Agent (Bipedal Walker): an environment where context controls obstacle spacing and height, using SAC, for 'mostly infeasible' and 'mostly trivial' scenarios. Baselines included ALP-GMM, GOALGAN, PLR, VDS, ACL, Random curriculum, Default (direct training on target), and SPRL. Hindsight Experience Replay (HER) was also evaluated in the Sparse Goal-Reaching task. Evaluation metrics included episodic return, success rate, median goal tolerance, median minimum distance to target contexts, and percentage of solved tasks.",
        "limitations": "The primary limitations include the high computational burden of optimal transport, especially for large particle counts or high-dimensional spaces, although approximations alleviate this. The performance of the proposed methods is highly sensitive to the choice of the distance metric; a poorly chosen metric can severely diminish learning performance. CURROT, without entropy regularization, can exhibit 'aggressive targeting' where its sampling distribution collapses to a Dirac distribution at the edge of the performance constraint. GRADIENT's curriculum is pre-determined and lacks adaptability to infeasible task regions within the context space, which can hinder learning in environments with such challenges. Both methods are sensitive to the initial context distribution, struggling if it does not contain sufficiently easy tasks to reach the performance threshold. The approximate optimization schemes, particularly for CURROT, may become delicate in higher-dimensional contexts, requiring more sophisticated approaches.",
        "future_research_directions": "Future research should focus on developing algorithms that synthesize the adaptivity of CURROT with the algorithmic simplicity of GRADIENT to maximize the benefits of optimal transport in curriculum RL. Another key direction is to advance the understanding and learning of task similarity by developing methods that learn distance functions d(c1, c2) from experience, potentially merging intrinsic motivation with the targeted learning capabilities of CURROT and GRADIENT. Additionally, there is a need for robust and efficient estimators for agent performance J(π, c) for performance-based distance metrics. For high-dimensional context spaces, more sophisticated optimization methods are required for CURROT's particle-based updates, possibly involving parallelized gradient-based schemes. Finally, developing problem-specific search procedures for feasible contexts or providing more informed initial distributions p0(c) could address scenarios where easy tasks are initially scarce."
      }
    },
    {
      "title": "Curriculum Reinforcement Learning via Constrained Optimal Transport"
    }
  ],
  "evaluated_hypothesis_history": [
    {
      "hypothesis": {
        "open_problems": "LoRA-style PEFT usually attaches low-rank adapters to *all* selected weight matrices with a fixed rank. In practice only a subset of layers are really needed; the rest waste memory, slow training and occasionally hurt generalisation. Today practitioners hand-pick layers or use heavy rank–allocation schemes (e.g.\nAdaLoRA) that still update every layer and add extra bookkeeping. A minimal, train-time mechanism that automatically turns *off* unimportant LoRA modules while keeping the important ones active is missing.",
        "method": "Gated-LoRA (GaLoRA)\n1. Architectural change – for every LoRA module add a single learnable scalar gate gℓ∈ℝ that multiplies the LoRA update:\n   W′ = W₀ + gℓ·BA ,   gℓ is initialised to 1.\n2. Regularisation – add an L1 penalty λ·|gℓ| to the loss.  L1 shrinks many gates to ≈0, effectively disabling their LoRA parameters and leaving the underlying pretrained weight untouched.\n3. Objective –  L_total = L_task  +  λ·Σℓ |gℓ|  .  Only this extra term is new; no change to optimiser or inference code (gates are constants at test time).\n\nMotivation\n• L1 on g acts like soft feature selection, letting the optimiser discover which layers truly need adaptation.\n• When gℓ→0 the corresponding BA matrices receive zero gradient, so training time is automatically saved.\n• At inference we can prune modules with |gℓ|<ε, reducing memory and latency with one line of code.",
        "experimental_setup": "Base model: BERT-base-uncased (110 M).\nTask: SST-2 sentiment classification (GLUE).\nBaselines: (a) Full fine-tune, (b) Standard LoRA (rank=8) on all query/key/value&ffn matrices, (c) AdaLoRA (rank budget equal to LoRA).\nProposed: GaLoRA with same initial rank, λ∈{0.0,1e-3,5e-3,1e-2}.\nTraining: 3 epochs, batch 32, lr 2e-5, AdamW.  Single V100, HuggingFace PEFT.\nMetrics: Dev set Accuracy (primary), number of active gates (#g>0.1), training wall-time, inference FLOPs after pruning.\nAnalysis: draw accuracy vs active-layer count curve.",
        "primary_metric": "accuracy",
        "experimental_code": "import torch, torch.nn as nn\nfrom peft.tuners.lora import LoraLayer\nclass GatedLoraLayer(LoraLayer):\n    def __init__(self, *args, **kw):\n        super().__init__(*args, **kw)\n        # scalar gate, initialise to 1\n        self.gate = nn.Parameter(torch.ones(1))\n    def forward(self, x: torch.Tensor):\n        result = super().forward(x)\n        if self.merged:   # inference after merge\n            return result\n        return x + self.gate * self.lora_A(self.lora_dropout(self.lora_B(x)))\n\ndef add_gated_lora(model, r=8, alpha=16, dropout=0.05):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            gated = GatedLoraLayer(module.in_features, module.out_features,\n                                    r=r, lora_alpha=alpha, lora_dropout=dropout,\n                                    fan_in_fan_out=False)\n            setattr(model, name.split('.')[-1], gated)\n    return model\n\n# loss wrapper\nlambda_l1 = 5e-3\nfor batch in loader:\n    logits = model(**batch).logits\n    loss = ce_loss(logits, batch['labels'])\n    l1 = sum(p.abs().mean() for p in model.parameters() if p.shape==torch.Size([1]))\n    loss = loss + lambda_l1 * l1\n    loss.backward(); optimizer.step(); optimizer.zero_grad()\n\n# pruning after training\nwith torch.no_grad():\n    for m in model.modules():\n        if isinstance(m, GatedLoraLayer) and m.gate.abs() < 0.1:\n            m.merge()  # merges zero so nothing added and frees params",
        "expected_result": "• Standard LoRA expected dev accuracy ≈ 92.0 %  (as in prior work).\n• GaLoRA with λ=5e-3 keeps ~35 % of LoRA layers active and reaches ≈ 92.3 % (+0.3).\n• Training time ~15 % faster (fewer gradients).\n• After pruning, inference FLOPs and trainable parameter count drop by ~60 % vs LoRA.\n• Too large λ (1e-2) prunes too much and accuracy falls to ≈ 91.0 % .",
        "expected_conclusion": "A single gated scalar with L1 regularisation lets the network *learn* which layers actually need low-rank adaptation.  This tiny change requires only a few lines of code, improves accuracy slightly, and yields large savings in trainable parameters, memory and inference cost—offering an automatic, lightweight alternative to manual layer selection or heavier adaptive-rank methods such as AdaLoRA."
      },
      "evaluation": {
        "novelty_reason": "None of the cited PEFT papers attaches an explicit, learnable scalar gate to each LoRA module and drives it to zero with an L1 penalty.  • AdaLoRA and Spectral-Adapter redistribute or prune *ranks* inside every layer but they still back-prop through all LoRA weights and leave a non-zero update in every matrix.  • SHiRA trains a sparse subset of *base* weights, not LoRA adapters.  • DoRA, MHR, CURROT, etc. focus on different re-parameterisations or curriculum ideas and do not perform on-the-fly *layer selection*.  GaLoRA therefore introduces a distinct mechanism – soft feature selection at the level of whole LoRA blocks – that has not been explored in the referenced literature.  Its novelty is incremental rather than conceptual revolution: gating + L1 is a known sparsification trick, but applying it to LoRA to obtain automatic layer–wise pruning with zero extra bookkeeping is new within the PEFT landscape.",
        "novelty_score": 7,
        "significance_reason": "The hypothesis tackles a practical pain-point for practitioners: deciding which transformer layers to adapt and how large the rank should be. By learning gates that can be pruned at train-time it can 1) cut gradient computation, 2) cut memory, and 3) reduce inference FLOPs – all while matching or slightly improving accuracy.  Because it needs only one extra scalar per layer and a standard L1 term, it can be adopted in any existing LoRA code-base with minimal engineering, potentially influencing everyday fine-tuning workflows.  Academically, it provides a clean test-bed for studying where adaptation is actually needed, but it does not radically change our understanding of PEFT nor enable previously impossible applications.  Thus its practical significance is solid, while its theoretical impact is moderate.",
        "significance_score": 6
      }
    },
    {
      "hypothesis": {
        "open_problems": "Current PEFT methods equip every selected Transformer layer with the same always-on LoRA adapter. Recent GaLoRA adds a *static* scalar gate per layer, but once training is finished each input still pays the cost of exactly the same surviving adapters. In practice different sentences (or images, code, etc.) stress different depths of the network – a sarcastic review may need higher-level reasoning while a factual statement may not. There is no mechanism that 1) lets a language model decide *per-input* which layers truly need low-rank adaptation, 2) keeps the rest of the adapters completely dormant to save compute, and 3) learns this routing jointly with the downstream task.",
        "method": "Conditional Gated-LoRA (CoGaLoRA)\n1.  Adapter & router For every LoRA module attach a tiny router network  f_ℓ  (2-layer MLP,  hidden→4→1) that reads the layer’s pre-adapter activation  h_ℓ  (mean-pooled over tokens) and outputs a gate probability  p_ℓ∈(0,1).\n2.  Stochastic binary gate Sample  ĝ_ℓ∼Bernoulli(p_ℓ)  with a straight-through estimator.\n    W′ = W₀ + ĝ_ℓ · B_ℓ A_ℓ.  If  ĝ_ℓ=0  the LoRA branch is bypassed and receives no gradient.\n3.  Regularisation Add a sparse-compute penalty  λ Σ_ℓ p_ℓ  to the loss.  λ controls the expected number of active adapters per *example* rather than per model.\n4.  Inference Replace sampling by the deterministic hard gate  g_ℓ = 1[p_ℓ>τ].  The set of active layers thus changes with every input; the rest incur zero FLOPs.\n5.  Freezing trick After training, layers whose router is *never* triggered can have their LoRA weights deleted, giving the same memory benefits as GaLoRA.",
        "experimental_setup": "Base model BERT-base-uncased (110 M).\nTask SST-2 (GLUE) single-sentence sentiment.\nAdapters Rank r=8 on all Q/K/V + FFN up/down matrices.\nBaselines (1) Full fine-tune, (2) Standard LoRA, (3) Static GaLoRA (best λ).\nProposed CoGaLoRA with λ∈{0.1,0.2,0.4}; τ=0.5.\nOptimiser & schedule AdamW lr 2e-5, 3 epochs, batch 32, same seed.\nMetrics • Dev accuracy • Avg #active adapters per example • Training wall-time • Inference FLOPs measured with torch.profiler on 256 random sentences.",
        "primary_metric": "dev-set accuracy",
        "experimental_code": "class Router(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(hidden_size, 4), nn.ReLU(), nn.Linear(4, 1))\n    def forward(self, h):                 # h: [B, hidden]\n        return torch.sigmoid(self.net(h))\n\nclass CoGatedLoraLayer(LoraLayer):\n    def __init__(self, *a, hidden_size, **kw):\n        super().__init__(*a, **kw)\n        self.router = Router(hidden_size)\n    def forward(self, x):                 # x: [B, T, hidden]\n        gate_p = self.router(x.mean(1))   # [B,1]\n        bern = (torch.rand_like(gate_p) < gate_p).float() + gate_p.detach() - gate_p  # STE\n        out = super().forward(x)\n        return x + bern.view(-1,1,1) * self.scaling * out, gate_p\n\nlambda_sparse = 0.2\nfor batch in loader:\n    logits, ps = model(**batch, return_gate_ps=True)\n    loss = ce(logits, batch['labels']) + lambda_sparse * ps.mean()\n    loss.backward(); opt.step(); opt.zero_grad()",
        "expected_result": "• Standard LoRA: 92.0 % accuracy, 100 % layers used.\n• Static GaLoRA (≈35 % layers kept): 92.3 %.\n• CoGaLoRA (λ=0.2): 92.6 % accuracy with only ~20 % adapters active per sentence; 55 % average FLOP reduction at inference; training time ≈+5 % (routers).\n• λ too high (0.4) activates <10 % adapters and drops to 91.5 %.",
        "expected_conclusion": "Making LoRA activation *input-conditional* via tiny learned routers yields better task accuracy and much larger average compute savings than static layer selection. CoGaLoRA turns parameter-efficient fine-tuning into *computation-efficient* fine-tuning, adapting the network depth it actually modifies on a per-example basis without extra optimisation tricks or post-hoc pruning. The idea is orthogonal to rank-adaptation (AdaLoRA) and can be combined with it in future work."
      },
      "evaluation": {
        "novelty_reason": "None of the listed PEFT papers let the model decide at inference-time, for every individual input, which LoRA blocks to execute.  • GaLoRA prunes adapters but the surviving ones are always active for every query.  • AdaLoRA reallocates rank per weight matrix, again statically.  • Multi-Head Adapter Routing (MHR) and other routing works learn a task--module matrix Z, not a per-example gate, and in practice still pay the cost of evaluating all adapters because mixing weights are applied to their outputs.  • SHiRA, DoRA, Spectral Adapter, etc. keep full compute during inference.  The proposed CoGaLoRA combines (1) a learnable router that reads the current hidden state, (2) stochastic gating with a straight-through estimator, and (3) a sparsity regulariser that directly penalises expected FLOPs.  This yields input-conditional, zero-FLOP deactivation of adapters – a mechanism absent from existing methods.  The idea is therefore novel with respect to both adapter selection (now dynamic) and compute optimisation (now tied to the downstream loss rather than post-hoc pruning).",
        "novelty_score": 8,
        "significance_reason": "If successful, CoGaLoRA turns parameter-efficient fine-tuning into compute-efficient inference, which is crucial for deploying many specialised LLMs on latency- or energy-constrained devices.  Saving ~50 % average FLOPs while slightly improving accuracy would translate into lower cloud bills and wider on-device adoption.  Academically, it opens a new axis in PEFT research – conditional depth adaptation – and is orthogonal to rank- or spectral methods, so it can compound with future advances.  The method is simple, adds minimal parameters, and uses standard training tricks, making it easy to reproduce.  Limitations: initial experiments are on BERT-base and a single task; gains on multi-billion-parameter LLMs and real hardware remain to be shown, and routing overhead may offset savings in highly optimised kernels.  Hence the potential impact is high but still provisional.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "(1) PEFT methods still spend the same compute for every input because all surviving adapters are executed, and (2) they store a *fixed* adapter for every layer although most sentences only need a handful of small modifications.  We lack a mechanism that, for each example,\n  • decides *which* layers to adapt,\n  • decides *how large* (rank-k) an adaptation is needed there, and\n  • materialises the corresponding parameters on-the-fly so that inactive layers occupy exactly 0 FLOPs *and* 0 RAM.\nThe challenge is to learn such depth-and-rank decisions jointly with the downstream objective while respecting a global latency / memory budget.",
        "method": "Rank-And-Depth Adaptive LoRA (RAD-LoRA)\n1.  Per-layer router  f_ℓ  (2-layer MLP, hidden→8→2+r_max) takes the mean-pooled pre-adapter activation h_ℓ.\n    • First logit z0 chooses a *binary depth gate* ĝ_ℓ∈{0,1} (straight-through Bernoulli).\n    • The remaining r_max logits pass through softmax to obtain a categorical rank  k_ℓ∈{0,…,r_max}.  Rank 0 means no adapter, reproducing depth gating; higher k activates rank-k low-rank update.\n2.  HyperLoRA  g_ℓ  – a tiny hyper-network (hidden→4r_max·d_in) that outputs the vectorised A_ℓ and B_ℓ **conditioned on k_ℓ**.  Weight generation is masked so FLOPs scale with k.\n3.  Update rule  W′ = W₀ + ĝ_ℓ · (B_ℓ[:k]  A_ℓ[:k]).\n4.  Budget regulariser  λ_c · Σℓ ĝ_ℓ + λ_r · Σℓ E[k_ℓ]  enforces expected #active layers and expected total rank.\n5.  Inference – replace stochastic gates by hard argmax; only active layers materialise their A/B, others allocate no memory (weights are generated on-the-fly then discarded).\n6.  Optional cache – if the same (k_ℓ,h_ℓ) pair repeats, reuse the previously generated matrices.",
        "experimental_setup": "Model  BERT-base-uncased.\nTask   SST-2 classification.\nRouters  f_ℓ shared across all layers (parameter tying).\nHyperLoRA   single linear layer per Transformer block.\nMax rank r_max = 8.\nBaselines  (1) Full FT, (2) LoRA-8 all layers, (3) GaLoRA (static pruning), (4) CoGaLoRA (depth gates only).\nRAD-LoRA   λ_c∈{0,0.1,0.2}, λ_r∈{0,0.05,0.1}.\nTraining   AdamW 2e-5, 3 epochs, batch 32.\nMetrics   dev accuracy, avg #active layers, avg total rank, wall-time, profiled inference FLOPs & RAM on 256 sentences.",
        "primary_metric": "Dev-set accuracy under a fixed compute budget (≤25 % of full LoRA FLOPs).",
        "experimental_code": "class Router(nn.Module):\n    def __init__(self, h, r_max):\n        super().__init__(); self.fc = nn.Sequential(nn.Linear(h,8), nn.ReLU(), nn.Linear(8,2+r_max))\n        self.r_max = r_max\n    def forward(self,h):\n        logits = self.fc(h)                # [B,2+r_max]\n        gate_p = torch.sigmoid(logits[:,0:1])\n        rank_p = torch.softmax(logits[:,1:],-1)  # categorical over r_max+1 ranks\n        gate = ((torch.rand_like(gate_p)<gate_p).float()+gate_p.detach()-gate_p) # STE\n        rank_idx = torch.multinomial(rank_p,1).squeeze(-1)   # STE via gumbel-softmax optional\n        return gate, rank_idx, gate_p, rank_p\nclass HyperLoRA(nn.Module):\n    def __init__(self,h,d_in,d_out,r_max):\n        super().__init__(); self.gen = nn.Linear(h, (d_in+d_out)*r_max)\n        self.d_in, self.d_out, self.r_max = d_in,d_out,r_max\n    def forward(self, h, k):\n        vec = self.gen(h)                  # [B, r_max*(d_in+d_out)]\n        A,B = torch.split(vec, [self.d_in*self.r_max, self.d_out*self.r_max], dim=-1)\n        A = A.view(-1,self.r_max,self.d_in); B = B.view(-1,self.r_max,self.d_out)\n        idx = k.unsqueeze(-1).unsqueeze(-1)\n        A = A.gather(1, idx.expand(-1,1,self.d_in)).squeeze(1)\n        B = B.gather(1, idx.expand(-1,1,self.d_out)).squeeze(1)\n        return A,B\n# inside Transformer layer\nh_pool = x.mean(1)                     # [B,hidden]\ngate, k, gp, rp = self.router(h_pool)\nA,B = self.hyper(h_pool,k)             # shapes depend on k\nupdate = (B.transpose(1,2) @ A) / max(k,1)  # low-rank product\nout = x + gate.view(-1,1,1)*self.alpha*F.linear(x,update)\nloss = task_loss + λ_c*gp.mean() + λ_r*(rp*torch.arange(r_max+1,device=x.device)).mean()",
        "expected_result": "At λ_c=0.1, λ_r=0.05 ⇒ 92.5 % accuracy with ≈3 active layers and total rank ≈12 (vs 72 ranks for full LoRA).  Inference FLOPs ↓60 %, GPU memory for A/B ↓95 % (weights generated transiently).  Ablations show: (i) removing rank branch reverts to CoGaLoRA accuracy 92.0 % and higher compute; (ii) removing gate branch equals AdaLoRA-style rank adaptation but still evaluates all layers, giving only 25 % FLOP saving.",
        "expected_conclusion": "RAD-LoRA converts parameter-efficient fine-tuning into *instance-adaptive depth *and* width* tuning: every input triggers just the minimal set of low-rank updates, generated on demand.  This halves inference compute and almost eliminates adapter storage while matching or exceeding prior PEFT accuracy.  The framework unifies dynamic layer selection and dynamic rank allocation, and opens the door to stateless, cache-friendly hyper-generated adapters for large LLM deployment on memory-constrained devices."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT papers either (a) adapt *all* layers with a fixed low-rank (LoRA, DoRA), (b) prune/adapt the *rank* during training but still evaluate every adapter at inference (AdaLoRA, Spectral-Adapter), or (c) learn binary *depth* gates so only a subset of layers’ adapters are executed (CoGaLoRA, GaLoRA). None of them jointly decide depth *and* rank on a *per-example* basis, nor do they materialise adapter weights on-the-fly with a tiny hyper-network so that inactive layers consume 0 RAM/FLOPs. RAD-LoRA’s combination of (1) a stochastic router that outputs both a depth gate and a categorical rank for each layer, (2) hyper-generated A/B matrices conditioned on that rank, and (3) budget-regularised training that trades accuracy for expected compute, is therefore not covered by the listed works. It turns PEFT into conditional computation, a paradigm absent from the current literature.",
        "novelty_score": 9,
        "significance_reason": "The hypothesis tackles two practical blockers for deploying fine-tuned LMs on edge devices: run-time compute and cumulative adapter storage. By activating only ~3 of 12 layers and generating just the necessary rank-k updates, RAD-LoRA promises >50 % FLOP reduction and ~95 % RAM saving while matching baseline accuracy. Academically, it unifies dynamic layer selection and rank adaptation under a single differentiable objective, potentially opening a new research line on instance-adaptive fine-tuning. Societally, enabling memory-constrained or energy-efficient inference broadens accessibility of language models. Although initial experiments are on BERT/SST-2, the method is architecture-agnostic and could scale to LLMs, giving it high but not maximal significance.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "State-of-the-art PEFT methods still (1) materialise a *different* set of A/B matrices for every transformer layer and (2) decide activation independently per layer.  Even RAD-LoRA—the most dynamic model so far—stores r_max·d·L hyper-net weights and cannot exploit cross-layer redundancy: for a 6-B LLM the hyper-LoRA already occupies >400 MB, well above on-device limits.  We lack a mechanism that  \n  • learns a *global* library of low-rank update atoms that are reused by any layer,  \n  • composes and sparsely routes those atoms *per token* (not just per sentence) so that only ≈1 % of update FLOPs are paid,  \n  • keeps library size and routing cost sub-linear in L, and  \n  • jointly trains library, token routers and task head under a single budget.\n",
        "method": "Shared-Atom Token-Conditional LoRA (SAT-LoRA)  \n1.  Atom library   A set of M rank-1 atoms {u_m⊗v_m} with u_m∈ℝ^{d_out}, v_m∈ℝ^{d_in}.  M≪L·r_max (e.g. 128 for BERT-base).  All layers draw from the *same* library.\n2.  Layer-specific mixer   Each layer ℓ learns a tiny linear map S_ℓ∈ℝ^{r_max×M} that selects up to r_max atoms.  No per-example cost.\n3.  Token router   For every token i at layer ℓ, a 1-hidden-layer MLP reads its pre-adapter activation h_{ℓ,i} and outputs a sparse gate g_{ℓ,i}∈{0,1}^{r_max} via k-hot Gumbel-Top-k (k=1–2).  The effective update is  \n     ΔW_{ℓ,i}=∑_{j=1}^{r_max} g_{ℓ,i,j} · (∑_{m} S_{ℓ,jm} u_m) (∑_{m} S_{ℓ,jm} v_m)^⊤.  \n   Only the k selected rank-1 outer products are materialised.\n4.  Budget loss   L_task + λ_F ·E[#active atoms] + λ_M ·||S||₁  enforces FLOP and memory budgets.\n5.  Weight tying & quantisation   The atom vectors are quantised to 8-bit once trained; S_ℓ is row-sparse and stored as <1 % density CSR.\n6.  Inference   For each token the router produces integer indices; the corresponding u, v rows are streamed from a shared table and multiplied with the current hidden state—compatible with CUDA scatter-gather.  Unused atoms incur 0 RAM/FLOPs.\n",
        "experimental_setup": "Base model  BERT-base-uncased.  \nTasks     (1) SST-2, (2) CoNLL-2003 NER (tests token-level routing).  \nLibrary   M=128 atoms, r_max=4, k=2, 8-bit post-training quant.  \nBaselines  Full FT, LoRA-8 all layers, RAD-LoRA, SHiRA (2 % weights).  \nBudgets    Target ≤15 % of LoRA FLOPs and ≤2 MB adapter parameters.  \nTraining   AdamW 2 e-5, 3 epochs, batch 32.  λ_F tuned in {0,0.01,0.05}.  \nMetrics    Dev accuracy / F1, average active atoms per token, total adapter RAM, kernel-level measured FLOPs.\n",
        "primary_metric": "Task score under 2 MB adapter budget; secondary: inference FLOPs/token.",
        "experimental_code": "class AtomLibrary(nn.Module):\n    def __init__(self, d_in, d_out, M):\n        super().__init__(); self.u = nn.Parameter(torch.randn(M,d_out)); self.v = nn.Parameter(torch.randn(M,d_in))\n    def forward(self, idx):                    # idx: [...]\n        return self.u[idx], self.v[idx]\nclass LayerMixer(nn.Module):                    # one per layer\n    def __init__(self,M,r):\n        super().__init__(); self.S = nn.Parameter(torch.randn(r,M)*0.01)\n    def forward(self):                          # returns top-Mixed vectors\n        U = self.S @ lib.u                      # [r,d_out]\n        V = self.S @ lib.v                      # [r,d_in]\n        return U,V\nclass TokenRouter(nn.Module):\n    def __init__(self,h,r,k):\n        super().__init__(); self.k,self.fc = k,nn.Linear(h,r)\n    def forward(self,h):                        # h:[B,T,h]\n        logits = self.fc(h)                     # [B,T,r]\n        g = F.gumbel_softmax(logits,hard=True,tau=0.5)  # k-hot via top-k mask later\n        if self.k<g.size(-1):                   # retain largest k\n            topk = g.topk(self.k)[0]\n            g = (g>=topk[...,-1:]).float()\n        return g\n# inside Transformer layer ℓ\ng = router(h)                                   # [B,T,r]\nU,V = mixer()                                   # [r,d_out], [r,d_in]\nupd = torch.einsum('btr,rd,rbtd->btd', g,U, V@h.transpose(1,2))\noutput = h + alpha*upd",
        "expected_result": "With λ_F=0.02 SAT-LoRA stores 1.9 MB (quantised atoms+S) and activates ≈2 atoms/token ⇒ 11 % of LoRA FLOPs.  Accuracy: SST-2 92.6 %, CoNLL 91.0 F1—matching full LoRA (92.4 %, 91.1 F1) and outperforming RAD-LoRA under same compute (+0.4 %).  Ablation: removing shared atoms (+220 MB) shows same accuracy but 8× memory; disabling token routing (-3 %) drops F1 by 1.3.  ",
        "expected_conclusion": "SAT-LoRA turns adapter tuning into a sparse *dictionary learning* problem: one tiny global set of low-rank atoms suffices to adapt *all layers and all tokens* while paying compute only where needed.  This collapses adapter storage by two orders of magnitude and further cuts FLOPs beyond per-sentence gating, making LLM fine-tuning practical on edge GPUs and even mobile NPUs.  The study opens a new axis—*cross-layer weight sharing*—complementary to depth, rank and sparsity work, and suggests dictionary-based PEFT as a fruitful path for resource-scarce NLP deployments."
      },
      "evaluation": {
        "novelty_reason": "Existing PEFT methods (LoRA, AdaLoRA, RAD-LoRA, DoRA, SHiRA, etc.) still instantiate a distinct low-rank (or sparse) update for every weight matrix. Some works (e.g. Multi-Head Routing, Poly) share a library of adapters, but the sharing is across *tasks* and still stores a full set of LoRA matrices per transformer layer. None of them (a) factor every layer’s update through a *single global dictionary of rank-1 atoms*, (b) route these atoms *at token-time* rather than sentence- or task-time, and (c) couple this with FLOP-aware sparsity so that only k≪r atoms are materialised per token. SAT-LoRA therefore introduces two orthogonal ideas that are absent from the prior art: cross-layer parameter tying of LoRA atoms and fine-grained token-conditional activation. The proposal is closer in spirit to MoE/FMoE routing but is, to our knowledge, the first to apply that paradigm specifically to low-rank *adapters* and to show how it collapses adapter storage while retaining per-layer expressive capacity. This combination is not covered in the related works list.",
        "novelty_score": 8,
        "significance_reason": "If SAT-LoRA works as hypothesised it would cut adapter RAM from hundreds of MB to ∼2 MB for a 6-B LLM and slash inference FLOPs by routing only ≈1 % of the update paths. That makes on-device or mobile fine-tuning of current LLMs realistic, which is a strong practical benefit. Academically, it opens a new design dimension (cross-layer dictionary learning) that could be combined with rank/bit-width/sparsity techniques already studied, and provides a bridge between PEFT and dynamic-MoE research. The technique is generic to any transformer, requires only scatter-gather kernels, and keeps training fully differentiable, so adoption barrier is low. Impact is somewhat contingent on empirical verification beyond small BERT-base tests, but potential gains and conceptual shift merit high significance.",
        "significance_score": 8
      }
    },
    {
      "hypothesis": {
        "open_problems": "Even with a global dictionary of rank-1 LoRA atoms (SAT-LoRA) we still need to store a separate, often wide, mixing matrix S_ℓ for every transformer layer.  For LLMs with thousands of layers these mixers outweigh the atoms themselves.  In addition, current token routers ignore (i) the layer’s relative position and type and (ii) downstream-task identity, both of which carry strong signals for which atoms should be active.  We therefore lack a PEFT mechanism that\n• produces layer-specific atom mixtures from a *single small generator* instead of O(L·M) parameters,\n• conditions token routing on *both* token state and task embedding, enabling multi-task & continual fine-tuning, and\n• retains the budget‐aware, per-token sparsity benefits of SAT-LoRA.",
        "method": "Hierarchical Functional Shared-Atom LoRA (HF-SAT-LoRA)\n1. Global atom dictionary  A set {u_m⊗v_m}_{m=1..M} shared by all layers (e.g. M=128, rank-1, 8-bit).\n2. Functional mixer  A 2-layer MLP g_θ(depth, type)→ℝ^M (≈2 k parameters) outputs the *real-valued* mixing weights for any layer on the fly; no S_ℓ tensors are stored.  Depth is normalised to [0,1]; type∈{attention-Q,K,V,MLP} is one-hot.\n3. Sparse token-&-task router  For token i at layer ℓ, router r_φ(h_{ℓ,i}, E(task))→{0,1}^k chooses k≤2 atoms via Gumbel-Top-k.  φ is shared across layers; E(task) is a learned 32-d task embedding.\n4. Update  ΔW_{ℓ,i}=∑_{j=1}^{k}g_top(j)·(a_j b_j^⊤) where (a_j,b_j) are the selected atoms.\n5. Budget loss  L_task+λ_F·E[k]+λ_M·||θ||² keeps compute and generator size bounded.\n6. Continual tuning  For a new task we freeze u,v and φ and fine-tune only θ plus a *task-specific* 4-bit bias δθ (≈1 kB per task).",
        "experimental_setup": "Base models  (1) BERT-base (110 M, L=12) and (2) Llama-7B (L=32).\nTasks  GLUE-SST-2, CoNLL-2003, Amazon-Review (multi-domain), BoolQ.\nBudgets  ≤1 MB total adapter weights, ≤10 % extra FLOPs/token.\nBaselines  Full FT, standard LoRA-8, RAD-LoRA, SAT-LoRA.\nTraining  AdamW 2e-5, 3 epochs, batch 32.  λ_F tuned in {0,0.02}.\nMetrics  Primary: dev accuracy/F1 under budget.  Secondary: adapter RAM, active atoms/token, wall-time.",
        "primary_metric": "Task performance (accuracy/F1) with ≤1 MB adapter and ≤10 % extra FLOPs.",
        "experimental_code": "class MixerMLP(nn.Module):\n    def __init__(self,M):\n        super().__init__(); self.fc=nn.Sequential(nn.Linear(6,128),nn.ReLU(),nn.Linear(128,M))\n    def forward(self,depth,kind):          # depth∈[0,1], kind one-hot(5)\n        x=torch.cat([depth[:,None],kind],-1)\n        return self.fc(x)                  # [B,M]\nclass TokenTaskRouter(nn.Module):\n    def __init__(self,h_dim,k):\n        super().__init__(); self.k=k; self.fc=nn.Linear(h_dim+32,128)\n    def forward(self,h,task_emb):          # h:[B,T,H]\n        B,T,_=h.shape; te=task_emb[:,None].expand(-1,T,-1)\n        logits=self.fc(torch.cat([h,te],-1))\n        g=F.gumbel_softmax(logits,hard=True,tau=0.5,dim=-1)\n        topk=g.topk(self.k)[0][...,-1:]\n        return (g>=topk).float()           # k-hot mask",
        "expected_result": "HF-SAT-LoRA reaches SST-2 92.8 % (+0.2 over SAT) and CoNLL 91.3 F1 using 0.95 MB total adapter (128 atoms + θ,φ) and 2 atoms/token (≈9 % LoRA FLOPs).  Mixer generator saves 98 % of S_ℓ storage vs SAT on Llama-7B (0.9 MB vs 45 MB).  Continual learning adds <2 kB per task while keeping previous task accuracy within −0.3 %.",
        "expected_conclusion": "Replacing per-layer mixing matrices with a tiny *functional mixer* and adding task-aware routing collapses LoRA adapter storage to sub-megabyte scale *independent of model depth*, enabling multi-task on-device fine-tuning of modern LLMs.  HF-SAT-LoRA unifies cross-layer weight sharing, functional parameter generation and conditional computation, charting a path toward truly scalable, memory-agnostic PEFT."
      },
      "evaluation": {
        "novelty_reason": "None of the surveyed PEFT papers combines (i) a single small function that generates the per-layer LoRA mixing weights from *continuous layer metadata* (depth, sub-module type) with (ii) a *task- and token-conditional* sparse router that decides, at run time, which globally shared rank-1 atoms to apply.  \n• SAT-LoRA already shares a global atom dictionary, but still stores an O(L·M) dense mixing matrix per layer and has no task-aware routing.  \n• AdaLoRA/DoRA/Spectral-Adapter/SHiRA focus on how to choose or re-parameterise low-rank updates inside each layer; they keep one adapter tensor per trained layer.  \n• Multi-Head Routing (MHR) and HyperTuning use routing or hyper-networks, but routing is over *pre-learned task-specific adapters* rather than generating layer-dependent mixtures on-the-fly, and they do not condition on layer metadata.  \n• HyperTuning’s hyper-model produces a full set of prefixes/LoRA weights per task, not a shared functional generator reused across all layers and tasks.  \nTherefore HF-SAT-LoRA’s idea of replacing every S_ℓ with gθ(depth,type) and coupling it with a Gumbel Top-k router that also sees a learned task embedding is new among existing PEFT literature.",
        "novelty_score": 8,
        "significance_reason": "Adapter memory is already the principal bottleneck when trying to PEFT very deep LLMs (e.g. 7-70 B parameters with ≳1000 layers) or when wanting to store dozens of tasks on-device.  By collapsing storage from O(L·M) to O(M+|θ|) (≈1 MB irrespective of depth) while retaining SAT-LoRA’s FLOP-efficient per-token sparsity, the proposal directly tackles a practical scalability pain-point.  Task-aware routing further opens a path to continual and multi-task tuning without catastrophic memory growth.  If the claimed 98 % storage reduction on Llama-7B with <1 % accuracy drop holds, this would meaningfully widen the set of devices (edge GPUs, phones) on which modern LLM fine-tuning is feasible, and academically it offers a novel connection between parameter-generation, cross-layer sharing and conditional computation.  Risks and open questions (generator expressivity, latency of the router) temper the impact, but the potential gain for both research and deployment is substantial.",
        "significance_score": 7
      }
    },
    {
      "hypothesis": {
        "open_problems": "Existing cross-layer LoRA sharing schemes (SAT-LoRA, HF-SAT-LoRA) still presuppose a *fixed host model*: the functional mixer g_θ is learned **per architecture** and its output dimensionality is hard-wired to the in/out size of every weight matrix.  Hence the learned mixers cannot be reused across (i) different hidden sizes (e.g. BERT-base → DeBERTa-large), (ii) pruned or compressed variants, or (iii) future model versions that add more layers.  Practically this forces practitioners to retrain a new generator for each model, while societally it prevents community-shared, device-ready adapter libraries.  We lack a PEFT mechanism that 1) keeps the global atom dictionary *model-agnostic*, 2) generates layer-specific LoRA updates for *arbitrary* (d_in , d_out) on-the-fly, and 3) still fits into a sub-megabyte memory budget with per-token sparsity.",
        "method": "Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2 k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.",
        "experimental_setup": "Source pre-training  – BERT-base (L=12, d=768) on GLUE-SST-2 (3 epochs).  Target models  – DistilBERT-6L-d=768, BERT-large-24L-d=1024, and TinyBERT-4L-d=312.  Downstream tasks  – CoNLL-2003, BoolQ, Amazon-Review.  Baselines  – (a) SAT-LoRA retrained for every model, (b) standard 8-rank LoRA, (c) full fine-tune.  Budgets  – ≤1 MB total adapter weights, ≤10 % extra FLOPs.  Metrics  – dev F1 / accuracy, extra RAM, atoms · token, *zero-shot* performance before δθ tuning, and after 500 fine-tune steps.",
        "primary_metric": "Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).",
        "experimental_code": "class DimProjector(nn.Module):\n    def __init__(self,d0=256):\n        super().__init__(); self.d0=d0; self.mlp=nn.Sequential(nn.Linear(1,64),nn.SiLU(),nn.Linear(64,d0*d0))\n    def forward(self,dim):                       # dim: int\n        w=self.mlp(torch.log10(torch.tensor([[dim]],dtype=torch.float32)))\n        return w.view(dim,self.d0)\n\nclass UniHyperLoRA(nn.Module):\n    def __init__(self,M=128,d0=256):\n        super().__init__(); self.u=nn.Parameter(torch.randn(M,d0)); self.v=nn.Parameter(torch.randn(M,d0))\n        self.mix=nn.Sequential(nn.Linear(6,128),nn.SiLU(),nn.Linear(128,M))   # depth+one-hot(5)\n        self.dim_proj=DimProjector(d0)\n    def forward(self,h,meta):                     # h:[B,T,H]\n        depth,kind,task=meta\n        g=self.mix(torch.cat([depth[:,None],kind],-1))          # [B,M]\n        mask=(g.topk(2).indices)                               # k=2\n        U_sel=self.u[mask]; V_sel=self.v[mask]\n        P_out=self.dim_proj(h.size(-1)); P_in=self.dim_proj(h.size(-1)).t()\n        delta=torch.einsum('bmk,kd->bmd',U_sel,P_out)           # simplified demo\n        # full std LoRA update skipped for brevity\n        return h+delta",
        "expected_result": "1) Zero-shot: without any extra tuning UniHyperLoRA transfers to BERT-large with <1 % accuracy loss vs source model. 2) After 500 update steps on each task, UniHyperLoRA matches SAT-LoRA that was *retrained* for that architecture while using 97 % less additional memory (0.97 MB vs 33 MB) and identical FLOPs. 3) On TinyBERT the projector correctly shrinks atoms; accuracy +1.1 % over plain LoRA under the same budget.",
        "expected_conclusion": "A single tiny hyper-network that conditions on layer *dimensions* as well as depth and type lets one global LoRA dictionary adapt *any* Transformer family without retraining architecture-specific mixers.  UniHyperLoRA therefore turns adapter weights into true reusable assets—enabling community-shared, phone-sized PEFT libraries and dramatically lowering the entry barrier for fine-tuning ever-evolving language models."
      },
      "evaluation": {
        "novelty_reason": "None of the cited works (DoRA, AdaLoRA, SAT-LoRA variants, HyperTuning, MHR, SHiRA, CURROT, etc.) attempt to *decouple* the LoRA generator from the host model architecture. They all (i) learn separate LoRA parameters for each weight matrix or (ii) share adapters only inside a *single* backbone, assuming fixed (d_in,d_out). UniHyperLoRA instead proposes a two-level hyper-network: a size-aware projector that maps an architecture-independent 256-d atom dictionary into any target shape on-the-fly, and a tiny continuous mixer conditioned on depth/type that is shared across *all* Transformers. This enables zero-shot transfer of the same sub-MB adapter to models with different hidden sizes, compression levels, or layer counts—capability not present in the related literature. While HyperTuning also uses a hyper-model, it produces parameters for the *same* frozen backbone and does not handle arbitrary matrix sizes nor enforce a universal atom basis. Therefore the proposed mechanism, especially the dimension-aware projection plus sparse routing of shared atoms, is a novel contribution.",
        "novelty_score": 8,
        "significance_reason": "Academically, a model-agnostic PEFT module opens a new research avenue: studying factorised adapter dictionaries that generalise across architectures, which could shift the focus from per-model fine-tuning to reusable, plug-and-play knowledge modules. Societally, the ability to keep adapter libraries under 1 MB and reuse them on future or pruned models directly benefits on-device and low-resource deployment, lowering compute and storage barriers. If empirical claims hold (≈1 % zero-shot loss and 97 % memory saving vs SAT-LoRA), the method would materially improve rapid adaptation workflows and community sharing. Limitations (need for projector generalisation proofs, reliance on LoRA form) temper, but do not negate, its impact.",
        "significance_score": 8
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "Existing cross-layer LoRA sharing schemes (SAT-LoRA, HF-SAT-LoRA) still presuppose a *fixed host model*: the functional mixer g_θ is learned **per architecture** and its output dimensionality is hard-wired to the in/out size of every weight matrix.  Hence the learned mixers cannot be reused across (i) different hidden sizes (e.g. BERT-base → DeBERTa-large), (ii) pruned or compressed variants, or (iii) future model versions that add more layers.  Practically this forces practitioners to retrain a new generator for each model, while societally it prevents community-shared, device-ready adapter libraries.  We lack a PEFT mechanism that 1) keeps the global atom dictionary *model-agnostic*, 2) generates layer-specific LoRA updates for *arbitrary* (d_in , d_out) on-the-fly, and 3) still fits into a sub-megabyte memory budget with per-token sparsity.",
      "method": "Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2 k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.",
      "experimental_setup": "Source pre-training  – BERT-base (L=12, d=768) on GLUE-SST-2 (3 epochs).  Target models  – DistilBERT-6L-d=768, BERT-large-24L-d=1024, and TinyBERT-4L-d=312.  Downstream tasks  – CoNLL-2003, BoolQ, Amazon-Review.  Baselines  – (a) SAT-LoRA retrained for every model, (b) standard 8-rank LoRA, (c) full fine-tune.  Budgets  – ≤1 MB total adapter weights, ≤10 % extra FLOPs.  Metrics  – dev F1 / accuracy, extra RAM, atoms · token, *zero-shot* performance before δθ tuning, and after 500 fine-tune steps.",
      "primary_metric": "Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).",
      "experimental_code": "class DimProjector(nn.Module):\n    def __init__(self,d0=256):\n        super().__init__(); self.d0=d0; self.mlp=nn.Sequential(nn.Linear(1,64),nn.SiLU(),nn.Linear(64,d0*d0))\n    def forward(self,dim):                       # dim: int\n        w=self.mlp(torch.log10(torch.tensor([[dim]],dtype=torch.float32)))\n        return w.view(dim,self.d0)\n\nclass UniHyperLoRA(nn.Module):\n    def __init__(self,M=128,d0=256):\n        super().__init__(); self.u=nn.Parameter(torch.randn(M,d0)); self.v=nn.Parameter(torch.randn(M,d0))\n        self.mix=nn.Sequential(nn.Linear(6,128),nn.SiLU(),nn.Linear(128,M))   # depth+one-hot(5)\n        self.dim_proj=DimProjector(d0)\n    def forward(self,h,meta):                     # h:[B,T,H]\n        depth,kind,task=meta\n        g=self.mix(torch.cat([depth[:,None],kind],-1))          # [B,M]\n        mask=(g.topk(2).indices)                               # k=2\n        U_sel=self.u[mask]; V_sel=self.v[mask]\n        P_out=self.dim_proj(h.size(-1)); P_in=self.dim_proj(h.size(-1)).t()\n        delta=torch.einsum('bmk,kd->bmd',U_sel,P_out)           # simplified demo\n        # full std LoRA update skipped for brevity\n        return h+delta",
      "expected_result": "1) Zero-shot: without any extra tuning UniHyperLoRA transfers to BERT-large with <1 % accuracy loss vs source model. 2) After 500 update steps on each task, UniHyperLoRA matches SAT-LoRA that was *retrained* for that architecture while using 97 % less additional memory (0.97 MB vs 33 MB) and identical FLOPs. 3) On TinyBERT the projector correctly shrinks atoms; accuracy +1.1 % over plain LoRA under the same budget.",
      "expected_conclusion": "A single tiny hyper-network that conditions on layer *dimensions* as well as depth and type lets one global LoRA dictionary adapt *any* Transformer family without retraining architecture-specific mixers.  UniHyperLoRA therefore turns adapter weights into true reusable assets—enabling community-shared, phone-sized PEFT libraries and dramatically lowering the entry barrier for fine-tuning ever-evolving language models."
    },
    "iterations": [
      {
        "iteration_id": 1,
        "method": "Universal Hyper‐LoRA (UniHyperLoRA)  1. Canonical atom dictionary  U∈ℝ^{M×d₀}, V∈ℝ^{M×d₀} with small anchor dimension d₀=256 (8-bit, M=128). 2. Dimension-aware projectors  For a target weight of shape (d_out , d_in) we compute two thin, *factored* projectors:   P_out(d_out)∈ℝ^{d_out×d₀},  P_in(d_in)∈ℝ^{d₀×d_in}.  They are produced by a shared hyper-MLP h_ψ(size) that takes the log-scaled dimension as input (≈4 kB total). 3. Continuous mixer  g_θ(depth_norm, type_id)→ℝ^M (two-layer sine-activated MLP, 2 k params) outputs real‐valued mixing weights that are quantised to 4-bit at save-time. 4. Token- & task-conditional sparse router r_φ(h_{ℓ,i}, E(task))→Top-k(M) (k≤2) chooses active atoms. 5. Low-rank update  ΔW_{ℓ,i}=∑_{j=1}^k g_top(j)·P_out u_j v_jᵀ P_in.  All matrix multiplications are fused with the base forward pass. 6. Zero-shot portability  When UniHyperLoRA is first trained on a *source* model (e.g. BERT-base), only U,V,θ,ψ,φ are updated.  To adapt a *new* target model with unseen shapes we **freeze** U,V,ψ,φ and fine-tune a 4-bit bias δθ (≈0.5 kB) per task; no architecture-specific retraining. 7. Budget loss  L_task+λ_F·E[k]+λ_Q·‖δθ‖² keeps FLOPs and extra params bounded.",
        "experimental_design": {
          "experiment_summary": "We first pre-train one universal UniHyperLoRA adapter on a single 12-layer BERT-base model with the SST-2 corpus.  This pre-training jointly learns (1) the canonical atom dictionary U,V, (2) the tiny continuous mixer g_θ, (3) the dimension-hyper-network h_ψ that predicts thin projectors P_in / P_out from log-scaled layer sizes, and (4) the sparse router r_φ that chooses at most k=2 atoms per token.  Only U,V,θ,ψ,φ are updated; the host model’s weights stay frozen.  After three epochs we freeze everything except a 4-bit bias δθ.\n\nFor the transfer phase we load the *same* frozen adapter weights into three unseen target architectures (DistilBERT-6L-768, BERT-large-1024, TinyBERT-4L-312).  Because the projector h_ψ is dimension-aware, it instantly produces size-compatible low-rank updates for every (d_out,d_in) pair, so no re-training is needed (zero-shot evaluation).  We then run a short 500-step PEFT fine-tuning on each downstream task (CoNLL-2003 NER, BoolQ, Amazon-Review) updating only δθ.\n\nWe compare against a strong baseline—SAT-LoRA—that must be re-learnt per architecture, as well as classical rank-8 LoRA and full fine-tuning, while enforcing the shared resource budget of ≤1 MB extra weights and ≤10 % extra FLOPs.  All experiments are executed on a single NVIDIA A100/H200 with 80 GB VRAM; the unified adapter occupies 0.97 MB in 4-bit form and adds <8 % theoretical FLOPs, satisfying the constraint.\n\nAfter training we report: (a) dev-set performance of the target model with the adapter inserted but *before* any task-specific updates (zero-shot portability) and (b) the score after 500 δθ optimisation steps.  We also log memory overhead, FLOP overhead, and average active atoms per token.  Results are expected to show that the single UniHyperLoRA file achieves near-lossless zero-shot transfer (≤1 % drop) and, after light tuning, matches or exceeds SAT-LoRA while using ~97 % less additional memory.",
          "evaluation_metrics": [
            "Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).",
            "Memory overhead (MB)",
            "FLOP overhead (%)",
            "Active atoms per token",
            "F1 Score",
            "Accuracy"
          ],
          "proposed_method": "Universal Hyper-LoRA (UniHyperLoRA) is a model-agnostic parameter-efficient fine-tuning mechanism that decouples the learned LoRA dictionary from the host architecture.\n1. Atom dictionary: Two 8-bit matrices U,V ∈ ℝ^{M×d₀} with M=128 atoms and anchor width d₀=256 hold the canonical bases that will be reused across *all* layers and models.\n2. Dimension-aware projectors: A shared two-layer MLP h_ψ takes log10(d_in) or log10(d_out) as a scalar input and outputs flattened weights for thin projectors P_in(d_in) ∈ ℝ^{d₀×d_in} and P_out(d_out) ∈ ℝ^{d_out×d₀}.  Because the mapping is continuous, it can generate projectors for unseen dimensions at inference time, enabling cross-size portability.\n3. Continuous mixer g_θ: Given the normalised depth of the layer (ℓ/L) and a one-hot encoding of the sub-module type (Q,K,V,O,FFN), a tiny sine-activated MLP (≈2 k parameters) produces real-valued mixing weights over the M atoms.  These weights are quantised to 4-bit when saved.\n4. Sparse router r_φ: A light router attends over the current hidden state h_{ℓ,i} concatenated with a learned task embedding and selects the Top-k (k≤2) atoms to activate, enforcing sparsity.\n5. Low-rank update: For every weight W_{ℓ,i} we form ΔW_{ℓ,i}= Σ_{j=1..k} g_top(j) · P_out u_j v_jᵀ P_in, and fuse it with the host forward pass.\n6. Zero-shot portability: After training on a source model we freeze U,V,ψ,φ and only fine-tune a 4-bit bias δθ per task, achieving <1 MB total adapter size.\n7. Budget regularisation: The loss L = L_task + λ_F·E[k] + λ_Q·‖δθ‖² penalises FLOPs and bias magnitude to respect the deployment budget.",
          "comparative_methods": [
            "SAT-LoRA"
          ],
          "models_to_use": [],
          "datasets_to_use": [
            "CoNLL-2003"
          ],
          "hyperparameters_to_search": {
            "learning_rate": "5e-5-5e-4",
            "batch_size": "32,64,128",
            "lambda_F": "0.01-0.1",
            "lambda_Q": "1e-6-1e-4",
            "router_top_k": "1,2,3"
          },
          "external_resources": {
            "hugging_face": {
              "models": [],
              "datasets": [
                {
                  "id": "eriktks/conll2003",
                  "author": "eriktks",
                  "sha": "3f1cce917ab38486481b062921eb137e7bd3c205",
                  "created_at": "2022-03-02T23:29:22+00:00",
                  "last_modified": "2024-01-18T09:34:17+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 21175,
                  "likes": 154,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "conll2003.py"
                    }
                  ],
                  "card_data": {
                    "license": [
                      "other"
                    ],
                    "language": [
                      "en"
                    ],
                    "tags": [],
                    "datasets": [],
                    "task_categories": [
                      "token-classification"
                    ],
                    "size_categories": [
                      "10K<n<100K"
                    ],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "task_categories:token-classification",
                    "task_ids:named-entity-recognition",
                    "task_ids:part-of-speech",
                    "annotations_creators:crowdsourced",
                    "language_creators:found",
                    "multilinguality:monolingual",
                    "source_datasets:extended|other-reuters-corpus",
                    "language:en",
                    "license:other",
                    "size_categories:10K<n<100K",
                    "region:us"
                  ],
                  "readme": "---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- other\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-reuters-corpus\ntask_categories:\n- token-classification\ntask_ids:\n- named-entity-recognition\n- part-of-speech\npaperswithcode_id: conll-2003\npretty_name: CoNLL-2003\ndataset_info:\n  features:\n  - name: id\n    dtype: string\n  - name: tokens\n    sequence: string\n  - name: pos_tags\n    sequence:\n      class_label:\n        names:\n          '0': '\"'\n          '1': ''''''\n          '2': '#'\n          '3': $\n          '4': (\n          '5': )\n          '6': ','\n          '7': .\n          '8': ':'\n          '9': '``'\n          '10': CC\n          '11': CD\n          '12': DT\n          '13': EX\n          '14': FW\n          '15': IN\n          '16': JJ\n          '17': JJR\n          '18': JJS\n          '19': LS\n          '20': MD\n          '21': NN\n          '22': NNP\n          '23': NNPS\n          '24': NNS\n          '25': NN|SYM\n          '26': PDT\n          '27': POS\n          '28': PRP\n          '29': PRP$\n          '30': RB\n          '31': RBR\n          '32': RBS\n          '33': RP\n          '34': SYM\n          '35': TO\n          '36': UH\n          '37': VB\n          '38': VBD\n          '39': VBG\n          '40': VBN\n          '41': VBP\n          '42': VBZ\n          '43': WDT\n          '44': WP\n          '45': WP$\n          '46': WRB\n  - name: chunk_tags\n    sequence:\n      class_label:\n        names:\n          '0': O\n          '1': B-ADJP\n          '2': I-ADJP\n          '3': B-ADVP\n          '4': I-ADVP\n          '5': B-CONJP\n          '6': I-CONJP\n          '7': B-INTJ\n          '8': I-INTJ\n          '9': B-LST\n          '10': I-LST\n          '11': B-NP\n          '12': I-NP\n          '13': B-PP\n          '14': I-PP\n          '15': B-PRT\n          '16': I-PRT\n          '17': B-SBAR\n          '18': I-SBAR\n          '19': B-UCP\n          '20': I-UCP\n          '21': B-VP\n          '22': I-VP\n  - name: ner_tags\n    sequence:\n      class_label:\n        names:\n          '0': O\n          '1': B-PER\n          '2': I-PER\n          '3': B-ORG\n          '4': I-ORG\n          '5': B-LOC\n          '6': I-LOC\n          '7': B-MISC\n          '8': I-MISC\n  config_name: conll2003\n  splits:\n  - name: train\n    num_bytes: 6931345\n    num_examples: 14041\n  - name: validation\n    num_bytes: 1739223\n    num_examples: 3250\n  - name: test\n    num_bytes: 1582054\n    num_examples: 3453\n  download_size: 982975\n  dataset_size: 10252622\ntrain-eval-index:\n- config: conll2003\n  task: token-classification\n  task_id: entity_extraction\n  splits:\n    train_split: train\n    eval_split: test\n  col_mapping:\n    tokens: tokens\n    ner_tags: tags\n  metrics:\n  - type: seqeval\n    name: seqeval\n---\n\n# Dataset Card for \"conll2003\"\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** [https://www.aclweb.org/anthology/W03-0419/](https://www.aclweb.org/anthology/W03-0419/)\n- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n- **Size of downloaded dataset files:** 4.85 MB\n- **Size of the generated dataset:** 10.26 MB\n- **Total amount of disk used:** 15.11 MB\n\n### Dataset Summary\n\nThe shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\nnot belong to the previous three groups.\n\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\ntagging scheme, whereas the original dataset uses IOB1.\n\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n\n### Supported Tasks and Leaderboards\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Languages\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n## Dataset Structure\n\n### Data Instances\n\n#### conll2003\n\n- **Size of downloaded dataset files:** 4.85 MB\n- **Size of the generated dataset:** 10.26 MB\n- **Total amount of disk used:** 15.11 MB\n\nAn example of 'train' looks as follows.\n\n```\n{\n    \"chunk_tags\": [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0],\n    \"id\": \"0\",\n    \"ner_tags\": [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    \"pos_tags\": [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7],\n    \"tokens\": [\"The\", \"European\", \"Commission\", \"said\", \"on\", \"Thursday\", \"it\", \"disagreed\", \"with\", \"German\", \"advice\", \"to\", \"consumers\", \"to\", \"shun\", \"British\", \"lamb\", \"until\", \"scientists\", \"determine\", \"whether\", \"mad\", \"cow\", \"disease\", \"can\", \"be\", \"transmitted\", \"to\", \"sheep\", \".\"]\n}\n```\n\nThe original data files have `-DOCSTART-` lines used to separate documents, but these lines are removed here.\nIndeed `-DOCSTART-` is a special line that acts as a boundary between two different documents, and it is filtered out in this implementation.\n\n### Data Fields\n\nThe data fields are the same among all splits.\n\n#### conll2003\n- `id`: a `string` feature.\n- `tokens`: a `list` of `string` features.\n- `pos_tags`: a `list` of classification labels (`int`). Full tagset with indices:\n\n```python\n{'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n 'WP': 44, 'WP$': 45, 'WRB': 46}\n```\n\n- `chunk_tags`: a `list` of classification labels (`int`). Full tagset with indices:\n\n```python\n{'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6, 'B-INTJ': 7, 'I-INTJ': 8,\n 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17,\n 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}\n```\n\n- `ner_tags`: a `list` of classification labels (`int`). Full tagset with indices:\n\n```python\n{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n```\n\n### Data Splits\n\n|  name   |train|validation|test|\n|---------|----:|---------:|---:|\n|conll2003|14041|      3250|3453|\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n#### Who are the source language producers?\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n#### Who are the annotators?\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Personal and Sensitive Information\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Discussion of Biases\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Other Known Limitations\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)\n\n### Licensing Information\n\nFrom the [CoNLL2003 shared task](https://www.clips.uantwerpen.be/conll2003/ner/) page:\n\n> The English data is a collection of news wire articles from the Reuters Corpus. The annotation has been done by people of the University of Antwerp. Because of copyright reasons we only make available the annotations. In order to build the complete data sets you will need access to the Reuters Corpus. It can be obtained for research purposes without any charge from NIST.\n\nThe copyrights are defined below, from the [Reuters Corpus page](https://trec.nist.gov/data/reuters/reuters.html):\n\n> The stories in the Reuters Corpus are under the copyright of Reuters Ltd and/or Thomson Reuters, and their use is governed by the following agreements:\n>\n> [Organizational agreement](https://trec.nist.gov/data/reuters/org_appl_reuters_v4.html)\n>\n> This agreement must be signed by the person responsible for the data at your organization, and sent to NIST.\n>\n> [Individual agreement](https://trec.nist.gov/data/reuters/ind_appl_reuters_v4.html)\n>\n> This agreement must be signed by all researchers using the Reuters Corpus at your organization, and kept on file at your organization.\n\n### Citation Information\n\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}\n\n```\n\n\n### Contributions\n\nThanks to [@jplu](https://github.com/jplu), [@vblagoje](https://github.com/vblagoje), [@lhoestq](https://github.com/lhoestq) for adding this dataset."
                }
              ]
            }
          },
          "experiment_code": {
            "train_py": "import os\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Optional, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport hydra\nfrom omegaconf import OmegaConf, DictConfig\n\nimport wandb\nimport optuna\n\nfrom .preprocess import build_dataloaders\nfrom .model import (\n    build_model_with_adapters,\n    compute_adapter_params,\n)\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    import random, numpy as np\n\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _apply_mode_overrides(cfg: DictConfig) -> None:\n    \"\"\"Apply trial/full specific overrides in-place.\"\"\"\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.max_steps = 2\n        cfg.training.epochs = 1\n        cfg.evaluation.eval_steps = 1\n        cfg.training.batch_size = min(int(cfg.training.batch_size), 2)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be 'trial' or 'full'\")\n\n\ndef _ensure_results_dir(cfg: DictConfig) -> None:\n    cfg.results_dir = os.path.abspath(cfg.results_dir)\n    os.makedirs(cfg.results_dir, exist_ok=True)\n\n\n# -----------------------------------------------------------------------------\n# Evaluation helper\n# -----------------------------------------------------------------------------\n\ndef _evaluate(\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    device: torch.device,\n    metric_name: str,\n) -> Dict[str, float]:\n    import evaluate as hf_evaluate\n\n    metric = hf_evaluate.load(metric_name, cache_dir=CACHE_DIR)\n    model.eval()\n\n    id2label: Dict[int, str] = {\n        int(k): v for k, v in getattr(model.config, \"id2label\", {}).items()\n    }\n\n    for batch in dataloader:\n        labels = batch.pop(\"labels\").to(device)\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            logits = model(**batch).logits\n        preds = logits.argmax(-1).cpu()\n        labels_cpu = labels.cpu()\n\n        if metric_name == \"seqeval\":\n            preds_list: List[List[str]] = []\n            refs_list: List[List[str]] = []\n            for pred_seq, label_seq in zip(preds, labels_cpu):\n                p_sent: List[str] = []\n                l_sent: List[str] = []\n                for p_id, l_id in zip(pred_seq.tolist(), label_seq.tolist()):\n                    if l_id == -100:\n                        continue\n                    p_sent.append(id2label.get(p_id, str(p_id)))\n                    l_sent.append(id2label.get(l_id, str(l_id)))\n                preds_list.append(p_sent)\n                refs_list.append(l_sent)\n            metric.add_batch(predictions=preds_list, references=refs_list)\n        else:\n            metric.add_batch(predictions=preds, references=labels_cpu)\n\n    result = metric.compute()\n    if metric_name == \"seqeval\":\n        return {\n            \"f1\": result.get(\"overall_f1\", 0.0),\n            \"accuracy\": result.get(\"overall_accuracy\", 0.0),\n        }\n    if isinstance(result, dict):\n        return result\n    return {metric_name: float(result)}\n\n\n# -----------------------------------------------------------------------------\n# Training utilities\n# -----------------------------------------------------------------------------\n\ndef _grad_global_norm(model: torch.nn.Module) -> float:\n    grads: List[torch.Tensor] = [p.grad for p in model.parameters() if p.grad is not None]\n    if not grads:\n        return 0.0\n    return torch.norm(torch.stack([g.norm(2) for g in grads]), 2).item()\n\n\ndef _training_step(\n    model: torch.nn.Module,\n    batch: Dict[str, torch.Tensor],\n    optimizer: torch.optim.Optimizer,\n    scheduler: Optional[torch.optim.lr_scheduler.LambdaLR],\n    device: torch.device,\n    scaler: Optional[torch.cuda.amp.GradScaler] = None,\n) -> Tuple[float, float]:\n    \"\"\"Returns (loss, grad_norm).\"\"\"\n    model.train()\n    labels = batch.pop(\"labels\").to(device)\n    batch = {k: v.to(device) for k, v in batch.items()}\n\n    if scaler is not None:\n        with torch.cuda.amp.autocast():\n            loss = model(**batch, labels=labels).loss\n        scaler.scale(loss).backward()\n        grad_norm = _grad_global_norm(model)\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        loss = model(**batch, labels=labels).loss\n        loss.backward()\n        grad_norm = _grad_global_norm(model)\n        optimizer.step()\n\n    optimizer.zero_grad(set_to_none=True)\n    if scheduler is not None:\n        scheduler.step()\n    return loss.item(), grad_norm\n\n\n# -----------------------------------------------------------------------------\n# Optuna helpers\n# -----------------------------------------------------------------------------\n\ndef _suggest_and_apply(trial: optuna.Trial, cfg: DictConfig):\n    search_space: Dict[str, Any] = cfg.optuna.get(\"search_space\", {})\n    for path, space in search_space.items():\n        if space[\"type\"] == \"loguniform\":\n            value = trial.suggest_float(path, space[\"low\"], space[\"high\"], log=True)\n        elif space[\"type\"] == \"uniform\":\n            value = trial.suggest_float(path, space[\"low\"], space[\"high\"], log=False)\n        elif space[\"type\"] == \"categorical\":\n            value = trial.suggest_categorical(path, space[\"choices\"])\n        elif space[\"type\"] == \"int\":\n            value = trial.suggest_int(path, space[\"low\"], space[\"high\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna space type {space['type']}\")\n        OmegaConf.update(cfg, path, value, merge=True)\n\n\ndef _objective(\n    trial: optuna.Trial,\n    cfg: DictConfig,\n    model_init_fn,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    device: torch.device,\n):\n    cfg_tmp = OmegaConf.deepcopy(cfg)\n    _suggest_and_apply(trial, cfg_tmp)\n\n    model = model_init_fn(cfg_tmp)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg_tmp.training.learning_rate,\n        weight_decay=cfg_tmp.training.weight_decay,\n    )\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    max_steps = min(100, cfg_tmp.training.max_steps)\n    eval_every = max(10, max_steps // 5)\n\n    train_iter = iter(train_loader)\n    best_val = -float(\"inf\")\n    step = 0\n    while step < max_steps:\n        try:\n            batch = next(train_iter)\n        except StopIteration:\n            train_iter = iter(train_loader)\n            batch = next(train_iter)\n\n        loss, _ = _training_step(model, batch, optimizer, None, device, scaler)\n        step += 1\n        if step % eval_every == 0 or step == max_steps:\n            metrics = _evaluate(model, val_loader, device, cfg_tmp.evaluation.metric)\n            key = \"f1\" if \"f1\" in metrics else list(metrics.keys())[0]\n            val_score = metrics[key]\n            trial.report(val_score, step)\n            best_val = max(best_val, val_score)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n    return best_val\n\n\n# -----------------------------------------------------------------------------\n# wandb helpers\n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=getattr(cfg, \"run_id\", cfg.run),\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n        dir=cfg.results_dir,\n    )\n    print(f\"wandb URL: {run.url}\")\n    return run\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Basic sanity ----------------------------------------------------------------\n    if cfg.run is None:\n        raise ValueError(\"Parameter 'run' must be supplied: python -m src.train run=<run_id> ...\")\n\n    _apply_mode_overrides(cfg)\n    _ensure_results_dir(cfg)\n\n    # Determinism -----------------------------------------------------------------\n    set_seed(int(cfg.training.seed))\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Data ------------------------------------------------------------------------\n    train_loader, val_loader, label_list = build_dataloaders(cfg, CACHE_DIR)\n    num_labels = len(label_list)\n\n    def model_init(local_cfg: DictConfig):\n        return build_model_with_adapters(local_cfg, num_labels, device)\n\n    # Optuna hyper-parameter search ----------------------------------------------\n    if cfg.optuna.n_trials > 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(\n            lambda trial: _objective(\n                trial, cfg, model_init, train_loader, val_loader, device\n            ),\n            n_trials=int(cfg.optuna.n_trials),\n        )\n        for k, v in study.best_trial.params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n        print(\n            f\"Optuna best trial {study.best_trial.number}: value={study.best_value:.4f} params={study.best_trial.params}\"\n        )\n\n    # Final training -------------------------------------------------------------\n    model = model_init(cfg)\n    optimizer = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n    )\n    lr_lambda = lambda cur: max(0.0, 1.0 - (cur / cfg.training.max_steps))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n\n    wb_run = _init_wandb(cfg)\n    if wb_run is not None:\n        wb_run.summary[\"adapter_param_MB\"] = compute_adapter_params(model)\n\n    global_step = 0\n    pbar = tqdm(total=cfg.training.max_steps, desc=\"Training\")\n    while global_step < cfg.training.max_steps:\n        for batch in train_loader:\n            loss_val, grad_norm_val = _training_step(\n                model, batch, optimizer, scheduler, device, scaler\n            )\n            global_step += 1\n            pbar.update(1)\n\n            # Logging -----------------------------------------------------------\n            if wb_run is not None:\n                metrics_to_log = {\n                    \"train_loss\": loss_val,\n                    \"grad_norm\": grad_norm_val,\n                    \"lr\": optimizer.param_groups[0][\"lr\"],\n                    \"step\": global_step,\n                }\n                if torch.cuda.is_available():\n                    metrics_to_log[\"gpu_mem_mb\"] = torch.cuda.memory_allocated() / (1024 ** 2)\n                wandb.log(metrics_to_log, step=global_step)\n\n            # Evaluation --------------------------------------------------------\n            if (\n                global_step % cfg.evaluation.eval_steps == 0\n                or global_step == cfg.training.max_steps\n            ):\n                eval_metrics = _evaluate(\n                    model, val_loader, device, cfg.evaluation.metric\n                )\n                if wb_run is not None:\n                    wandb.log(\n                        {f\"eval_{k}\": v for k, v in eval_metrics.items()},\n                        step=global_step,\n                    )\n            if global_step >= cfg.training.max_steps:\n                break\n    pbar.close()\n\n    final_metrics = _evaluate(model, val_loader, device, cfg.evaluation.metric)\n    print(f\"Final metrics: {final_metrics}\")\n\n    # -------------------------------------------------------------------------\n    # Save checkpoint and finish wandb\n    # -------------------------------------------------------------------------\n    if wb_run is not None:\n        for k, v in final_metrics.items():\n            wb_run.summary[f\"final_{k}\"] = v\n\n        model_fp = Path(cfg.results_dir) / f\"{cfg.run}_final_model.pth\"\n        torch.save(model.state_dict(), model_fp)\n        artifact = wandb.Artifact(name=f\"{cfg.run}_model\", type=\"model\")\n        artifact.add_file(str(model_fp))\n        wb_run.log_artifact(artifact)\n        wb_run.finish()\n\n\nif __name__ == \"__main__\":\n    main()",
            "evaluate_py": "import argparse\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Any\n\nimport torch  # Must be imported before use throughout file\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import (\n    confusion_matrix,\n)\n\nfrom .preprocess import build_dataloaders\nfrom .model import build_model_with_adapters\n\nPRIMARY_METRIC_STRING = (\n    \"Target-model dev score under 1 MB / 10 %-FLOP budget (report zero-shot and post-tune).\"\n)\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Helpers\n# -----------------------------------------------------------------------------\n\ndef _load_wandb_credentials():\n    cfg_file = Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\"\n    cfg = OmegaConf.load(cfg_file)\n    return cfg.wandb.entity, cfg.wandb.project\n\n\ndef _sanitize(s: str) -> str:\n    \"\"\"Return a safe filename component (no slashes, spaces, etc.).\"\"\"\n    return re.sub(r\"[^A-Za-z0-9\\-_]\", \"_\", s)\n\n\ndef _fetch_run(rid: str, entity: str, project: str):\n    api = wandb.Api()\n    return api.run(f\"{entity}/{project}/{rid}\")\n\n\n# -----------------------------------------------------------------------------\n# In-depth run processing helpers\n# -----------------------------------------------------------------------------\n\ndef _reconstruct_cfg(wandb_cfg: Dict[str, Any]) -> OmegaConf:\n    \"\"\"Convert the flattened wandb config dict back into OmegaConf style.\"\"\"\n    cfg_nested: Dict[str, Any] = {}\n    for k, v in wandb_cfg.items():\n        cur = cfg_nested\n        parts = k.split(\".\")\n        for p in parts[:-1]:\n            cur = cur.setdefault(p, {})\n        cur[parts[-1]] = v\n    return OmegaConf.create(cfg_nested)\n\n\ndef _generate_predictions(\n    cfg: OmegaConf, model_state_path: str\n) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n    \"\"\"Load model weights and dataset, then produce predictions (validation split).\"\"\"\n    device = \"cpu\"\n\n    # Build loaders & model ----------------------------------------------------\n    loaders = build_dataloaders(cfg, CACHE_DIR)\n    _, val_loader, label_list = loaders\n    num_labels = len(label_list)\n\n    model = build_model_with_adapters(cfg, num_labels=num_labels, device=device)\n    state_dict = torch.load(model_state_path, map_location=device)\n    model.load_state_dict(state_dict, strict=False)\n    model.eval()\n\n    preds_all: List[int] = []\n    labels_all: List[int] = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            labels = batch.pop(\"labels\")\n            batch = {k: v for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            preds = logits.argmax(-1)\n            if cfg.evaluation.metric == \"seqeval\":\n                for p_seq, l_seq in zip(preds, labels):\n                    for p_id, l_id in zip(p_seq.tolist(), l_seq.tolist()):\n                        if l_id == -100:\n                            continue\n                        preds_all.append(p_id)\n                        labels_all.append(l_id)\n            else:\n                preds_all.extend(preds.tolist())\n                labels_all.extend(labels.tolist())\n    return np.array(preds_all), np.array(labels_all), label_list\n\n\ndef _plot_confusion_matrix(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    labels: List[str],\n    save_path: Path,\n):\n    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=labels,\n        yticklabels=labels,\n    )\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\n\n# -----------------------------------------------------------------------------\n# STEP-1: Per-run processing\n# -----------------------------------------------------------------------------\n\ndef _export_metrics_and_figures(run, save_dir: Path) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Export history & generate per-run figures. Returns (preds, labels) if computed.\"\"\"\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # 1. Save history & summary JSON ------------------------------------------\n    history_df = run.history()\n    summary_dict = dict(run.summary._json_dict)\n    cfg_dict = dict(run.config)\n\n    with open(save_dir / \"metrics.json\", \"w\") as fp:\n        json.dump({\"summary\": summary_dict, \"config\": cfg_dict}, fp, indent=2)\n    history_df.to_parquet(save_dir / \"history.parquet\")\n\n    # 2. Learning-curve figure -------------------------------------------------\n    fig_fp = save_dir / f\"{_sanitize(run.id)}_learning_curve.pdf\"\n    plt.figure()\n    metric_cols = [c for c in history_df.columns if re.match(r\"(train|eval)_.*\", str(c))]\n    for col in metric_cols:\n        sns.lineplot(x=history_df.index, y=history_df[col], label=col)\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Metric value\")\n    plt.title(run.id)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(fig_fp)\n    plt.close()\n    print(f\"Saved figure: {fig_fp}\")\n\n    # 3. Confusion matrix ------------------------------------------------------\n    preds_np = labels_np = None\n    try:\n        art = next(a for a in run.logged_artifacts() if a.type == \"model\")\n        model_dir = art.download()\n        state_path = next(Path(model_dir).glob(\"*.pth\"))\n        cfg = _reconstruct_cfg(cfg_dict)\n        preds_np, labels_np, label_list = _generate_predictions(cfg, str(state_path))\n\n        cm_path = save_dir / f\"{_sanitize(run.id)}_confusion_matrix.pdf\"\n        _plot_confusion_matrix(labels_np, preds_np, label_list, cm_path)\n        print(f\"Saved figure: {cm_path}\")\n\n        # Save raw preds / labels\n        np.save(save_dir / \"preds.npy\", preds_np)\n        np.save(save_dir / \"labels.npy\", labels_np)\n    except StopIteration:\n        print(f\"[WARN] No model artifact found for run {run.id}; skipping confusion matrix.\")\n    except Exception as e:\n        print(f\"[WARN] Failed to generate confusion matrix for {run.id}: {e}\")\n\n    return preds_np, labels_np\n\n\n# -----------------------------------------------------------------------------\n# STEP-2: Aggregated analysis & comparison\n# -----------------------------------------------------------------------------\n\ndef _mcnemar(y1: np.ndarray, y2: np.ndarray, y_true: np.ndarray) -> Dict[str, Any]:\n    \"\"\"McNemar's exact binomial test between two classifiers.\"\"\"\n    assert y1.shape == y2.shape == y_true.shape\n    n01 = ((y1 == y_true) & (y2 != y_true)).sum()\n    n10 = ((y1 != y_true) & (y2 == y_true)).sum()\n    from math import comb\n\n    n = n01 + n10\n    if n == 0:\n        p_val = 1.0\n    else:\n        k = min(n01, n10)\n        p_val = 2 * sum(comb(n, i) * (0.5 ** n) for i in range(0, k + 1))\n        p_val = min(1.0, p_val)\n    return {\"n01\": int(n01), \"n10\": int(n10), \"p_value\": float(p_val)}\n\n\ndef _aggregate(runs: Dict[str, wandb.apis.public.Run], out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    metrics_by_name: Dict[str, Dict[str, float]] = {}\n    preds_map: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}\n\n    # Collect numerical summary metrics --------------------------------------\n    for rid, run in runs.items():\n        summary = run.summary._json_dict\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                metrics_by_name.setdefault(k, {})[rid] = v\n        run_dir = out_dir.parent / _sanitize(rid)\n        preds_fp = run_dir / \"preds.npy\"\n        labels_fp = run_dir / \"labels.npy\"\n        if preds_fp.exists() and labels_fp.exists():\n            preds_map[rid] = (np.load(preds_fp), np.load(labels_fp))\n\n    # Primary metric selection ------------------------------------------------\n    pref_keys = [\n        \"final_f1\",\n        \"final_accuracy\",\n        \"f1\",\n        \"accuracy\",\n        \"eval_f1\",\n    ]\n    selected_key = next((k for k in pref_keys if k in metrics_by_name), None)\n    if selected_key is None:\n        raise RuntimeError(\"Cannot find a common metric across runs.\")\n\n    proposed_vals = {\n        rid: v for rid, v in metrics_by_name[selected_key].items() if \"proposed\" in rid\n    }\n    baseline_vals = {\n        rid: v\n        for rid, v in metrics_by_name[selected_key].items()\n        if any(tok in rid for tok in [\"baseline\", \"comparative\"])\n    }\n\n    best_proposed_id = max(proposed_vals, key=proposed_vals.get)\n    best_baseline_id = max(baseline_vals, key=baseline_vals.get)\n    best_proposed_val = proposed_vals[best_proposed_id]\n    best_baseline_val = baseline_vals[best_baseline_id]\n    gap_pct = (best_proposed_val - best_baseline_val) / best_baseline_val * 100.0\n\n    aggregate = {\n        \"primary_metric\": PRIMARY_METRIC_STRING,\n        \"metrics\": metrics_by_name,\n        \"best_proposed\": {\"run_id\": best_proposed_id, \"value\": best_proposed_val},\n        \"best_baseline\": {\"run_id\": best_baseline_id, \"value\": best_baseline_val},\n        \"gap\": gap_pct,\n    }\n\n    # Statistical significance -----------------------------------------------\n    if best_proposed_id in preds_map and best_baseline_id in preds_map:\n        yp, yt = preds_map[best_proposed_id]\n        yb, _ = preds_map[best_baseline_id]\n        sig = _mcnemar(yp, yb, yt)\n        aggregate[\"stat_tests\"] = {\"mcnemar\": sig}\n    else:\n        aggregate[\"stat_tests\"] = {\"mcnemar\": \"NA\"}\n\n    out_fp = out_dir / \"aggregated_metrics.json\"\n    with open(out_fp, \"w\") as fp:\n        json.dump(aggregate, fp, indent=2)\n    print(f\"Aggregated metrics saved → {out_fp}\")\n\n    # Comparison figures ------------------------------------------------------\n    bar_fig = out_dir / f\"comparison_{_sanitize(selected_key)}_bar_chart.pdf\"\n    plt.figure(figsize=(10, 6))\n    sns.barplot(\n        x=list(metrics_by_name[selected_key].keys()),\n        y=list(metrics_by_name[selected_key].values()),\n        palette=\"viridis\",\n    )\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(selected_key)\n    plt.title(\"Comparison on \" + selected_key)\n    for i, v in enumerate(metrics_by_name[selected_key].values()):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.tight_layout()\n    plt.savefig(bar_fig)\n    plt.close()\n    print(f\"Saved figure: {bar_fig}\")\n\n    metrics_df = pd.DataFrame(metrics_by_name)\n    melted = metrics_df.melt(var_name=\"metric\", value_name=\"value\", ignore_index=False)\n    melted = melted.reset_index().rename(columns={\"index\": \"run_id\"})\n    box_fig = out_dir / \"comparison_all_metrics_boxplot.pdf\"\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(data=melted, x=\"metric\", y=\"value\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Value\")\n    plt.title(\"Metric distribution across runs\")\n    plt.tight_layout()\n    plt.savefig(box_fig)\n    plt.close()\n    print(f\"Saved figure: {box_fig}\")\n\n\n# -----------------------------------------------------------------------------\n# CLI\n# -----------------------------------------------------------------------------\n\ndef _parse_args():\n    p = argparse.ArgumentParser(\n        description=\"Evaluate multiple wandb runs and generate comparison reports.\"\n    )\n    p.add_argument(\"results_dir\", type=str)\n    p.add_argument(\n        \"run_ids\",\n        type=str,\n        help=\"JSON string list of run IDs, e.g. '[\\\"run1\\\", \\\"run2\\\"]'\",\n    )\n    return p.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = _parse_args()\n    results_root = Path(args.results_dir)\n    run_id_list: List[str] = json.loads(args.run_ids)\n\n    entity, project = _load_wandb_credentials()\n\n    runs: Dict[str, wandb.apis.public.Run] = {}\n    for rid in run_id_list:\n        run = _fetch_run(rid, entity, project)\n        runs[rid] = run\n        _export_metrics_and_figures(run, results_root / _sanitize(rid))\n\n    _aggregate(runs, results_root / \"comparison\")",
            "preprocess_py": "from typing import Tuple, List\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n)\nfrom torch.utils.data import DataLoader\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\n\n# -----------------------------------------------------------------------------\n# Token-classification helpers\n# -----------------------------------------------------------------------------\n\ndef _tokenize_align_labels(examples, tokenizer, label_all_tokens: bool):\n    tokenized = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        max_length=tokenizer.model_max_length,\n    )\n    labels = []\n    for i, label_seq in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_seq[word_idx])\n            else:\n                label_ids.append(label_seq[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized[\"labels\"] = labels\n    return tokenized\n\n\n# -----------------------------------------------------------------------------\n# Data-loading entry point\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(cfg: DictConfig, cache_dir: str):\n    ds_cfg = cfg.dataset\n    tokenizer = AutoTokenizer.from_pretrained(\n        cfg.model.host_model_name, cache_dir=cache_dir, use_fast=True\n    )\n\n    task_name = ds_cfg.name.lower()\n\n    # Trial-mode dataset reduction -------------------------------------------\n    limit = 64 if getattr(cfg, \"mode\", \"full\") == \"trial\" else None\n\n    if task_name == \"conll2003\":\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n        if limit is not None:\n            train_ds = train_ds.select(range(min(len(train_ds), limit)))\n            val_ds = val_ds.select(range(min(len(val_ds), limit)))\n        train_ds = train_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=train_ds.column_names,\n        )\n        val_ds = val_ds.map(\n            lambda ex: _tokenize_align_labels(ex, tokenizer, False),\n            batched=True,\n            remove_columns=val_ds.column_names,\n        )\n        collator = DataCollatorForTokenClassification(tokenizer)\n        label_list = raw[\"train\"].features[\"ner_tags\"].feature.names\n    else:\n        raw = load_dataset(ds_cfg.hf_id, cache_dir=cache_dir)\n        train_ds = raw[ds_cfg.splits.train]\n        val_ds = raw[ds_cfg.splits.validation]\n        if limit is not None:\n            train_ds = train_ds.select(range(min(len(train_ds), limit)))\n            val_ds = val_ds.select(range(min(len(val_ds), limit)))\n\n        def tok_fn(ex):\n            out = tokenizer(ex[\"text\"], truncation=True, max_length=ds_cfg.max_length)\n            out[\"labels\"] = ex[\"label\"]\n            return out\n\n        train_ds = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names)\n        val_ds = val_ds.map(tok_fn, batched=True, remove_columns=val_ds.column_names)\n        collator = DataCollatorWithPadding(tokenizer)\n        label_list = raw[\"train\"].features[\"label\"].names\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=int(cfg.training.batch_size),\n        shuffle=True,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=int(cfg.training.batch_size),\n        shuffle=False,\n        collate_fn=collator,\n    )\n    return train_loader, val_loader, label_list",
            "model_py": "import math\nfrom typing import List\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import AutoConfig, AutoModelForTokenClassification\nfrom omegaconf import DictConfig\n\n__all__ = [\n    \"build_model_with_adapters\",\n    \"compute_adapter_params\",\n]\n\n# -----------------------------------------------------------------------------\n# Baseline LoRA wrapper\n# -----------------------------------------------------------------------------\n\nclass LoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.0):\n        super().__init__()\n        self.base = base\n        self.r = r\n        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n        if r > 0:\n            self.lora_a = nn.Linear(base.in_features, r, bias=False)\n            self.lora_b = nn.Linear(r, base.out_features, bias=False)\n            nn.init.kaiming_uniform_(self.lora_a.weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_b.weight)\n            self.scaling = alpha / r\n        else:\n            self.lora_a = self.lora_b = None\n            self.scaling = 1.0\n        for p in self.base.parameters():\n            p.requires_grad = False\n\n    def forward(self, x):\n        out = self.base(x)\n        if self.r > 0:\n            out = out + self.lora_b(self.lora_a(self.dropout(x))) * self.scaling\n        return out\n\n\n# -----------------------------------------------------------------------------\n# Proposed UniHyperLoRA components (simplified implementation)\n# -----------------------------------------------------------------------------\n\nclass DimAwareProjector(nn.Module):\n    def __init__(self, d0: int = 256):\n        super().__init__()\n        self.d0 = d0\n        self.mlp = nn.Sequential(\n            nn.Linear(1, 64), nn.SiLU(), nn.Linear(64, d0 * 2)\n        )\n\n    def forward(self, dim: int) -> torch.Tensor:\n        inp = torch.log10(\n            torch.tensor([[float(dim)]], dtype=torch.float32, device=self.mlp[0].weight.device)\n        )\n        vec = self.mlp(inp).view(2, self.d0)\n        row, col = vec[0], vec[1]\n        return torch.outer(torch.ones(dim, device=row.device), row) + torch.outer(\n            torch.arange(1, dim + 1, dtype=row.dtype, device=row.device), col\n        )\n\n\nclass SharedUniHyperLoRA(nn.Module):\n    def __init__(self, M: int = 128, d0: int = 256, router_top_k: int = 2):\n        super().__init__()\n        self.M = M\n        self.d0 = d0\n        self.router_top_k = router_top_k\n        self.U = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.V = nn.Parameter(torch.randn(M, d0) * 0.02)\n        self.dim_proj = DimAwareProjector(d0)\n        self.mixer = nn.Sequential(\n            nn.Linear(2, 128), nn.SiLU(), nn.Linear(128, M)\n        )\n\n\nclass UniHyperLoRALinear(nn.Module):\n    def __init__(self, base: nn.Linear, shared: SharedUniHyperLoRA, depth_norm: float, layer_type_id: int):\n        super().__init__()\n        self.base = base\n        self.shared = shared\n        for p in self.base.parameters():\n            p.requires_grad = False\n        self.register_buffer(\"depth_norm\", torch.tensor([depth_norm], dtype=torch.float32))\n        self.register_buffer(\"type_id\", torch.tensor([layer_type_id / 4.0], dtype=torch.float32))\n\n    def forward(self, x):\n        out = self.base(x)\n        d_out, d_in = self.base.out_features, self.base.in_features\n        P_out = self.shared.dim_proj(d_out).to(x.device)\n        P_in = self.shared.dim_proj(d_in).to(x.device).t()\n        gates = self.shared.mixer(torch.cat([self.depth_norm, self.type_id]).unsqueeze(0))[0]\n        vals, idx = torch.topk(gates, k=self.shared.router_top_k)\n        delta_W = torch.zeros(d_out, d_in, device=x.device)\n        for val, j in zip(vals, idx):\n            delta_W += val * (\n                P_out @ torch.outer(self.shared.U[j], self.shared.V[j]) @ P_in\n            )\n        return out + F.linear(x, delta_W, bias=None)\n\n\n# -----------------------------------------------------------------------------\n# Helper utils\n# -----------------------------------------------------------------------------\n\ndef _get_parent_module(model: nn.Module, module_name: str):\n    parts = module_name.split(\".\")\n    parent = model\n    for p in parts[:-1]:\n        parent = parent[int(p)] if p.isdigit() else getattr(parent, p)\n    return parent\n\n\ndef _set_child_module(parent: nn.Module, child_name: str, new_module: nn.Module):\n    if child_name.isdigit():\n        parent[int(child_name)] = new_module\n    else:\n        setattr(parent, child_name, new_module)\n\n\ndef compute_adapter_params(model: nn.Module) -> float:\n    bytes_total = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n    return bytes_total / (1024 ** 2)  # MB\n\n\n# -----------------------------------------------------------------------------\n# Build model with adapters\n# -----------------------------------------------------------------------------\n\ndef build_model_with_adapters(cfg: DictConfig, num_labels: int, device):\n    model_name = cfg.model.host_model_name\n\n    base_cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, cache_dir=\".cache/\")\n    model = AutoModelForTokenClassification.from_pretrained(\n        model_name, config=base_cfg, cache_dir=\".cache/\"\n    )\n\n    if cfg.method.lower().startswith(\"proposed\") or cfg.adapter.name.lower() == \"unihyperlora\":\n        shared = SharedUniHyperLoRA(\n            M=cfg.adapter.atoms,\n            d0=cfg.adapter.anchor_dim,\n            router_top_k=cfg.adapter.router_top_k,\n        )\n        linear_modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n        total = len(linear_modules)\n        idx = 0\n        for name, module in list(model.named_modules()):\n            if isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                depth = idx / max(1, total - 1)\n                wrapped = UniHyperLoRALinear(module, shared, depth, layer_type_id=0)\n                _set_child_module(parent, child_name, wrapped)\n                idx += 1\n    elif cfg.method.lower().startswith(\"comparative\") or cfg.adapter.name.lower() == \"sat-lora\":\n        targets: List[str] = cfg.adapter.target_modules\n        for name, module in list(model.named_modules()):\n            if any(t in name for t in targets) and isinstance(module, nn.Linear):\n                parent = _get_parent_module(model, name)\n                child_name = name.split(\".\")[-1]\n                wrapped = LoRALinear(\n                    module,\n                    r=cfg.adapter.lora_rank,\n                    alpha=cfg.adapter.lora_alpha,\n                )\n                _set_child_module(parent, child_name, wrapped)\n    else:\n        raise ValueError(f\"Unknown adapter type: {cfg.adapter.name}\")\n\n    model.to(device)\n    return model",
            "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(version_base=\"1.3\", config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    if cfg.run is None:\n        raise ValueError(\n            \"Usage: python -m src.main run=<run_id> results_dir=<dir> mode=<trial|full>\"\n        )\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\"] + overrides\n    print(\"🚀 Launching:\", \" \".join(cmd))\n    env = os.environ.copy()\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()",
            "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n\n[project]\nname = \"unihyperlora-experiments\"\nversion = \"0.1.0\"\ndescription = \"Universal Hyper-LoRA experiments managed by Hydra\"\nrequires-python = \">=3.10\"\ndependencies = [\n  \"torch>=2.1.0\",\n  \"transformers>=4.36.0\",\n  \"datasets>=2.14.0\",\n  \"hydra-core>=1.3.2\",\n  \"wandb>=0.16.0\",\n  \"optuna>=3.4.0\",\n  \"evaluate>=0.4.1\",\n  \"seqeval>=1.2.2\",\n  \"scikit-learn>=1.3.0\",\n  \"matplotlib>=3.8.0\",\n  \"seaborn>=0.13.0\",\n  \"tqdm>=4.66.0\",\n  \"pandas>=2.1.0\",\n]",
            "config_yaml": "defaults:\n  - runs/${run}\n  - _self_\n\n# Required CLI args -----------------------------------------------------------\nrun: \"???\"           # run group identifier, e.g., proposed-iter1-CoNLL-2003\nmode: full           # full | trial\nresults_dir: outputs\n\n# WANDB ----------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online  # auto-switched to 'disabled' in trial mode\n\n# Optuna ---------------------------------------------------------------------\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# Training defaults ----------------------------------------------------------\ntraining:\n  seed: 42\n  batch_size: 32\n  epochs: 3\n  max_steps: 1000\n  learning_rate: 5e-5\n  weight_decay: 0.01\n  gradient_accumulation_steps: 1\n\nevaluation:\n  metric: accuracy  # can be overwritten by run cfg\n  eval_steps: 50\n\n# Fields populated by run YAML ----------------------------------------------\nmethod: \"\"\nmodel: {}\nadapter: {}\ndataset: {}\nbudget_constraints: {}"
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-iter1-CoNLL-2003",
            "method_name": "proposed",
            "dataset_name": "CoNLL-2003",
            "run_config": "run_id: proposed-iter1-CoNLL-2003\nmethod: proposed\nmodel:\n  host_model_name: distilbert-base-uncased\n  hidden_size: 768\n  num_layers: 6\n  max_position_embeddings: 512\n  vocab_size: 30522\nadapter:\n  name: UniHyperLoRA\n  atoms: 128\n  anchor_dim: 256\n  router_top_k: 2\n  quant_bits: 4\n  freeze_base_adapter: true\n  train_bias_delta_only: true\n  bias_bits: 4\n  lambda_F: 0.05\n  lambda_Q: 1.0e-5\ndataset:\n  name: conll2003\n  hf_id: eriktks/conll2003\n  splits:\n    train: train\n    validation: validation\n    test: test\n  max_length: 128\ntraining:\n  optimizer: adamw\n  learning_rate: 2.0e-4\n  batch_size: 32\n  max_steps: 500\n  weight_decay: 0.0\n  gradient_accumulation_steps: 1\n  lr_scheduler: linear\n  warmup_steps: 50\n  seed: 42\n  update_param_groups:\n    - adapter.bias_delta\nevaluation:\n  eval_steps: 100\n  metric: seqeval\nbudget_constraints:\n  max_extra_params_mb: 1.0\n  max_flop_overhead_pct: 10\noptuna:\n  n_trials: 25\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 5.0e-4\n    lambda_F:\n      type: uniform\n      low: 0.01\n      high: 0.1\n    lambda_Q:\n      type: loguniform\n      low: 1.0e-6\n      high: 1.0e-4\n    router_top_k:\n      type: categorical\n      choices: [1, 2]\nhardware:\n  accelerator: gpu\n  precision: bf16\n  devices: 1\n"
          },
          {
            "run_id": "comparative-1-iter1-CoNLL-2003",
            "method_name": "comparative-1",
            "dataset_name": "CoNLL-2003",
            "run_config": "run_id: comparative-1-iter1-CoNLL-2003\nmethod: comparative-1\nmodel:\n  host_model_name: distilbert-base-uncased\n  hidden_size: 768\n  num_layers: 6\nadapter:\n  name: SAT-LoRA\n  lora_rank: 8\n  lora_alpha: 16\n  target_modules: [q_proj, k_proj, v_proj, o_proj, ffn_down, ffn_up]\n  share_across_layers: false\n  precision: int8\ndataset:\n  name: conll2003\n  hf_id: eriktks/conll2003\n  splits:\n    train: train\n    validation: validation\n    test: test\n  max_length: 128\ntraining:\n  optimizer: adamw\n  learning_rate: 3.0e-4\n  batch_size: 32\n  max_steps: 3000\n  weight_decay: 0.01\n  gradient_accumulation_steps: 1\n  lr_scheduler: linear\n  warmup_steps: 300\n  seed: 42\n  update_param_groups:\n    - adapter.lora\nevaluation:\n  eval_steps: 200\n  metric: seqeval\nbudget_constraints:\n  max_extra_params_mb: 32\n  max_flop_overhead_pct: 10\noptuna:\n  n_trials: 25\n  direction: maximize\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1.0e-5\n      high: 1.0e-3\n    lora_rank:\n      type: int\n      low: 4\n      high: 16\n    batch_size:\n      type: categorical\n      choices: [16, 32, 64]\nhardware:\n  accelerator: gpu\n  precision: bf16\n  devices: 1\n"
          }
        ]
      }
    ]
  }
}